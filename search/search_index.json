{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Getting started","text":""},{"location":"#getting-started-with-pyiceberg","title":"Getting started with PyIceberg","text":"<p>PyIceberg is a Python implementation for accessing Iceberg tables, without the need of a JVM.</p>"},{"location":"#installation","title":"Installation","text":"<p>Before installing PyIceberg, make sure that you're on an up-to-date version of <code>pip</code>:</p> <pre><code>pip install --upgrade pip\n</code></pre> <p>You can install the latest release version from pypi:</p> <pre><code>pip install \"pyiceberg[s3fs,hive]\"\n</code></pre> <p>You can mix and match optional dependencies depending on your needs:</p> Key Description: hive Support for the Hive metastore hive-kerberos Support for Hive metastore in Kerberos environment glue Support for AWS Glue dynamodb Support for AWS DynamoDB bigquery Support for Google Cloud BigQuery sql-postgres Support for SQL Catalog backed by Postgresql sql-sqlite Support for SQL Catalog backed by SQLite pyarrow PyArrow as a FileIO implementation to interact with the object store pandas Installs both PyArrow and Pandas duckdb Installs both PyArrow and DuckDB ray Installs PyArrow, Pandas, and Ray bodo Installs Bodo daft Installs Daft polars Installs Polars s3fs S3FS as a FileIO implementation to interact with the object store adlfs ADLFS as a FileIO implementation to interact with the object store snappy Support for snappy Avro compression gcsfs GCSFS as a FileIO implementation to interact with the object store rest-sigv4 Support for generating AWS SIGv4 authentication headers for REST Catalogs pyiceberg-core Installs iceberg-rust powered core datafusion Installs both PyArrow and Apache DataFusion hf Support for Hugging Face Hub gcp-auth Support for Google Cloud authentication entra-auth Support for Azure Entra authentication <p>You either need to install <code>s3fs</code>, <code>adlfs</code>, <code>gcsfs</code>, or <code>pyarrow</code> to be able to fetch files from an object store.</p>"},{"location":"#connecting-to-a-catalog","title":"Connecting to a catalog","text":"<p>Iceberg leverages the catalog to have one centralized place to organize the tables. This can be a traditional Hive catalog to store your Iceberg tables next to the rest, a vendor solution like the AWS Glue catalog, or an implementation of Icebergs' own REST protocol. Checkout the configuration page to find all the configuration details.</p> <p>For the sake of demonstration, we'll configure the catalog to use the <code>SqlCatalog</code> implementation, which will store information in a local <code>sqlite</code> database. We'll also configure the catalog to store data files in the local filesystem instead of an object store. This should not be used in production due to the limited scalability.</p> <p>Create a temporary location for Iceberg:</p> <pre><code>mkdir /tmp/warehouse\n</code></pre> <p>Open a Python 3 REPL to set up the catalog:</p> <pre><code>from pyiceberg.catalog import load_catalog\n\nwarehouse_path = \"/tmp/warehouse\"\ncatalog = load_catalog(\n    \"default\",\n    **{\n        'type': 'sql',\n        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n        \"warehouse\": f\"file://{warehouse_path}\",\n    },\n)\n</code></pre> <p>The <code>sql</code> catalog works for testing locally without needing another service. If you want to try out another catalog, please check out the configuration.</p>"},{"location":"#write-a-pyarrow-dataframe","title":"Write a PyArrow dataframe","text":"<p>Let's take the Taxi dataset, and write this to an Iceberg table.</p> <p>First download one month of data:</p> <pre><code>curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet -o /tmp/yellow_tripdata_2023-01.parquet\n</code></pre> <p>Load it into your PyArrow dataframe:</p> <pre><code>import pyarrow.parquet as pq\n\ndf = pq.read_table(\"/tmp/yellow_tripdata_2023-01.parquet\")\n</code></pre> <p>Create a new Iceberg table:</p> <pre><code>catalog.create_namespace(\"default\")\n\ntable = catalog.create_table(\n    \"default.taxi_dataset\",\n    schema=df.schema,\n)\n</code></pre> <p>Append the dataframe to the table:</p> <pre><code>table.append(df)\nlen(table.scan().to_arrow())\n</code></pre> <p>3066766 rows have been written to the table.</p> <p>Now generate a tip-per-mile feature to train the model on:</p> <pre><code>import pyarrow.compute as pc\n\ndf = df.append_column(\"tip_per_mile\", pc.divide(df[\"tip_amount\"], df[\"trip_distance\"]))\n</code></pre> <p>Evolve the schema of the table with the new column:</p> <pre><code>with table.update_schema() as update_schema:\n    update_schema.union_by_name(df.schema)\n</code></pre> <p>And now we can write the new dataframe to the Iceberg table:</p> <pre><code>table.overwrite(df)\nprint(table.scan().to_arrow())\n</code></pre> <p>And the new column is there:</p> <pre><code>taxi_dataset(\n  1: VendorID: optional long,\n  2: tpep_pickup_datetime: optional timestamp,\n  3: tpep_dropoff_datetime: optional timestamp,\n  4: passenger_count: optional double,\n  5: trip_distance: optional double,\n  6: RatecodeID: optional double,\n  7: store_and_fwd_flag: optional string,\n  8: PULocationID: optional long,\n  9: DOLocationID: optional long,\n  10: payment_type: optional long,\n  11: fare_amount: optional double,\n  12: extra: optional double,\n  13: mta_tax: optional double,\n  14: tip_amount: optional double,\n  15: tolls_amount: optional double,\n  16: improvement_surcharge: optional double,\n  17: total_amount: optional double,\n  18: congestion_surcharge: optional double,\n  19: airport_fee: optional double,\n  20: tip_per_mile: optional double\n),\n</code></pre> <p>And we can see that 2371784 rows have a tip-per-mile:</p> <pre><code>df = table.scan(row_filter=\"tip_per_mile &gt; 0\").to_arrow()\nlen(df)\n</code></pre>"},{"location":"#explore-iceberg-data-and-metadata-files","title":"Explore Iceberg data and metadata files","text":"<p>Since the catalog was configured to use the local filesystem, we can explore how Iceberg saved data and metadata files from the above operations.</p> <pre><code>find /tmp/warehouse/\n</code></pre>"},{"location":"#try-it-yourself-with-jupyter-notebooks","title":"Try it yourself with Jupyter Notebooks","text":"<p>PyIceberg provides Jupyter notebooks for hands-on experimentation with the examples above and more. Check out the Notebooks for Experimentation guide.</p>"},{"location":"#more-details","title":"More details","text":"<p>For the details, please check the CLI or Python API page.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":""},{"location":"SUMMARY/#summary","title":"Summary","text":"<ul> <li>Getting started</li> <li>Configuration</li> <li>CLI</li> <li>API<ul> <li>Row Filter Syntax</li> <li>Expression DSL</li> </ul> </li> <li>Contributing</li> <li>Community</li> <li>Releases<ul> <li>Verify a release</li> <li>How to release</li> <li>Release Notes</li> <li>Nightly Build</li> </ul> </li> <li>Code Reference</li> </ul>"},{"location":"api/","title":"API","text":""},{"location":"api/#python-api","title":"Python API","text":"<p>(Py)Iceberg is catalog centric. Meaning that reading/writing data goes via a catalog. First step is to instantiate a catalog to load a table. Let's use the following configuration in <code>.pyiceberg.yaml</code> to define a REST catalog called <code>prod</code>:</p> <pre><code>catalog:\n  prod:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n</code></pre> <p>Note that multiple catalogs can be defined in the same <code>.pyiceberg.yaml</code>, for example, in the case of a Hive and REST catalog:</p> <pre><code>catalog:\n  hive:\n    uri: thrift://127.0.0.1:9083\n    s3.endpoint: http://127.0.0.1:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n  rest:\n    uri: https://rest-server:8181/\n    warehouse: my-warehouse\n</code></pre> <p>The different catalogs can be loaded in PyIceberg by their name: <code>load_catalog(name=\"hive\")</code> and <code>load_catalog(name=\"rest\")</code>. An overview of the configuration options can be found on the configuration page.</p> <p>This information must be placed inside a file called <code>.pyiceberg.yaml</code> located either in the <code>$HOME</code> or <code>%USERPROFILE%</code> directory (depending on whether the operating system is Unix-based or Windows-based, respectively), in the current working directory, or in the <code>$PYICEBERG_HOME</code> directory (if the corresponding environment variable is set).</p> <p>It is also possible to load a catalog without using a <code>.pyiceberg.yaml</code> by passing in the properties directly:</p> <pre><code>from pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\n    \"docs\",\n    **{\n        \"uri\": \"http://127.0.0.1:8181\",\n        \"s3.endpoint\": \"http://127.0.0.1:9000\",\n        \"py-io-impl\": \"pyiceberg.io.pyarrow.PyArrowFileIO\",\n        \"s3.access-key-id\": \"admin\",\n        \"s3.secret-access-key\": \"password\",\n    }\n)\n</code></pre> <p>Next, create a namespace:</p> <pre><code>catalog.create_namespace(\"docs_example\")\n</code></pre> <p>Or, list existing namespaces:</p> <pre><code>ns = catalog.list_namespaces()\n\nassert ns == [(\"docs_example\",)]\n</code></pre> <p>Next, update the namespace properties.</p> <pre><code># Load namespace properties\nproperties = catalog.load_namespace_properties(\"docs_example\")\n\n# Update namespace properties with additions and removals.\ncatalog.update_namespace_properties(\"docs_example\", removals={\"remove-meee!\"}, updates={\"owner\": \"iceberg\"})\n</code></pre> <p>Finally, drop the namespace (if you want!)</p> <pre><code># Drop a namespace\ncatalog.drop_namespace(\"docs_example\")\n</code></pre>"},{"location":"api/#create-a-table","title":"Create a table","text":"<p>To create a table from a catalog:</p> <pre><code>from pyiceberg.schema import Schema\nfrom pyiceberg.types import (\n    TimestampType,\n    FloatType,\n    DoubleType,\n    StringType,\n    NestedField,\n    StructType,\n)\n\nschema = Schema(\n    NestedField(field_id=1, name=\"datetime\", field_type=TimestampType(), required=True),\n    NestedField(field_id=2, name=\"symbol\", field_type=StringType(), required=True),\n    NestedField(field_id=3, name=\"bid\", field_type=FloatType(), required=False),\n    NestedField(field_id=4, name=\"ask\", field_type=DoubleType(), required=False),\n    NestedField(\n        field_id=5,\n        name=\"details\",\n        field_type=StructType(\n            NestedField(\n                field_id=4, name=\"created_by\", field_type=StringType(), required=False\n            ),\n        ),\n        required=False,\n    ),\n)\n\nfrom pyiceberg.partitioning import PartitionSpec, PartitionField\n\npartition_spec = PartitionSpec(\n    PartitionField(\n        source_id=1, field_id=1000, transform=\"day\", name=\"datetime_day\"\n    )\n)\n\nfrom pyiceberg.table.sorting import SortOrder, SortField\n\n# Sort on the symbol\nsort_order = SortOrder(SortField(source_id=2, transform='identity'))\n\ncatalog.create_table(\n    identifier=\"docs_example.bids\",\n    schema=schema,\n    partition_spec=partition_spec,\n    sort_order=sort_order,\n)\n</code></pre> <p>When the table is created, all IDs in the schema are re-assigned to ensure uniqueness.</p> <p>To create a table using a pyarrow schema:</p> <pre><code>import pyarrow as pa\n\nschema = pa.schema([\n        pa.field(\"foo\", pa.string(), nullable=True),\n        pa.field(\"bar\", pa.int32(), nullable=False),\n        pa.field(\"baz\", pa.bool_(), nullable=True),\n])\n\ncatalog.create_table(\n    identifier=\"docs_example.bids\",\n    schema=schema,\n)\n</code></pre> <p>Another API to create a table is using the <code>create_table_transaction</code>. This follows the same APIs when making updates to a table. This is a friendly API for both setting the partition specification and sort-order, because you don't have to deal with field-IDs.</p> <pre><code>with catalog.create_table_transaction(identifier=\"docs_example.bids\", schema=schema) as txn:\n    with txn.update_schema() as update_schema:\n        update_schema.add_column(path=\"new_column\", field_type='string')\n\n    with txn.update_spec() as update_spec:\n        update_spec.add_identity(\"symbol\")\n\n    txn.set_properties(test_a=\"test_aa\", test_b=\"test_b\", test_c=\"test_c\")\n</code></pre>"},{"location":"api/#register-a-table","title":"Register a table","text":"<p>To register a table using existing metadata:</p> <pre><code>catalog.register_table(\n    identifier=\"docs_example.bids\",\n    metadata_location=\"s3://warehouse/path/to/metadata.json\"\n)\n</code></pre>"},{"location":"api/#load-a-table","title":"Load a table","text":"<p>There are two ways of reading an Iceberg table; through a catalog, and by pointing at the Iceberg metadata directly. Reading through a catalog is preferred, and directly pointing at the metadata is read-only.</p>"},{"location":"api/#catalog-table","title":"Catalog table","text":"<p>Loading the <code>bids</code> table:</p> <pre><code>table = catalog.load_table(\"docs_example.bids\")\n# Equivalent to:\ntable = catalog.load_table((\"docs_example\", \"bids\"))\n# The tuple syntax can be used if the namespace or table contains a dot.\n</code></pre> <p>This returns a <code>Table</code> that represents an Iceberg table that can be queried and altered.</p>"},{"location":"api/#static-table","title":"Static table","text":"<p>To load a table directly from a <code>metadata.json</code> file (i.e., without using a catalog), you can use a <code>StaticTable</code> as follows:</p> <pre><code>from pyiceberg.table import StaticTable\n\nstatic_table = StaticTable.from_metadata(\n    \"s3://warehouse/wh/nyc.db/taxis/metadata/00002-6ea51ce3-62aa-4197-9cf8-43d07c3440ca.metadata.json\"\n)\n</code></pre> <p>The static-table does not allow for write operations. If your table metadata directory contains a <code>version-hint.text</code> file, you can just specify  the table root path, and the latest <code>metadata.json</code> file will be resolved automatically:</p> <pre><code>from pyiceberg.table import StaticTable\n\nstatic_table = StaticTable.from_metadata(\n    \"s3://warehouse/wh/nyc.db/taxis\"\n)\n</code></pre>"},{"location":"api/#check-if-a-table-exists","title":"Check if a table exists","text":"<p>To check whether the <code>bids</code> table exists:</p> <pre><code>catalog.table_exists(\"docs_example.bids\")\n</code></pre> <p>Returns <code>True</code> if the table already exists.</p>"},{"location":"api/#rename-a-table","title":"Rename a table","text":"<p>To rename a table:</p> <pre><code>catalog.rename_table(\n    from_identifier=\"docs_example.bids\",\n    to_identifier=\"docs_example.bids_backup\"\n)\n</code></pre>"},{"location":"api/#drop-a-table","title":"Drop a table","text":"<p>To drop a table:</p> <pre><code>catalog.drop_table(\"docs_example.bids\")\n</code></pre> <p>To drop a table and purge all data and metadata files:</p> <pre><code>catalog.purge_table(\"docs_example.bids\")\n</code></pre>"},{"location":"api/#write-to-a-table","title":"Write to a table","text":"<p>Reading and writing is being done using Apache Arrow. Arrow is an in-memory columnar format for fast data interchange and in-memory analytics. Let's consider the following Arrow Table:</p> <pre><code>import pyarrow as pa\n\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"lat\": 52.371807, \"long\": 4.896029},\n        {\"city\": \"San Francisco\", \"lat\": 37.773972, \"long\": -122.431297},\n        {\"city\": \"Drachten\", \"lat\": 53.11254, \"long\": 6.0989},\n        {\"city\": \"Paris\", \"lat\": 48.864716, \"long\": 2.349014},\n    ],\n)\n</code></pre> <p>Next, create a table using the Arrow schema:</p> <pre><code>from pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"default\")\n\ntbl = catalog.create_table(\"default.cities\", schema=df.schema)\n</code></pre> <p>Next, write the data to the table. Both <code>append</code> and <code>overwrite</code> produce the same result, since the table is empty on creation:</p> <p>Fast append</p> <p>PyIceberg defaults to the fast append to minimize the amount of data written. This enables fast commit operations, reducing the possibility of conflicts. The downside of the fast append is that it creates more metadata than a merge commit. Compaction is planned and will automatically rewrite all the metadata when a threshold is hit, to maintain performant reads.</p> <pre><code>tbl.append(df)\n\n# or\n\ntbl.overwrite(df)\n</code></pre> <p>Now, the data is written to the table, and the table can be read using <code>tbl.scan().to_arrow()</code>:</p> <pre><code>pyarrow.Table\ncity: string\nlat: double\nlong: double\n----\ncity: [[\"Amsterdam\",\"San Francisco\",\"Drachten\",\"Paris\"]]\nlat: [[52.371807,37.773972,53.11254,48.864716]]\nlong: [[4.896029,-122.431297,6.0989,2.349014]]\n</code></pre> <p>If we want to add more data, we can use <code>.append()</code> again:</p> <pre><code>tbl.append(pa.Table.from_pylist(\n    [{\"city\": \"Groningen\", \"lat\": 53.21917, \"long\": 6.56667}],\n))\n</code></pre> <p>When reading the table <code>tbl.scan().to_arrow()</code> you can see that <code>Groningen</code> is now also part of the table:</p> <pre><code>pyarrow.Table\ncity: string\nlat: double\nlong: double\n----\ncity: [[\"Amsterdam\",\"San Francisco\",\"Drachten\",\"Paris\"],[\"Groningen\"]]\nlat: [[52.371807,37.773972,53.11254,48.864716],[53.21917]]\nlong: [[4.896029,-122.431297,6.0989,2.349014],[6.56667]]\n</code></pre> <p>The nested lists indicate the different Arrow buffers. Each of the writes produce a Parquet file where each row group translates into an Arrow buffer. In the case where the table is large, PyIceberg also allows the option to stream the buffers using the Arrow RecordBatchReader, avoiding pulling everything into memory right away:</p> <pre><code>for buf in tbl.scan().to_arrow_batch_reader():\n    print(f\"Buffer contains {len(buf)} rows\")\n</code></pre> <p>To avoid any type inconsistencies during writing, you can convert the Iceberg table schema to Arrow:</p> <pre><code>df = pa.Table.from_pylist(\n    [{\"city\": \"Groningen\", \"lat\": 53.21917, \"long\": 6.56667}], schema=table.schema().as_arrow()\n)\n\ntbl.append(df)\n</code></pre> <p>You can delete some of the data from the table by calling <code>tbl.delete()</code> with a desired <code>delete_filter</code>. This will use the Iceberg metadata to only open up the Parquet files that contain relevant information.</p> <pre><code>tbl.delete(delete_filter=\"city == 'Paris'\")\n</code></pre> <p>In the above example, any records where the city field value equals to <code>Paris</code> will be deleted. Running <code>tbl.scan().to_arrow()</code> will now yield:</p> <pre><code>pyarrow.Table\ncity: string\nlat: double\nlong: double\n----\ncity: [[\"Amsterdam\",\"San Francisco\",\"Drachten\"],[\"Groningen\"]]\nlat: [[52.371807,37.773972,53.11254],[53.21917]]\nlong: [[4.896029,-122.431297,6.0989],[6.56667]]\n</code></pre> <p>In the case of <code>tbl.delete(delete_filter=\"city == 'Groningen'\")</code>, the whole Parquet file will be dropped without checking it contents, since from the Iceberg metadata PyIceberg can derive that all the content in the file matches the predicate.</p>"},{"location":"api/#partial-overwrites","title":"Partial overwrites","text":"<p>When using the <code>overwrite</code> API, you can use an <code>overwrite_filter</code> to delete data that matches the filter before appending new data into the table. For example, consider the following Iceberg table:</p> <pre><code>import pyarrow as pa\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"lat\": 52.371807, \"long\": 4.896029},\n        {\"city\": \"San Francisco\", \"lat\": 37.773972, \"long\": -122.431297},\n        {\"city\": \"Drachten\", \"lat\": 53.11254, \"long\": 6.0989},\n        {\"city\": \"Paris\", \"lat\": 48.864716, \"long\": 2.349014},\n    ],\n)\n\nfrom pyiceberg.catalog import load_catalog\ncatalog = load_catalog(\"default\")\n\ntbl = catalog.create_table(\"default.cities\", schema=df.schema)\n\ntbl.append(df)\n</code></pre> <p>You can overwrite the record of <code>Paris</code> with a record of <code>New York</code>:</p> <pre><code>from pyiceberg.expressions import EqualTo\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"New York\", \"lat\": 40.7128, \"long\": 74.0060},\n    ]\n)\ntbl.overwrite(df, overwrite_filter=EqualTo('city', \"Paris\"))\n</code></pre> <p>This produces the following result with <code>tbl.scan().to_arrow()</code>:</p> <pre><code>pyarrow.Table\ncity: large_string\nlat: double\nlong: double\n----\ncity: [[\"New York\"],[\"Amsterdam\",\"San Francisco\",\"Drachten\"]]\nlat: [[40.7128],[52.371807,37.773972,53.11254]]\nlong: [[74.006],[4.896029,-122.431297,6.0989]]\n</code></pre> <p>If the PyIceberg table is partitioned, you can use <code>tbl.dynamic_partition_overwrite(df)</code> to replace the existing partitions with new ones provided in the dataframe. The partitions to be replaced are detected automatically from the provided arrow table. For example, with an iceberg table with a partition specified on <code>\"city\"</code> field:</p> <pre><code>from pyiceberg.schema import Schema\nfrom pyiceberg.types import DoubleType, NestedField, StringType\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntbl = catalog.create_table(\n    \"default.cities\",\n    schema=schema,\n    partition_spec=PartitionSpec(PartitionField(source_id=1, field_id=1001, transform=IdentityTransform(), name=\"city_identity\"))\n)\n</code></pre> <p>And we want to overwrite the data for the partition of <code>\"Paris\"</code>:</p> <pre><code>import pyarrow as pa\n\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"lat\": 52.371807, \"long\": 4.896029},\n        {\"city\": \"San Francisco\", \"lat\": 37.773972, \"long\": -122.431297},\n        {\"city\": \"Drachten\", \"lat\": 53.11254, \"long\": 6.0989},\n        {\"city\": \"Paris\", \"lat\": -48.864716, \"long\": -2.349014},\n    ],\n)\ntbl.append(df)\n</code></pre> <p>Then we can call <code>dynamic_partition_overwrite</code> with this arrow table:</p> <pre><code>df_corrected = pa.Table.from_pylist([\n    {\"city\": \"Paris\", \"lat\": 48.864716, \"long\": 2.349014}\n])\ntbl.dynamic_partition_overwrite(df_corrected)\n</code></pre> <p>This produces the following result with <code>tbl.scan().to_arrow()</code>:</p> <pre><code>pyarrow.Table\ncity: large_string\nlat: double\nlong: double\n----\ncity: [[\"Paris\"],[\"Amsterdam\"],[\"Drachten\"],[\"San Francisco\"]]\nlat: [[48.864716],[52.371807],[53.11254],[37.773972]]\nlong: [[2.349014],[4.896029],[6.0989],[-122.431297]]\n</code></pre>"},{"location":"api/#upsert","title":"Upsert","text":"<p>PyIceberg supports upsert operations, meaning that it is able to merge an Arrow table into an Iceberg table. Rows are considered the same based on the identifier field. If a row is already in the table, it will update that row. If a row cannot be found, it will insert that new row.</p> <p>Consider the following table, with some data:</p> <pre><code>from pyiceberg.schema import Schema\nfrom pyiceberg.types import IntegerType, NestedField, StringType\n\nimport pyarrow as pa\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=True),\n    NestedField(2, \"inhabitants\", IntegerType(), required=True),\n    # Mark City as the identifier field, also known as the primary-key\n    identifier_field_ids=[1]\n)\n\ntbl = catalog.create_table(\"default.cities\", schema=schema)\n\narrow_schema = pa.schema(\n    [\n        pa.field(\"city\", pa.string(), nullable=False),\n        pa.field(\"inhabitants\", pa.int32(), nullable=False),\n    ]\n)\n\n# Write some data\ndf = pa.Table.from_pylist(\n    [\n        {\"city\": \"Amsterdam\", \"inhabitants\": 921402},\n        {\"city\": \"San Francisco\", \"inhabitants\": 808988},\n        {\"city\": \"Drachten\", \"inhabitants\": 45019},\n        {\"city\": \"Paris\", \"inhabitants\": 2103000},\n    ],\n    schema=arrow_schema\n)\ntbl.append(df)\n</code></pre> <p>Next, we'll upsert a table into the Iceberg table:</p> <pre><code>df = pa.Table.from_pylist(\n    [\n        # Will be updated, the inhabitants has been updated\n        {\"city\": \"Drachten\", \"inhabitants\": 45505},\n\n        # New row, will be inserted\n        {\"city\": \"Berlin\", \"inhabitants\": 3432000},\n\n        # Ignored, already exists in the table\n        {\"city\": \"Paris\", \"inhabitants\": 2103000},\n    ],\n    schema=arrow_schema\n)\nupd = tbl.upsert(df)\n\nassert upd.rows_updated == 1\nassert upd.rows_inserted == 1\n</code></pre> <p>PyIceberg will automatically detect which rows need to be updated, inserted or can simply be ignored.</p>"},{"location":"api/#inspecting-tables","title":"Inspecting tables","text":"<p>To explore the table metadata, tables can be inspected.</p> <p>Time Travel</p> <p>To inspect a tables's metadata with the time travel feature, call the inspect table method with the <code>snapshot_id</code> argument. Time travel is supported on all metadata tables except <code>snapshots</code> and <code>refs</code>. <pre><code>table.inspect.entries(snapshot_id=805611270568163028)\n</code></pre></p>"},{"location":"api/#snapshots","title":"Snapshots","text":"<p>Inspect the snapshots of the table:</p> <pre><code>table.inspect.snapshots()\n</code></pre> <pre><code>pyarrow.Table\ncommitted_at: timestamp[ms] not null\nsnapshot_id: int64 not null\nparent_id: int64\noperation: string\nmanifest_list: string not null\nsummary: map&lt;string, string&gt;\n  child 0, entries: struct&lt;key: string not null, value: string&gt; not null\n      child 0, key: string not null\n      child 1, value: string\n----\ncommitted_at: [[2024-03-15 15:01:25.682,2024-03-15 15:01:25.730,2024-03-15 15:01:25.772]]\nsnapshot_id: [[805611270568163028,3679426539959220963,5588071473139865870]]\nparent_id: [[null,805611270568163028,3679426539959220963]]\noperation: [[\"append\",\"overwrite\",\"append\"]]\nmanifest_list: [[\"s3://warehouse/default/table_metadata_snapshots/metadata/snap-805611270568163028-0-43637daf-ea4b-4ceb-b096-a60c25481eb5.avro\",\"s3://warehouse/default/table_metadata_snapshots/metadata/snap-3679426539959220963-0-8be81019-adf1-4bb6-a127-e15217bd50b3.avro\",\"s3://warehouse/default/table_metadata_snapshots/metadata/snap-5588071473139865870-0-1382dd7e-5fbc-4c51-9776-a832d7d0984e.avro\"]]\nsummary: [[keys:[\"added-files-size\",\"added-data-files\",\"added-records\",\"total-data-files\",\"total-delete-files\",\"total-records\",\"total-files-size\",\"total-position-deletes\",\"total-equality-deletes\"]values:[\"5459\",\"1\",\"3\",\"1\",\"0\",\"3\",\"5459\",\"0\",\"0\"],keys:[\"added-files-size\",\"added-data-files\",\"added-records\",\"total-data-files\",\"total-records\",...,\"total-equality-deletes\",\"total-files-size\",\"deleted-data-files\",\"deleted-records\",\"removed-files-size\"]values:[\"5459\",\"1\",\"3\",\"1\",\"3\",...,\"0\",\"5459\",\"1\",\"3\",\"5459\"],keys:[\"added-files-size\",\"added-data-files\",\"added-records\",\"total-data-files\",\"total-delete-files\",\"total-records\",\"total-files-size\",\"total-position-deletes\",\"total-equality-deletes\"]values:[\"5459\",\"1\",\"3\",\"2\",\"0\",\"6\",\"10918\",\"0\",\"0\"]]]\n</code></pre>"},{"location":"api/#partitions","title":"Partitions","text":"<p>Inspect the partitions of the table:</p> <pre><code>table.inspect.partitions()\n</code></pre> <pre><code>pyarrow.Table\npartition: struct&lt;dt_month: int32, dt_day: date32[day]&gt; not null\n  child 0, dt_month: int32\n  child 1, dt_day: date32[day]\nspec_id: int32 not null\nrecord_count: int64 not null\nfile_count: int32 not null\ntotal_data_file_size_in_bytes: int64 not null\nposition_delete_record_count: int64 not null\nposition_delete_file_count: int32 not null\nequality_delete_record_count: int64 not null\nequality_delete_file_count: int32 not null\nlast_updated_at: timestamp[ms]\nlast_updated_snapshot_id: int64\n----\npartition: [\n  -- is_valid: all not null\n  -- child 0 type: int32\n[null,null,612]\n  -- child 1 type: date32[day]\n[null,2021-02-01,null]]\nspec_id: [[2,1,0]]\nrecord_count: [[1,1,2]]\nfile_count: [[1,1,2]]\ntotal_data_file_size_in_bytes: [[641,641,1260]]\nposition_delete_record_count: [[0,0,0]]\nposition_delete_file_count: [[0,0,0]]\nequality_delete_record_count: [[0,0,0]]\nequality_delete_file_count: [[0,0,0]]\nlast_updated_at: [[2024-04-13 18:59:35.981,2024-04-13 18:59:35.465,2024-04-13 18:59:35.003]]\n</code></pre>"},{"location":"api/#entries","title":"Entries","text":"<p>To show all the table's current manifest entries for both data and delete files.</p> <pre><code>table.inspect.entries()\n</code></pre> <pre><code>pyarrow.Table\nstatus: int8 not null\nsnapshot_id: int64 not null\nsequence_number: int64 not null\nfile_sequence_number: int64 not null\ndata_file: struct&lt;content: int8 not null, file_path: string not null, file_format: string not null, partition: struct&lt;&gt; not null, record_count: int64 not null, file_size_in_bytes: int64 not null, column_sizes: map&lt;int32, int64&gt;, value_counts: map&lt;int32, int64&gt;, null_value_counts: map&lt;int32, int64&gt;, nan_value_counts: map&lt;int32, int64&gt;, lower_bounds: map&lt;int32, binary&gt;, upper_bounds: map&lt;int32, binary&gt;, key_metadata: binary, split_offsets: list&lt;item: int64&gt;, equality_ids: list&lt;item: int32&gt;, sort_order_id: int32&gt; not null\n  child 0, content: int8 not null\n  child 1, file_path: string not null\n  child 2, file_format: string not null\n  child 3, partition: struct&lt;&gt; not null\n  child 4, record_count: int64 not null\n  child 5, file_size_in_bytes: int64 not null\n  child 6, column_sizes: map&lt;int32, int64&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n          child 0, key: int32 not null\n          child 1, value: int64\n  child 7, value_counts: map&lt;int32, int64&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n          child 0, key: int32 not null\n          child 1, value: int64\n  child 8, null_value_counts: map&lt;int32, int64&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n          child 0, key: int32 not null\n          child 1, value: int64\n  child 9, nan_value_counts: map&lt;int32, int64&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n          child 0, key: int32 not null\n          child 1, value: int64\n  child 10, lower_bounds: map&lt;int32, binary&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: binary&gt; not null\n          child 0, key: int32 not null\n          child 1, value: binary\n  child 11, upper_bounds: map&lt;int32, binary&gt;\n      child 0, entries: struct&lt;key: int32 not null, value: binary&gt; not null\n          child 0, key: int32 not null\n          child 1, value: binary\n  child 12, key_metadata: binary\n  child 13, split_offsets: list&lt;item: int64&gt;\n      child 0, item: int64\n  child 14, equality_ids: list&lt;item: int32&gt;\n      child 0, item: int32\n  child 15, sort_order_id: int32\nreadable_metrics: struct&lt;city: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: string, upper_bound: string&gt; not null, lat: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null, long: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null&gt;\n  child 0, city: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: string, upper_bound: string&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: string\n      child 5, upper_bound: string\n  child 1, lat: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: double\n      child 5, upper_bound: double\n  child 2, long: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: double\n      child 5, upper_bound: double\n----\nstatus: [[1]]\nsnapshot_id: [[6245626162224016531]]\nsequence_number: [[1]]\nfile_sequence_number: [[1]]\ndata_file: [\n  -- is_valid: all not null\n  -- child 0 type: int8\n[0]\n  -- child 1 type: string\n[\"s3://warehouse/default/cities/data/00000-0-80766b66-e558-4150-a5cf-85e4c609b9fe.parquet\"]\n  -- child 2 type: string\n[\"PARQUET\"]\n  -- child 3 type: struct&lt;&gt;\n    -- is_valid: all not null\n  -- child 4 type: int64\n[4]\n  -- child 5 type: int64\n[1656]\n  -- child 6 type: map&lt;int32, int64&gt;\n[keys:[1,2,3]values:[140,135,135]]\n  -- child 7 type: map&lt;int32, int64&gt;\n[keys:[1,2,3]values:[4,4,4]]\n  -- child 8 type: map&lt;int32, int64&gt;\n[keys:[1,2,3]values:[0,0,0]]\n  -- child 9 type: map&lt;int32, int64&gt;\n[keys:[]values:[]]\n  -- child 10 type: map&lt;int32, binary&gt;\n[keys:[1,2,3]values:[416D7374657264616D,8602B68311E34240,3A77BB5E9A9B5EC0]]\n  -- child 11 type: map&lt;int32, binary&gt;\n[keys:[1,2,3]values:[53616E204672616E636973636F,F5BEF1B5678E4A40,304CA60A46651840]]\n  -- child 12 type: binary\n[null]\n  -- child 13 type: list&lt;item: int64&gt;\n[[4]]\n  -- child 14 type: list&lt;item: int32&gt;\n[null]\n  -- child 15 type: int32\n[null]]\nreadable_metrics: [\n  -- is_valid: all not null\n  -- child 0 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: string, upper_bound: string&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[140]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: string\n[\"Amsterdam\"]\n    -- child 5 type: string\n[\"San Francisco\"]\n  -- child 1 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[135]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: double\n[37.773972]\n    -- child 5 type: double\n[53.11254]\n  -- child 2 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[135]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: double\n[-122.431297]\n    -- child 5 type: double\n[6.0989]]\n</code></pre>"},{"location":"api/#references","title":"References","text":"<p>To show a table's known snapshot references:</p> <pre><code>table.inspect.refs()\n</code></pre> <pre><code>pyarrow.Table\nname: string not null\ntype: string not null\nsnapshot_id: int64 not null\nmax_reference_age_in_ms: int64\nmin_snapshots_to_keep: int32\nmax_snapshot_age_in_ms: int64\n----\nname: [[\"main\",\"testTag\"]]\ntype: [[\"BRANCH\",\"TAG\"]]\nsnapshot_id: [[2278002651076891950,2278002651076891950]]\nmax_reference_age_in_ms: [[null,604800000]]\nmin_snapshots_to_keep: [[null,10]]\nmax_snapshot_age_in_ms: [[null,604800000]]\n</code></pre>"},{"location":"api/#manifests","title":"Manifests","text":"<p>To show a table's current file manifests:</p> <pre><code>table.inspect.manifests()\n</code></pre> <pre><code>pyarrow.Table\ncontent: int8 not null\npath: string not null\nlength: int64 not null\npartition_spec_id: int32 not null\nadded_snapshot_id: int64 not null\nadded_data_files_count: int32 not null\nexisting_data_files_count: int32 not null\ndeleted_data_files_count: int32 not null\nadded_delete_files_count: int32 not null\nexisting_delete_files_count: int32 not null\ndeleted_delete_files_count: int32 not null\npartition_summaries: list&lt;item: struct&lt;contains_null: bool not null, contains_nan: bool, lower_bound: string, upper_bound: string&gt;&gt; not null\n  child 0, item: struct&lt;contains_null: bool not null, contains_nan: bool, lower_bound: string, upper_bound: string&gt;\n      child 0, contains_null: bool not null\n      child 1, contains_nan: bool\n      child 2, lower_bound: string\n      child 3, upper_bound: string\n----\ncontent: [[0]]\npath: [[\"s3://warehouse/default/table_metadata_manifests/metadata/3bf5b4c6-a7a4-4b43-a6ce-ca2b4887945a-m0.avro\"]]\nlength: [[6886]]\npartition_spec_id: [[0]]\nadded_snapshot_id: [[3815834705531553721]]\nadded_data_files_count: [[1]]\nexisting_data_files_count: [[0]]\ndeleted_data_files_count: [[0]]\nadded_delete_files_count: [[0]]\nexisting_delete_files_count: [[0]]\ndeleted_delete_files_count: [[0]]\npartition_summaries: [[    -- is_valid: all not null\n    -- child 0 type: bool\n[false]\n    -- child 1 type: bool\n[false]\n    -- child 2 type: string\n[\"test\"]\n    -- child 3 type: string\n[\"test\"]]]\n</code></pre>"},{"location":"api/#metadata-log-entries","title":"Metadata Log Entries","text":"<p>To show table metadata log entries:</p> <pre><code>table.inspect.metadata_log_entries()\n</code></pre> <pre><code>pyarrow.Table\ntimestamp: timestamp[ms] not null\nfile: string not null\nlatest_snapshot_id: int64\nlatest_schema_id: int32\nlatest_sequence_number: int64\n----\ntimestamp: [[2024-04-28 17:03:00.214,2024-04-28 17:03:00.352,2024-04-28 17:03:00.445,2024-04-28 17:03:00.498]]\nfile: [[\"s3://warehouse/default/table_metadata_log_entries/metadata/00000-0b3b643b-0f3a-4787-83ad-601ba57b7319.metadata.json\",\"s3://warehouse/default/table_metadata_log_entries/metadata/00001-f74e4b2c-0f89-4f55-822d-23d099fd7d54.metadata.json\",\"s3://warehouse/default/table_metadata_log_entries/metadata/00002-97e31507-e4d9-4438-aff1-3c0c5304d271.metadata.json\",\"s3://warehouse/default/table_metadata_log_entries/metadata/00003-6c8b7033-6ad8-4fe4-b64d-d70381aeaddc.metadata.json\"]]\nlatest_snapshot_id: [[null,3958871664825505738,1289234307021405706,7640277914614648349]]\nlatest_schema_id: [[null,0,0,0]]\nlatest_sequence_number: [[null,0,0,0]]\n</code></pre>"},{"location":"api/#history","title":"History","text":"<p>To show a table's history:</p> <pre><code>table.inspect.history()\n</code></pre> <pre><code>pyarrow.Table\nmade_current_at: timestamp[ms] not null\nsnapshot_id: int64 not null\nparent_id: int64\nis_current_ancestor: bool not null\n----\nmade_current_at: [[2024-06-18 16:17:48.768,2024-06-18 16:17:49.240,2024-06-18 16:17:49.343,2024-06-18 16:17:49.511]]\nsnapshot_id: [[4358109269873137077,3380769165026943338,4358109269873137077,3089420140651211776]]\nparent_id: [[null,4358109269873137077,null,4358109269873137077]]\nis_current_ancestor: [[true,false,true,true]]\n</code></pre>"},{"location":"api/#files","title":"Files","text":"<p>Inspect the data files in the current snapshot of the table:</p> <pre><code>table.inspect.files()\n</code></pre> <pre><code>pyarrow.Table\ncontent: int8 not null\nfile_path: string not null\nfile_format: dictionary&lt;values=string, indices=int32, ordered=0&gt; not null\nspec_id: int32 not null\nrecord_count: int64 not null\nfile_size_in_bytes: int64 not null\ncolumn_sizes: map&lt;int32, int64&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n      child 0, key: int32 not null\n      child 1, value: int64\nvalue_counts: map&lt;int32, int64&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n      child 0, key: int32 not null\n      child 1, value: int64\nnull_value_counts: map&lt;int32, int64&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n      child 0, key: int32 not null\n      child 1, value: int64\nnan_value_counts: map&lt;int32, int64&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: int64&gt; not null\n      child 0, key: int32 not null\n      child 1, value: int64\nlower_bounds: map&lt;int32, binary&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: binary&gt; not null\n      child 0, key: int32 not null\n      child 1, value: binary\nupper_bounds: map&lt;int32, binary&gt;\n  child 0, entries: struct&lt;key: int32 not null, value: binary&gt; not null\n      child 0, key: int32 not null\n      child 1, value: binary\nkey_metadata: binary\nsplit_offsets: list&lt;item: int64&gt;\n  child 0, item: int64\nequality_ids: list&lt;item: int32&gt;\n  child 0, item: int32\nsort_order_id: int32\nreadable_metrics: struct&lt;city: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: large_string, upper_bound: large_string&gt; not null, lat: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null, long: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null&gt;\n  child 0, city: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: string, upper_bound: string&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: large_string\n      child 5, upper_bound: large_string\n  child 1, lat: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: double\n      child 5, upper_bound: double\n  child 2, long: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt; not null\n      child 0, column_size: int64\n      child 1, value_count: int64\n      child 2, null_value_count: int64\n      child 3, nan_value_count: int64\n      child 4, lower_bound: double\n      child 5, upper_bound: double\n----\ncontent: [[0,0]]\nfile_path: [[\"s3://warehouse/default/table_metadata_files/data/00000-0-9ea7d222-6457-467f-bad5-6fb125c9aa5f.parquet\",\"s3://warehouse/default/table_metadata_files/data/00000-0-afa8893c-de71-4710-97c9-6b01590d0c44.parquet\"]]\nfile_format: [[\"PARQUET\",\"PARQUET\"]]\nspec_id: [[0,0]]\nrecord_count: [[3,3]]\nfile_size_in_bytes: [[5459,5459]]\ncolumn_sizes: [[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[49,78,128,94,118,...,118,118,94,78,109],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[49,78,128,94,118,...,118,118,94,78,109]]]\nvalue_counts: [[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[3,3,3,3,3,...,3,3,3,3,3],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[3,3,3,3,3,...,3,3,3,3,3]]]\nnull_value_counts: [[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[1,1,1,1,1,...,1,1,1,1,1],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[1,1,1,1,1,...,1,1,1,1,1]]]\nnan_value_counts: [[keys:[]values:[],keys:[]values:[]]]\nlower_bounds: [[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[00,61,61616161616161616161616161616161,01000000,0100000000000000,...,009B6ACA38F10500,009B6ACA38F10500,9E4B0000,01,00000000000000000000000000000000],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[00,61,61616161616161616161616161616161,01000000,0100000000000000,...,009B6ACA38F10500,009B6ACA38F10500,9E4B0000,01,00000000000000000000000000000000]]]\nupper_bounds:[[keys:[1,2,3,4,5,...,8,9,10,11,12]values:[00,61,61616161616161616161616161616161,01000000,0100000000000000,...,009B6ACA38F10500,009B6ACA38F10500,9E4B0000,01,00000000000000000000000000000000],keys:[1,2,3,4,5,...,8,9,10,11,12]values:[00,61,61616161616161616161616161616161,01000000,0100000000000000,...,009B6ACA38F10500,009B6ACA38F10500,9E4B0000,01,00000000000000000000000000000000]]]\nkey_metadata: [[0100,0100]]\nsplit_offsets:[[[],[]]]\nequality_ids:[[[],[]]]\nsort_order_id:[[[],[]]]\nreadable_metrics: [\n  -- is_valid: all not null\n  -- child 0 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: large_string, upper_bound: large_string&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[140]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: large_string\n[\"Amsterdam\"]\n    -- child 5 type: large_string\n[\"San Francisco\"]\n  -- child 1 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[135]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: double\n[37.773972]\n    -- child 5 type: double\n[53.11254]\n  -- child 2 type: struct&lt;column_size: int64, value_count: int64, null_value_count: int64, nan_value_count: int64, lower_bound: double, upper_bound: double&gt;\n    -- is_valid: all not null\n    -- child 0 type: int64\n[135]\n    -- child 1 type: int64\n[4]\n    -- child 2 type: int64\n[0]\n    -- child 3 type: int64\n[null]\n    -- child 4 type: double\n[-122.431297]\n    -- child 5 type: double\n[6.0989]]\n</code></pre> <p>Info</p> <p>Content refers to type of content stored by the data file: <code>0</code> - <code>Data</code>, <code>1</code> - <code>Position Deletes</code>, <code>2</code> - <code>Equality Deletes</code></p> <p>To show only data files or delete files in the current snapshot, use <code>table.inspect.data_files()</code> and <code>table.inspect.delete_files()</code> respectively.</p>"},{"location":"api/#add-files","title":"Add Files","text":"<p>Expert Iceberg users may choose to commit existing parquet files to the Iceberg table as data files, without rewriting them.</p> <p>Name Mapping and Field IDs</p> <p><code>add_files</code> can work with Parquet files both with and without field IDs in their metadata: - Files with field IDs: When field IDs are present in the Parquet metadata, they must match the corresponding field IDs in the Iceberg table schema. This is common for files generated by tools like Spark or when using or other libraries with explicit field ID metadata. - Files without field IDs: When field IDs are absent, the table must have a Name Mapping to map field names to Iceberg field IDs. <code>add_files</code> will automatically create a Name Mapping based on the table's current schema if one doesn't already exist.</p> <p>Partitions</p> <p><code>add_files</code> only requires the client to read the existing parquet files' metadata footer to infer the partition value of each file. This implementation also supports adding files to Iceberg tables with partition transforms like <code>MonthTransform</code>, and <code>TruncateTransform</code> which preserve the order of the values after the transformation (Any Transform that has the <code>preserves_order</code> property set to True is supported). Please note that if the column statistics of the <code>PartitionField</code>'s source column are not present in the parquet metadata, the partition value is inferred as <code>None</code>.</p> <p>Maintenance Operations</p> <p>Because <code>add_files</code> commits the existing parquet files to the Iceberg Table as any other data file, destructive maintenance operations like expiring snapshots will remove them.</p> <p>Check Duplicate Files</p> <p>The <code>check_duplicate_files</code> parameter determines whether the method validates that the specified <code>file_paths</code> do not already exist in the Iceberg table. When set to True (the default), the method performs a validation against the table\u2019s current data files to prevent accidental duplication, helping to maintain data consistency by ensuring the same file is not added multiple times. While this check is important for data integrity, it can introduce performance overhead for tables with a large number of files. Setting check_duplicate_files=False can improve performance but increases the risk of duplicate files, which may lead to data inconsistencies or table corruption. It is strongly recommended to keep this parameter enabled unless duplicate file handling is strictly enforced elsewhere.</p>"},{"location":"api/#usage","title":"Usage","text":"Parameter Required? Type Description <code>file_paths</code> \u2714\ufe0f List[str] The list of full file paths to be added as data files to the table <code>snapshot_properties</code> Dict[str, str] Properties to set for the new snapshot. Defaults to an empty dictionary <code>check_duplicate_files</code> bool Whether to check for duplicate files. Defaults to <code>True</code>"},{"location":"api/#example","title":"Example","text":"<p>Add files to Iceberg table:</p> <pre><code># Given that these parquet files have schema consistent with the Iceberg table\n\nfile_paths = [\n    \"s3a://warehouse/default/existing-1.parquet\",\n    \"s3a://warehouse/default/existing-2.parquet\",\n]\n\n# They can be added to the table without rewriting them\n\ntbl.add_files(file_paths=file_paths)\n\n# A new snapshot is committed to the table with manifests pointing to the existing parquet files\n</code></pre> <p>Add files to Iceberg table with custom snapshot properties:</p> <pre><code># Assume an existing Iceberg table object `tbl`\n\nfile_paths = [\n    \"s3a://warehouse/default/existing-1.parquet\",\n    \"s3a://warehouse/default/existing-2.parquet\",\n]\n\n# Custom snapshot properties\nsnapshot_properties = {\"abc\": \"def\"}\n\n# Enable duplicate file checking\ncheck_duplicate_files = True\n\n# Add the Parquet files to the Iceberg table without rewriting\ntbl.add_files(\n    file_paths=file_paths,\n    snapshot_properties=snapshot_properties,\n    check_duplicate_files=check_duplicate_files\n)\n\n# NameMapping must have been set to enable reads\nassert tbl.name_mapping() is not None\n\n# Verify that the snapshot property was set correctly\nassert tbl.metadata.snapshots[-1].summary[\"abc\"] == \"def\"\n</code></pre>"},{"location":"api/#schema-evolution","title":"Schema evolution","text":"<p>PyIceberg supports full schema evolution through the Python API. It takes care of setting the field-IDs and makes sure that only non-breaking changes are done (can be overridden).</p> <p>In the examples below, the <code>.update_schema()</code> is called from the table itself.</p> <pre><code>with table.update_schema() as update:\n    update.add_column(\"some_field\", IntegerType(), \"doc\")\n</code></pre> <p>You can also initiate a transaction if you want to make more changes than just evolving the schema:</p> <pre><code>with table.transaction() as transaction:\n    with transaction.update_schema() as update_schema:\n        update.add_column(\"some_other_field\", IntegerType(), \"doc\")\n    # ... Update properties etc\n</code></pre>"},{"location":"api/#union-by-name","title":"Union by Name","text":"<p>Using <code>.union_by_name()</code> you can merge another schema into an existing schema without having to worry about field-IDs:</p> <pre><code>from pyiceberg.catalog import load_catalog\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import NestedField, StringType, DoubleType, LongType\n\ncatalog = load_catalog()\n\nschema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n)\n\ntable = catalog.create_table(\"default.locations\", schema)\n\nnew_schema = Schema(\n    NestedField(1, \"city\", StringType(), required=False),\n    NestedField(2, \"lat\", DoubleType(), required=False),\n    NestedField(3, \"long\", DoubleType(), required=False),\n    NestedField(10, \"population\", LongType(), required=False),\n)\n\nwith table.update_schema() as update:\n    update.union_by_name(new_schema)\n</code></pre> <p>Now the table has the union of the two schemas <code>print(table.schema())</code>:</p> <pre><code>table {\n  1: city: optional string\n  2: lat: optional double\n  3: long: optional double\n  4: population: optional long\n}\n</code></pre>"},{"location":"api/#add-column","title":"Add column","text":"<p>Using <code>add_column</code> you can add a column, without having to worry about the field-id:</p> <pre><code>with table.update_schema() as update:\n    update.add_column(\"retries\", IntegerType(), \"Number of retries to place the bid\")\n    # In a struct\n    update.add_column(\"details\", StructType())\n\nwith table.update_schema() as update:\n    update.add_column((\"details\", \"confirmed_by\"), StringType(), \"Name of the exchange\")\n</code></pre> <p>A complex type must exist before columns can be added to it. Fields in complex types are added in a tuple.</p>"},{"location":"api/#rename-column","title":"Rename column","text":"<p>Renaming a field in an Iceberg table is simple:</p> <pre><code>with table.update_schema() as update:\n    update.rename_column(\"retries\", \"num_retries\")\n    # This will rename `confirmed_by` to `processed_by` in the `details` struct\n    update.rename_column((\"details\", \"confirmed_by\"), \"processed_by\")\n</code></pre>"},{"location":"api/#move-column","title":"Move column","text":"<p>Move order of fields:</p> <pre><code>with table.update_schema() as update:\n    update.move_first(\"symbol\")\n    # This will move `bid` after `ask`\n    update.move_after(\"bid\", \"ask\")\n    # This will move `confirmed_by` before `exchange` in the `details` struct\n    update.move_before((\"details\", \"confirmed_by\"), (\"details\", \"exchange\"))\n</code></pre>"},{"location":"api/#update-column","title":"Update column","text":"<p>Update a fields' type, description or required.</p> <pre><code>with table.update_schema() as update:\n    # Promote a float to a double\n    update.update_column(\"bid\", field_type=DoubleType())\n    # Make a field optional\n    update.update_column(\"symbol\", required=False)\n    # Update the documentation\n    update.update_column(\"symbol\", doc=\"Name of the share on the exchange\")\n</code></pre> <p>Be careful, some operations are not compatible, but can still be done at your own risk by setting <code>allow_incompatible_changes</code>:</p> <pre><code>with table.update_schema(allow_incompatible_changes=True) as update:\n    # Incompatible change, cannot require an optional field\n    update.update_column(\"symbol\", required=True)\n</code></pre>"},{"location":"api/#delete-column","title":"Delete column","text":"<p>Delete a field, careful this is a incompatible change (readers/writers might expect this field):</p> <pre><code>with table.update_schema(allow_incompatible_changes=True) as update:\n    update.delete_column(\"some_field\")\n    # In a struct\n    update.delete_column((\"details\", \"confirmed_by\"))\n</code></pre>"},{"location":"api/#partition-evolution","title":"Partition evolution","text":"<p>PyIceberg supports partition evolution. See the partition evolution for more details.</p> <p>The API to use when evolving partitions is the <code>update_spec</code> API on the table.</p> <pre><code>with table.update_spec() as update:\n    update.add_field(\"id\", BucketTransform(16), \"bucketed_id\")\n    update.add_field(\"event_ts\", DayTransform(), \"day_ts\")\n</code></pre> <p>Updating the partition spec can also be done as part of a transaction with other operations.</p> <pre><code>with table.transaction() as transaction:\n    with transaction.update_spec() as update_spec:\n        update_spec.add_field(\"id\", BucketTransform(16), \"bucketed_id\")\n        update_spec.add_field(\"event_ts\", DayTransform(), \"day_ts\")\n    # ... Update properties etc\n</code></pre>"},{"location":"api/#add-fields","title":"Add fields","text":"<p>New partition fields can be added via the <code>add_field</code> API which takes in the field name to partition on, the partition transform, and an optional partition name. If the partition name is not specified, one will be created.</p> <pre><code>with table.update_spec() as update:\n    update.add_field(\"id\", BucketTransform(16), \"bucketed_id\")\n    update.add_field(\"event_ts\", DayTransform(), \"day_ts\")\n    # identity is a shortcut API for adding an IdentityTransform\n    update.identity(\"some_field\")\n</code></pre>"},{"location":"api/#remove-fields","title":"Remove fields","text":"<p>Partition fields can also be removed via the <code>remove_field</code> API if it no longer makes sense to partition on those fields.</p> <pre><code>with table.update_spec() as update:\n    # Remove the partition field with the name\n    update.remove_field(\"some_partition_name\")\n</code></pre>"},{"location":"api/#rename-fields","title":"Rename fields","text":"<p>Partition fields can also be renamed via the <code>rename_field</code> API.</p> <pre><code>with table.update_spec() as update:\n    # Rename the partition field with the name bucketed_id to sharded_id\n    update.rename_field(\"bucketed_id\", \"sharded_id\")\n</code></pre>"},{"location":"api/#sort-order-updates","title":"Sort order updates","text":"<p>Users can update the sort order on existing tables for new data. See sorting for more details.</p> <p>The API to use when updating a sort order is the <code>update_sort_order</code> API on the table.</p> <p>Sort orders can only be updated by adding a new sort order. They cannot be deleted or modified.</p>"},{"location":"api/#updating-a-sort-order-on-a-table","title":"Updating a sort order on a table","text":"<p>To create a new sort order, you can use either the <code>asc</code> or <code>desc</code> API depending on whether you want you data sorted in ascending or descending order. Both take the name of the field, the sort order transform, and a null order that describes the order of null values when sorted.</p> <pre><code>with table.update_sort_order() as update:\n    update.desc(\"event_ts\", DayTransform(), NullOrder.NULLS_FIRST)\n    update.asc(\"some_field\", IdentityTransform(), NullOrder.NULLS_LAST)\n</code></pre>"},{"location":"api/#table-properties","title":"Table properties","text":"<p>Set and remove properties through the <code>Transaction</code> API:</p> <pre><code>with table.transaction() as transaction:\n    transaction.set_properties(abc=\"def\")\n\nassert table.properties == {\"abc\": \"def\"}\n\nwith table.transaction() as transaction:\n    transaction.remove_properties(\"abc\")\n\nassert table.properties == {}\n</code></pre> <p>Or, without context manager:</p> <pre><code>table = table.transaction().set_properties(abc=\"def\").commit_transaction()\n\nassert table.properties == {\"abc\": \"def\"}\n\ntable = table.transaction().remove_properties(\"abc\").commit_transaction()\n\nassert table.properties == {}\n</code></pre>"},{"location":"api/#snapshot-properties","title":"Snapshot properties","text":"<p>Optionally, Snapshot properties can be set while writing to a table using <code>append</code> or <code>overwrite</code> API:</p> <pre><code>tbl.append(df, snapshot_properties={\"abc\": \"def\"})\n\n# or\n\ntbl.overwrite(df, snapshot_properties={\"abc\": \"def\"})\n\nassert tbl.metadata.snapshots[-1].summary[\"abc\"] == \"def\"\n</code></pre>"},{"location":"api/#snapshot-management","title":"Snapshot Management","text":"<p>Manage snapshots with operations through the <code>Table</code> API:</p> <pre><code># To run a specific operation\ntable.manage_snapshots().create_tag(snapshot_id, \"tag123\").commit()\n# To run multiple operations\ntable.manage_snapshots()\n    .create_tag(snapshot_id1, \"tag123\")\n    .create_tag(snapshot_id2, \"tag456\")\n    .commit()\n# Operations are applied on commit.\n</code></pre> <p>You can also use context managers to make more changes:</p> <pre><code>with table.manage_snapshots() as ms:\n    ms.create_branch(snapshot_id1, \"Branch_A\").create_tag(snapshot_id2, \"tag789\")\n</code></pre>"},{"location":"api/#tags","title":"Tags","text":"<p>Tags are named references to snapshots that are immutable. They can be used to mark important snapshots for long-term retention or to reference specific table versions.</p> <p>Create a tag pointing to a specific snapshot:</p> <pre><code># Create a tag with default retention\ntable.manage_snapshots().create_tag(\n    snapshot_id=snapshot_id,\n    tag_name=\"v1.0.0\"\n).commit()\n\n# Create a tag with custom max reference age\ntable.manage_snapshots().create_tag(\n    snapshot_id=snapshot_id,\n    tag_name=\"v1.0.0\",\n    max_ref_age_ms=604800000  # 7 days\n).commit()\n</code></pre> <p>Remove an existing tag:</p> <pre><code>table.manage_snapshots().remove_tag(\"v1.0.0\").commit()\n</code></pre>"},{"location":"api/#branching","title":"Branching","text":"<p>Branches are mutable named references to snapshots that can be updated over time. They allow for independent lineages of table changes, enabling use cases like development branches, testing environments, or parallel workflows.</p> <p>Create a branch pointing to a specific snapshot:</p> <pre><code># Create a branch with default settings\ntable.manage_snapshots().create_branch(\n    snapshot_id=snapshot_id,\n    branch_name=\"dev\"\n).commit()\n\n# Create a branch with retention policies\ntable.manage_snapshots().create_branch(\n    snapshot_id=snapshot_id,\n    branch_name=\"dev\",\n    max_ref_age_ms=604800000,        # Max age of the branch reference (7 days)\n    max_snapshot_age_ms=259200000,   # Max age of snapshots to keep (3 days)\n    min_snapshots_to_keep=10         # Minimum number of snapshots to retain\n).commit()\n</code></pre> <p>Remove an existing branch:</p> <pre><code>table.manage_snapshots().remove_branch(\"dev\").commit()\n</code></pre>"},{"location":"api/#table-maintenance","title":"Table Maintenance","text":"<p>PyIceberg provides table maintenance operations through the <code>table.maintenance</code> API. This provides a clean interface for performing maintenance tasks like snapshot expiration.</p>"},{"location":"api/#snapshot-expiration","title":"Snapshot Expiration","text":"<p>Expire old snapshots to clean up table metadata and reduce storage costs:</p> <pre><code># Expire snapshots older than three days\nfrom datetime import datetime, timedelta\ntable.maintenance.expire_snapshots().older_than(\n    datetime.now() - timedelta(days=3)\n).commit()\n\n# Expire a specific snapshot by ID\ntable.maintenance.expire_snapshots().by_id(12345).commit()\n\n# Context manager usage (recommended for multiple operations)\nwith table.maintenance.expire_snapshots() as expire:\n    expire.by_id(12345)\n    expire.by_id(67890)\n    # Automatically commits when exiting the context\n</code></pre>"},{"location":"api/#real-world-example","title":"Real-world Example","text":"<pre><code>def cleanup_old_snapshots(table_name: str, snapshot_ids: list[int]):\n    \"\"\"Remove specific snapshots from a table.\"\"\"\n    catalog = load_catalog(\"production\")\n    table = catalog.load_table(table_name)\n\n    # Use context manager for safe transaction handling\n    with table.maintenance.expire_snapshots() as expire:\n        for snapshot_id in snapshot_ids:\n            expire.by_id(snapshot_id)\n\n    print(f\"Expired {len(snapshot_ids)} snapshots from {table_name}\")\n\n# Usage\ncleanup_old_snapshots(\"analytics.user_events\", [12345, 67890, 11111])\n</code></pre>"},{"location":"api/#views","title":"Views","text":"<p>PyIceberg supports view operations.</p>"},{"location":"api/#check-if-a-view-exists","title":"Check if a view exists","text":"<pre><code>from pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"default\")\ncatalog.view_exists(\"default.bar\")\n</code></pre>"},{"location":"api/#table-statistics-management","title":"Table Statistics Management","text":"<p>Manage table statistics with operations through the <code>Table</code> API:</p> <pre><code># To run a specific operation\ntable.update_statistics().set_statistics(statistics_file=statistics_file).commit()\n# To run multiple operations\ntable.update_statistics()\n  .set_statistics(statistics_file1)\n  .remove_statistics(snapshot_id2)\n  .commit()\n# Operations are applied on commit.\n</code></pre> <p>You can also use context managers to make more changes:</p> <pre><code>with table.update_statistics() as update:\n    update.set_statistics(statistics_file)\n    update.remove_statistics(snapshot_id2)\n</code></pre>"},{"location":"api/#query-the-data","title":"Query the data","text":"<p>To query a table, a table scan is needed. A table scan accepts a filter, columns, optionally a limit and a snapshot ID:</p> <pre><code>from pyiceberg.catalog import load_catalog\nfrom pyiceberg.expressions import GreaterThanOrEqual\n\ncatalog = load_catalog(\"default\")\ntable = catalog.load_table(\"nyc.taxis\")\n\nscan = table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n    limit=100,\n)\n\n# Or filter using a string predicate\nscan = table.scan(\n    row_filter=\"trip_distance &gt; 10.0\",\n)\n\n[task.file.file_path for task in scan.plan_files()]\n</code></pre> <p>The low level API <code>plan_files</code> methods returns a set of tasks that provide the files that might contain matching rows:</p> <pre><code>[\n  \"s3://warehouse/wh/nyc/taxis/data/00003-4-42464649-92dd-41ad-b83b-dea1a2fe4b58-00001.parquet\"\n]\n</code></pre> <p>In this case it is up to the engine itself to filter the file itself. Below, <code>to_arrow()</code> and <code>to_duckdb()</code> that already do this for you.</p>"},{"location":"api/#apache-arrow","title":"Apache Arrow","text":"<p>Requirements</p> <p>This requires <code>pyarrow</code> to be installed.</p> <p>Using PyIceberg it is filter out data from a huge table and pull it into a PyArrow table:</p> <pre><code>table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_arrow()\n</code></pre> <p>This will return a PyArrow table:</p> <pre><code>pyarrow.Table\nVendorID: int64\ntpep_pickup_datetime: timestamp[us, tz=+00:00]\ntpep_dropoff_datetime: timestamp[us, tz=+00:00]\n----\nVendorID: [[2,1,2,1,1,...,2,2,2,2,2],[2,1,1,1,2,...,1,1,2,1,2],...,[2,2,2,2,2,...,2,6,6,2,2],[2,2,2,2,2,...,2,2,2,2,2]]\ntpep_pickup_datetime: [[2021-04-01 00:28:05.000000,...,2021-04-30 23:44:25.000000]]\ntpep_dropoff_datetime: [[2021-04-01 00:47:59.000000,...,2021-05-01 00:14:47.000000]]\n</code></pre> <p>This will only pull in the files that that might contain matching rows.</p> <p>One can also return a PyArrow RecordBatchReader, if reading one record batch at a time is preferred:</p> <pre><code>table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_arrow_batch_reader()\n</code></pre>"},{"location":"api/#pandas","title":"Pandas","text":"<p>Requirements</p> <p>This requires <code>pandas</code> to be installed.</p> <p>PyIceberg makes it easy to filter out data from a huge table and pull it into a Pandas dataframe locally. This will only fetch the relevant Parquet files for the query and apply the filter. This will reduce IO and therefore improve performance and reduce cost.</p> <pre><code>table.scan(\n    row_filter=\"trip_distance &gt;= 10.0\",\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_pandas()\n</code></pre> <p>This will return a Pandas dataframe:</p> <pre><code>        VendorID      tpep_pickup_datetime     tpep_dropoff_datetime\n0              2 2021-04-01 00:28:05+00:00 2021-04-01 00:47:59+00:00\n1              1 2021-04-01 00:39:01+00:00 2021-04-01 00:57:39+00:00\n2              2 2021-04-01 00:14:42+00:00 2021-04-01 00:42:59+00:00\n3              1 2021-04-01 00:17:17+00:00 2021-04-01 00:43:38+00:00\n4              1 2021-04-01 00:24:04+00:00 2021-04-01 00:56:20+00:00\n...          ...                       ...                       ...\n116976         2 2021-04-30 23:56:18+00:00 2021-05-01 00:29:13+00:00\n116977         2 2021-04-30 23:07:41+00:00 2021-04-30 23:37:18+00:00\n116978         2 2021-04-30 23:38:28+00:00 2021-05-01 00:12:04+00:00\n116979         2 2021-04-30 23:33:00+00:00 2021-04-30 23:59:00+00:00\n116980         2 2021-04-30 23:44:25+00:00 2021-05-01 00:14:47+00:00\n\n[116981 rows x 3 columns]\n</code></pre> <p>It is recommended to use Pandas 2 or later, because it stores the data in an Apache Arrow backend which avoids copies of data.</p>"},{"location":"api/#duckdb","title":"DuckDB","text":"<p>Requirements</p> <p>This requires DuckDB to be installed.</p> <p>A table scan can also be converted into a in-memory DuckDB table:</p> <pre><code>con = table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_duckdb(table_name=\"distant_taxi_trips\")\n</code></pre> <p>Using the cursor that we can run queries on the DuckDB table:</p> <pre><code>print(\n    con.execute(\n        \"SELECT tpep_dropoff_datetime - tpep_pickup_datetime AS duration FROM distant_taxi_trips LIMIT 4\"\n    ).fetchall()\n)\n[\n    (datetime.timedelta(seconds=1194),),\n    (datetime.timedelta(seconds=1118),),\n    (datetime.timedelta(seconds=1697),),\n    (datetime.timedelta(seconds=1581),),\n]\n</code></pre>"},{"location":"api/#ray","title":"Ray","text":"<p>Requirements</p> <p>This requires Ray to be installed.</p> <p>A table scan can also be converted into a Ray dataset:</p> <pre><code>ray_dataset = table.scan(\n    row_filter=GreaterThanOrEqual(\"trip_distance\", 10.0),\n    selected_fields=(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"),\n).to_ray()\n</code></pre> <p>This will return a Ray dataset:</p> <pre><code>Dataset(\n    num_blocks=1,\n    num_rows=1168798,\n    schema={\n        VendorID: int64,\n        tpep_pickup_datetime: timestamp[us, tz=UTC],\n        tpep_dropoff_datetime: timestamp[us, tz=UTC]\n    }\n)\n</code></pre> <p>Using Ray Dataset API to interact with the dataset:</p> <pre><code>print(ray_dataset.take(2))\n[\n    {\n        \"VendorID\": 2,\n        \"tpep_pickup_datetime\": datetime.datetime(2008, 12, 31, 23, 23, 50),\n        \"tpep_dropoff_datetime\": datetime.datetime(2009, 1, 1, 0, 34, 31),\n    },\n    {\n        \"VendorID\": 2,\n        \"tpep_pickup_datetime\": datetime.datetime(2008, 12, 31, 23, 5, 3),\n        \"tpep_dropoff_datetime\": datetime.datetime(2009, 1, 1, 16, 10, 18),\n    },\n]\n</code></pre>"},{"location":"api/#bodo","title":"Bodo","text":"<p>PyIceberg interfaces closely with Bodo Dataframes (see Bodo Iceberg Quick Start), which provides a drop-in replacement for Pandas that applies query, compiler and HPC optimizations automatically. Bodo accelerates and scales Python code from single laptops to large clusters without code rewrites.</p> <p>Requirements</p> <p>This requires <code>bodo</code> to be installed.</p> <pre><code>pip install pyiceberg['bodo']\n</code></pre> <p>A table can be read easily into a Bodo Dataframe to perform Pandas operations:</p> <pre><code>df = table.to_bodo()  # equivalent to `bodo.pandas.read_iceberg_table(table)`\ndf = df[df[\"trip_distance\"] &gt;= 10.0]\ndf = df[[\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]]\nprint(df)\n</code></pre> <p>This creates a lazy query, optimizes it, and runs it on all available cores (print triggers execution):</p> <pre><code>        VendorID tpep_pickup_datetime tpep_dropoff_datetime\n0              2  2023-01-01 00:27:12   2023-01-01 00:49:56\n1              2  2023-01-01 00:09:29   2023-01-01 00:29:23\n2              1  2023-01-01 00:13:30   2023-01-01 00:44:00\n3              2  2023-01-01 00:41:41   2023-01-01 01:19:32\n4              2  2023-01-01 00:22:39   2023-01-01 01:30:45\n...          ...                  ...                   ...\n245478         2  2023-01-31 22:32:57   2023-01-31 23:01:48\n245479         2  2023-01-31 22:03:26   2023-01-31 22:46:13\n245480         2  2023-01-31 23:25:56   2023-02-01 00:05:42\n245481         2  2023-01-31 23:18:00   2023-01-31 23:46:00\n245482         2  2023-01-31 23:18:00   2023-01-31 23:41:00\n\n[245483 rows x 3 columns]\n</code></pre> <p>Bodo is optimized to take advantage of Iceberg features such as hidden partitioning and various statistics for efficient reads.</p>"},{"location":"api/#daft","title":"Daft","text":"<p>PyIceberg interfaces closely with Daft Dataframes (see also: Daft integration with Iceberg) which provides a full lazily optimized query engine interface on top of PyIceberg tables.</p> <p>Requirements</p> <p>This requires Daft to be installed.</p> <p>A table can be read easily into a Daft Dataframe:</p> <pre><code>df = table.to_daft()  # equivalent to `daft.read_iceberg(table)`\ndf = df.where(df[\"trip_distance\"] &gt;= 10.0)\ndf = df.select(\"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\")\n</code></pre> <p>This returns a Daft Dataframe which is lazily materialized. Printing <code>df</code> will display the schema:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 VendorID \u2506 tpep_pickup_datetime          \u2506 tpep_dropoff_datetime         \u2502\n\u2502 ---      \u2506 ---                           \u2506 ---                           \u2502\n\u2502 Int64    \u2506 Timestamp(Microseconds, None) \u2506 Timestamp(Microseconds, None) \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(No data to display: Dataframe not materialized)\n</code></pre> <p>We can execute the Dataframe to preview the first few rows of the query with <code>df.show()</code>.</p> <p>This is correctly optimized to take advantage of Iceberg features such as hidden partitioning and file-level statistics for efficient reads.</p> <pre><code>df.show(2)\n</code></pre> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 VendorID \u2506 tpep_pickup_datetime          \u2506 tpep_dropoff_datetime         \u2502\n\u2502 ---      \u2506 ---                           \u2506 ---                           \u2502\n\u2502 Int64    \u2506 Timestamp(Microseconds, None) \u2506 Timestamp(Microseconds, None) \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2        \u2506 2008-12-31T23:23:50.000000    \u2506 2009-01-01T00:34:31.000000    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2        \u2506 2008-12-31T23:05:03.000000    \u2506 2009-01-01T16:10:18.000000    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 2 rows)\n</code></pre>"},{"location":"api/#polars","title":"Polars","text":"<p>PyIceberg interfaces closely with Polars Dataframes and LazyFrame which provides a full lazily optimized query engine interface on top of PyIceberg tables.</p> <p>Requirements</p> <p>This requires <code>polars</code> to be installed.</p> <pre><code>pip install pyiceberg['polars']\n</code></pre> <p>PyIceberg data can be analyzed and accessed through Polars using either DataFrame or LazyFrame. If your code utilizes the Apache Iceberg data scanning and retrieval API and then analyzes the resulting DataFrame in Polars, use the <code>table.scan().to_polars()</code> API. If the intent is to utilize Polars' high-performance filtering and retrieval functionalities, use LazyFrame exported from the Iceberg table with the <code>table.to_polars()</code> API.</p> <pre><code># Get LazyFrame\niceberg_table.to_polars()\n\n# Get Data Frame\niceberg_table.scan().to_polars()\n</code></pre>"},{"location":"api/#working-with-polars-dataframe","title":"Working with Polars DataFrame","text":"<p>PyIceberg makes it easy to filter out data from a huge table and pull it into a Polars dataframe locally. This will only fetch the relevant Parquet files for the query and apply the filter. This will reduce IO and therefore improve performance and reduce cost.</p> <pre><code>schema = Schema(\n    NestedField(field_id=1, name='ticket_id', field_type=LongType(), required=True),\n    NestedField(field_id=2, name='customer_id', field_type=LongType(), required=True),\n    NestedField(field_id=3, name='issue', field_type=StringType(), required=False),\n    NestedField(field_id=4, name='created_at', field_type=TimestampType(), required=True),\n  required=True\n)\n\niceberg_table = catalog.create_table(\n    identifier='default.product_support_issues',\n    schema=schema\n)\n\npa_table_data = pa.Table.from_pylist(\n    [\n        {'ticket_id': 1, 'customer_id': 546, 'issue': 'User Login issue', 'created_at': 1650020000000000},\n        {'ticket_id': 2, 'customer_id': 547, 'issue': 'Payment not going through', 'created_at': 1650028640000000},\n        {'ticket_id': 3, 'customer_id': 548, 'issue': 'Error on checkout', 'created_at': 1650037280000000},\n        {'ticket_id': 4, 'customer_id': 549, 'issue': 'Unable to reset password', 'created_at': 1650045920000000},\n        {'ticket_id': 5, 'customer_id': 550, 'issue': 'Account locked', 'created_at': 1650054560000000},\n        {'ticket_id': 6, 'customer_id': 551, 'issue': 'Order not received', 'created_at': 1650063200000000},\n        {'ticket_id': 7, 'customer_id': 552, 'issue': 'Refund not processed', 'created_at': 1650071840000000},\n        {'ticket_id': 8, 'customer_id': 553, 'issue': 'Shipping address issue', 'created_at': 1650080480000000},\n        {'ticket_id': 9, 'customer_id': 554, 'issue': 'Product damaged', 'created_at': 1650089120000000},\n        {'ticket_id': 10, 'customer_id': 555, 'issue': 'Unable to apply discount code', 'created_at': 1650097760000000},\n        {'ticket_id': 11, 'customer_id': 556, 'issue': 'Website not loading', 'created_at': 1650106400000000},\n        {'ticket_id': 12, 'customer_id': 557, 'issue': 'Incorrect order received', 'created_at': 1650115040000000},\n        {'ticket_id': 13, 'customer_id': 558, 'issue': 'Unable to track order', 'created_at': 1650123680000000},\n        {'ticket_id': 14, 'customer_id': 559, 'issue': 'Order delayed', 'created_at': 1650132320000000},\n        {'ticket_id': 15, 'customer_id': 560, 'issue': 'Product not as described', 'created_at': 1650140960000000},\n        {'ticket_id': 16, 'customer_id': 561, 'issue': 'Unable to contact support', 'created_at': 1650149600000000},\n        {'ticket_id': 17, 'customer_id': 562, 'issue': 'Duplicate charge', 'created_at': 1650158240000000},\n        {'ticket_id': 18, 'customer_id': 563, 'issue': 'Unable to update profile', 'created_at': 1650166880000000},\n        {'ticket_id': 19, 'customer_id': 564, 'issue': 'App crashing', 'created_at': 1650175520000000},\n        {'ticket_id': 20, 'customer_id': 565, 'issue': 'Unable to download invoice', 'created_at': 1650184160000000},\n        {'ticket_id': 21, 'customer_id': 566, 'issue': 'Incorrect billing amount', 'created_at': 1650192800000000},\n    ], schema=iceberg_table.schema().as_arrow()\n)\n\niceberg_table.append(\n    df=pa_table_data\n)\n\ntable.scan(\n    row_filter=\"ticket_id &gt; 10\",\n).to_polars()\n</code></pre> <p>This will return a Polars DataFrame:</p> <pre><code>shape: (11, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticket_id \u2506 customer_id \u2506 issue                      \u2506 created_at          \u2502\n\u2502 ---       \u2506 ---         \u2506 ---                        \u2506 ---                 \u2502\n\u2502 i64       \u2506 i64         \u2506 str                        \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11        \u2506 556         \u2506 Website not loading        \u2506 2022-04-16 10:53:20 \u2502\n\u2502 12        \u2506 557         \u2506 Incorrect order received   \u2506 2022-04-16 13:17:20 \u2502\n\u2502 13        \u2506 558         \u2506 Unable to track order      \u2506 2022-04-16 15:41:20 \u2502\n\u2502 14        \u2506 559         \u2506 Order delayed              \u2506 2022-04-16 18:05:20 \u2502\n\u2502 15        \u2506 560         \u2506 Product not as described   \u2506 2022-04-16 20:29:20 \u2502\n\u2502 \u2026         \u2506 \u2026           \u2506 \u2026                          \u2506 \u2026                   \u2502\n\u2502 17        \u2506 562         \u2506 Duplicate charge           \u2506 2022-04-17 01:17:20 \u2502\n\u2502 18        \u2506 563         \u2506 Unable to update profile   \u2506 2022-04-17 03:41:20 \u2502\n\u2502 19        \u2506 564         \u2506 App crashing               \u2506 2022-04-17 06:05:20 \u2502\n\u2502 20        \u2506 565         \u2506 Unable to download invoice \u2506 2022-04-17 08:29:20 \u2502\n\u2502 21        \u2506 566         \u2506 Incorrect billing amount   \u2506 2022-04-17 10:53:20 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"api/#working-with-polars-lazyframe","title":"Working with Polars LazyFrame","text":"<p>PyIceberg supports creation of a Polars LazyFrame based on an Iceberg Table.</p> <p>using the above code example:</p> <pre><code>lf = iceberg_table.to_polars().filter(pl.col(\"ticket_id\") &gt; 10)\nprint(lf.collect())\n</code></pre> <p>This above code snippet returns a Polars LazyFrame and defines a filter to be executed by Polars:</p> <pre><code>shape: (11, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 ticket_id \u2506 customer_id \u2506 issue                      \u2506 created_at          \u2502\n\u2502 ---       \u2506 ---         \u2506 ---                        \u2506 ---                 \u2502\n\u2502 i64       \u2506 i64         \u2506 str                        \u2506 datetime[\u03bcs]        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 11        \u2506 556         \u2506 Website not loading        \u2506 2022-04-16 10:53:20 \u2502\n\u2502 12        \u2506 557         \u2506 Incorrect order received   \u2506 2022-04-16 13:17:20 \u2502\n\u2502 13        \u2506 558         \u2506 Unable to track order      \u2506 2022-04-16 15:41:20 \u2502\n\u2502 14        \u2506 559         \u2506 Order delayed              \u2506 2022-04-16 18:05:20 \u2502\n\u2502 15        \u2506 560         \u2506 Product not as described   \u2506 2022-04-16 20:29:20 \u2502\n\u2502 \u2026         \u2506 \u2026           \u2506 \u2026                          \u2506 \u2026                   \u2502\n\u2502 17        \u2506 562         \u2506 Duplicate charge           \u2506 2022-04-17 01:17:20 \u2502\n\u2502 18        \u2506 563         \u2506 Unable to update profile   \u2506 2022-04-17 03:41:20 \u2502\n\u2502 19        \u2506 564         \u2506 App crashing               \u2506 2022-04-17 06:05:20 \u2502\n\u2502 20        \u2506 565         \u2506 Unable to download invoice \u2506 2022-04-17 08:29:20 \u2502\n\u2502 21        \u2506 566         \u2506 Incorrect billing amount   \u2506 2022-04-17 10:53:20 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"api/#apache-datafusion","title":"Apache DataFusion","text":"<p>PyIceberg integrates with Apache DataFusion through the Custom Table Provider interface (FFI_TableProvider) exposed through <code>iceberg-rust</code>.</p> <p>Requirements</p> <p>This requires <code>datafusion</code> and <code>pyiceberg-core</code> to be installed.</p> <p>Experimental Feature</p> <p>The DataFusion integration is considered experimental.</p> <p>The integration has a few caveats:</p> <ul> <li>Only works with <code>datafusion == 51</code>, aligns with the version used in <code>pyiceberg-core</code></li> <li>Depends directly on <code>iceberg-rust</code> instead of PyIceberg's implementation</li> <li>Has limited features compared to the full PyIceberg API</li> </ul> <p>The integration will improve as both DataFusion and <code>iceberg-rust</code> matures.</p> <p>PyIceberg tables can be registered directly with DataFusion's SessionContext using the table provider interface.</p> <pre><code>from datafusion import SessionContext\nfrom pyiceberg.catalog import load_catalog\nimport pyarrow as pa\n\n# Load catalog and create/load a table\ncatalog = load_catalog(\"catalog\", type=\"in-memory\")\ncatalog.create_namespace_if_not_exists(\"default\")\n\n# Create some sample data\ndata = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\niceberg_table = catalog.create_table(\"default.test\", schema=data.schema)\niceberg_table.append(data)\n\n# Register the table with DataFusion\nctx = SessionContext()\nctx.register_table(\"test\", iceberg_table)\n\n# Query the table using DataFusion SQL\nctx.table(\"test\").show()\n</code></pre> <p>This will output:</p> <pre><code>DataFrame()\n+---+---+\n| x | y |\n+---+---+\n| 1 | 4 |\n| 2 | 5 |\n| 3 | 6 |\n+---+---+\n</code></pre>"},{"location":"cli/","title":"CLI","text":""},{"location":"cli/#python-cli","title":"Python CLI","text":"<p>Pyiceberg comes with a CLI that's available after installing the <code>pyiceberg</code> package.</p> <p>You can pass the path to the Catalog using the <code>--uri</code> and <code>--credential</code> argument, but it is recommended to setup a <code>~/.pyiceberg.yaml</code> config as described in the Catalog section.</p> <pre><code>\u279c  pyiceberg --help\nUsage: pyiceberg [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --catalog TEXT\n  --verbose BOOLEAN\n  --output [text|json]\n  --ugi TEXT\n  --uri TEXT\n  --credential TEXT\n  --help                Show this message and exit.\n\nCommands:\n  create      Operation to create a namespace.\n  describe    Describe a namespace or a table.\n  drop        Operations to drop a namespace or table.\n  files       List all the files of the table.\n  list        List tables or namespaces.\n  list-refs   List all the refs in the provided table.\n  location    Return the location of the table.\n  properties  Properties on tables/namespaces.\n  rename      Rename a table.\n  schema      Get the schema of the table.\n  spec        Return the partition spec of the table.\n  uuid        Return the UUID of the table.\n  version     Print pyiceberg version.\n</code></pre> <p>This example assumes that you have a default catalog set. If you want to load another catalog, for example, the rest example above. Then you need to set <code>--catalog rest</code>.</p> <pre><code>\u279c  pyiceberg list\ndefault\nnyc\n</code></pre> <pre><code>\u279c  pyiceberg list nyc\nnyc.taxis\n</code></pre> <pre><code>\u279c  pyiceberg describe nyc.taxis\nTable format version  1\nMetadata location     file:/.../nyc.db/taxis/metadata/00000-aa3a3eac-ea08-4255-b890-383a64a94e42.metadata.json\nTable UUID            6cdfda33-bfa3-48a7-a09e-7abb462e3460\nLast Updated          1661783158061\nPartition spec        []\nSort order            []\nCurrent schema        Schema, id=0\n\u251c\u2500\u2500 1: VendorID: optional long\n\u251c\u2500\u2500 2: tpep_pickup_datetime: optional timestamptz\n\u251c\u2500\u2500 3: tpep_dropoff_datetime: optional timestamptz\n\u251c\u2500\u2500 4: passenger_count: optional double\n\u251c\u2500\u2500 5: trip_distance: optional double\n\u251c\u2500\u2500 6: RatecodeID: optional double\n\u251c\u2500\u2500 7: store_and_fwd_flag: optional string\n\u251c\u2500\u2500 8: PULocationID: optional long\n\u251c\u2500\u2500 9: DOLocationID: optional long\n\u251c\u2500\u2500 10: payment_type: optional long\n\u251c\u2500\u2500 11: fare_amount: optional double\n\u251c\u2500\u2500 12: extra: optional double\n\u251c\u2500\u2500 13: mta_tax: optional double\n\u251c\u2500\u2500 14: tip_amount: optional double\n\u251c\u2500\u2500 15: tolls_amount: optional double\n\u251c\u2500\u2500 16: improvement_surcharge: optional double\n\u251c\u2500\u2500 17: total_amount: optional double\n\u251c\u2500\u2500 18: congestion_surcharge: optional double\n\u2514\u2500\u2500 19: airport_fee: optional double\nCurrent snapshot      Operation.APPEND: id=5937117119577207079, schema_id=0\nSnapshots             Snapshots\n\u2514\u2500\u2500 Snapshot 5937117119577207079, schema 0: file:/.../nyc.db/taxis/metadata/snap-5937117119577207079-1-94656c4f-4c66-4600-a4ca-f30377300527.avro\nProperties            owner                 root\nwrite.format.default  parquet\n</code></pre> <p>Or output in JSON for automation:</p> <pre><code>\u279c  pyiceberg --output json describe nyc.taxis | jq\n{\n  \"identifier\": [\n    \"nyc\",\n    \"taxis\"\n  ],\n  \"metadata_location\": \"file:/.../nyc.db/taxis/metadata/00000-aa3a3eac-ea08-4255-b890-383a64a94e42.metadata.json\",\n  \"metadata\": {\n    \"location\": \"file:/.../nyc.db/taxis\",\n    \"table-uuid\": \"6cdfda33-bfa3-48a7-a09e-7abb462e3460\",\n    \"last-updated-ms\": 1661783158061,\n    \"last-column-id\": 19,\n    \"schemas\": [\n      {\n        \"type\": \"struct\",\n        \"fields\": [\n          {\n            \"id\": 1,\n            \"name\": \"VendorID\",\n            \"type\": \"long\",\n            \"required\": false\n          },\n...\n          {\n            \"id\": 19,\n            \"name\": \"airport_fee\",\n            \"type\": \"double\",\n            \"required\": false\n          }\n        ],\n        \"schema-id\": 0,\n        \"identifier-field-ids\": []\n      }\n    ],\n    \"current-schema-id\": 0,\n    \"partition-specs\": [\n      {\n        \"spec-id\": 0,\n        \"fields\": []\n      }\n    ],\n    \"default-spec-id\": 0,\n    \"last-partition-id\": 999,\n    \"properties\": {\n      \"owner\": \"root\",\n      \"write.format.default\": \"parquet\"\n    },\n    \"current-snapshot-id\": 5937117119577207000,\n    \"snapshots\": [\n      {\n        \"snapshot-id\": 5937117119577207000,\n        \"timestamp-ms\": 1661783158061,\n        \"manifest-list\": \"file:/.../nyc.db/taxis/metadata/snap-5937117119577207079-1-94656c4f-4c66-4600-a4ca-f30377300527.avro\",\n        \"summary\": {\n          \"operation\": \"append\",\n          \"spark.app.id\": \"local-1661783139151\",\n          \"added-data-files\": \"1\",\n          \"added-records\": \"2979431\",\n          \"added-files-size\": \"46600777\",\n          \"changed-partition-count\": \"1\",\n          \"total-records\": \"2979431\",\n          \"total-files-size\": \"46600777\",\n          \"total-data-files\": \"1\",\n          \"total-delete-files\": \"0\",\n          \"total-position-deletes\": \"0\",\n          \"total-equality-deletes\": \"0\"\n        },\n        \"schema-id\": 0\n      }\n    ],\n    \"snapshot-log\": [\n      {\n        \"snapshot-id\": \"5937117119577207079\",\n        \"timestamp-ms\": 1661783158061\n      }\n    ],\n    \"metadata-log\": [],\n    \"sort-orders\": [\n      {\n        \"order-id\": 0,\n        \"fields\": []\n      }\n    ],\n    \"default-sort-order-id\": 0,\n    \"refs\": {\n      \"main\": {\n        \"snapshot-id\": 5937117119577207000,\n        \"type\": \"branch\"\n      }\n    },\n    \"format-version\": 1,\n    \"schema\": {\n      \"type\": \"struct\",\n      \"fields\": [\n        {\n          \"id\": 1,\n          \"name\": \"VendorID\",\n          \"type\": \"long\",\n          \"required\": false\n        },\n...\n        {\n          \"id\": 19,\n          \"name\": \"airport_fee\",\n          \"type\": \"double\",\n          \"required\": false\n        }\n      ],\n      \"schema-id\": 0,\n      \"identifier-field-ids\": []\n    },\n    \"partition-spec\": []\n  }\n}\n</code></pre> <p>You can also add, update or remove properties on tables or namespaces:</p> <pre><code>\u279c  pyiceberg properties set table nyc.taxis write.metadata.delete-after-commit.enabled true\nSet write.metadata.delete-after-commit.enabled=true on nyc.taxis\n\n\u279c  pyiceberg properties get table nyc.taxis\nwrite.metadata.delete-after-commit.enabled  true\n\n\u279c  pyiceberg properties remove table nyc.taxis write.metadata.delete-after-commit.enabled\nProperty write.metadata.delete-after-commit.enabled removed from nyc.taxis\n\n\u279c  pyiceberg properties get table nyc.taxis write.metadata.delete-after-commit.enabled\nCould not find property write.metadata.delete-after-commit.enabled on nyc.taxis\n</code></pre>"},{"location":"community/","title":"Community","text":""},{"location":"community/#join-the-community","title":"Join the community","text":"<p>Apache Iceberg tracks issues in GitHub and prefers to receive contributions as pull requests.</p> <p>Community discussions happen primarily on the dev mailing list, on Apache Iceberg Slack workspace in the #python channel, and on specific GitHub issues.</p>"},{"location":"community/#iceberg-community-events","title":"Iceberg Community Events","text":"<p>The PyIceberg community sync is on the last Tuesday of every month. The calendar event is located on the Iceberg Dev Events calendar.</p>"},{"location":"community/#community-guidelines","title":"Community Guidelines","text":""},{"location":"community/#apache-iceberg-community-guidelines","title":"Apache Iceberg Community Guidelines","text":"<p>The Apache Iceberg community is built on the principles described in the Apache Way and all who engage with the community are expected to be respectful, open, come with the best interests of the community in mind, and abide by the Apache Foundation Code of Conduct.</p>"},{"location":"community/#participants-with-corporate-interests","title":"Participants with Corporate Interests","text":"<p>A wide range of corporate entities have interests that overlap in both features and frameworks related to Iceberg and while we encourage engagement and contributions, the community is not a venue for marketing, solicitation, or recruitment.</p> <p>Any vendor who wants to participate in the Apache Iceberg community Slack workspace should create a dedicated vendor channel for their organization prefixed by <code>vendor-</code>.</p> <p>This space can be used to discuss features and integration with Iceberg related to the vendor offering.  This space should not be used to promote competing vendor products/services or disparage other vendor offerings.  Discussion should be focused on questions asked by the community and not to expand/introduce/redirect users to alternate offerings.</p>"},{"location":"community/#marketing-solicitation-recruiting","title":"Marketing / Solicitation / Recruiting","text":"<p>The Apache Iceberg community is a space for everyone to operate free of influence. The development lists, Slack workspace, and GitHub should not be used to market products or services.  Solicitation or overt promotion should not be performed in common channels or through direct messages.</p> <p>Recruitment of community members should not be conducted through direct messages or community channels, but opportunities related to contributing to or using Iceberg can be posted to the <code>#jobs</code> channel.</p> <p>For questions regarding any of the guidelines above, please contact a PMC member</p>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#configuration","title":"Configuration","text":""},{"location":"configuration/#setting-configuration-values","title":"Setting Configuration Values","text":"<p>There are three ways to pass in configuration:</p> <ul> <li>Using the <code>.pyiceberg.yaml</code> configuration file (Recommended)</li> <li>Through environment variables</li> <li>By passing in credentials through the CLI or the Python API</li> </ul> <p>The configuration file can be stored in either the directory specified by the <code>PYICEBERG_HOME</code> environment variable, the home directory, or current working directory (in this order).</p> <p>To change the path searched for the <code>.pyiceberg.yaml</code>, you can overwrite the <code>PYICEBERG_HOME</code> environment variable.</p> <p>Another option is through environment variables:</p> <pre><code>export PYICEBERG_CATALOG__DEFAULT__URI=thrift://localhost:9083\nexport PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID=username\nexport PYICEBERG_CATALOG__DEFAULT__S3__SECRET_ACCESS_KEY=password\n</code></pre> <p>The environment variable picked up by Iceberg starts with <code>PYICEBERG_</code> and then follows the yaml structure below, where a double underscore <code>__</code> represents a nested field, and the underscore <code>_</code> is converted into a dash <code>-</code>.</p> <p>For example, <code>PYICEBERG_CATALOG__DEFAULT__S3__ACCESS_KEY_ID</code>, sets <code>s3.access-key-id</code> on the <code>default</code> catalog.</p>"},{"location":"configuration/#tables","title":"Tables","text":"<p>Iceberg tables support table properties to configure table behavior.</p>"},{"location":"configuration/#write-options","title":"Write options","text":"Key Options Default Description <code>write.parquet.compression-codec</code> <code>{uncompressed,zstd,gzip,snappy}</code> zstd Sets the Parquet compression coddec. <code>write.parquet.compression-level</code> Integer null Parquet compression level for the codec. If not set, it is up to PyIceberg <code>write.parquet.row-group-limit</code> Number of rows 1048576 The upper bound of the number of entries within a single row group <code>write.parquet.page-size-bytes</code> Size in bytes 1MB Set a target threshold for the approximate encoded size of data pages within a column chunk <code>write.parquet.page-row-limit</code> Number of rows 20000 Set a target threshold for the maximum number of rows within a column chunk <code>write.parquet.dict-size-bytes</code> Size in bytes 2MB Set the dictionary page size limit per row group <code>write.metadata.previous-versions-max</code> Integer 100 The max number of previous version metadata files to keep before deleting after commit. <code>write.metadata.delete-after-commit.enabled</code> Boolean False Whether to automatically delete old tracked metadata files after each table commit. It will retain a number of the most recent metadata files, which can be set using property <code>write.metadata.previous-versions-max</code>. <code>write.object-storage.enabled</code> Boolean False Enables the <code>ObjectStoreLocationProvider</code> that adds a hash component to file paths. <code>write.object-storage.partitioned-paths</code> Boolean True Controls whether partition values are included in file paths when object storage is enabled <code>write.py-location-provider.impl</code> String of form <code>module.ClassName</code> null Optional, custom <code>LocationProvider</code> implementation <code>write.data.path</code> String pointing to location <code>{metadata.location}/data</code> Sets the location under which data is written. <code>write.metadata.path</code> String pointing to location <code>{metadata.location}/metadata</code> Sets the location under which metadata is written."},{"location":"configuration/#table-behavior-options","title":"Table behavior options","text":"Key Options Default Description <code>commit.manifest.target-size-bytes</code> Size in bytes 8388608 (8MB) Target size when merging manifest files <code>commit.manifest.min-count-to-merge</code> Number of manifests 100 Minimum number of manifests to accumulate before merging <code>commit.manifest-merge.enabled</code> Boolean False Controls whether to automatically merge manifests on writes <p>Fast append</p> <p>Unlike Java implementation, PyIceberg default to the fast append and thus <code>commit.manifest-merge.enabled</code> is set to <code>False</code> by default.</p>"},{"location":"configuration/#fileio","title":"FileIO","text":"<p>Iceberg works with the concept of a FileIO which is a pluggable module for reading, writing, and deleting files. By default, PyIceberg will try to initialize the FileIO that's suitable for the scheme (<code>s3://</code>, <code>gs://</code>, etc.) and will use the first one that's installed.</p> <ul> <li>s3, s3a, s3n: <code>PyArrowFileIO</code>, <code>FsspecFileIO</code></li> <li>gs: <code>PyArrowFileIO</code></li> <li>file: <code>PyArrowFileIO</code></li> <li>hdfs: <code>PyArrowFileIO</code></li> <li>abfs, abfss: <code>FsspecFileIO</code></li> <li>oss: <code>PyArrowFileIO</code></li> <li>hf: <code>FsspecFileIO</code></li> </ul> <p>You can also set the FileIO explicitly:</p> Key Example Description py-io-impl pyiceberg.io.fsspec.FsspecFileIO Sets the FileIO explicitly to an implementation, and will fail explicitly if it can't be loaded <p>For the FileIO there are several configuration options available:</p>"},{"location":"configuration/#s3","title":"S3","text":"Key Example Description s3.endpoint https://10.0.19.25/ Configure an alternative endpoint of the S3 service for the FileIO to access. This could be used to use S3FileIO with any s3-compatible object storage service that has a different endpoint, or access a private S3 endpoint in a virtual private cloud. s3.access-key-id admin Configure the static access key id used to access the FileIO. s3.secret-access-key password Configure the static secret access key used to access the FileIO. s3.session-token AQoDYXdzEJr... Configure the static session token used to access the FileIO. s3.profile-name default Configure the AWS profile used to access the S3 FileIO. s3.role-session-name session An optional identifier for the assumed role session. s3.role-arn arn:aws:... AWS Role ARN. If provided instead of access_key and secret_key, temporary credentials will be fetched by assuming this role. s3.signer bearer Configure the signature version of the FileIO. s3.signer.uri http://my.signer:8080/s3 Configure the remote signing uri if it differs from the catalog uri. Remote signing is only implemented for <code>FsspecFileIO</code>. The final request is sent to <code>&lt;s3.signer.uri&gt;/&lt;s3.signer.endpoint&gt;</code>. s3.signer.endpoint v1/main/s3-sign Configure the remote signing endpoint. Remote signing is only implemented for <code>FsspecFileIO</code>. The final request is sent to <code>&lt;s3.signer.uri&gt;/&lt;s3.signer.endpoint&gt;</code>. (default : v1/aws/s3/sign). s3.region us-west-2 Configure the default region used to initialize an <code>S3FileSystem</code>. <code>PyArrowFileIO</code> attempts to automatically tries to resolve the region if this isn't set (only supported for AWS S3 Buckets). s3.resolve-region False Only supported for <code>PyArrowFileIO</code>, when enabled, it will always try to resolve the location of the bucket (only supported for AWS S3 Buckets). s3.proxy-uri http://my.proxy.com:8080 Configure the proxy server to be used by the FileIO. s3.connect-timeout 60.0 Configure socket connection timeout, in seconds. s3.request-timeout 60.0 Configure socket read timeouts on Windows and macOS, in seconds. s3.force-virtual-addressing False Whether to use virtual addressing of buckets. If true, then virtual addressing is always enabled. If false, then virtual addressing is only enabled if endpoint_override is empty. This can be used for non-AWS backends that only support virtual hosted-style access. s3.retry-strategy-impl None Ability to set a custom S3 retry strategy. A full path to a class needs to be given that extends the S3RetryStrategy base class. s3.anonymous True Configure whether to use anonymous connection. If False (default), uses key/secret if configured or boto's credential resolver."},{"location":"configuration/#hdfs","title":"HDFS","text":"Key Example Description hdfs.host https://10.0.19.25/ Configure the HDFS host to connect to hdfs.port 9000 Configure the HDFS port to connect to. hdfs.user user Configure the HDFS username used for connection. hdfs.kerberos_ticket kerberos_ticket Configure the path to the Kerberos ticket cache."},{"location":"configuration/#azure-data-lake","title":"Azure Data lake","text":"Key Example Description adls.connection-string AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqF...;BlobEndpoint=http://localhost/ A connection string. This could be used to use FileIO with any adls-compatible object storage service that has a different endpoint (like azurite). adls.account-name devstoreaccount1 The account that you want to connect to adls.account-key Eby8vdM02xNOcqF... The key to authentication against the account. adls.sas-token NuHOuuzdQN7VRM%2FOpOeqBlawRCA845IY05h9eu1Yte4%3D The shared access signature adls.tenant-id ad667be4-b811-11ed-afa1-0242ac120002 The tenant-id adls.client-id ad667be4-b811-11ed-afa1-0242ac120002 The client-id adls.client-secret oCA3R6P*ka#oa1Sms2J74z... The client-secret adls.account-host accountname1.blob.core.windows.net The storage account host. See AzureBlobFileSystem for reference adls.blob-storage-authority .blob.core.windows.net The hostname[:port] of the Blob Service. Defaults to <code>.blob.core.windows.net</code>. Useful for connecting to a local emulator, like azurite. See AzureFileSystem for reference adls.dfs-storage-authority .dfs.core.windows.net The hostname[:port] of the Data Lake Gen 2 Service. Defaults to <code>.dfs.core.windows.net</code>. Useful for connecting to a local emulator, like azurite. See AzureFileSystem for reference adls.blob-storage-scheme https Either <code>http</code> or <code>https</code>. Defaults to <code>https</code>. Useful for connecting to a local emulator, like azurite. See AzureFileSystem for reference adls.dfs-storage-scheme https Either <code>http</code> or <code>https</code>. Defaults to <code>https</code>. Useful for connecting to a local emulator, like azurite. See AzureFileSystem for reference adls.token eyJ0eXAiOiJKV1QiLCJhbGci... Static access token for authenticating with ADLS. Used for OAuth2 flows."},{"location":"configuration/#google-cloud-storage","title":"Google Cloud Storage","text":"Key Example Description gcs.project-id my-gcp-project Configure Google Cloud Project for GCS FileIO. gcs.oauth2.token ya29.dr.AfM... String representation of the access token used for temporary access. gcs.oauth2.token-expires-at 1690971805918 Configure expiration for credential generated with an access token. Milliseconds since epoch gcs.access read_only Configure client to have specific access. Must be one of 'read_only', 'read_write', or 'full_control' gcs.consistency md5 Configure the check method when writing files. Must be one of 'none', 'size', or 'md5' gcs.cache-timeout 60 Configure the cache expiration time in seconds for object metadata cache gcs.requester-pays False Configure whether to use requester-pays requests gcs.session-kwargs {} Configure a dict of parameters to pass on to aiohttp.ClientSession; can contain, for example, proxy settings. gcs.service.host http://0.0.0.0:4443 Configure an alternative endpoint for the GCS FileIO to access (format protocol://host:port) If not given, defaults to the value of environment variable \"STORAGE_EMULATOR_HOST\"; if that is not set either, will use the standard Google endpoint. gcs.default-location US Configure the default location where buckets are created, like 'US' or 'EUROPE-WEST3'. gcs.version-aware False Configure whether to support object versioning on the GCS bucket."},{"location":"configuration/#alibaba-cloud-object-storage-service-oss","title":"Alibaba Cloud Object Storage Service (OSS)","text":"<p>PyIceberg uses S3FileSystem class to connect to OSS bucket as the service is compatible with S3 SDK as long as the endpoint is addressed with virtual hosted style.</p> Key Example Description s3.endpoint https://s3.oss-your-bucket-region.aliyuncs.com/ Configure an endpoint of the OSS service for the FileIO to access. Be sure to use S3 compatible endpoint as given in the example. s3.access-key-id admin Configure the static access key id used to access the FileIO. s3.secret-access-key password Configure the static secret access key used to access the FileIO. s3.session-token AQoDYXdzEJr... Configure the static session token used to access the FileIO. s3.force-virtual-addressing True Whether to use virtual addressing of buckets. This is set to <code>True</code> by default as OSS can only be accessed with virtual hosted style address. s3.anonymous True Configure whether to use anonymous connection. If False (default), uses key/secret if configured or standard AWS configuration methods."},{"location":"configuration/#hugging-face","title":"Hugging Face","text":"Key Example Description hf.endpoint https://huggingface.co Configure the endpoint for Hugging Face hf.token hf_xxx The Hugging Face token to access HF Datasets repositories"},{"location":"configuration/#pyarrow","title":"PyArrow","text":"Key Example Description pyarrow.use-large-types-on-read True Use large PyArrow types i.e. large_string, large_binary and large_list field types on table scans. The default value is True."},{"location":"configuration/#location-providers","title":"Location Providers","text":"<p>Apache Iceberg uses the concept of a <code>LocationProvider</code> to manage file paths for a table's data files. In PyIceberg, the <code>LocationProvider</code> module is designed to be pluggable, allowing customization for specific use cases, and to additionally determine metadata file locations. The <code>LocationProvider</code> for a table can be specified through table properties.</p> <p>Both data file and metadata file locations can be customized by configuring the table properties <code>write.data.path</code> and <code>write.metadata.path</code>, respectively.</p> <p>For more granular control, you can override the <code>LocationProvider</code>'s <code>new_data_location</code> and <code>new_metadata_location</code> methods to define custom logic for generating file paths. See <code>Loading a Custom Location Provider</code>.</p> <p>PyIceberg defaults to the <code>SimpleLocationProvider</code> for managing file paths.</p>"},{"location":"configuration/#simple-location-provider","title":"Simple Location Provider","text":"<p>The <code>SimpleLocationProvider</code> provides paths prefixed by <code>{location}/data/</code>, where <code>location</code> comes from the table metadata. This can be overridden by setting <code>write.data.path</code> table configuration.</p> <p>For example, a non-partitioned table might have a data file with location:</p> <pre><code>s3://bucket/ns/table/data/0000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n</code></pre> <p>When the table is partitioned, files under a given partition are grouped into a subdirectory, with that partition key and value as the directory name - this is known as the Hive-style partition path format. For example, a table partitioned over a string column <code>category</code> might have a data file with location:</p> <pre><code>s3://bucket/ns/table/data/category=orders/0000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n</code></pre>"},{"location":"configuration/#object-store-location-provider","title":"Object Store Location Provider","text":"<p>PyIceberg offers the <code>ObjectStoreLocationProvider</code>, and an optional partition-exclusion optimization, designed for tables stored in object storage. For additional context and motivation concerning these configurations, see their documentation for Iceberg's Java implementation.</p> <p>When several files are stored under the same prefix, cloud object stores such as S3 often throttle requests on prefixes, resulting in slowdowns. The <code>ObjectStoreLocationProvider</code> counteracts this by injecting deterministic hashes, in the form of binary directories, into file paths, to distribute files across a larger number of object store prefixes.</p> <p>Paths are prefixed by <code>{location}/data/</code>, where <code>location</code> comes from the table metadata, in a similar manner to the <code>SimpleLocationProvider</code>. This can be overridden by setting <code>write.data.path</code> table configuration.</p> <p>For example, a table partitioned over a string column <code>category</code> might have a data file with location: (note the additional binary directories)</p> <pre><code>s3://bucket/ns/table/data/0101/0110/1001/10110010/category=orders/0000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n</code></pre> <p>The <code>ObjectStoreLocationProvider</code> is enabled for a table by explicitly setting its <code>write.object-storage.enabled</code> table property to <code>True</code>.</p>"},{"location":"configuration/#partition-exclusion","title":"Partition Exclusion","text":"<p>When the <code>ObjectStoreLocationProvider</code> is used, the table property <code>write.object-storage.partitioned-paths</code>, which defaults to <code>True</code>, can be set to <code>False</code> as an additional optimization for object stores. This omits partition keys and values from data file paths entirely to further reduce key size. With it disabled, the same data file above would instead be written to: (note the absence of <code>category=orders</code>)</p> <pre><code>s3://bucket/ns/table/data/1101/0100/1011/00111010-00000-0-5affc076-96a4-48f2-9cd2-d5efbc9f0c94-00001.parquet\n</code></pre>"},{"location":"configuration/#loading-a-custom-location-provider","title":"Loading a Custom Location Provider","text":"<p>Similar to FileIO, a custom <code>LocationProvider</code> may be provided for a table by concretely subclassing the abstract base class <code>LocationProvider</code>.</p> <p>The table property <code>write.py-location-provider.impl</code> should be set to the fully-qualified name of the custom <code>LocationProvider</code> (i.e. <code>mymodule.MyLocationProvider</code>). Recall that a <code>LocationProvider</code> is configured per-table, permitting different location provision for different tables. Note also that Iceberg's Java implementation uses a different table property, <code>write.location-provider.impl</code>, for custom Java implementations.</p> <p>An example, custom <code>LocationProvider</code> implementation is shown below.</p> <pre><code>import uuid\n\nclass UUIDLocationProvider(LocationProvider):\n    def __init__(self, table_location: str, table_properties: Properties):\n        super().__init__(table_location, table_properties)\n\n    def new_data_location(self, data_file_name: str, partition_key: Optional[PartitionKey] = None) -&gt; str:\n        # Can use any custom method to generate a file path given the partitioning information and file name\n        prefix = f\"{self.table_location}/{uuid.uuid4()}\"\n        return f\"{prefix}/{partition_key.to_path()}/{data_file_name}\" if partition_key else f\"{prefix}/{data_file_name}\"\n</code></pre>"},{"location":"configuration/#catalogs","title":"Catalogs","text":"<p>PyIceberg currently has native catalog type support for REST, SQL, Hive, Glue and DynamoDB. Alternatively, you can also directly set the catalog implementation:</p> Key Example Description type rest Type of catalog, one of <code>rest</code>, <code>sql</code>, <code>hive</code>, <code>glue</code>, <code>dymamodb</code>. Default to <code>rest</code> py-catalog-impl mypackage.mymodule.MyCatalog Sets the catalog explicitly to an implementation, and will fail explicitly if it can't be loaded"},{"location":"configuration/#rest-catalog","title":"REST Catalog","text":"<pre><code>catalog:\n  default:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n\n  default-mtls-secured-catalog:\n    uri: https://rest-catalog/ws/\n    ssl:\n      client:\n        cert: /absolute/path/to/client.crt\n        key: /absolute/path/to/client.key\n      cabundle: /absolute/path/to/cabundle.pem\n</code></pre> Key Example Description uri https://rest-catalog/ws URI identifying the REST Server warehouse myWarehouse Warehouse location or identifier to request from the catalog service. May be used to determine server-side overrides, such as the warehouse location. snapshot-loading-mode refs The snapshots to return in the body of the metadata. Setting the value to <code>all</code> would return the full set of snapshots currently valid for the table. Setting the value to <code>refs</code> would load all snapshots referenced by branches or tags. <code>header.X-Iceberg-Access-Delegation</code> <code>vended-credentials</code> Signal to the server that the client supports delegated access via a comma-separated list of access mechanisms. The server may choose to supply access via any or none of the requested mechanisms. When using <code>vended-credentials</code>, the server provides temporary credentials to the client. When using <code>remote-signing</code>, the server signs requests on behalf of the client. (default: <code>vended-credentials</code>)"},{"location":"configuration/#headers-in-rest-catalog","title":"Headers in REST Catalog","text":"<p>To configure custom headers in REST Catalog, include them in the catalog properties with <code>header.&lt;Header-Name&gt;</code>. This ensures that all HTTP requests to the REST service include the specified headers.</p> <pre><code>catalog:\n  default:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n    header.content-type: application/vnd.api+json\n</code></pre>"},{"location":"configuration/#authentication-options","title":"Authentication Options","text":""},{"location":"configuration/#legacy-oauth2","title":"Legacy OAuth2","text":"<p>Legacy OAuth2 Properties will be removed in PyIceberg 1.0 in place of pluggable AuthManager properties below</p> Key Example Description oauth2-server-uri https://auth-service/cc Authentication URL to use for client credentials authentication (default: uri + 'v1/oauth/tokens') token FEW23.DFSDF.FSDF Bearer token value to use for <code>Authorization</code> header credential client_id:client_secret Credential to use for OAuth2 credential flow when initializing the catalog scope openid offline corpds:ds:profile Desired scope of the requested security token (default : catalog) resource rest_catalog.iceberg.com URI for the target resource or service audience rest_catalog Logical name of target resource or service"},{"location":"configuration/#sigv4","title":"SigV4","text":"Key Example Description rest.sigv4-enabled true Sign requests to the REST Server using AWS SigV4 protocol rest.signing-region us-east-1 The region to use when SigV4 signing a request rest.signing-name execute-api The service signing name to use when SigV4 signing a request"},{"location":"configuration/#pluggable-authentication-via-authmanager","title":"Pluggable Authentication via AuthManager","text":"<p>The RESTCatalog supports pluggable authentication via the <code>auth</code> configuration block. This allows you to specify which how the access token will be fetched and managed for use with the HTTP requests to the RESTCatalog server. The authentication method is selected by setting the <code>auth.type</code> property, and additional configuration can be provided as needed for each method.</p>"},{"location":"configuration/#supported-authentication-types","title":"Supported Authentication Types","text":"<ul> <li><code>noop</code>: No authentication (no Authorization header sent).</li> <li><code>basic</code>: HTTP Basic authentication.</li> <li><code>oauth2</code>: OAuth2 client credentials flow.</li> <li><code>custom</code>: Custom authentication manager (requires <code>auth.impl</code>).</li> <li><code>google</code>: Google Authentication support</li> <li><code>entra</code>: Microsoft Entra ID (Azure AD) authentication support</li> </ul>"},{"location":"configuration/#configuration-properties","title":"Configuration Properties","text":"<p>The <code>auth</code> block is structured as follows:</p> <pre><code>catalog:\n  default:\n    type: rest\n    uri: http://rest-catalog/ws/\n    auth:\n      type: &lt;auth_type&gt;\n      &lt;auth_type&gt;:\n        # Type-specific configuration\n      impl: &lt;custom_class_path&gt;  # Only for custom auth\n</code></pre>"},{"location":"configuration/#property-reference","title":"Property Reference","text":"Property Required Description <code>auth.type</code> Yes The authentication type to use (<code>noop</code>, <code>basic</code>, <code>oauth2</code>, or <code>custom</code>). <code>auth.impl</code> Conditionally The fully qualified class path for a custom AuthManager. Required if <code>auth.type</code> is <code>custom</code>. <code>auth.basic</code> If type is <code>basic</code> Block containing <code>username</code> and <code>password</code> for HTTP Basic authentication. <code>auth.oauth2</code> If type is <code>oauth2</code> Block containing OAuth2 configuration (see below). <code>auth.custom</code> If type is <code>custom</code> Block containing configuration for the custom AuthManager. <code>auth.google</code> If type is <code>google</code> Block containing <code>credentials_path</code> to a service account file (if using). Will default to using Application Default Credentials. <code>auth.entra</code> If type is <code>entra</code> Block containing Entra ID configuration. Will default to using DefaultAzureCredential."},{"location":"configuration/#examples","title":"Examples","text":"<p>No Authentication:</p> <pre><code>auth:\n  type: noop\n</code></pre> <p>Basic Authentication:</p> <pre><code>auth:\n  type: basic\n  basic:\n    username: myuser\n    password: mypass\n</code></pre> <p>OAuth2 Authentication:</p> <pre><code>auth:\n  type: oauth2\n  oauth2:\n    client_id: my-client-id\n    client_secret: my-client-secret\n    token_url: https://auth.example.com/oauth/token\n    scope: read\n    refresh_margin: 60         # (optional) seconds before expiry to refresh\n    expires_in: 3600           # (optional) fallback if server does not provide\n</code></pre> <p>Custom Authentication:</p> <pre><code>auth:\n  type: custom\n  impl: mypackage.module.MyAuthManager\n  custom:\n    property1: value1\n    property2: value2\n</code></pre>"},{"location":"configuration/#notes","title":"Notes","text":"<ul> <li>If <code>auth.type</code> is <code>custom</code>, you must specify <code>auth.impl</code> with the full class path to your custom AuthManager.</li> <li>If <code>auth.type</code> is not <code>custom</code>, specifying <code>auth.impl</code> is not allowed.</li> <li>The configuration block under each type (e.g., <code>basic</code>, <code>oauth2</code>, <code>custom</code>) is passed as keyword arguments to the corresponding AuthManager.</li> </ul>"},{"location":"configuration/#common-integrations-examples","title":"Common Integrations &amp; Examples","text":""},{"location":"configuration/#aws-glue","title":"AWS Glue","text":"<pre><code>catalog:\n  s3_tables_catalog:\n    type: rest\n    uri: https://glue.&lt;region&gt;.amazonaws.com/iceberg\n    warehouse: &lt;account-id&gt;:s3tablescatalog/&lt;table-bucket-name&gt;\n    rest.sigv4-enabled: true\n    rest.signing-name: glue\n    rest.signing-region: &lt;region&gt;\n</code></pre>"},{"location":"configuration/#unity-catalog","title":"Unity Catalog","text":"<pre><code>catalog:\n  unity_catalog:\n    type: rest\n    uri: https://&lt;workspace-url&gt;/api/2.1/unity-catalog/iceberg-rest\n    warehouse: &lt;uc-catalog-name&gt;\n    token: &lt;databricks-pat-token&gt;\n</code></pre>"},{"location":"configuration/#r2-data-catalog","title":"R2 Data Catalog","text":"<pre><code>catalog:\n  r2_catalog:\n    type: rest\n    uri: &lt;r2-catalog-uri&gt;\n    warehouse: &lt;r2-warehouse-name&gt;\n    token: &lt;r2-token&gt;\n</code></pre>"},{"location":"configuration/#lakekeeper","title":"Lakekeeper","text":"<pre><code>catalog:\n  lakekeeper_catalog:\n    type: rest\n    uri: &lt;lakekeeper-catalog-uri&gt;\n    warehouse: &lt;lakekeeper-warehouse-name&gt;\n    credential: &lt;client-id&gt;:&lt;client-secret&gt;\n    oauth2-server-uri: http://localhost:30080/realms/&lt;keycloak-realm-name&gt;/protocol/openid-connect/token\n    scope: lakekeeper\n</code></pre>"},{"location":"configuration/#apache-polaris","title":"Apache Polaris","text":"<pre><code>catalog:\n  polaris_catalog:\n    type: rest\n    uri: https://&lt;account&gt;.snowflakecomputing.com/polaris/api/catalog\n    warehouse: &lt;polaris-catalog-name&gt;\n    credential: &lt;client-id&gt;:&lt;client-secret&gt;\n    header.X-Iceberg-Access-Delegation: vended-credentials\n    scope: PRINCIPAL_ROLE:ALL\n    token-refresh-enabled: true\n    py-io-impl: pyiceberg.io.fsspec.FsspecFileIO\n</code></pre>"},{"location":"configuration/#apache-gravitino","title":"Apache Gravitino","text":"<pre><code>catalog:\n  gravitino_catalog:\n    type: rest\n    uri: &lt;gravitino-catalog-uri&gt;\n    header.X-Iceberg-Access-Delegation: vended-credentials\n    auth:\n      type: noop\n</code></pre>"},{"location":"configuration/#gcp-biglake-metastore-catalog-rest","title":"GCP BigLake Metastore Catalog REST","text":"<pre><code>catalog:\n  biglake_catalog:\n    type: rest\n    uri: https://biglake.googleapis.com/iceberg/v1/restcatalog\n    warehouse: gs://&lt;bucket-name&gt;  # Use bq://projects/&lt;gcp-project-id&gt; for federation option (see docs)\n    auth:\n      type: google\n    header.x-goog-user-project: &lt;gcp-project-id&gt;\n    header.X-Iceberg-Access-Delegation: \"\" # For user-credentials authentication, set to empty string.\n</code></pre> <p>Metastore Authentication Models</p> <p>If your BigLake Metastore catalog is configured for \"user credentials\" authentication instead of \"vendor credentials\", set the <code>header.X-Iceberg-Access-Delegation</code> header to an empty string as shown above.  Standard GCP Application Default Credentials (ADC) will be used to authenticate requests to the BigLake Metastore REST API. You can retrieve the configuration details for your BigLake Iceberg catalog at the GCP Console BigLake Metastore page. Select your catalog, then find the necessary parameters such as <code>uri</code>, <code>warehouse</code>, and authentication method (e.g. user-creds or vendor).</p>"},{"location":"configuration/#microsoft-onelake-iceberg-rest-catalog","title":"Microsoft OneLake Iceberg REST Catalog","text":"<p>See OneLake table APIs for Iceberg for detailed documentation.</p> <p>Using Entra ID authentication (recommended):</p> <pre><code>catalog:\n  onelake_catalog:\n    type: rest\n    uri: https://onelake.table.fabric.microsoft.com/iceberg\n    warehouse: &lt;fabric_workspace_id&gt;/&lt;fabric_data_item_id&gt;\n    auth:\n      type: entra\n    adls.account-name: onelake\n    adls.account-host: onelake.blob.fabric.microsoft.com\n</code></pre> <p>Using static token:</p> <pre><code>catalog:\n  onelake_catalog:\n    type: rest\n    uri: https://onelake.table.fabric.microsoft.com/iceberg\n    warehouse: &lt;fabric_workspace_id&gt;/&lt;fabric_data_item_id&gt; # Example : DB0CE1EE-B014-47D3-8F0C-9D64C39C0FC2/F470A1D2-6D6D-4C9D-8796-46286C80B7C0\n    token: &lt;token&gt;\n    adls.account-name: onelake\n    adls.account-host: onelake.blob.fabric.microsoft.com\n    adls.credential: &lt;credential&gt;\n</code></pre> <p>OneLake Authentication</p> <p>Use the <code>entra</code> auth type for Entra ID (Azure AD) authentication via DefaultAzureCredential, which supports environment variables, managed identity, Azure CLI, and more. Install with <code>pip install pyiceberg[entra-auth]</code>.</p>"},{"location":"configuration/#sql-catalog","title":"SQL Catalog","text":"<p>The SQL catalog requires a database for its backend. PyIceberg supports PostgreSQL and SQLite through psycopg2. The database connection has to be configured using the <code>uri</code> property. The init_catalog_tables is optional and defaults to True. If it is set to False, the catalog tables will not be created when the SQLCatalog is initialized. See SQLAlchemy's documentation for URL format:</p> <p>For PostgreSQL:</p> <pre><code>catalog:\n  default:\n    type: sql\n    uri: postgresql+psycopg2://username:password@localhost/mydatabase\n    init_catalog_tables: false\n</code></pre> <p>In the case of SQLite:</p> <p>Development only</p> <p>SQLite is not built for concurrency, you should use this catalog for exploratory or development purposes.</p> <pre><code>catalog:\n  default:\n    type: sql\n    uri: sqlite:////tmp/pyiceberg.db\n    init_catalog_tables: false\n</code></pre> Key Example Default Description uri postgresql+psycopg2://username:password@localhost/mydatabase SQLAlchemy backend URL for the catalog database (see documentation for URL format) echo true false SQLAlchemy engine echo param to log all statements to the default log handler pool_pre_ping true false SQLAlchemy engine pool_pre_ping param to test connections for liveness upon each checkout"},{"location":"configuration/#in-memory-catalog","title":"In Memory Catalog","text":"<p>The in-memory catalog is built on top of <code>SqlCatalog</code> and uses SQLite in-memory database for its backend.</p> <p>It is useful for test, demo, and playground but not in production as it does not support concurrent access.</p> <pre><code>catalog:\n  default:\n    type: in-memory\n    warehouse: /tmp/pyiceberg/warehouse\n</code></pre> Key Example Default Description warehouse /tmp/pyiceberg/warehouse file:///tmp/iceberg/warehouse The directory where the in-memory catalog will store its data files."},{"location":"configuration/#hive-catalog","title":"Hive Catalog","text":"<pre><code>catalog:\n  default:\n    uri: thrift://localhost:9083\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n</code></pre> Key Example Description hive.hive2-compatible true Using Hive 2.x compatibility mode hive.kerberos-authentication true Using authentication via Kerberos hive.kerberos-service-name hive Kerberos service name (default hive) ugi t-1234:secret Hadoop UGI for Hive client. <p>When using Hive 2.x, make sure to set the compatibility flag:</p> <pre><code>catalog:\n  default:\n...\n    hive.hive2-compatible: true\n</code></pre>"},{"location":"configuration/#glue-catalog","title":"Glue Catalog","text":"<p>Your AWS credentials can be passed directly through the Python API. Otherwise, please refer to How to configure AWS credentials to set your AWS account credentials locally.</p> <pre><code>catalog:\n  default:\n    type: glue\n    glue.access-key-id: &lt;ACCESS_KEY_ID&gt;\n    glue.secret-access-key: &lt;SECRET_ACCESS_KEY&gt;\n    glue.session-token: &lt;SESSION_TOKEN&gt;\n    glue.region: &lt;REGION_NAME&gt;\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n</code></pre> <pre><code>catalog:\n  default:\n    type: glue\n    glue.profile-name: &lt;PROFILE_NAME&gt;\n    glue.region: &lt;REGION_NAME&gt;\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n</code></pre> <p>Client-specific Properties</p> <p><code>glue.*</code> properties are for Glue Catalog only. If you want to use the same credentials for both Glue Catalog and S3 FileIO, you can set the <code>client.*</code> properties. See the Unified AWS Credentials section for more details.</p> Key Example Description glue.id 111111111111 Configure the 12-digit ID of the Glue Catalog glue.skip-archive true Configure whether to skip the archival of older table versions. Default to true glue.endpoint https://glue.us-east-1.amazonaws.com Configure an alternative endpoint of the Glue service for GlueCatalog to access glue.profile-name default Configure the AWS profile used to access the Glue Catalog glue.region us-east-1 Set the region of the Glue Catalog glue.access-key-id admin Configure the static access key id used to access the Glue Catalog glue.secret-access-key password Configure the static secret access key used to access the Glue Catalog glue.session-token AQoDYXdzEJr... Configure the static session token used to access the Glue Catalog glue.max-retries 10 Configure the maximum number of retries for the Glue service calls glue.retry-mode standard Configure the retry mode for the Glue service. Default to standard. <p>Removed Properties</p> <p>The properties <code>profile_name</code>, <code>region_name</code>, <code>aws_access_key_id</code>, <code>aws_secret_access_key</code>, and <code>aws_session_token</code> were deprecated and removed in 0.8.0</p>"},{"location":"configuration/#dynamodb-catalog","title":"DynamoDB Catalog","text":"<p>If you want to use AWS DynamoDB as the catalog, you can use the last two ways to configure the pyiceberg and refer How to configure AWS credentials to set your AWS account credentials locally. If you want to use the same credentials for both Dynamodb Catalog and S3 FileIO, you can set the <code>client.*</code> properties.</p> <pre><code>catalog:\n  default:\n    type: dynamodb\n    table-name: iceberg\n</code></pre> <p>If you prefer to pass the credentials explicitly to the client instead of relying on environment variables,</p> <pre><code>catalog:\n  default:\n    type: dynamodb\n    table-name: iceberg\n    dynamodb.access-key-id: &lt;ACCESS_KEY_ID&gt;\n    dynamodb.secret-access-key: &lt;SECRET_ACCESS_KEY&gt;\n    dynamodb.session-token: &lt;SESSION_TOKEN&gt;\n    dynamodb.region: &lt;REGION_NAME&gt;\n    s3.endpoint: http://localhost:9000\n    s3.access-key-id: admin\n    s3.secret-access-key: password\n</code></pre> <p>Client-specific Properties</p> <p><code>dynamodb.*</code> properties are for DynamoDB Catalog only. If you want to use the same credentials for both DynamoDB Catalog and S3 FileIO, you can set the <code>client.*</code> properties. See the Unified AWS Credentials section for more details.</p> Key Example Description dynamodb.profile-name default Configure the static profile used to access the DynamoDB Catalog dynamodb.region us-east-1 Set the region of the DynamoDB Catalog dynamodb.access-key-id admin Configure the static access key id used to access the DynamoDB Catalog dynamodb.secret-access-key password Configure the static secret access key used to access the DynamoDB Catalog dynamodb.session-token AQoDYXdzEJr... Configure the static session token used to access the DynamoDB Catalog <p>Removed Properties</p> <p>The properties <code>profile_name</code>, <code>region_name</code>, <code>aws_access_key_id</code>, <code>aws_secret_access_key</code>, and <code>aws_session_token</code> were deprecated and removed in 0.8.0</p>"},{"location":"configuration/#custom-catalog-implementations","title":"Custom Catalog Implementations","text":"<p>If you want to load any custom catalog implementation, you can set catalog configurations like the following:</p> <pre><code>catalog:\n  default:\n    py-catalog-impl: mypackage.mymodule.MyCatalog\n    custom-key1: value1\n    custom-key2: value2\n</code></pre>"},{"location":"configuration/#unified-aws-credentials","title":"Unified AWS Credentials","text":"<p>You can explicitly set the AWS credentials for both Glue/DynamoDB Catalog and S3 FileIO by configuring <code>client.*</code> properties. For example:</p> <pre><code>catalog:\n  default:\n    type: glue\n    client.access-key-id: &lt;ACCESS_KEY_ID&gt;\n    client.secret-access-key: &lt;SECRET_ACCESS_KEY&gt;\n    client.region: &lt;REGION_NAME&gt;\n</code></pre> <p>configures the AWS credentials for both Glue Catalog and S3 FileIO.</p> Key Example Description client.region us-east-1 Set the region of both the Glue/DynamoDB Catalog and the S3 FileIO client.access-key-id admin Configure the static access key id used to access both the Glue/DynamoDB Catalog and the S3 FileIO client.secret-access-key password Configure the static secret access key used to access both the Glue/DynamoDB Catalog and the S3 FileIO client.session-token AQoDYXdzEJr... Configure the static session token used to access both the Glue/DynamoDB Catalog and the S3 FileIO client.profile-name default Configure the AWS profile used to access both the Glue/DynamoDB Catalog and the S3 FileIO client.role-session-name session An optional identifier for the assumed role session. client.role-arn arn:aws:... AWS Role ARN. If provided instead of access_key and secret_key, temporary credentials will be fetched by assuming this role. <p>Properties Priority</p> <p><code>client.*</code> properties will be overridden by service-specific properties if they are set. For example, if <code>client.region</code> is set to <code>us-west-1</code> and <code>s3.region</code> is set to <code>us-east-1</code>, the S3 FileIO will use <code>us-east-1</code> as the region.</p>"},{"location":"configuration/#concurrency","title":"Concurrency","text":"<p>PyIceberg uses multiple threads to parallelize operations. The number of workers can be configured by supplying a <code>max-workers</code> entry in the configuration file, or by setting the <code>PYICEBERG_MAX_WORKERS</code> environment variable. The default value depends on the system hardware and Python version. See the Python documentation for more details.</p>"},{"location":"configuration/#backward-compatibility","title":"Backward Compatibility","text":"<p>Previous versions of Java (<code>&lt;1.4.0</code>) implementations incorrectly assume the optional attribute <code>current-snapshot-id</code> to be a required attribute in TableMetadata. This means that if <code>current-snapshot-id</code> is missing in the metadata file (e.g. on table creation), the application will throw an exception without being able to load the table. This assumption has been corrected in more recent Iceberg versions. However, it is possible to force PyIceberg to create a table with a metadata file that will be compatible with previous versions. This can be configured by setting the <code>legacy-current-snapshot-id</code> property as \"True\" in the configuration file, or by setting the <code>PYICEBERG_LEGACY_CURRENT_SNAPSHOT_ID</code> environment variable. Refer to the PR discussion for more details on the issue</p>"},{"location":"configuration/#nanoseconds-support","title":"Nanoseconds Support","text":"<p>PyIceberg currently only supports upto microsecond precision in its TimestampType. PyArrow timestamp types in 's' and 'ms' will be upcast automatically to 'us' precision timestamps on write. Timestamps in 'ns' precision can also be downcast automatically on write if desired. This can be configured by setting the <code>downcast-ns-timestamp-to-us-on-write</code> property as \"True\" in the configuration file, or by setting the <code>PYICEBERG_DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE</code> environment variable. Refer to the nanoseconds timestamp proposal document for more details on the long term roadmap for nanoseconds support</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>We welcome contributions to Apache Iceberg! To learn more about contributing to Apache Iceberg, please refer to the official Iceberg contribution guidelines. These guidelines are intended as helpful suggestions to make the contribution process as seamless as possible, and are not strict rules.</p> <p>If you would like to discuss your proposed change before contributing, we encourage you to visit our Community page. There, you will find various ways to connect with the community, including Slack and our mailing lists. Alternatively, you can open a new issue directly in the GitHub repository.</p> <p>For first-time contributors, feel free to check out our good first issues for an easy way to get started.</p>"},{"location":"contributing/#contributing-to-pyiceberg","title":"Contributing to PyIceberg","text":"<p>The PyIceberg Project is hosted on GitHub at https://github.com/apache/iceberg-python.</p> <p>For development, uv is used for dependency management and packaging. uv is a Python package installer and resolver, written in Rust, that serves as a drop-in replacement for pip, and virtualenv.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<p>Install uv and set up the development environment:</p> <pre><code>make install\n</code></pre> <p>This will install uv if needed, create a virtual environment in <code>.venv</code>, and install all dependencies.</p> <p>If you only want to just install uv:</p> <pre><code>make install-uv\n</code></pre>"},{"location":"contributing/#python-version-selection","title":"Python Version Selection","text":"<p>You can specify which Python version to use when creating your virtual environment:</p> <pre><code>PYTHON=3.12 make install # Create environment with Python 3.12\nmake test # Run tests against Python 3.12\n</code></pre> <p>Tip: <code>uv python list</code> shows available interpreters. <code>uv python install 3.12</code> can install one if needed.</p>"},{"location":"contributing/#ide-setup","title":"IDE Setup","text":"<p>After running <code>make install</code>, configure your IDE to use the Python interpreter at <code>.venv/bin/python</code>.</p> <p>To set up IDEA with uv:</p> <ul> <li>Open up the Python project in IntelliJ</li> <li>Make sure that you're on latest main</li> <li>Go to File -&gt; Project Structure (\u2318;)</li> <li>Go to Platform Settings -&gt; SDKs</li> <li>Add Python SDK -&gt; Virtualenv Environment -&gt; Existing environment</li> <li>Point to <code>.venv/bin/python</code></li> </ul> <p>VS Code:</p> <ul> <li>Press Cmd/Ctrl+Shift+P -&gt; \"Python: Select Interpreter\"</li> <li>Choose <code>.venv/bin/python</code></li> </ul>"},{"location":"contributing/#advanced-uv-usage","title":"Advanced uv Usage","text":"<p>For full control over your environment, you can use uv commands directly. See the uv documentation to learn more about:</p> <ul> <li>Managing dependencies with <code>uv add</code> and <code>uv remove</code></li> <li>Python version management with <code>uv python</code></li> <li>Running commands with <code>uv run</code></li> <li>Lock file management with <code>uv.lock</code></li> </ul>"},{"location":"contributing/#lock-file-management","title":"Lock File Management","text":"<p><code>uv.lock</code> is a cross-platform lockfile that contains exact information about the project's dependencies. See the uv.lock documentation for more details.</p> <p>When modifying dependencies in <code>pyproject.toml</code>, regenerate the lock file:</p> <pre><code>make uv-lock\n</code></pre> <p>Separately, to verify that the lock file is up to date without modifying it:</p> <pre><code>make uv-lock-check\n</code></pre>"},{"location":"contributing/#installation-from-source","title":"Installation from source","text":"<p>Clone the repository for local development:</p> <pre><code>git clone https://github.com/apache/iceberg-python.git\ncd iceberg-python\npip3 install -e \".[s3fs,hive]\"\n</code></pre> <p>Install it directly for GitHub (not recommended), but sometimes handy:</p> <pre><code>pip install \"git+https://github.com/apache/iceberg-python.git#egg=pyiceberg[pyarrow]\"\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<p><code>prek</code> is used for autoformatting and linting:</p> <pre><code>make lint\n</code></pre> <p><code>prek</code> will automatically fix the violations such as import orders, formatting etc. Pylint errors you need to fix yourself.</p> <p>In addition to manually running <code>make lint</code>, you can install the pre-commit hooks in your local repo with <code>prek install</code>. By doing this, linting is run automatically every time you make a commit.</p> <p>You can bump the integrations to the latest version using <code>prek auto-update</code>. This will check if there is a newer version of <code>{ruff,mypy,...}</code> and update the yaml.</p>"},{"location":"contributing/#cleaning","title":"Cleaning","text":"<p>Removal of old cached files generated during the Cython build process:</p> <pre><code>make clean\n</code></pre> <p>Helps prevent build failures and unexpected behavior by removing outdated files, ensuring that only up-to-date sources are used &amp; the build environment is always clean.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>For Python, <code>pytest</code> is used a testing framework in combination with <code>coverage</code> to enforce 90%+ code coverage.</p> <pre><code>make test\n</code></pre> <p>By default, S3 and ADLS tests are ignored because that require minio and azurite to be running. To run the S3 suite:</p> <pre><code>make test-s3\n</code></pre> <p>To run the ADLS suite:</p> <pre><code>make test-adls\n</code></pre> <p>To pass additional arguments to pytest, you can use <code>PYTEST_ARGS</code>.</p>"},{"location":"contributing/#run-pytest-in-verbose-mode","title":"Run pytest in verbose mode","text":"<pre><code>make test PYTEST_ARGS=\"-v\"\n</code></pre>"},{"location":"contributing/#run-pytest-with-pdb-enabled","title":"Run pytest with pdb enabled","text":"<pre><code>make test PYTEST_ARGS=\"--pdb\"\n</code></pre> <p>To see all available pytest arguments, run <code>make test PYTEST_ARGS=\"--help\"</code>.</p>"},{"location":"contributing/#integration-tests","title":"Integration tests","text":"<p>PyIceberg has integration tests with Apache Spark. Spark will create a new database and provision some tables that PyIceberg can query against.</p> <pre><code>make test-integration\n</code></pre> <p>This will restart the containers, to get to a clean state, and then run the PyTest suite. In case something changed in the Dockerfile or the provision script, you can run:</p> <pre><code>make test-integration-rebuild\n</code></pre> <p>To rebuild the containers from scratch.</p>"},{"location":"contributing/#running-integration-tests-against-rest-catalogs","title":"Running Integration Tests against REST Catalogs","text":"<p>Do not run against production catalogs</p> <p>The integration tests will delete data throughout the entirety of your catalog. Running these integration tests against production catalogs will result in data loss.</p> <p>PyIceberg supports the ability to run our catalog tests against an arbitrary REST Catalog.</p> <p>In order to run the test catalog, you will need to specify which REST catalog to run against with the <code>PYICEBERG_TEST_CATALOG</code> environment variable</p> <pre><code>export PYICEBERG_TEST_CATALOG=test_catalog\n</code></pre> <p>The catalog in question can be configured either through the ~/.pyiceberg.yaml file or through environment variables.</p> <pre><code>catalog:\n  test_catalog:\n    uri: http://rest-catalog/ws/\n    credential: t-1234:secret\n</code></pre> <pre><code>export PYICEBERG_CATALOG__TEST_CATALOG__URI=thrift://localhost:9083\nexport PYICEBERG_CATALOG__TEST_CATALOG__ACCESS_KEY_ID=username\nexport PYICEBERG_CATALOG__TEST_CATALOG__SECRET_ACCESS_KEY=password\n</code></pre>"},{"location":"contributing/#notebooks-for-experimentation","title":"Notebooks for Experimentation","text":"<p>PyIceberg provides Jupyter notebooks for quick experimentation and learning. Two Make commands are available depending on your needs:</p>"},{"location":"contributing/#pyiceberg-examples-make-notebook","title":"PyIceberg Examples (<code>make notebook</code>)","text":"<p>For basic PyIceberg experimentation without additional infrastructure:</p> <pre><code>make notebook\n</code></pre> <p>This will install notebook dependencies and launch Jupyter Lab in the <code>notebooks/</code> directory.</p> <p>PyIceberg Example Notebook (<code>notebooks/pyiceberg_example.ipynb</code>) is based on the Getting Started with PyIceberg page. It demonstrates basic PyIceberg operations like creating catalogs, schemas, and querying tables without requiring any external services.</p>"},{"location":"contributing/#spark-integration-examples-make-notebook-infra","title":"Spark Integration Examples (<code>make notebook-infra</code>)","text":"<p>For working with PyIceberg alongside Spark, use the infrastructure-enabled notebook environment:</p> <pre><code>make notebook-infra\n</code></pre> <p>This command spins up the full integration test infrastructure via Docker Compose, including:</p> <ul> <li>Spark (with Spark Connect)</li> <li>Iceberg REST Catalog (using the <code>apache/iceberg-rest-fixture</code> image)</li> <li>Hive Metastore</li> <li>S3-compatible object storage (Minio)</li> </ul> <p>Spark Example Notebook (<code>notebooks/spark_integration_example.ipynb</code>) is based on the Spark Getting Started guide. This notebook demonstrates how to work with PyIceberg alongside Spark, leveraging the Docker-based testing setup for a complete local development environment.</p> <p>After running <code>make notebook-infra</code>, open <code>spark_integration_example.ipynb</code> in the Jupyter Lab interface to explore Spark integration capabilities.</p>"},{"location":"contributing/#code-standards","title":"Code standards","text":"<p>Below are the formalized conventions that we adhere to in the PyIceberg project. The goal of this is to have a common agreement on how to evolve the codebase, but also using it as guidelines for newcomers to the project.</p>"},{"location":"contributing/#api-compatibility","title":"API Compatibility","text":"<p>It is important to keep the Python public API compatible across versions. The Python official PEP-8 defines public methods as: Public attributes should have no leading underscores. This means not removing any methods without any notice, or removing or renaming any existing parameters. Adding new optional parameters is okay.</p> <p>If you want to remove a method, please add a deprecation notice by annotating the function using <code>@deprecated</code>:</p> <pre><code>from pyiceberg.utils.deprecated import deprecated\n\n\n@deprecated(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"Please use load_something_else() instead\",\n)\ndef load_something():\n    pass\n</code></pre> <p>Which will warn:</p> <pre><code>Call to load_something, deprecated in 0.1.0, will be removed in 0.2.0. Please use load_something_else() instead.\n</code></pre> <p>If you want to remove a property or notify about a behavior change, please add a deprecation notice by calling the deprecation_message function:</p> <pre><code>from pyiceberg.utils.deprecated import deprecation_message\n\ndeprecation_message(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"The old_property is deprecated. Please use the something_else property instead.\",\n)\n</code></pre> <p>Which will warn:</p> <pre><code>Deprecated in 0.1.0, will be removed in 0.2.0. The old_property is deprecated. Please use the something_else property instead.\n</code></pre>"},{"location":"contributing/#logging","title":"Logging","text":"<p>PyIceberg uses Python's standard logging module. You can control the logging level using either:</p> <p>CLI option:</p> <pre><code>pyiceberg --log-level DEBUG describe my_table\n</code></pre> <p>Environment variable:</p> <pre><code>export PYICEBERG_LOG_LEVEL=DEBUG\npyiceberg describe my_table\n</code></pre> <p>Valid log levels are: <code>DEBUG</code>, <code>INFO</code>, <code>WARNING</code> (default), <code>ERROR</code>, <code>CRITICAL</code>.</p> <p>Debug logging is particularly useful for troubleshooting issues with FileIO implementations, catalog connections, and other integration points.</p>"},{"location":"contributing/#type-annotations","title":"Type annotations","text":"<p>For the type annotation the types from the <code>Typing</code> package are used.</p>"},{"location":"contributing/#third-party-libraries","title":"Third party libraries","text":"<p>PyIceberg naturally integrates into the rich Python ecosystem, however it is important to be hesitant adding third party packages. Adding a lot of packages makes the library heavyweight, and causes incompatibilities with other projects if they use a different version of the library. Also, big libraries such as <code>s3fs</code>, <code>adlfs</code>, <code>pyarrow</code>, <code>thrift</code> should be optional to avoid downloading everything, while not being sure if is actually being used.</p>"},{"location":"expression-dsl/","title":"Expression DSL","text":""},{"location":"expression-dsl/#expression-dsl","title":"Expression DSL","text":"<p>The PyIceberg library provides a powerful expression Domain Specific Language (DSL) for building complex row filter expressions. This guide will help you understand how to use the expression DSL effectively. This DSL allows you to build type-safe expressions for use in the <code>row_filter</code> scan argument.</p> <p>They are composed of terms, predicates, and logical operators.</p>"},{"location":"expression-dsl/#basic-concepts","title":"Basic Concepts","text":""},{"location":"expression-dsl/#terms","title":"Terms","text":"<p>Terms are the basic building blocks of expressions. They represent references to fields in your data:</p> <pre><code>from pyiceberg.expressions import Reference\n\n# Create a reference to a field named \"age\"\nage_field = Reference(\"age\")\n</code></pre>"},{"location":"expression-dsl/#predicates","title":"Predicates","text":"<p>Predicates are expressions that evaluate to a boolean value. They can be combined using logical operators.</p>"},{"location":"expression-dsl/#literal-predicates","title":"Literal Predicates","text":"<pre><code>from pyiceberg.expressions import EqualTo, NotEqualTo, LessThan, LessThanOrEqual, GreaterThan, GreaterThanOrEqual\n\n# age equals 18\nage_equals_18 = EqualTo(\"age\", 18)\n\n# age is not equal to 18\nage_not_equals_18 = NotEqualTo(\"age\", 18)\n\n# age is less than 18\nage_less_than_18 = LessThan(\"age\", 18)\n\n# Less than or equal to\nage_less_than_or_equal_18 = LessThanOrEqual(\"age\", 18)\n\n# Greater than\nage_greater_than_18 = GreaterThan(\"age\", 18)\n\n# Greater than or equal to\nage_greater_than_or_equal_18 = GreaterThanOrEqual(\"age\", 18)\n</code></pre>"},{"location":"expression-dsl/#set-predicates","title":"Set Predicates","text":"<pre><code>from pyiceberg.expressions import In, NotIn\n\n# age is one of 18, 19, 20\nage_in_set = In(\"age\", [18, 19, 20])\n\n# age is not 18, 19, oer 20\nage_not_in_set = NotIn(\"age\", [18, 19, 20])\n</code></pre>"},{"location":"expression-dsl/#unary-predicates","title":"Unary Predicates","text":"<pre><code>from pyiceberg.expressions import IsNull, NotNull\n\n# Is null\nname_is_null = IsNull(\"name\")\n\n# Is not null\nname_is_not_null = NotNull(\"name\")\n</code></pre>"},{"location":"expression-dsl/#string-predicates","title":"String Predicates","text":"<pre><code>from pyiceberg.expressions import StartsWith, NotStartsWith\n\n# TRUE for 'Johnathan', FALSE for 'Johan'\nname_starts_with = StartsWith(\"name\", \"John\")\n\n# FALSE for 'Johnathan', TRUE for 'Johan'\nname_not_starts_with = NotStartsWith(\"name\", \"John\")\n</code></pre>"},{"location":"expression-dsl/#logical-operators","title":"Logical Operators","text":"<p>You can combine predicates using logical operators:</p> <pre><code>from pyiceberg.expressions import And, Or, Not\n\n# TRUE for 25, FALSE for 67 and 15\nage_between = And(\n    GreaterThanOrEqual(\"age\", 18),\n    LessThanOrEqual(\"age\", 65)\n)\n\n# FALSE for 25, TRUE for 67 and 15\nage_outside = Or(\n    LessThan(\"age\", 18),\n    GreaterThan(\"age\", 65)\n)\n\n# NOT operator\nnot_adult = Not(GreaterThanOrEqual(\"age\", 18))\n</code></pre>"},{"location":"expression-dsl/#advanced-usage","title":"Advanced Usage","text":""},{"location":"expression-dsl/#complex-expressions","title":"Complex Expressions","text":"<p>You can build complex expressions by combining multiple predicates and operators:</p> <pre><code>from pyiceberg.expressions import And, Or, Not, EqualTo, GreaterThan, LessThan, In\n\n# (age &gt;= 18 AND age &lt;= 65) AND (status = 'active' OR status = 'pending')\ncomplex_filter = And(\n    And(\n        GreaterThanOrEqual(\"age\", 18),\n        LessThanOrEqual(\"age\", 65)\n    ),\n    Or(\n        EqualTo(\"status\", \"active\"),\n        EqualTo(\"status\", \"pending\")\n    )\n)\n\n# NOT (age &lt; 18 OR age &gt; 65)\nage_in_range = Not(\n    Or(\n        LessThan(\"age\", 18),\n        GreaterThan(\"age\", 65)\n    )\n)\n</code></pre>"},{"location":"expression-dsl/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use Type Hints: Always use type hints when working with expressions to catch type-related errors early.</p> </li> <li> <p>Break Down Complex Expressions: For complex expressions, break them down into smaller, more manageable parts:</p> </li> </ol> <pre><code># Instead of this:\ncomplex_filter = And(\n    And(\n        GreaterThanOrEqual(\"age\", 18),\n        LessThanOrEqual(\"age\", 65)\n    ),\n    Or(\n        EqualTo(\"status\", \"active\"),\n        EqualTo(\"status\", \"pending\")\n    )\n)\n\n# Do this:\nage_range = And(\n    GreaterThanOrEqual(\"age\", 18),\n    LessThanOrEqual(\"age\", 65)\n)\n\nstatus_filter = Or(\n    EqualTo(\"status\", \"active\"),\n    EqualTo(\"status\", \"pending\")\n)\n\ncomplex_filter = And(age_range, status_filter)\n</code></pre>"},{"location":"expression-dsl/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>Null Handling: Be careful when using <code>IsNull</code> and <code>NotNull</code> predicates with required fields. The expression DSL will automatically optimize these cases:</li> <li><code>IsNull</code> (and <code>IsNaN</code> for doubles/floats) on a required field will always return <code>False</code></li> <li> <p><code>NotNull</code> (and <code>NotNaN</code> for doubles/floats) on a required field will always return <code>True</code></p> </li> <li> <p>String Comparisons: When using string predicates like <code>StartsWith</code>, ensure that the field type is a string type.</p> </li> </ol>"},{"location":"expression-dsl/#examples","title":"Examples","text":"<p>Here are some practical examples of using the expression DSL:</p>"},{"location":"expression-dsl/#basic-filtering","title":"Basic Filtering","text":"<pre><code>from datetime import datetime\nfrom pyiceberg.expressions import (\n    And,\n    EqualTo,\n    GreaterThanOrEqual,\n    LessThanOrEqual,\n    GreaterThan,\n    In\n)\n\nactive_adult_users_filter = And(\n    EqualTo(\"status\", \"active\"),\n    GreaterThanOrEqual(\"age\", 18)\n)\n\n\nhigh_value_customers = And(\n    GreaterThan(\"total_spent\", 1000),\n    In(\"membership_level\", [\"gold\", \"platinum\"])\n)\n\ndate_range_filter = And(\n    GreaterThanOrEqual(\"created_at\", datetime(2024, 1, 1)),\n    LessThanOrEqual(\"created_at\", datetime(2024, 12, 31))\n)\n</code></pre>"},{"location":"expression-dsl/#multi-condition-filter","title":"Multi-Condition Filter","text":"<pre><code>from pyiceberg.expressions import And, Or, Not, EqualTo, GreaterThan\n\ncomplex_filter = And(\n    Not(EqualTo(\"status\", \"deleted\")),\n    Or(\n        And(\n            EqualTo(\"type\", \"premium\"),\n            GreaterThan(\"subscription_months\", 12)\n        ),\n        EqualTo(\"type\", \"enterprise\")\n    )\n)\n</code></pre>"},{"location":"how-to-release/","title":"How to release","text":""},{"location":"how-to-release/#how-to-release","title":"How to Release","text":"<p>This guide outlines the process for releasing PyIceberg in accordance with the Apache Release Process. The steps include:</p> <ol> <li>Preparing for a release</li> <li>Publishing a Release Candidate (RC)</li> <li>Community Voting and Validation</li> <li>Publishing the Final Release (if the vote passes)</li> <li>Post-Release Step</li> </ol>"},{"location":"how-to-release/#requirements","title":"Requirements","text":"<ul> <li>A GPG key must be registered and published in the Apache Iceberg KEYS file. Follow the instructions for setting up a GPG key and uploading it to the KEYS file.<ul> <li>Permission to update the <code>KEYS</code> artifact in the Apache release distribution (requires Iceberg PMC privileges).</li> </ul> </li> <li>SVN Access<ul> <li>Permission to upload artifacts to the Apache development distribution (requires Apache Committer access).</li> <li>Permission to upload artifacts to the Apache release distribution (requires Apache PMC access).</li> </ul> </li> <li>PyPI Access<ul> <li>The <code>twine</code> package must be installed for uploading releases to PyPi.</li> <li>A PyPI account with publishing permissions for the pyiceberg project.</li> </ul> </li> </ul>"},{"location":"how-to-release/#preparing-for-a-release","title":"Preparing for a Release","text":""},{"location":"how-to-release/#remove-deprecated-apis","title":"Remove Deprecated APIs","text":"<p>Before running the release candidate, we want to remove any APIs that were marked for removal under the <code>@deprecated</code> tag for this release. See #1269.</p> <p>For example, the API with the following deprecation tag should be removed when preparing for the 0.2.0 release.</p> <pre><code>@deprecated(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"Please use load_something_else() instead\",\n)\n</code></pre> <p>We also have the <code>deprecation_message</code> function. We need to change the behavior according to what is noted in the message of that deprecation.</p> <pre><code>deprecation_message(\n    deprecated_in=\"0.1.0\",\n    removed_in=\"0.2.0\",\n    help_message=\"The old_property is deprecated. Please use the something_else property instead.\",\n)\n</code></pre>"},{"location":"how-to-release/#update-library-version","title":"Update Library Version","text":"<p>Update the version in <code>pyproject.toml</code> and <code>pyiceberg/__init__.py</code> to match the release version. See #1276.</p>"},{"location":"how-to-release/#publishing-a-release-candidate-rc","title":"Publishing a Release Candidate (RC)","text":""},{"location":"how-to-release/#release-types","title":"Release Types","text":""},{"location":"how-to-release/#majorminor-release","title":"Major/Minor Release","text":"<ul> <li>Use the <code>main</code> branch for the release.</li> <li>Includes new features, enhancements, and any necessary backward-compatible changes.</li> <li>Examples: <code>0.8.0</code>, <code>0.9.0</code>, <code>1.0.0</code>.</li> </ul>"},{"location":"how-to-release/#patch-release","title":"Patch Release","text":"<ul> <li>Use the branch corresponding to the patch version, such as <code>pyiceberg-0.8.x</code>.</li> <li>Focuses on critical bug fixes or security patches that maintain backward compatibility.</li> <li>Examples: <code>0.8.1</code>, <code>0.8.2</code>.</li> </ul> <p>To create a patch branch from the latest release tag:</p> <pre><code># Fetch all tags\ngit fetch --tags\n\n# Assuming 0.8.0 is the latest release tag\ngit checkout -b pyiceberg-0.8.x pyiceberg-0.8.0\n\n# Cherry-pick commits for the upcoming patch release\ngit cherry-pick &lt;commit&gt;\n</code></pre>"},{"location":"how-to-release/#create-tag","title":"Create Tag","text":"<p>Ensure you are on the correct branch:</p> <ul> <li>For a major/minor release, use the <code>main</code> branch</li> <li>For a patch release, use the branch corresponding to the patch version, i.e. <code>pyiceberg-0.6.x</code>.</li> </ul> <p>Create a signed tag:</p> <p>Replace <code>VERSION</code> and <code>RC</code> with the appropriate values for the release.</p> <pre><code>export VERSION=0.7.0\nexport RC=1\n\nexport VERSION_WITH_RC=${VERSION}rc${RC}\nexport GIT_TAG=pyiceberg-${VERSION_WITH_RC}\n\ngit tag -s ${GIT_TAG} -m \"PyIceberg ${VERSION_WITH_RC}\"\ngit push git@github.com:apache/iceberg-python.git ${GIT_TAG}\n</code></pre>"},{"location":"how-to-release/#create-artifacts","title":"Create Artifacts","text":"<p>The <code>Python Build Release Candidate</code> Github Action will run automatically upon tag push.</p> <p>This action will generate artifacts that will include both source distribution (<code>sdist</code>) and binary distributions (<code>wheels</code> using <code>cibuildwheel</code>) for each architectures.</p> <p>This action will generate two final artifacts:</p> <ul> <li><code>svn-release-candidate-${VERSION}rc${RC}</code> for SVN</li> <li><code>pypi-release-candidate-${VERSION}rc${RC}</code> for PyPi</li> </ul> <p>If <code>gh</code> is available, watch the GitHub Action progress using:</p> <pre><code>RUN_ID=$(gh run list --repo apache/iceberg-python --workflow \"Python Build Release Candidate\" --branch \"${GIT_TAG}\" --event push --json databaseId -q '.[0].databaseId')\necho \"Waiting for workflow to complete, this will take several minutes...\"\ngh run watch $RUN_ID --repo apache/iceberg-python\n</code></pre> <p>and download the artifacts using:</p> <pre><code>gh run download $RUN_ID --repo apache/iceberg-python\n</code></pre>"},{"location":"how-to-release/#publish-release-candidate-rc","title":"Publish Release Candidate (RC)","text":""},{"location":"how-to-release/#upload-to-apache-dev-svn","title":"Upload to Apache Dev SVN","text":""},{"location":"how-to-release/#download-artifacts-sign-and-generate-checksums","title":"Download Artifacts, Sign, and Generate Checksums","text":"<p>Download the SVN artifact from the GitHub Action and unzip it.</p> <p>Navigate to the artifact directory. Generate signature and checksum files:</p> <ul> <li><code>.asc</code> files: GPG-signed versions of each artifact to ensure authenticity.</li> <li><code>.sha512</code> files: SHA-512 checksums for verifying file integrity.</li> </ul> <pre><code>(\n    cd svn-release-candidate-${VERSION}rc${RC}\n\n    for name in $(ls pyiceberg-*.whl pyiceberg-*.tar.gz)\n    do\n        gpg --yes --armor --output \"${name}.asc\" --detach-sig \"${name}\"\n        shasum -a 512 \"${name}\" &gt; \"${name}.sha512\"\n    done\n)\n</code></pre> <p>The parentheses <code>()</code> create a subshell. Any changes to the directory (<code>cd</code>) are limited to this subshell, so the current directory in the parent shell remains unchanged.</p>"},{"location":"how-to-release/#upload-artifacts-to-apache-dev-svn","title":"Upload Artifacts to Apache Dev SVN","text":"<p>Now, upload the files from the same directory:</p> <pre><code>export SVN_TMP_DIR=/tmp/iceberg-${VERSION}/\nsvn checkout https://dist.apache.org/repos/dist/dev/iceberg $SVN_TMP_DIR\n\nexport SVN_TMP_DIR_VERSIONED=${SVN_TMP_DIR}pyiceberg-$VERSION_WITH_RC/\nmkdir -p $SVN_TMP_DIR_VERSIONED\ncp svn-release-candidate-${VERSION}rc${RC}/* $SVN_TMP_DIR_VERSIONED\nsvn add $SVN_TMP_DIR_VERSIONED\nsvn ci -m \"PyIceberg ${VERSION_WITH_RC}\" ${SVN_TMP_DIR_VERSIONED}\n</code></pre> <p>Verify the artifact is uploaded to https://dist.apache.org/repos/dist/dev/iceberg.</p>"},{"location":"how-to-release/#remove-old-artifacts-from-apache-dev-svn","title":"Remove Old Artifacts From Apache Dev SVN","text":"<p>Clean up old RC artifacts:</p> <pre><code>svn delete https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-&lt;OLD_RC_VERSION&gt; -m \"Remove old RC artifacts\"\n</code></pre>"},{"location":"how-to-release/#upload-to-pypi","title":"Upload to PyPi","text":""},{"location":"how-to-release/#download-artifacts","title":"Download Artifacts","text":"<p>Download the PyPi artifact from the GitHub Action and unzip it.</p>"},{"location":"how-to-release/#upload-artifacts-to-pypi","title":"Upload Artifacts to PyPi","text":"<p>Update the artifact directory to PyPi using <code>twine</code>. This won't bump the version for everyone that hasn't pinned their version, since it is set to an RC pre-release and those are ignored.</p> <p>Note</p> <p><code>twine</code> might require an PyPi API token.</p> <pre><code>twine upload pypi-release-candidate-${VERSION}rc${RC}/*\n</code></pre> <p>Verify the artifact is uploaded to PyPi.</p>"},{"location":"how-to-release/#vote","title":"Vote","text":""},{"location":"how-to-release/#generate-vote-email","title":"Generate Vote Email","text":"<p>Final step is to generate the email to the dev mail list:</p> <pre><code>export GIT_TAG_REF=$(git show-ref ${GIT_TAG})\nexport GIT_TAG_HASH=${GIT_TAG_REF:0:40}\nexport LAST_COMMIT_ID=$(git rev-list ${GIT_TAG} 2&gt; /dev/null | head -n 1)\n\ncat &lt;&lt; EOF &gt; release-announcement-email.txt\nTo: dev@iceberg.apache.org\nSubject: [VOTE] PyIceberg $VERSION_WITH_RC\nHi Everyone,\n\nI propose that we release the following RC as the official PyIceberg $VERSION release.\n\nA summary of the high level features:\n\n* &lt;Add summary by hand&gt;\n\nThe commit ID is $LAST_COMMIT_ID\n\n* This corresponds to the tag: $GIT_TAG ($GIT_TAG_HASH)\n* https://github.com/apache/iceberg-python/releases/tag/$GIT_TAG\n* https://github.com/apache/iceberg-python/tree/$LAST_COMMIT_ID\n\nThe release tarball, signature, and checksums are here:\n\n* https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-$VERSION_WITH_RC/\n\nYou can find the KEYS file here:\n\n* https://downloads.apache.org/iceberg/KEYS\n\nConvenience binary artifacts are staged on pypi:\n\nhttps://pypi.org/project/pyiceberg/$VERSION_WITH_RC/\n\nAnd can be installed using: pip3 install pyiceberg==$VERSION_WITH_RC\n\nInstructions for verifying a release can be found here:\n\n* https://py.iceberg.apache.org/verify-release/\n\nPlease download, verify, and test.\n\nPlease vote in the next 72 hours.\n[ ] +1 Release this as PyIceberg $VERSION\n[ ] +0\n[ ] -1 Do not release this because...\nEOF\n</code></pre>"},{"location":"how-to-release/#send-vote-email","title":"Send Vote Email","text":"<p>Verify the content of <code>release-announcement-email.txt</code> and send it to <code>dev@iceberg.apache.org</code> with the corresponding subject line.</p>"},{"location":"how-to-release/#vote-has-failed","title":"Vote has failed","text":"<p>If there are concerns with the RC, address the issues and generate another RC.</p>"},{"location":"how-to-release/#publish-the-final-release-vote-has-passed","title":"Publish the Final Release (Vote has passed)","text":"<p>A minimum of 3 binding +1 votes is required to pass an RC. Once the vote has been passed, you can close the vote thread by concluding it:</p> <pre><code>Thanks everyone for voting! The 72 hours have passed, and a minimum of 3 binding votes have been cast:\n\n+1 Foo Bar (non-binding)\n...\n+1 Fokko Driesprong (binding)\n\nThe release candidate has been accepted as PyIceberg &lt;VERSION&gt;. Thanks everyone, when all artifacts are published the announcement will be sent out.\n\nKind regards,\n</code></pre>"},{"location":"how-to-release/#upload-the-accepted-rc-to-apache-release-svn","title":"Upload the accepted RC to Apache Release SVN","text":"<p>Note</p> <p>Only a PMC member has the permission to upload an artifact to the SVN release dist.</p> <pre><code>export SVN_DEV_DIR_VERSIONED=\"https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-${VERSION_WITH_RC}\"\nexport SVN_RELEASE_DIR_VERSIONED=\"https://dist.apache.org/repos/dist/release/iceberg/pyiceberg-${VERSION}\"\n\nsvn mv ${SVN_DEV_DIR_VERSIONED} ${SVN_RELEASE_DIR_VERSIONED} -m \"PyIceberg: Add release ${VERSION}\"\n</code></pre> <p>Verify the artifact is uploaded to https://dist.apache.org/repos/dist/release/iceberg.</p>"},{"location":"how-to-release/#remove-old-artifacts-from-apache-release-svn","title":"Remove Old Artifacts From Apache Release SVN","text":"<p>We only want to host the latest release. Clean up old release artifacts:</p> <pre><code>svn delete https://dist.apache.org/repos/dist/release/iceberg/pyiceberg-&lt;OLD_RELEASE_VERSION&gt; -m \"Remove old release artifacts\"\n</code></pre>"},{"location":"how-to-release/#upload-the-accepted-release-to-pypi","title":"Upload the accepted release to PyPi","text":"<p>The latest version can be pushed to PyPi. Check out the Apache SVN and make sure to publish the right version with <code>twine</code>:</p> <p>Note</p> <p><code>twine</code> might require an PyPi API token.</p> <pre><code>svn checkout https://dist.apache.org/repos/dist/release/iceberg /tmp/iceberg-dist-release/\ncd /tmp/iceberg-dist-release/pyiceberg-${VERSION}\ntwine upload pyiceberg-*.whl pyiceberg-*.tar.gz\n</code></pre> <p>Verify the artifact is uploaded to PyPi.</p>"},{"location":"how-to-release/#post-release","title":"Post Release","text":""},{"location":"how-to-release/#send-out-release-announcement-email","title":"Send out Release Announcement Email","text":"<p>Send out an announcement on the dev mail list:</p> <pre><code>To: dev@iceberg.apache.org\nSubject: [ANNOUNCE] PyIceberg &lt;VERSION&gt;\n\nI'm pleased to announce the release of PyIceberg &lt;VERSION&gt;!\n\nApache Iceberg is an open table format for huge analytic datasets. Iceberg\ndelivers high query performance for tables with tens of petabytes of data,\nalong with atomic commits, concurrent writes, and SQL-compatible table\nevolution.\n\nThis Python release can be downloaded from: https://pypi.org/project/pyiceberg/&lt;VERSION&gt;/\n\nThanks to everyone for contributing!\n</code></pre>"},{"location":"how-to-release/#create-a-github-release-note","title":"Create a Github Release Note","text":"<p>Create a new Release Note on the iceberg-python Github repository.</p> <p>Input the tag in Choose a tag with the newly approved released version (e.g. <code>0.7.0</code>) and set it to Create new tag on publish. Pick the target commit version as the commit ID the release was approved on. For example: </p> <p>Then, select the previous release version as the Previous tag to use the diff between the two versions in generating the release notes.</p> <p>Generate release notes.</p> <p>Set as the latest release and Publish.</p> <p>Make sure to check the <code>changelog</code> label on GitHub to see if anything needs to be highlighted.</p>"},{"location":"how-to-release/#release-the-docs","title":"Release the docs","text":"<p>Run the <code>Release Docs</code> Github Action.</p>"},{"location":"how-to-release/#update-the-github-template","title":"Update the Github template","text":"<p>Make sure to create a PR to update the GitHub issues template with the latest version.</p>"},{"location":"how-to-release/#misc","title":"Misc","text":""},{"location":"how-to-release/#set-up-gpg-key-and-upload-to-apache-iceberg-keys-file","title":"Set up GPG key and Upload to Apache Iceberg KEYS file","text":"<p>To set up GPG key locally, see the instructions.</p> <p>To install gpg on a M1 based Mac, a couple of additional steps are required: https://gist.github.com/phortuin/cf24b1cca3258720c71ad42977e1ba57.</p> <p>Then, published GPG key to the Apache Iceberg KEYS file:</p> <pre><code>svn co https://dist.apache.org/repos/dist/release/iceberg icebergsvn\ncd icebergsvn\necho \"\" &gt;&gt; KEYS # append a newline\ngpg --list-sigs &lt;YOUR KEY ID HERE&gt; &gt;&gt; KEYS # append signatures\ngpg --armor --export &lt;YOUR KEY ID HERE&gt; &gt;&gt; KEYS # append public key block\nsvn commit -m \"add key for &lt;YOUR NAME HERE&gt;\" # this requires Iceberg PMC privileges\n</code></pre> <p>Note</p> <p>Updating the <code>KEYS</code> artifact in the <code>release/</code> distribution requires Iceberg PMC privileges. Please work with a PMC member to update the file.</p>"},{"location":"nightly-build/","title":"Nightly Build","text":""},{"location":"nightly-build/#nightly-build","title":"Nightly Build","text":"<p>A nightly build of PyIceberg is available on testpypi, https://test.pypi.org/project/pyiceberg/.</p> <p>To install the nightly build,</p> <pre><code>pip install -i https://test.pypi.org/simple/ --pre pyiceberg\n</code></pre> <p>For Testing Purposes Only</p> <p>Nightly builds are for testing purposes only and have not been validated. Please use at your own risk, as they may contain untested changes, potential bugs, or incomplete features. Additionally, ensure compliance with any applicable licenses, as these builds may include changes that have not been reviewed for legal or licensing implications.</p>"},{"location":"row-filter-syntax/","title":"Row Filter Syntax","text":""},{"location":"row-filter-syntax/#row-filter-syntax","title":"Row Filter Syntax","text":"<p>In addition to the primary Expression DSL, PyIceberg provides a string-based statement interface for filtering rows in Iceberg tables. This guide explains the syntax and provides examples for supported operations.</p> <p>The row filter syntax is designed to be similar to SQL WHERE clauses. Here are the basic components:</p>"},{"location":"row-filter-syntax/#column-references","title":"Column References","text":"<p>Columns can be referenced using either unquoted or quoted identifiers:</p> <pre><code>column_name\n\"column.name\"\n</code></pre>"},{"location":"row-filter-syntax/#literals","title":"Literals","text":"<p>The following literal types are supported:</p> <ul> <li>Strings: <code>'hello world'</code></li> <li>Numbers: <code>42</code>, <code>-42</code>, <code>3.14</code></li> <li>Booleans: <code>true</code>, <code>false</code> (case insensitive)</li> </ul>"},{"location":"row-filter-syntax/#comparison-operations","title":"Comparison Operations","text":""},{"location":"row-filter-syntax/#basic-comparisons","title":"Basic Comparisons","text":"<pre><code>column = 42\ncolumn != 42\ncolumn &gt; 42\ncolumn &gt;= 42\ncolumn &lt; 42\ncolumn &lt;= 42\n</code></pre> <p>Note</p> <p>The <code>==</code> operator is an alias for <code>=</code> and <code>&lt;&gt;</code> is an alias for <code>!=</code></p>"},{"location":"row-filter-syntax/#string-comparisons","title":"String Comparisons","text":"<pre><code>column = 'hello'\ncolumn != 'world'\n</code></pre>"},{"location":"row-filter-syntax/#null-checks","title":"NULL Checks","text":"<p>Check for NULL values using the <code>IS NULL</code> and <code>IS NOT NULL</code> operators:</p> <pre><code>column IS NULL\ncolumn IS NOT NULL\n</code></pre>"},{"location":"row-filter-syntax/#nan-checks","title":"NaN Checks","text":"<p>For floating-point columns, you can check for NaN values:</p> <pre><code>column IS NAN\ncolumn IS NOT NAN\n</code></pre>"},{"location":"row-filter-syntax/#in-and-not-in","title":"IN and NOT IN","text":"<p>Check if a value is in a set of values:</p> <pre><code>column IN ('a', 'b', 'c')\ncolumn NOT IN (1, 2, 3)\n</code></pre>"},{"location":"row-filter-syntax/#like-operations","title":"LIKE Operations","text":"<p>The LIKE operator supports pattern matching with a wildcard <code>%</code> at the end of the string:</p> <pre><code>column LIKE 'prefix%'\ncolumn NOT LIKE 'prefix%'\n</code></pre> <p>Important</p> <p>The <code>%</code> wildcard is only supported at the end of the pattern. Using it in the middle or beginning of the pattern will raise an error.</p>"},{"location":"row-filter-syntax/#between","title":"BETWEEN","text":"<p>The BETWEEN operator filters a column against an inclusive range of two comparable literals, e.g. <code>a between 1 and 2</code> is equivalent to <code>a &gt;= 1 and a &lt;= 2</code>.</p> <pre><code>column BETWEEN 1 AND 2\ncolumn BETWEEN 1.0 AND 2.0\ncolumn BETWEEN '2025-01-01' AND '2025-01-02'\ncolumn BETWEEN '2025-01-01T00:00:00.000000' AND '2025-01-02T12:00:00.000000'\n</code></pre>"},{"location":"row-filter-syntax/#logical-operations","title":"Logical Operations","text":"<p>Combine multiple conditions using logical operators:</p> <pre><code>column1 = 42 AND column2 = 'hello'\ncolumn1 &gt; 0 OR column2 IS NULL\nNOT (column1 = 42)\n</code></pre> <p>Tip</p> <p>Parentheses can be used to group logical operations for clarity: <pre><code>(column1 = 42 AND column2 = 'hello') OR column3 IS NULL\n</code></pre></p>"},{"location":"row-filter-syntax/#complete-examples","title":"Complete Examples","text":"<p>Here are some complete examples showing how to combine different operations:</p> <pre><code>-- Complex filter with multiple conditions\nstatus = 'active' AND age &gt; 18 AND NOT (country IN ('US', 'CA'))\n\n-- Filter with string pattern matching\nname LIKE 'John%' AND age &gt;= 21\n\n-- Filter with NULL checks and numeric comparisons\nprice IS NOT NULL AND price &gt; 100 AND quantity &gt; 0\n\n-- Filter with multiple logical operations\n(status = 'pending' OR status = 'processing') AND NOT (priority = 'low')\n</code></pre>"},{"location":"row-filter-syntax/#common-pitfalls","title":"Common Pitfalls","text":"<ol> <li>String Quoting: Always use single quotes for string literals. Double quotes are reserved for column identifiers.</li> </ol> <pre><code>-- Correct\nname = 'John'\n\n-- Incorrect\nname = \"John\"\n</code></pre> <ol> <li>Wildcard Usage: The <code>%</code> wildcard in LIKE patterns can only appear at the end.</li> </ol> <pre><code>-- Correct\nname LIKE 'John%'\n\n-- Incorrect (will raise an error)\nname LIKE '%John%'\n</code></pre> <ol> <li>Case Sensitivity: Boolean literals (<code>true</code>/<code>false</code>) are case insensitive, but string comparisons are case sensitive.</li> </ol> <pre><code>-- All valid\nis_active = true\nis_active = TRUE\nis_active = True\n\n-- Case sensitive\nstatus = 'Active'  -- Will not match 'active'\n</code></pre>"},{"location":"row-filter-syntax/#best-practices","title":"Best Practices","text":"<ol> <li>For complex use cases, use the primary Expression DSL</li> <li>When using multiple conditions, consider the order of operations (NOT &gt; AND &gt; OR)</li> <li>For string comparisons, be consistent with case usage</li> </ol>"},{"location":"verify-release/","title":"Verify a release","text":""},{"location":"verify-release/#verifying-a-release","title":"Verifying a release","text":"<p>Each PyIceberg release is validated by the community by holding a vote. A community release manager will prepare a release candidate and call a vote on the Iceberg dev list. To validate the release candidate, community members will test it out in their downstream projects and environments.</p> <p>In addition to testing in downstream projects, community members also check the release\u2019s signatures, checksums, and license documentation.</p>"},{"location":"verify-release/#validating-a-release-candidate","title":"Validating a release candidate","text":"<p>Release announcements include links to the following:</p> <ul> <li>A source tarball</li> <li>A signature (.asc)</li> <li>A checksum (.sha512)</li> <li>KEYS file</li> <li>GitHub change comparison</li> </ul> <p>After downloading the source tarball, signature, checksum, and KEYS file, here are instructions on how to verify signatures, checksums, and documentation.</p>"},{"location":"verify-release/#verifying-signatures","title":"Verifying signatures","text":"<p>First, import the keys.</p> <pre><code>curl https://downloads.apache.org/iceberg/KEYS -o KEYS\ngpg --import KEYS\n</code></pre> <p>Set an environment variable to the version to verify and path to use</p> <pre><code>export PYICEBERG_VERSION=&lt;version&gt; # e.g. 0.6.1rc3\n</code></pre> <p>And a temp folder for the artifacts</p> <pre><code>export PYICEBERG_VERIFICATION_DIR=/tmp/pyiceberg/${PYICEBERG_VERSION}\n</code></pre> <p>Next, verify the <code>.asc</code> file.</p> <pre><code>svn checkout https://dist.apache.org/repos/dist/dev/iceberg/pyiceberg-$PYICEBERG_VERSION/ ${PYICEBERG_VERIFICATION_DIR}\n\ncd ${PYICEBERG_VERIFICATION_DIR}\n\nfor name in $(ls pyiceberg-*.whl pyiceberg-*.tar.gz)\ndo\n    gpg --verify ${name}.asc ${name}\ndone\n</code></pre>"},{"location":"verify-release/#verifying-checksums","title":"Verifying checksums","text":"<pre><code>cd ${PYICEBERG_VERIFICATION_DIR}\nfor name in $(ls pyiceberg-*.whl.sha512 pyiceberg-*.tar.gz.sha512)\ndo\n    shasum -a 512 --check ${name}\ndone\n</code></pre>"},{"location":"verify-release/#verifying-license-documentation","title":"Verifying License Documentation","text":"<pre><code>export PYICEBERG_RELEASE_VERSION=${PYICEBERG_VERSION/rc?/}  # remove rcX qualifier\ntar xzf pyiceberg-${PYICEBERG_RELEASE_VERSION}.tar.gz\ncd pyiceberg-${PYICEBERG_RELEASE_VERSION}\n</code></pre> <p>Run RAT checks to validate license header:</p> <pre><code>./dev/check-license\n</code></pre>"},{"location":"verify-release/#testing","title":"Testing","text":"<p>This section explains how to run the tests of the source distribution.</p> <p>Python Version</p> <p>Make sure you're using a supported Python version</p> <p>First step is to install the package:</p> <pre><code>make install\n</code></pre> <p>To run the full test coverage, with both unit tests and integration tests:</p> <pre><code>make test-coverage\n</code></pre> <p>This will spin up Docker containers to facilitate running test coverage.</p>"},{"location":"verify-release/#cast-the-vote","title":"Cast the vote","text":"<p>Votes are cast by replying to the release candidate announcement email on the dev mailing list with either <code>+1</code>, <code>0</code>, or <code>-1</code>. For example :</p> <p>[ ] +1 Release this as PyIceberg 0.3.0</p> <p>[ ] +0</p> <p>[ ] -1 Do not release this because\u2026</p> <p>In addition to your vote, it\u2019s customary to specify if your vote is binding or non-binding. Only members of the Project Management Committee have formally binding votes. If you\u2019re unsure, you can specify that your vote is non-binding. To read more about voting in the Apache framework, checkout the Voting information page on the Apache foundation\u2019s website.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>pyiceberg<ul> <li>avro<ul> <li>codecs<ul> <li>bzip2</li> <li>codec</li> <li>deflate</li> <li>snappy_codec</li> <li>zstandard_codec</li> </ul> </li> <li>decoder</li> <li>encoder</li> <li>file</li> <li>reader</li> <li>resolver</li> <li>writer</li> </ul> </li> <li>catalog<ul> <li>bigquery_metastore</li> <li>dynamodb</li> <li>glue</li> <li>hive</li> <li>memory</li> <li>noop</li> <li>rest<ul> <li>auth</li> <li>response</li> <li>scan_planning</li> </ul> </li> <li>sql</li> </ul> </li> <li>cli<ul> <li>console</li> <li>output</li> </ul> </li> <li>conversions</li> <li>exceptions</li> <li>expressions<ul> <li>literals</li> <li>parser</li> <li>visitors</li> </ul> </li> <li>io<ul> <li>fsspec</li> <li>pyarrow</li> </ul> </li> <li>manifest</li> <li>partitioning</li> <li>schema</li> <li>serializers</li> <li>table<ul> <li>delete_file_index</li> <li>inspect</li> <li>locations</li> <li>maintenance</li> <li>metadata</li> <li>name_mapping</li> <li>puffin</li> <li>refs</li> <li>snapshots</li> <li>sorting</li> <li>statistics</li> <li>update<ul> <li>schema</li> <li>snapshot</li> <li>sorting</li> <li>spec</li> <li>statistics</li> <li>validate</li> </ul> </li> <li>upsert_util</li> </ul> </li> <li>transforms</li> <li>typedef</li> <li>types</li> <li>utils<ul> <li>bin_packing</li> <li>concurrent</li> <li>config</li> <li>datetime</li> <li>decimal</li> <li>deprecated</li> <li>lazydict</li> <li>parsing</li> <li>properties</li> <li>schema_conversion</li> <li>singleton</li> <li>truncate</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/pyiceberg/","title":"pyiceberg","text":""},{"location":"reference/pyiceberg/conversions/","title":"conversions","text":"<p>Utility module for various conversions around PrimitiveType implementations.</p> This module enables <ul> <li>Converting partition strings to built-in python objects.</li> <li>Converting a value to a byte buffer.</li> <li>Converting a byte buffer to a value.</li> <li>Converting a json-single field serialized field</li> </ul> Note <p>Conversion logic varies based on the PrimitiveType implementation. Therefore conversion functions are defined here as generic functions using the @singledispatch decorator. For each PrimitiveType implementation, a concrete function is registered for each generic conversion function. For PrimitiveType implementations that share the same conversion logic, registrations can be stacked.</p>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions._","title":"<code>_(_, val)</code>","text":"<p>Convert JSON string into Python UUID.</p> Source code in <code>pyiceberg/conversions.py</code> <pre><code>@from_json.register(UUIDType)\ndef _(_: UUIDType, val: str | bytes | uuid.UUID) -&gt; uuid.UUID:\n    \"\"\"Convert JSON string into Python UUID.\"\"\"\n    if isinstance(val, str):\n        return uuid.UUID(val)\n    elif isinstance(val, bytes):\n        return uuid.UUID(bytes=val)\n    else:\n        return val\n</code></pre>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.from_bytes","title":"<code>from_bytes(primitive_type, b)</code>","text":"<p>Convert bytes to a built-in python value.</p> <p>Parameters:</p> Name Type Description Default <code>primitive_type</code> <code>PrimitiveType</code> <p>An implementation of the PrimitiveType base class.</p> required <code>b</code> <code>bytes</code> <p>The bytes to convert.</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>@singledispatch  # type: ignore\ndef from_bytes(primitive_type: PrimitiveType, b: bytes) -&gt; L:  # type: ignore\n    \"\"\"Convert bytes to a built-in python value.\n\n    Args:\n        primitive_type (PrimitiveType): An implementation of the PrimitiveType base class.\n        b (bytes): The bytes to convert.\n    \"\"\"\n    raise TypeError(f\"Cannot deserialize bytes, type {primitive_type} not supported: {b!r}\")\n</code></pre>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.from_json","title":"<code>from_json(primitive_type, val)</code>","text":"<p>Convert JSON value types into built-in python values.</p> <p>https://iceberg.apache.org/spec/#json-single-value-serialization</p> <p>Parameters:</p> Name Type Description Default <code>primitive_type</code> <code>PrimitiveType</code> <p>An implementation of the PrimitiveType base class.</p> required <code>val</code> <code>Any</code> <p>The arbitrary JSON value to convert into the right form</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>@singledispatch  # type: ignore\ndef from_json(primitive_type: PrimitiveType, val: Any) -&gt; L:  # type: ignore\n    \"\"\"Convert JSON value types into built-in python values.\n\n    https://iceberg.apache.org/spec/#json-single-value-serialization\n\n    Args:\n        primitive_type (PrimitiveType): An implementation of the PrimitiveType base class.\n        val (Any): The arbitrary JSON value to convert into the right form\n    \"\"\"\n    raise TypeError(f\"Cannot deserialize bytes, type {primitive_type} not supported: {str(val)}\")\n</code></pre>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.handle_none","title":"<code>handle_none(func)</code>","text":"<p>Handle cases where partition values are <code>None</code> or \"HIVE_DEFAULT_PARTITION\".</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>A function registered to the singledispatch function <code>partition_to_py</code>.</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>def handle_none(func: Callable) -&gt; Callable:  # type: ignore\n    \"\"\"Handle cases where partition values are `None` or \"__HIVE_DEFAULT_PARTITION__\".\n\n    Args:\n        func (Callable): A function registered to the singledispatch function `partition_to_py`.\n    \"\"\"\n\n    def wrapper(primitive_type: PrimitiveType, value_str: str | None) -&gt; Any:\n        if value_str is None:\n            return None\n        elif value_str == \"__HIVE_DEFAULT_PARTITION__\":\n            return None\n        return func(primitive_type, value_str)\n\n    return wrapper\n</code></pre>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.partition_to_py","title":"<code>partition_to_py(primitive_type, value_str)</code>","text":"<p>Convert a partition string to a python built-in.</p> <p>Parameters:</p> Name Type Description Default <code>primitive_type</code> <code>PrimitiveType</code> <p>An implementation of the PrimitiveType base class.</p> required <code>value_str</code> <code>str</code> <p>A string representation of a partition value.</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>@singledispatch\ndef partition_to_py(primitive_type: PrimitiveType, value_str: str) -&gt; int | float | str | uuid.UUID | bytes | Decimal:\n    \"\"\"Convert a partition string to a python built-in.\n\n    Args:\n        primitive_type (PrimitiveType): An implementation of the PrimitiveType base class.\n        value_str (str): A string representation of a partition value.\n    \"\"\"\n    raise TypeError(f\"Cannot convert '{value_str}' to unsupported type: {primitive_type}\")\n</code></pre>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.to_bytes","title":"<code>to_bytes(primitive_type, _)</code>","text":"<p>Convert a built-in python value to bytes.</p> <p>This conversion follows the serialization scheme for storing single values as individual binary values defined in the Iceberg specification that can be found at https://iceberg.apache.org/spec/#appendix-d-single-value-serialization</p> <p>Parameters:</p> Name Type Description Default <code>primitive_type</code> <code>PrimitiveType</code> <p>An implementation of the PrimitiveType base class.</p> required <code>_</code> <code>bool | bytes | Decimal | date | datetime | float | int | str | time | UUID</code> <p>The value to convert to bytes (The type of this value depends on which dispatched function is used--check dispatchable functions for type hints).</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>@singledispatch\ndef to_bytes(\n    primitive_type: PrimitiveType, _: bool | bytes | Decimal | date | datetime | float | int | str | time | uuid.UUID\n) -&gt; bytes:\n    \"\"\"Convert a built-in python value to bytes.\n\n    This conversion follows the serialization scheme for storing single values as individual binary values\n    defined in the Iceberg specification that can be found at\n    https://iceberg.apache.org/spec/#appendix-d-single-value-serialization\n\n    Args:\n        primitive_type (PrimitiveType): An implementation of the PrimitiveType base class.\n        _: The value to convert to bytes (The type of this value depends on which dispatched function is\n            used--check dispatchable functions for type hints).\n    \"\"\"\n    raise TypeError(f\"scale does not match {primitive_type}\")\n</code></pre>"},{"location":"reference/pyiceberg/conversions/#pyiceberg.conversions.to_json","title":"<code>to_json(primitive_type, val)</code>","text":"<p>Convert built-in python values into JSON value types.</p> <p>https://iceberg.apache.org/spec/#json-single-value-serialization</p> <p>Parameters:</p> Name Type Description Default <code>primitive_type</code> <code>PrimitiveType</code> <p>An implementation of the PrimitiveType base class.</p> required <code>val</code> <code>Any</code> <p>The arbitrary built-in value to convert into the right form</p> required Source code in <code>pyiceberg/conversions.py</code> <pre><code>@singledispatch  # type: ignore\ndef to_json(primitive_type: PrimitiveType, val: Any) -&gt; L:  # type: ignore\n    \"\"\"Convert built-in python values into JSON value types.\n\n    https://iceberg.apache.org/spec/#json-single-value-serialization\n\n    Args:\n        primitive_type (PrimitiveType): An implementation of the PrimitiveType base class.\n        val (Any): The arbitrary built-in value to convert into the right form\n    \"\"\"\n    raise TypeError(f\"Cannot deserialize bytes, type {primitive_type} not supported: {val}\")\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/","title":"exceptions","text":""},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.AuthorizationExpiredError","title":"<code>AuthorizationExpiredError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>When the credentials are expired when performing an action on the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class AuthorizationExpiredError(RESTError):\n    \"\"\"When the credentials are expired when performing an action on the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.BadRequestError","title":"<code>BadRequestError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when an invalid request is being made.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class BadRequestError(RESTError):\n    \"\"\"Raises when an invalid request is being made.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.CommitFailedException","title":"<code>CommitFailedException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Commit failed, refresh and try again.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class CommitFailedException(Exception):\n    \"\"\"Commit failed, refresh and try again.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.CommitStateUnknownException","title":"<code>CommitStateUnknownException</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Commit failed due to unknown reason.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class CommitStateUnknownException(RESTError):\n    \"\"\"Commit failed due to unknown reason.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.ForbiddenError","title":"<code>ForbiddenError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when you don't have the credentials to perform the action on the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class ForbiddenError(RESTError):\n    \"\"\"Raises when you don't have the credentials to perform the action on the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NamespaceAlreadyExistsError","title":"<code>NamespaceAlreadyExistsError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a name-space being created already exists in the catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NamespaceAlreadyExistsError(Exception):\n    \"\"\"Raised when a name-space being created already exists in the catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NamespaceNotEmptyError","title":"<code>NamespaceNotEmptyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a name-space being dropped is not empty.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NamespaceNotEmptyError(Exception):\n    \"\"\"Raised when a name-space being dropped is not empty.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchIcebergTableError","title":"<code>NoSuchIcebergTableError</code>","text":"<p>               Bases: <code>NoSuchTableError</code></p> <p>Raises when the table found in the REST catalog is not an iceberg table.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchIcebergTableError(NoSuchTableError):\n    \"\"\"Raises when the table found in the REST catalog is not an iceberg table.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchIdentifierError","title":"<code>NoSuchIdentifierError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when the identifier can't be found in the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchIdentifierError(Exception):\n    \"\"\"Raises when the identifier can't be found in the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchNamespaceError","title":"<code>NoSuchNamespaceError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a referenced name-space is not found.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchNamespaceError(Exception):\n    \"\"\"Raised when a referenced name-space is not found.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchPlanTaskError","title":"<code>NoSuchPlanTaskError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a scan plan task is not found.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchPlanTaskError(Exception):\n    \"\"\"Raised when a scan plan task is not found.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchPropertyException","title":"<code>NoSuchPropertyException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>When a property is missing.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchPropertyException(Exception):\n    \"\"\"When a property is missing.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchTableError","title":"<code>NoSuchTableError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when the table can't be found in the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchTableError(Exception):\n    \"\"\"Raises when the table can't be found in the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NoSuchViewError","title":"<code>NoSuchViewError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when the view can't be found in the REST catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NoSuchViewError(Exception):\n    \"\"\"Raises when the view can't be found in the REST catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.NotInstalledError","title":"<code>NotInstalledError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>When an optional dependency is not installed.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class NotInstalledError(Exception):\n    \"\"\"When an optional dependency is not installed.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.OAuthError","title":"<code>OAuthError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when there is an error with the OAuth call.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class OAuthError(RESTError):\n    \"\"\"Raises when there is an error with the OAuth call.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.RESTError","title":"<code>RESTError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when there is an unknown response from the REST Catalog.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class RESTError(Exception):\n    \"\"\"Raises when there is an unknown response from the REST Catalog.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.ServerError","title":"<code>ServerError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when there is an unhandled exception on the server side.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class ServerError(RESTError):\n    \"\"\"Raises when there is an unhandled exception on the server side.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.ServiceUnavailableError","title":"<code>ServiceUnavailableError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when the service doesn't respond.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class ServiceUnavailableError(RESTError):\n    \"\"\"Raises when the service doesn't respond.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.SignError","title":"<code>SignError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when unable to sign a S3 request.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class SignError(Exception):\n    \"\"\"Raises when unable to sign a S3 request.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.TableAlreadyExistsError","title":"<code>TableAlreadyExistsError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when creating a table with a name that already exists.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class TableAlreadyExistsError(Exception):\n    \"\"\"Raised when creating a table with a name that already exists.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.UnauthorizedError","title":"<code>UnauthorizedError</code>","text":"<p>               Bases: <code>RESTError</code></p> <p>Raises when you don't have the proper authorization.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class UnauthorizedError(RESTError):\n    \"\"\"Raises when you don't have the proper authorization.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raises when there is an issue with the schema.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class ValidationError(Exception):\n    \"\"\"Raises when there is an issue with the schema.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.ValidationException","title":"<code>ValidationException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when validation fails.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class ValidationException(Exception):\n    \"\"\"Raised when validation fails.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/exceptions/#pyiceberg.exceptions.WaitingForLockException","title":"<code>WaitingForLockException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Need to wait for a lock, try again.</p> Source code in <code>pyiceberg/exceptions.py</code> <pre><code>class WaitingForLockException(Exception):\n    \"\"\"Need to wait for a lock, try again.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/","title":"manifest","text":""},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFile","title":"<code>DataFile</code>","text":"<p>               Bases: <code>Record</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class DataFile(Record):\n    @classmethod\n    def from_args(cls, _table_format_version: TableVersion = DEFAULT_READ_VERSION, **arguments: Any) -&gt; DataFile:\n        struct = DATA_FILE_TYPE[_table_format_version]\n        return super()._bind(struct, **arguments)\n\n    @property\n    def content(self) -&gt; DataFileContent:\n        return self._data[0]\n\n    @property\n    def file_path(self) -&gt; str:\n        return self._data[1]\n\n    @property\n    def file_format(self) -&gt; FileFormat:\n        return self._data[2]\n\n    @property\n    def partition(self) -&gt; Record:\n        return self._data[3]\n\n    @property\n    def record_count(self) -&gt; int:\n        return self._data[4]\n\n    @property\n    def file_size_in_bytes(self) -&gt; int:\n        return self._data[5]\n\n    @property\n    def column_sizes(self) -&gt; dict[int, int]:\n        return self._data[6]\n\n    @property\n    def value_counts(self) -&gt; dict[int, int]:\n        return self._data[7]\n\n    @property\n    def null_value_counts(self) -&gt; dict[int, int]:\n        return self._data[8]\n\n    @property\n    def nan_value_counts(self) -&gt; dict[int, int]:\n        return self._data[9]\n\n    @property\n    def lower_bounds(self) -&gt; dict[int, bytes]:\n        return self._data[10]\n\n    @property\n    def upper_bounds(self) -&gt; dict[int, bytes]:\n        return self._data[11]\n\n    @property\n    def key_metadata(self) -&gt; bytes | None:\n        return self._data[12]\n\n    @property\n    def split_offsets(self) -&gt; list[int] | None:\n        return self._data[13]\n\n    @property\n    def equality_ids(self) -&gt; list[int] | None:\n        return self._data[14]\n\n    @property\n    def sort_order_id(self) -&gt; int | None:\n        return self._data[15]\n\n    # Spec ID should not be stored in the file\n    _spec_id: int\n\n    @property\n    def spec_id(self) -&gt; int:\n        return self._spec_id\n\n    @spec_id.setter\n    def spec_id(self, value: int) -&gt; None:\n        self._spec_id = value\n\n    def __setattr__(self, name: str, value: Any) -&gt; None:\n        \"\"\"Assign a key/value to a DataFile.\"\"\"\n        # The file_format is written as a string, so we need to cast it to the Enum\n        if name == \"file_format\":\n            value = FileFormat[value]\n        super().__setattr__(name, value)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of the file path.\"\"\"\n        return hash(self.file_path)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare the datafile with another object.\n\n        If it is a datafile, it will compare based on the file_path.\n        \"\"\"\n        return self.file_path == other.file_path if isinstance(other, DataFile) else False\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFile.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare the datafile with another object.</p> <p>If it is a datafile, it will compare based on the file_path.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare the datafile with another object.\n\n    If it is a datafile, it will compare based on the file_path.\n    \"\"\"\n    return self.file_path == other.file_path if isinstance(other, DataFile) else False\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFile.__hash__","title":"<code>__hash__()</code>","text":"<p>Return the hash of the file path.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of the file path.\"\"\"\n    return hash(self.file_path)\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFile.__setattr__","title":"<code>__setattr__(name, value)</code>","text":"<p>Assign a key/value to a DataFile.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __setattr__(self, name: str, value: Any) -&gt; None:\n    \"\"\"Assign a key/value to a DataFile.\"\"\"\n    # The file_format is written as a string, so we need to cast it to the Enum\n    if name == \"file_format\":\n        value = FileFormat[value]\n    super().__setattr__(name, value)\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFileContent","title":"<code>DataFileContent</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class DataFileContent(int, Enum):\n    DATA = 0\n    POSITION_DELETES = 1\n    EQUALITY_DELETES = 2\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the DataFileContent class.\"\"\"\n        return f\"DataFileContent.{self.name}\"\n\n    @staticmethod\n    def from_rest_type(content_type: str) -&gt; DataFileContent:\n        \"\"\"Convert REST API content type string to DataFileContent.\n\n        Args:\n            content_type: REST API content type.\n\n        Returns:\n            The corresponding DataFileContent enum value.\n\n        Raises:\n            ValueError: If the content type is unknown.\n        \"\"\"\n        mapping = {\n            \"data\": DataFileContent.DATA,\n            \"position-deletes\": DataFileContent.POSITION_DELETES,\n            \"equality-deletes\": DataFileContent.EQUALITY_DELETES,\n        }\n        if content_type not in mapping:\n            raise ValueError(f\"Invalid file content value: {content_type}\")\n        return mapping[content_type]\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFileContent.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the DataFileContent class.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the DataFileContent class.\"\"\"\n    return f\"DataFileContent.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.DataFileContent.from_rest_type","title":"<code>from_rest_type(content_type)</code>  <code>staticmethod</code>","text":"<p>Convert REST API content type string to DataFileContent.</p> <p>Parameters:</p> Name Type Description Default <code>content_type</code> <code>str</code> <p>REST API content type.</p> required <p>Returns:</p> Type Description <code>DataFileContent</code> <p>The corresponding DataFileContent enum value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the content type is unknown.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>@staticmethod\ndef from_rest_type(content_type: str) -&gt; DataFileContent:\n    \"\"\"Convert REST API content type string to DataFileContent.\n\n    Args:\n        content_type: REST API content type.\n\n    Returns:\n        The corresponding DataFileContent enum value.\n\n    Raises:\n        ValueError: If the content type is unknown.\n    \"\"\"\n    mapping = {\n        \"data\": DataFileContent.DATA,\n        \"position-deletes\": DataFileContent.POSITION_DELETES,\n        \"equality-deletes\": DataFileContent.EQUALITY_DELETES,\n    }\n    if content_type not in mapping:\n        raise ValueError(f\"Invalid file content value: {content_type}\")\n    return mapping[content_type]\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.FileFormat","title":"<code>FileFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class FileFormat(str, Enum):\n    AVRO = \"AVRO\"\n    PARQUET = \"PARQUET\"\n    ORC = \"ORC\"\n    PUFFIN = \"PUFFIN\"\n\n    @classmethod\n    def _missing_(cls, value: object) -&gt; None | str:\n        for member in cls:\n            if member.value == str(value).upper():\n                return member\n        return None\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the FileFormat class.\"\"\"\n        return f\"FileFormat.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.FileFormat.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the FileFormat class.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the FileFormat class.\"\"\"\n    return f\"FileFormat.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestContent","title":"<code>ManifestContent</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestContent(int, Enum):\n    DATA = 0\n    DELETES = 1\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the ManifestContent class.\"\"\"\n        return f\"ManifestContent.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestContent.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the ManifestContent class.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the ManifestContent class.\"\"\"\n    return f\"ManifestContent.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestEntryStatus","title":"<code>ManifestEntryStatus</code>","text":"<p>               Bases: <code>int</code>, <code>Enum</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestEntryStatus(int, Enum):\n    EXISTING = 0\n    ADDED = 1\n    DELETED = 2\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the ManifestEntryStatus class.\"\"\"\n        return f\"ManifestEntryStatus.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestEntryStatus.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the ManifestEntryStatus class.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the ManifestEntryStatus class.\"\"\"\n    return f\"ManifestEntryStatus.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestFile","title":"<code>ManifestFile</code>","text":"<p>               Bases: <code>Record</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestFile(Record):\n    @classmethod\n    def from_args(cls, _table_format_version: TableVersion = DEFAULT_READ_VERSION, **arguments: Any) -&gt; ManifestFile:\n        return super()._bind(**arguments, struct=MANIFEST_LIST_FILE_SCHEMAS[_table_format_version])\n\n    @property\n    def manifest_path(self) -&gt; str:\n        return self._data[0]\n\n    @property\n    def manifest_length(self) -&gt; int:\n        return self._data[1]\n\n    @property\n    def partition_spec_id(self) -&gt; int:\n        return self._data[2]\n\n    @property\n    def content(self) -&gt; ManifestContent:\n        return self._data[3]\n\n    @property\n    def sequence_number(self) -&gt; int:\n        return self._data[4]\n\n    @sequence_number.setter\n    def sequence_number(self, value: int) -&gt; None:\n        self._data[4] = value\n\n    @property\n    def min_sequence_number(self) -&gt; int:\n        return self._data[5]\n\n    @min_sequence_number.setter\n    def min_sequence_number(self, value: int) -&gt; None:\n        self._data[5] = value\n\n    @property\n    def added_snapshot_id(self) -&gt; int | None:\n        return self._data[6]\n\n    @property\n    def added_files_count(self) -&gt; int | None:\n        return self._data[7]\n\n    @property\n    def existing_files_count(self) -&gt; int | None:\n        return self._data[8]\n\n    @property\n    def deleted_files_count(self) -&gt; int | None:\n        return self._data[9]\n\n    @property\n    def added_rows_count(self) -&gt; int | None:\n        return self._data[10]\n\n    @property\n    def existing_rows_count(self) -&gt; int | None:\n        return self._data[11]\n\n    @property\n    def deleted_rows_count(self) -&gt; int | None:\n        return self._data[12]\n\n    @property\n    def partitions(self) -&gt; list[PartitionFieldSummary] | None:\n        return self._data[13]\n\n    @property\n    def key_metadata(self) -&gt; bytes | None:\n        return self._data[14]\n\n    def has_added_files(self) -&gt; bool:\n        return self.added_files_count is None or self.added_files_count &gt; 0\n\n    def has_existing_files(self) -&gt; bool:\n        return self.existing_files_count is None or self.existing_files_count &gt; 0\n\n    def fetch_manifest_entry(self, io: FileIO, discard_deleted: bool = True) -&gt; list[ManifestEntry]:\n        \"\"\"\n        Read the manifest entries from the manifest file.\n\n        Args:\n            io: The FileIO to fetch the file.\n            discard_deleted: Filter on live entries.\n\n        Returns:\n            An Iterator of manifest entries.\n        \"\"\"\n        input_file = io.new_input(self.manifest_path)\n        with AvroFile[ManifestEntry](\n            input_file,\n            MANIFEST_ENTRY_SCHEMAS[DEFAULT_READ_VERSION],\n            read_types={-1: ManifestEntry, 2: DataFile},\n            read_enums={0: ManifestEntryStatus, 101: FileFormat, 134: DataFileContent},\n        ) as reader:\n            return [\n                _inherit_from_manifest(entry, self)\n                for entry in reader\n                if not discard_deleted or entry.status != ManifestEntryStatus.DELETED\n            ]\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the ManifestFile class.\"\"\"\n        return self.manifest_path == other.manifest_path if isinstance(other, ManifestFile) else False\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of manifest_path.\"\"\"\n        return hash(self.manifest_path)\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestFile.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the ManifestFile class.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the ManifestFile class.\"\"\"\n    return self.manifest_path == other.manifest_path if isinstance(other, ManifestFile) else False\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestFile.__hash__","title":"<code>__hash__()</code>","text":"<p>Return the hash of manifest_path.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of manifest_path.\"\"\"\n    return hash(self.manifest_path)\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestFile.fetch_manifest_entry","title":"<code>fetch_manifest_entry(io, discard_deleted=True)</code>","text":"<p>Read the manifest entries from the manifest file.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>FileIO</code> <p>The FileIO to fetch the file.</p> required <code>discard_deleted</code> <code>bool</code> <p>Filter on live entries.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[ManifestEntry]</code> <p>An Iterator of manifest entries.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def fetch_manifest_entry(self, io: FileIO, discard_deleted: bool = True) -&gt; list[ManifestEntry]:\n    \"\"\"\n    Read the manifest entries from the manifest file.\n\n    Args:\n        io: The FileIO to fetch the file.\n        discard_deleted: Filter on live entries.\n\n    Returns:\n        An Iterator of manifest entries.\n    \"\"\"\n    input_file = io.new_input(self.manifest_path)\n    with AvroFile[ManifestEntry](\n        input_file,\n        MANIFEST_ENTRY_SCHEMAS[DEFAULT_READ_VERSION],\n        read_types={-1: ManifestEntry, 2: DataFile},\n        read_enums={0: ManifestEntryStatus, 101: FileFormat, 134: DataFileContent},\n    ) as reader:\n        return [\n            _inherit_from_manifest(entry, self)\n            for entry in reader\n            if not discard_deleted or entry.status != ManifestEntryStatus.DELETED\n        ]\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestListWriter","title":"<code>ManifestListWriter</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestListWriter(ABC):\n    _format_version: TableVersion\n    _output_file: OutputFile\n    _meta: dict[str, str]\n    _manifest_files: list[ManifestFile]\n    _commit_snapshot_id: int\n    _writer: AvroOutputFile[ManifestFile]\n\n    def __init__(self, format_version: TableVersion, output_file: OutputFile, meta: dict[str, Any]):\n        self._format_version = format_version\n        self._output_file = output_file\n        self._meta = meta\n        self._manifest_files = []\n\n    def __enter__(self) -&gt; ManifestListWriter:\n        \"\"\"Open the writer for writing.\"\"\"\n        self._writer = AvroOutputFile[ManifestFile](\n            output_file=self._output_file,\n            record_schema=MANIFEST_LIST_FILE_SCHEMAS[DEFAULT_READ_VERSION],\n            file_schema=MANIFEST_LIST_FILE_SCHEMAS[self._format_version],\n            schema_name=\"manifest_file\",\n            metadata=self._meta,\n        )\n        self._writer.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ) -&gt; None:\n        \"\"\"Close the writer.\"\"\"\n        self._writer.__exit__(exc_type, exc_value, traceback)\n        return\n\n    @abstractmethod\n    def prepare_manifest(self, manifest_file: ManifestFile) -&gt; ManifestFile: ...\n\n    def add_manifests(self, manifest_files: list[ManifestFile]) -&gt; ManifestListWriter:\n        self._writer.write_block([self.prepare_manifest(manifest_file) for manifest_file in manifest_files])\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestListWriter.__enter__","title":"<code>__enter__()</code>","text":"<p>Open the writer for writing.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __enter__(self) -&gt; ManifestListWriter:\n    \"\"\"Open the writer for writing.\"\"\"\n    self._writer = AvroOutputFile[ManifestFile](\n        output_file=self._output_file,\n        record_schema=MANIFEST_LIST_FILE_SCHEMAS[DEFAULT_READ_VERSION],\n        file_schema=MANIFEST_LIST_FILE_SCHEMAS[self._format_version],\n        schema_name=\"manifest_file\",\n        metadata=self._meta,\n    )\n    self._writer.__enter__()\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestListWriter.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Close the writer.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_value: BaseException | None,\n    traceback: TracebackType | None,\n) -&gt; None:\n    \"\"\"Close the writer.\"\"\"\n    self._writer.__exit__(exc_type, exc_value, traceback)\n    return\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestWriter","title":"<code>ManifestWriter</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>class ManifestWriter(ABC):\n    closed: bool\n    _spec: PartitionSpec\n    _schema: Schema\n    _output_file: OutputFile\n    _writer: AvroOutputFile[ManifestEntry]\n    _snapshot_id: int\n    _added_files: int\n    _added_rows: int\n    _existing_files: int\n    _existing_rows: int\n    _deleted_files: int\n    _deleted_rows: int\n    _min_sequence_number: int | None\n    _partitions: list[Record]\n    _compression: AvroCompressionCodec\n\n    def __init__(\n        self,\n        spec: PartitionSpec,\n        schema: Schema,\n        output_file: OutputFile,\n        snapshot_id: int,\n        avro_compression: AvroCompressionCodec,\n    ) -&gt; None:\n        self.closed = False\n        self._spec = spec\n        self._schema = schema\n        self._output_file = output_file\n        self._snapshot_id = snapshot_id\n\n        self._added_files = 0\n        self._added_rows = 0\n        self._existing_files = 0\n        self._existing_rows = 0\n        self._deleted_files = 0\n        self._deleted_rows = 0\n        self._min_sequence_number = None\n        self._partitions = []\n        self._compression = avro_compression\n\n    def __enter__(self) -&gt; ManifestWriter:\n        \"\"\"Open the writer.\"\"\"\n        self._writer = self.new_writer()\n        self._writer.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_value: BaseException | None,\n        traceback: TracebackType | None,\n    ) -&gt; None:\n        \"\"\"Close the writer.\"\"\"\n        if (self._added_files + self._existing_files + self._deleted_files) == 0:\n            # This is just a guard to ensure that we don't write empty manifest files\n            raise ValueError(\"An empty manifest file has been written\")\n\n        self.closed = True\n        self._writer.__exit__(exc_type, exc_value, traceback)\n\n    @abstractmethod\n    def content(self) -&gt; ManifestContent: ...\n\n    @property\n    @abstractmethod\n    def version(self) -&gt; TableVersion: ...\n\n    @property\n    def _meta(self) -&gt; dict[str, str]:\n        return {\n            \"schema\": self._schema.model_dump_json(),\n            \"partition-spec\": to_json(self._spec.fields).decode(\"utf-8\"),\n            \"partition-spec-id\": str(self._spec.spec_id),\n            \"format-version\": str(self.version),\n            AVRO_CODEC_KEY: self._compression,\n        }\n\n    def _with_partition(self, format_version: TableVersion) -&gt; Schema:\n        data_file_type = data_file_with_partition(\n            format_version=format_version, partition_type=self._spec.partition_type(self._schema)\n        )\n        return manifest_entry_schema_with_data_file(format_version=format_version, data_file=data_file_type)\n\n    def new_writer(self) -&gt; AvroOutputFile[ManifestEntry]:\n        return AvroOutputFile[ManifestEntry](\n            output_file=self._output_file,\n            file_schema=self._with_partition(self.version),\n            record_schema=self._with_partition(DEFAULT_READ_VERSION),\n            schema_name=\"manifest_entry\",\n            metadata=self._meta,\n        )\n\n    @abstractmethod\n    def prepare_entry(self, entry: ManifestEntry) -&gt; ManifestEntry: ...\n\n    def to_manifest_file(self) -&gt; ManifestFile:\n        \"\"\"Return the manifest file.\"\"\"\n        # once the manifest file is generated, no more entries can be added\n        self.closed = True\n        min_sequence_number = self._min_sequence_number or UNASSIGNED_SEQ\n        return ManifestFile.from_args(\n            manifest_path=self._output_file.location,\n            manifest_length=len(self._writer.output_file),\n            partition_spec_id=self._spec.spec_id,\n            content=self.content(),\n            sequence_number=UNASSIGNED_SEQ,\n            min_sequence_number=min_sequence_number,\n            added_snapshot_id=self._snapshot_id,\n            added_files_count=self._added_files,\n            existing_files_count=self._existing_files,\n            deleted_files_count=self._deleted_files,\n            added_rows_count=self._added_rows,\n            existing_rows_count=self._existing_rows,\n            deleted_rows_count=self._deleted_rows,\n            partitions=construct_partition_summaries(self._spec, self._schema, self._partitions),\n            key_metadata=None,\n        )\n\n    def add_entry(self, entry: ManifestEntry) -&gt; ManifestWriter:\n        if self.closed:\n            raise RuntimeError(\"Cannot add entry to closed manifest writer\")\n        if entry.status == ManifestEntryStatus.ADDED:\n            self._added_files += 1\n            self._added_rows += entry.data_file.record_count\n        elif entry.status == ManifestEntryStatus.EXISTING:\n            self._existing_files += 1\n            self._existing_rows += entry.data_file.record_count\n        elif entry.status == ManifestEntryStatus.DELETED:\n            self._deleted_files += 1\n            self._deleted_rows += entry.data_file.record_count\n        else:\n            raise ValueError(f\"Unknown entry: {entry.status}\")\n\n        self._partitions.append(entry.data_file.partition)\n\n        if (\n            (entry.status == ManifestEntryStatus.ADDED or entry.status == ManifestEntryStatus.EXISTING)\n            and entry.sequence_number is not None\n            and (self._min_sequence_number is None or entry.sequence_number &lt; self._min_sequence_number)\n        ):\n            self._min_sequence_number = entry.sequence_number\n\n        self._writer.write_block([self.prepare_entry(entry)])\n        return self\n\n    def add(self, entry: ManifestEntry) -&gt; ManifestWriter:\n        self.add_entry(\n            ManifestEntry.from_args(\n                status=ManifestEntryStatus.ADDED,\n                snapshot_id=self._snapshot_id,\n                sequence_number=entry.sequence_number if entry.sequence_number != UNASSIGNED_SEQ else None,\n                data_file=entry.data_file,\n            )\n        )\n\n        return self\n\n    def delete(self, entry: ManifestEntry) -&gt; ManifestWriter:\n        self.add_entry(\n            ManifestEntry.from_args(\n                status=ManifestEntryStatus.DELETED,\n                snapshot_id=self._snapshot_id,\n                sequence_number=entry.sequence_number,\n                file_sequence_number=entry.file_sequence_number,\n                data_file=entry.data_file,\n            )\n        )\n        return self\n\n    def existing(self, entry: ManifestEntry) -&gt; ManifestWriter:\n        self.add_entry(\n            ManifestEntry.from_args(\n                status=ManifestEntryStatus.EXISTING,\n                snapshot_id=entry.snapshot_id,\n                sequence_number=entry.sequence_number,\n                file_sequence_number=entry.file_sequence_number,\n                data_file=entry.data_file,\n            )\n        )\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestWriter.__enter__","title":"<code>__enter__()</code>","text":"<p>Open the writer.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __enter__(self) -&gt; ManifestWriter:\n    \"\"\"Open the writer.\"\"\"\n    self._writer = self.new_writer()\n    self._writer.__enter__()\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestWriter.__exit__","title":"<code>__exit__(exc_type, exc_value, traceback)</code>","text":"<p>Close the writer.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def __exit__(\n    self,\n    exc_type: type[BaseException] | None,\n    exc_value: BaseException | None,\n    traceback: TracebackType | None,\n) -&gt; None:\n    \"\"\"Close the writer.\"\"\"\n    if (self._added_files + self._existing_files + self._deleted_files) == 0:\n        # This is just a guard to ensure that we don't write empty manifest files\n        raise ValueError(\"An empty manifest file has been written\")\n\n    self.closed = True\n    self._writer.__exit__(exc_type, exc_value, traceback)\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.ManifestWriter.to_manifest_file","title":"<code>to_manifest_file()</code>","text":"<p>Return the manifest file.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def to_manifest_file(self) -&gt; ManifestFile:\n    \"\"\"Return the manifest file.\"\"\"\n    # once the manifest file is generated, no more entries can be added\n    self.closed = True\n    min_sequence_number = self._min_sequence_number or UNASSIGNED_SEQ\n    return ManifestFile.from_args(\n        manifest_path=self._output_file.location,\n        manifest_length=len(self._writer.output_file),\n        partition_spec_id=self._spec.spec_id,\n        content=self.content(),\n        sequence_number=UNASSIGNED_SEQ,\n        min_sequence_number=min_sequence_number,\n        added_snapshot_id=self._snapshot_id,\n        added_files_count=self._added_files,\n        existing_files_count=self._existing_files,\n        deleted_files_count=self._deleted_files,\n        added_rows_count=self._added_rows,\n        existing_rows_count=self._existing_rows,\n        deleted_rows_count=self._deleted_rows,\n        partitions=construct_partition_summaries(self._spec, self._schema, self._partitions),\n        key_metadata=None,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/manifest/#pyiceberg.manifest.read_manifest_list","title":"<code>read_manifest_list(input_file)</code>","text":"<p>Read the manifests from the manifest list.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>InputFile</code> <p>The input file where the stream can be read from.</p> required <p>Returns:</p> Type Description <code>Iterator[ManifestFile]</code> <p>An iterator of ManifestFiles that are part of the list.</p> Source code in <code>pyiceberg/manifest.py</code> <pre><code>def read_manifest_list(input_file: InputFile) -&gt; Iterator[ManifestFile]:\n    \"\"\"\n    Read the manifests from the manifest list.\n\n    Args:\n        input_file: The input file where the stream can be read from.\n\n    Returns:\n        An iterator of ManifestFiles that are part of the list.\n    \"\"\"\n    with AvroFile[ManifestFile](\n        input_file,\n        MANIFEST_LIST_FILE_SCHEMAS[DEFAULT_READ_VERSION],\n        read_types={-1: ManifestFile, 508: PartitionFieldSummary},\n        read_enums={517: ManifestContent},\n    ) as reader:\n        yield from reader\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/","title":"partitioning","text":""},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionField","title":"<code>PartitionField</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>PartitionField represents how one partition value is derived from the source column via transformation.</p> <p>Attributes:</p> Name Type Description <code>source_id(int)</code> <p>The source column id of table's schema.</p> <code>field_id(int)</code> <p>The partition field id across all the table partition specs.</p> <code>transform(Transform)</code> <p>The transform used to produce partition values from source column.</p> <code>name(str)</code> <p>The name of this partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>class PartitionField(IcebergBaseModel):\n    \"\"\"PartitionField represents how one partition value is derived from the source column via transformation.\n\n    Attributes:\n        source_id(int): The source column id of table's schema.\n        field_id(int): The partition field id across all the table partition specs.\n        transform(Transform): The transform used to produce partition values from source column.\n        name(str): The name of this partition field.\n    \"\"\"\n\n    source_id: int = Field(alias=\"source-id\")\n    field_id: int = Field(alias=\"field-id\")\n    transform: Annotated[  # type: ignore\n        Transform,\n        BeforeValidator(parse_transform),\n        PlainSerializer(lambda c: str(c), return_type=str),  # pylint: disable=W0108\n        WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    ] = Field()\n    name: str = Field()\n\n    def __init__(\n        self,\n        source_id: int | None = None,\n        field_id: int | None = None,\n        transform: Transform[Any, Any] | None = None,\n        name: str | None = None,\n        **data: Any,\n    ):\n        if source_id is not None:\n            data[\"source-id\"] = source_id\n        if field_id is not None:\n            data[\"field-id\"] = field_id\n        if transform is not None:\n            data[\"transform\"] = transform\n        if name is not None:\n            data[\"name\"] = name\n\n        super().__init__(**data)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def map_source_ids_onto_source_id(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if \"source-id\" not in data and (source_ids := data[\"source-ids\"]):\n                if isinstance(source_ids, list):\n                    if len(source_ids) == 0:\n                        raise ValueError(\"Empty source-ids is not allowed\")\n                    if len(source_ids) &gt; 1:\n                        raise ValueError(\"Multi argument transforms are not yet supported\")\n                    data[\"source-id\"] = source_ids[0]\n        return data\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the PartitionField class.\"\"\"\n        return f\"{self.field_id}: {self.name}: {self.transform}({self.source_id})\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionField.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the PartitionField class.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the PartitionField class.\"\"\"\n    return f\"{self.field_id}: {self.name}: {self.transform}({self.source_id})\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec","title":"<code>PartitionSpec</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>PartitionSpec captures the transformation from table data to partition values.</p> <p>Attributes:</p> Name Type Description <code>spec_id(int)</code> <p>any change to PartitionSpec will produce a new specId.</p> <code>fields(Tuple[PartitionField)</code> <p>list of partition fields to produce partition values.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>class PartitionSpec(IcebergBaseModel):\n    \"\"\"\n    PartitionSpec captures the transformation from table data to partition values.\n\n    Attributes:\n        spec_id(int): any change to PartitionSpec will produce a new specId.\n        fields(Tuple[PartitionField): list of partition fields to produce partition values.\n    \"\"\"\n\n    spec_id: int = Field(alias=\"spec-id\", default=INITIAL_PARTITION_SPEC_ID)\n    fields: tuple[PartitionField, ...] = Field(default_factory=tuple)\n\n    def __init__(\n        self,\n        *fields: PartitionField,\n        **data: Any,\n    ):\n        if fields:\n            data[\"fields\"] = tuple(fields)\n        super().__init__(**data)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"\n        Produce a boolean to return True if two objects are considered equal.\n\n        Note:\n            Equality of PartitionSpec is determined by spec_id and partition fields only.\n        \"\"\"\n        if not isinstance(other, PartitionSpec):\n            return False\n        return self.spec_id == other.spec_id and self.fields == other.fields\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        Produce a human-readable string representation of PartitionSpec.\n\n        Note:\n            Only include list of partition fields in the PartitionSpec's string representation.\n        \"\"\"\n        result_str = \"[\"\n        if self.fields:\n            result_str += \"\\n  \" + \"\\n  \".join([str(field) for field in self.fields]) + \"\\n\"\n        result_str += \"]\"\n        return result_str\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the PartitionSpec class.\"\"\"\n        fields = f\"{', '.join(repr(column) for column in self.fields)}, \" if self.fields else \"\"\n        return f\"PartitionSpec({fields}spec_id={self.spec_id})\"\n\n    def is_unpartitioned(self) -&gt; bool:\n        return not self.fields\n\n    @property\n    def last_assigned_field_id(self) -&gt; int:\n        if self.fields:\n            return max(pf.field_id for pf in self.fields)\n        return PARTITION_FIELD_ID_START - 1\n\n    @cached_property\n    def source_id_to_fields_map(self) -&gt; dict[int, list[PartitionField]]:\n        source_id_to_fields_map: dict[int, list[PartitionField]] = {}\n        for partition_field in self.fields:\n            existing = source_id_to_fields_map.get(partition_field.source_id, [])\n            existing.append(partition_field)\n            source_id_to_fields_map[partition_field.source_id] = existing\n        return source_id_to_fields_map\n\n    def fields_by_source_id(self, field_id: int) -&gt; list[PartitionField]:\n        return self.source_id_to_fields_map.get(field_id, [])\n\n    def compatible_with(self, other: PartitionSpec) -&gt; bool:\n        \"\"\"Produce a boolean to return True if two PartitionSpec are considered compatible.\"\"\"\n        if self == other:\n            return True\n        if len(self.fields) != len(other.fields):\n            return False\n        return all(\n            this_field.source_id == that_field.source_id\n            and this_field.transform == that_field.transform\n            and this_field.name == that_field.name\n            for this_field, that_field in zip(self.fields, other.fields, strict=True)\n        )\n\n    def partition_type(self, schema: Schema) -&gt; StructType:\n        \"\"\"Produce a struct of the PartitionSpec.\n\n        The partition fields should be optional:\n\n        - All partition transforms are required to produce null if the input value is null, so it can\n          happen when the source column is optional.\n        - Partition fields may be added later, in which case not all files would have the result field,\n          and it may be null.\n\n        There is a case where we can guarantee that a partition field in the first and only partition spec\n        that uses a required source column will never be null, but it doesn't seem worth tracking this case.\n\n        :param schema: The schema to bind to.\n        :return: A StructType that represents the PartitionSpec, with a NestedField for each PartitionField.\n        \"\"\"\n        nested_fields = []\n        schema_ids = schema._lazy_id_to_field\n        for field in self.fields:\n            if source_field := schema_ids.get(field.source_id):\n                result_type = field.transform.result_type(source_field.field_type)\n                nested_fields.append(NestedField(field.field_id, field.name, result_type, required=source_field.required))\n            else:\n                # Since the source field has been drop we cannot determine the type\n                nested_fields.append(NestedField(field.field_id, field.name, UnknownType()))\n        return StructType(*nested_fields)\n\n    def partition_to_path(self, data: Record, schema: Schema) -&gt; str:\n        partition_type = self.partition_type(schema)\n        field_types = partition_type.fields\n\n        field_strs = []\n        value_strs = []\n        for pos in range(len(self.fields)):\n            partition_field = self.fields[pos]\n            value_str = partition_field.transform.to_human_string(field_types[pos].field_type, value=data[pos])\n\n            value_strs.append(quote_plus(value_str, safe=\"\"))\n            field_strs.append(quote_plus(partition_field.name, safe=\"\"))\n\n        path = \"/\".join([field_str + \"=\" + value_str for field_str, value_str in zip(field_strs, value_strs, strict=True)])\n        return path\n\n    def check_compatible(self, schema: Schema, allow_missing_fields: bool = False) -&gt; None:\n        # if the underlying field is dropped, we cannot check they are compatible -- continue\n        schema_fields = schema._lazy_id_to_field\n        parents = schema._lazy_id_to_parent\n\n        for field in self.fields:\n            source_field = schema_fields.get(field.source_id)\n\n            if allow_missing_fields and source_field is None:\n                continue\n\n            if isinstance(field.transform, VoidTransform):\n                continue\n\n            if not source_field:\n                raise ValidationError(f\"Cannot find source column for partition field: {field}\")\n\n            source_type = source_field.field_type\n            if not source_type.is_primitive:\n                raise ValidationError(f\"Cannot partition by non-primitive source field: {source_field}\")\n            if not field.transform.can_transform(source_type):\n                raise ValidationError(\n                    f\"Invalid source field {source_field.name} with type {source_type} \" + f\"for transform: {field.transform}\"\n                )\n\n            # The only valid parent types for a PartitionField are StructTypes. This must be checked recursively\n            parent_id = parents.get(field.source_id)\n            while parent_id:\n                parent_type = schema.find_type(parent_id)\n                if not parent_type.is_struct:\n                    raise ValidationError(f\"Invalid partition field parent: {parent_type}\")\n                parent_id = parents.get(parent_id)\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Produce a boolean to return True if two objects are considered equal.</p> Note <p>Equality of PartitionSpec is determined by spec_id and partition fields only.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"\n    Produce a boolean to return True if two objects are considered equal.\n\n    Note:\n        Equality of PartitionSpec is determined by spec_id and partition fields only.\n    \"\"\"\n    if not isinstance(other, PartitionSpec):\n        return False\n    return self.spec_id == other.spec_id and self.fields == other.fields\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the PartitionSpec class.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the PartitionSpec class.\"\"\"\n    fields = f\"{', '.join(repr(column) for column in self.fields)}, \" if self.fields else \"\"\n    return f\"PartitionSpec({fields}spec_id={self.spec_id})\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.__str__","title":"<code>__str__()</code>","text":"<p>Produce a human-readable string representation of PartitionSpec.</p> Note <p>Only include list of partition fields in the PartitionSpec's string representation.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Produce a human-readable string representation of PartitionSpec.\n\n    Note:\n        Only include list of partition fields in the PartitionSpec's string representation.\n    \"\"\"\n    result_str = \"[\"\n    if self.fields:\n        result_str += \"\\n  \" + \"\\n  \".join([str(field) for field in self.fields]) + \"\\n\"\n    result_str += \"]\"\n    return result_str\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.compatible_with","title":"<code>compatible_with(other)</code>","text":"<p>Produce a boolean to return True if two PartitionSpec are considered compatible.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def compatible_with(self, other: PartitionSpec) -&gt; bool:\n    \"\"\"Produce a boolean to return True if two PartitionSpec are considered compatible.\"\"\"\n    if self == other:\n        return True\n    if len(self.fields) != len(other.fields):\n        return False\n    return all(\n        this_field.source_id == that_field.source_id\n        and this_field.transform == that_field.transform\n        and this_field.name == that_field.name\n        for this_field, that_field in zip(self.fields, other.fields, strict=True)\n    )\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpec.partition_type","title":"<code>partition_type(schema)</code>","text":"<p>Produce a struct of the PartitionSpec.</p> <p>The partition fields should be optional:</p> <ul> <li>All partition transforms are required to produce null if the input value is null, so it can   happen when the source column is optional.</li> <li>Partition fields may be added later, in which case not all files would have the result field,   and it may be null.</li> </ul> <p>There is a case where we can guarantee that a partition field in the first and only partition spec that uses a required source column will never be null, but it doesn't seem worth tracking this case.</p> <p>:param schema: The schema to bind to. :return: A StructType that represents the PartitionSpec, with a NestedField for each PartitionField.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def partition_type(self, schema: Schema) -&gt; StructType:\n    \"\"\"Produce a struct of the PartitionSpec.\n\n    The partition fields should be optional:\n\n    - All partition transforms are required to produce null if the input value is null, so it can\n      happen when the source column is optional.\n    - Partition fields may be added later, in which case not all files would have the result field,\n      and it may be null.\n\n    There is a case where we can guarantee that a partition field in the first and only partition spec\n    that uses a required source column will never be null, but it doesn't seem worth tracking this case.\n\n    :param schema: The schema to bind to.\n    :return: A StructType that represents the PartitionSpec, with a NestedField for each PartitionField.\n    \"\"\"\n    nested_fields = []\n    schema_ids = schema._lazy_id_to_field\n    for field in self.fields:\n        if source_field := schema_ids.get(field.source_id):\n            result_type = field.transform.result_type(source_field.field_type)\n            nested_fields.append(NestedField(field.field_id, field.name, result_type, required=source_field.required))\n        else:\n            # Since the source field has been drop we cannot determine the type\n            nested_fields.append(NestedField(field.field_id, field.name, UnknownType()))\n    return StructType(*nested_fields)\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor","title":"<code>PartitionSpecVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>class PartitionSpecVisitor(Generic[T], ABC):\n    @abstractmethod\n    def identity(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit identity partition field.\"\"\"\n\n    @abstractmethod\n    def bucket(self, field_id: int, source_name: str, source_id: int, num_buckets: int) -&gt; T:\n        \"\"\"Visit bucket partition field.\"\"\"\n\n    @abstractmethod\n    def truncate(self, field_id: int, source_name: str, source_id: int, width: int) -&gt; T:\n        \"\"\"Visit truncate partition field.\"\"\"\n\n    @abstractmethod\n    def year(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit year partition field.\"\"\"\n\n    @abstractmethod\n    def month(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit month partition field.\"\"\"\n\n    @abstractmethod\n    def day(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit day partition field.\"\"\"\n\n    @abstractmethod\n    def hour(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit hour partition field.\"\"\"\n\n    @abstractmethod\n    def always_null(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n        \"\"\"Visit void partition field.\"\"\"\n\n    @abstractmethod\n    def unknown(self, field_id: int, source_name: str, source_id: int, transform: str) -&gt; T:\n        \"\"\"Visit unknown partition field.\"\"\"\n        raise ValueError(f\"Unknown transform is not supported: {transform}\")\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.always_null","title":"<code>always_null(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit void partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef always_null(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit void partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.bucket","title":"<code>bucket(field_id, source_name, source_id, num_buckets)</code>  <code>abstractmethod</code>","text":"<p>Visit bucket partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef bucket(self, field_id: int, source_name: str, source_id: int, num_buckets: int) -&gt; T:\n    \"\"\"Visit bucket partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.day","title":"<code>day(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit day partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef day(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit day partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.hour","title":"<code>hour(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit hour partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef hour(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit hour partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.identity","title":"<code>identity(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit identity partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef identity(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit identity partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.month","title":"<code>month(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit month partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef month(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit month partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.truncate","title":"<code>truncate(field_id, source_name, source_id, width)</code>  <code>abstractmethod</code>","text":"<p>Visit truncate partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef truncate(self, field_id: int, source_name: str, source_id: int, width: int) -&gt; T:\n    \"\"\"Visit truncate partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.unknown","title":"<code>unknown(field_id, source_name, source_id, transform)</code>  <code>abstractmethod</code>","text":"<p>Visit unknown partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef unknown(self, field_id: int, source_name: str, source_id: int, transform: str) -&gt; T:\n    \"\"\"Visit unknown partition field.\"\"\"\n    raise ValueError(f\"Unknown transform is not supported: {transform}\")\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.PartitionSpecVisitor.year","title":"<code>year(field_id, source_name, source_id)</code>  <code>abstractmethod</code>","text":"<p>Visit year partition field.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>@abstractmethod\ndef year(self, field_id: int, source_name: str, source_id: int) -&gt; T:\n    \"\"\"Visit year partition field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.partition_record_value","title":"<code>partition_record_value(partition_field, value, schema)</code>","text":"<p>Return the Partition Record representation of the value.</p> <p>The value is first converted to internal partition representation. For example, UUID is converted to bytes[16], DateType to days since epoch, etc.</p> <p>Then the corresponding PartitionField's transform is applied to return the final partition record value.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def partition_record_value(partition_field: PartitionField, value: Any, schema: Schema) -&gt; Any:\n    \"\"\"\n    Return the Partition Record representation of the value.\n\n    The value is first converted to internal partition representation.\n    For example, UUID is converted to bytes[16], DateType to days since epoch, etc.\n\n    Then the corresponding PartitionField's transform is applied to return\n    the final partition record value.\n    \"\"\"\n    iceberg_type = schema.find_field(name_or_id=partition_field.source_id).field_type\n    return _to_partition_representation(iceberg_type, value)\n</code></pre>"},{"location":"reference/pyiceberg/partitioning/#pyiceberg.partitioning.validate_partition_name","title":"<code>validate_partition_name(field_name, partition_transform, source_id, schema, partition_names)</code>","text":"<p>Validate that a partition field name doesn't conflict with schema field names.</p> Source code in <code>pyiceberg/partitioning.py</code> <pre><code>def validate_partition_name(\n    field_name: str,\n    partition_transform: Transform[Any, Any],\n    source_id: int,\n    schema: Schema,\n    partition_names: set[str],\n) -&gt; None:\n    \"\"\"Validate that a partition field name doesn't conflict with schema field names.\"\"\"\n    try:\n        schema_field = schema.find_field(field_name)\n    except ValueError:\n        return  # No conflict if field doesn't exist in schema\n\n    if isinstance(partition_transform, (IdentityTransform, VoidTransform)):\n        # For identity and void transforms, allow conflict only if sourced from the same schema field\n        if schema_field.field_id != source_id:\n            raise ValueError(f\"Cannot create identity partition sourced from different field in schema: {field_name}\")\n    else:\n        raise ValueError(f\"Cannot create partition with a name that exists in schema: {field_name}\")\n    if not field_name:\n        raise ValueError(\"Undefined name\")\n    if field_name in partition_names:\n        raise ValueError(f\"Partition name has to be unique: {field_name}\")\n</code></pre>"},{"location":"reference/pyiceberg/schema/","title":"schema","text":""},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Accessor","title":"<code>Accessor</code>  <code>dataclass</code>","text":"<p>An accessor for a specific position in a container that implements the StructProtocol.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@dataclass(init=True, eq=True, frozen=True)\nclass Accessor:\n    \"\"\"An accessor for a specific position in a container that implements the StructProtocol.\"\"\"\n\n    position: int\n    inner: Accessor | None = None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Accessor class.\"\"\"\n        return f\"Accessor(position={self.position},inner={self.inner})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Accessor class.\"\"\"\n        return self.__str__()\n\n    def get(self, container: StructProtocol) -&gt; Any:\n        \"\"\"Return the value at self.position in `container`.\n\n        Args:\n            container (StructProtocol): A container to access at position `self.position`.\n\n        Returns:\n            Any: The value at position `self.position` in the container.\n        \"\"\"\n        pos = self.position\n        val = container[pos]\n        inner = self\n        while inner.inner:\n            inner = inner.inner\n            val = val[inner.position]\n\n        return val\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Accessor.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Accessor class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Accessor class.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Accessor.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Accessor class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Accessor class.\"\"\"\n    return f\"Accessor(position={self.position},inner={self.inner})\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Accessor.get","title":"<code>get(container)</code>","text":"<p>Return the value at self.position in <code>container</code>.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>StructProtocol</code> <p>A container to access at position <code>self.position</code>.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The value at position <code>self.position</code> in the container.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def get(self, container: StructProtocol) -&gt; Any:\n    \"\"\"Return the value at self.position in `container`.\n\n    Args:\n        container (StructProtocol): A container to access at position `self.position`.\n\n    Returns:\n        Any: The value at position `self.position` in the container.\n    \"\"\"\n    pos = self.position\n    val = container[pos]\n    inner = self\n    while inner.inner:\n        inner = inner.inner\n        val = val[inner.position]\n\n    return val\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor","title":"<code>PartnerAccessor</code>","text":"<p>               Bases: <code>Generic[P]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class PartnerAccessor(Generic[P], ABC):\n    @abstractmethod\n    def schema_partner(self, partner: P | None) -&gt; P | None:\n        \"\"\"Return the equivalent of the schema as a struct.\"\"\"\n\n    @abstractmethod\n    def field_partner(self, partner_struct: P | None, field_id: int, field_name: str) -&gt; P | None:\n        \"\"\"Return the equivalent struct field by name or id in the partner struct.\"\"\"\n\n    @abstractmethod\n    def list_element_partner(self, partner_list: P | None) -&gt; P | None:\n        \"\"\"Return the equivalent list element in the partner list.\"\"\"\n\n    @abstractmethod\n    def map_key_partner(self, partner_map: P | None) -&gt; P | None:\n        \"\"\"Return the equivalent map key in the partner map.\"\"\"\n\n    @abstractmethod\n    def map_value_partner(self, partner_map: P | None) -&gt; P | None:\n        \"\"\"Return the equivalent map value in the partner map.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.field_partner","title":"<code>field_partner(partner_struct, field_id, field_name)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent struct field by name or id in the partner struct.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef field_partner(self, partner_struct: P | None, field_id: int, field_name: str) -&gt; P | None:\n    \"\"\"Return the equivalent struct field by name or id in the partner struct.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.list_element_partner","title":"<code>list_element_partner(partner_list)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent list element in the partner list.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef list_element_partner(self, partner_list: P | None) -&gt; P | None:\n    \"\"\"Return the equivalent list element in the partner list.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.map_key_partner","title":"<code>map_key_partner(partner_map)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent map key in the partner map.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map_key_partner(self, partner_map: P | None) -&gt; P | None:\n    \"\"\"Return the equivalent map key in the partner map.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.map_value_partner","title":"<code>map_value_partner(partner_map)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent map value in the partner map.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map_value_partner(self, partner_map: P | None) -&gt; P | None:\n    \"\"\"Return the equivalent map value in the partner map.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PartnerAccessor.schema_partner","title":"<code>schema_partner(partner)</code>  <code>abstractmethod</code>","text":"<p>Return the equivalent of the schema as a struct.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef schema_partner(self, partner: P | None) -&gt; P | None:\n    \"\"\"Return the equivalent of the schema as a struct.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor","title":"<code>PreOrderSchemaVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class PreOrderSchemaVisitor(Generic[T], ABC):\n    @abstractmethod\n    def schema(self, schema: Schema, struct_result: Callable[[], T]) -&gt; T:\n        \"\"\"Visit a Schema.\"\"\"\n\n    @abstractmethod\n    def struct(self, struct: StructType, field_results: builtins.list[Callable[[], T]]) -&gt; T:\n        \"\"\"Visit a StructType.\"\"\"\n\n    @abstractmethod\n    def field(self, field: NestedField, field_result: Callable[[], T]) -&gt; T:\n        \"\"\"Visit a NestedField.\"\"\"\n\n    @abstractmethod\n    def list(self, list_type: ListType, element_result: Callable[[], T]) -&gt; T:\n        \"\"\"Visit a ListType.\"\"\"\n\n    @abstractmethod\n    def map(self, map_type: MapType, key_result: Callable[[], T], value_result: Callable[[], T]) -&gt; T:\n        \"\"\"Visit a MapType.\"\"\"\n\n    @abstractmethod\n    def primitive(self, primitive: PrimitiveType) -&gt; T:\n        \"\"\"Visit a PrimitiveType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.field","title":"<code>field(field, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a NestedField.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef field(self, field: NestedField, field_result: Callable[[], T]) -&gt; T:\n    \"\"\"Visit a NestedField.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.list","title":"<code>list(list_type, element_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef list(self, list_type: ListType, element_result: Callable[[], T]) -&gt; T:\n    \"\"\"Visit a ListType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.map","title":"<code>map(map_type, key_result, value_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map(self, map_type: MapType, key_result: Callable[[], T], value_result: Callable[[], T]) -&gt; T:\n    \"\"\"Visit a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.primitive","title":"<code>primitive(primitive)</code>  <code>abstractmethod</code>","text":"<p>Visit a PrimitiveType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef primitive(self, primitive: PrimitiveType) -&gt; T:\n    \"\"\"Visit a PrimitiveType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.schema","title":"<code>schema(schema, struct_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a Schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef schema(self, schema: Schema, struct_result: Callable[[], T]) -&gt; T:\n    \"\"\"Visit a Schema.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PreOrderSchemaVisitor.struct","title":"<code>struct(struct, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a StructType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef struct(self, struct: StructType, field_results: builtins.list[Callable[[], T]]) -&gt; T:\n    \"\"\"Visit a StructType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor","title":"<code>PrimitiveWithPartnerVisitor</code>","text":"<p>               Bases: <code>SchemaWithPartnerVisitor[P, T]</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class PrimitiveWithPartnerVisitor(SchemaWithPartnerVisitor[P, T]):\n    def primitive(self, primitive: PrimitiveType, primitive_partner: P | None) -&gt; T:\n        \"\"\"Visit a PrimitiveType.\"\"\"\n        if isinstance(primitive, BooleanType):\n            return self.visit_boolean(primitive, primitive_partner)\n        elif isinstance(primitive, IntegerType):\n            return self.visit_integer(primitive, primitive_partner)\n        elif isinstance(primitive, LongType):\n            return self.visit_long(primitive, primitive_partner)\n        elif isinstance(primitive, FloatType):\n            return self.visit_float(primitive, primitive_partner)\n        elif isinstance(primitive, DoubleType):\n            return self.visit_double(primitive, primitive_partner)\n        elif isinstance(primitive, DecimalType):\n            return self.visit_decimal(primitive, primitive_partner)\n        elif isinstance(primitive, DateType):\n            return self.visit_date(primitive, primitive_partner)\n        elif isinstance(primitive, TimeType):\n            return self.visit_time(primitive, primitive_partner)\n        elif isinstance(primitive, TimestampType):\n            return self.visit_timestamp(primitive, primitive_partner)\n        elif isinstance(primitive, TimestampNanoType):\n            return self.visit_timestamp_ns(primitive, primitive_partner)\n        elif isinstance(primitive, TimestamptzType):\n            return self.visit_timestamptz(primitive, primitive_partner)\n        elif isinstance(primitive, TimestamptzNanoType):\n            return self.visit_timestamptz_ns(primitive, primitive_partner)\n        elif isinstance(primitive, StringType):\n            return self.visit_string(primitive, primitive_partner)\n        elif isinstance(primitive, UUIDType):\n            return self.visit_uuid(primitive, primitive_partner)\n        elif isinstance(primitive, FixedType):\n            return self.visit_fixed(primitive, primitive_partner)\n        elif isinstance(primitive, BinaryType):\n            return self.visit_binary(primitive, primitive_partner)\n        elif isinstance(primitive, UnknownType):\n            return self.visit_unknown(primitive, primitive_partner)\n        else:\n            raise ValueError(f\"Type not recognized: {primitive}\")\n\n    @abstractmethod\n    def visit_boolean(self, boolean_type: BooleanType, partner: P | None) -&gt; T:\n        \"\"\"Visit a BooleanType.\"\"\"\n\n    @abstractmethod\n    def visit_integer(self, integer_type: IntegerType, partner: P | None) -&gt; T:\n        \"\"\"Visit a IntegerType.\"\"\"\n\n    @abstractmethod\n    def visit_long(self, long_type: LongType, partner: P | None) -&gt; T:\n        \"\"\"Visit a LongType.\"\"\"\n\n    @abstractmethod\n    def visit_float(self, float_type: FloatType, partner: P | None) -&gt; T:\n        \"\"\"Visit a FloatType.\"\"\"\n\n    @abstractmethod\n    def visit_double(self, double_type: DoubleType, partner: P | None) -&gt; T:\n        \"\"\"Visit a DoubleType.\"\"\"\n\n    @abstractmethod\n    def visit_decimal(self, decimal_type: DecimalType, partner: P | None) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_date(self, date_type: DateType, partner: P | None) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_time(self, time_type: TimeType, partner: P | None) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamp(self, timestamp_type: TimestampType, partner: P | None) -&gt; T:\n        \"\"\"Visit a TimestampType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamp_ns(self, timestamp_ns_type: TimestampNanoType, partner: P | None) -&gt; T:\n        \"\"\"Visit a TimestampNanoType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamptz(self, timestamptz_type: TimestamptzType, partner: P | None) -&gt; T:\n        \"\"\"Visit a TimestamptzType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamptz_ns(self, timestamptz_ns_type: TimestamptzNanoType, partner: P | None) -&gt; T:\n        \"\"\"Visit a TimestamptzNanoType.\"\"\"\n\n    @abstractmethod\n    def visit_string(self, string_type: StringType, partner: P | None) -&gt; T:\n        \"\"\"Visit a StringType.\"\"\"\n\n    @abstractmethod\n    def visit_uuid(self, uuid_type: UUIDType, partner: P | None) -&gt; T:\n        \"\"\"Visit a UUIDType.\"\"\"\n\n    @abstractmethod\n    def visit_fixed(self, fixed_type: FixedType, partner: P | None) -&gt; T:\n        \"\"\"Visit a FixedType.\"\"\"\n\n    @abstractmethod\n    def visit_binary(self, binary_type: BinaryType, partner: P | None) -&gt; T:\n        \"\"\"Visit a BinaryType.\"\"\"\n\n    @abstractmethod\n    def visit_unknown(self, unknown_type: UnknownType, partner: P | None) -&gt; T:\n        \"\"\"Visit a UnknownType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.primitive","title":"<code>primitive(primitive, primitive_partner)</code>","text":"<p>Visit a PrimitiveType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def primitive(self, primitive: PrimitiveType, primitive_partner: P | None) -&gt; T:\n    \"\"\"Visit a PrimitiveType.\"\"\"\n    if isinstance(primitive, BooleanType):\n        return self.visit_boolean(primitive, primitive_partner)\n    elif isinstance(primitive, IntegerType):\n        return self.visit_integer(primitive, primitive_partner)\n    elif isinstance(primitive, LongType):\n        return self.visit_long(primitive, primitive_partner)\n    elif isinstance(primitive, FloatType):\n        return self.visit_float(primitive, primitive_partner)\n    elif isinstance(primitive, DoubleType):\n        return self.visit_double(primitive, primitive_partner)\n    elif isinstance(primitive, DecimalType):\n        return self.visit_decimal(primitive, primitive_partner)\n    elif isinstance(primitive, DateType):\n        return self.visit_date(primitive, primitive_partner)\n    elif isinstance(primitive, TimeType):\n        return self.visit_time(primitive, primitive_partner)\n    elif isinstance(primitive, TimestampType):\n        return self.visit_timestamp(primitive, primitive_partner)\n    elif isinstance(primitive, TimestampNanoType):\n        return self.visit_timestamp_ns(primitive, primitive_partner)\n    elif isinstance(primitive, TimestamptzType):\n        return self.visit_timestamptz(primitive, primitive_partner)\n    elif isinstance(primitive, TimestamptzNanoType):\n        return self.visit_timestamptz_ns(primitive, primitive_partner)\n    elif isinstance(primitive, StringType):\n        return self.visit_string(primitive, primitive_partner)\n    elif isinstance(primitive, UUIDType):\n        return self.visit_uuid(primitive, primitive_partner)\n    elif isinstance(primitive, FixedType):\n        return self.visit_fixed(primitive, primitive_partner)\n    elif isinstance(primitive, BinaryType):\n        return self.visit_binary(primitive, primitive_partner)\n    elif isinstance(primitive, UnknownType):\n        return self.visit_unknown(primitive, primitive_partner)\n    else:\n        raise ValueError(f\"Type not recognized: {primitive}\")\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_binary","title":"<code>visit_binary(binary_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a BinaryType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_binary(self, binary_type: BinaryType, partner: P | None) -&gt; T:\n    \"\"\"Visit a BinaryType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_boolean","title":"<code>visit_boolean(boolean_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a BooleanType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_boolean(self, boolean_type: BooleanType, partner: P | None) -&gt; T:\n    \"\"\"Visit a BooleanType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_date","title":"<code>visit_date(date_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_date(self, date_type: DateType, partner: P | None) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_decimal","title":"<code>visit_decimal(decimal_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_decimal(self, decimal_type: DecimalType, partner: P | None) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_double","title":"<code>visit_double(double_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a DoubleType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_double(self, double_type: DoubleType, partner: P | None) -&gt; T:\n    \"\"\"Visit a DoubleType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_fixed","title":"<code>visit_fixed(fixed_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a FixedType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_fixed(self, fixed_type: FixedType, partner: P | None) -&gt; T:\n    \"\"\"Visit a FixedType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_float","title":"<code>visit_float(float_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a FloatType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_float(self, float_type: FloatType, partner: P | None) -&gt; T:\n    \"\"\"Visit a FloatType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_integer","title":"<code>visit_integer(integer_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a IntegerType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_integer(self, integer_type: IntegerType, partner: P | None) -&gt; T:\n    \"\"\"Visit a IntegerType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_long","title":"<code>visit_long(long_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a LongType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_long(self, long_type: LongType, partner: P | None) -&gt; T:\n    \"\"\"Visit a LongType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_string","title":"<code>visit_string(string_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a StringType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_string(self, string_type: StringType, partner: P | None) -&gt; T:\n    \"\"\"Visit a StringType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_time","title":"<code>visit_time(time_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_time(self, time_type: TimeType, partner: P | None) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_timestamp","title":"<code>visit_timestamp(timestamp_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestampType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamp(self, timestamp_type: TimestampType, partner: P | None) -&gt; T:\n    \"\"\"Visit a TimestampType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_timestamp_ns","title":"<code>visit_timestamp_ns(timestamp_ns_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestampNanoType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamp_ns(self, timestamp_ns_type: TimestampNanoType, partner: P | None) -&gt; T:\n    \"\"\"Visit a TimestampNanoType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_timestamptz","title":"<code>visit_timestamptz(timestamptz_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestamptzType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamptz(self, timestamptz_type: TimestamptzType, partner: P | None) -&gt; T:\n    \"\"\"Visit a TimestamptzType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_timestamptz_ns","title":"<code>visit_timestamptz_ns(timestamptz_ns_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestamptzNanoType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamptz_ns(self, timestamptz_ns_type: TimestamptzNanoType, partner: P | None) -&gt; T:\n    \"\"\"Visit a TimestamptzNanoType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_unknown","title":"<code>visit_unknown(unknown_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a UnknownType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_unknown(self, unknown_type: UnknownType, partner: P | None) -&gt; T:\n    \"\"\"Visit a UnknownType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.PrimitiveWithPartnerVisitor.visit_uuid","title":"<code>visit_uuid(uuid_type, partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a UUIDType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_uuid(self, uuid_type: UUIDType, partner: P | None) -&gt; T:\n    \"\"\"Visit a UUIDType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema","title":"<code>Schema</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>A table Schema.</p> Example <p>from pyiceberg import schema from pyiceberg import types</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class Schema(IcebergBaseModel):\n    \"\"\"A table Schema.\n\n    Example:\n        &gt;&gt;&gt; from pyiceberg import schema\n        &gt;&gt;&gt; from pyiceberg import types\n    \"\"\"\n\n    type: Literal[\"struct\"] = \"struct\"\n    fields: tuple[NestedField, ...] = Field(default_factory=tuple)\n    schema_id: int = Field(alias=\"schema-id\", default=INITIAL_SCHEMA_ID)\n    identifier_field_ids: list[int] = Field(alias=\"identifier-field-ids\", default_factory=list)\n\n    _name_to_id: dict[str, int] = PrivateAttr()\n\n    def __init__(self, *fields: NestedField, **data: Any):\n        if fields:\n            data[\"fields\"] = fields\n        super().__init__(**data)\n        self._name_to_id = index_by_name(self)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Schema class.\"\"\"\n        return \"table {\\n\" + \"\\n\".join([\"  \" + str(field) for field in self.columns]) + \"\\n}\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Schema class.\"\"\"\n        columns_repr = \", \".join(repr(column) for column in self.columns)\n        return f\"Schema({columns_repr}, schema_id={self.schema_id}, identifier_field_ids={self.identifier_field_ids})\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of an instance of the Literal class.\"\"\"\n        return len(self.fields)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Schema class.\"\"\"\n        if not other:\n            return False\n\n        if not isinstance(other, Schema):\n            return False\n\n        if len(self.columns) != len(other.columns):\n            return False\n\n        identifier_field_ids_is_equal = self.identifier_field_ids == other.identifier_field_ids\n        schema_is_equal = all(lhs == rhs for lhs, rhs in zip(self.columns, other.columns, strict=True))\n\n        return identifier_field_ids_is_equal and schema_is_equal\n\n    @model_validator(mode=\"after\")\n    def check_schema(self) -&gt; Schema:\n        if self.identifier_field_ids:\n            for field_id in self.identifier_field_ids:\n                self._validate_identifier_field(field_id)\n\n        return self\n\n    @property\n    def columns(self) -&gt; tuple[NestedField, ...]:\n        \"\"\"A tuple of the top-level fields.\"\"\"\n        return self.fields\n\n    @cached_property\n    def _lazy_id_to_field(self) -&gt; dict[int, NestedField]:\n        \"\"\"Return an index of field ID to NestedField instance.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return index_by_id(self)\n\n    @cached_property\n    def _lazy_id_to_parent(self) -&gt; dict[int, int]:\n        \"\"\"Returns an index of field ID to parent field IDs.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return _index_parents(self)\n\n    @cached_property\n    def _lazy_name_to_id_lower(self) -&gt; dict[str, int]:\n        \"\"\"Return an index of lower-case field names to field IDs.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return {name.lower(): field_id for name, field_id in self._name_to_id.items()}\n\n    @cached_property\n    def _lazy_id_to_name(self) -&gt; dict[int, str]:\n        \"\"\"Return an index of field ID to full name.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return index_name_by_id(self)\n\n    @cached_property\n    def _lazy_id_to_accessor(self) -&gt; dict[int, Accessor]:\n        \"\"\"Return an index of field ID to accessor.\n\n        This is calculated once when called for the first time. Subsequent calls to this method will use a cached index.\n        \"\"\"\n        return build_position_accessors(self)\n\n    def as_struct(self) -&gt; StructType:\n        \"\"\"Return the schema as a struct.\"\"\"\n        return StructType(*self.fields)\n\n    def as_arrow(self) -&gt; pa.Schema:\n        \"\"\"Return the schema as an Arrow schema.\"\"\"\n        from pyiceberg.io.pyarrow import schema_to_pyarrow\n\n        return schema_to_pyarrow(self)\n\n    def find_field(self, name_or_id: str | int, case_sensitive: bool = True) -&gt; NestedField:\n        \"\"\"Find a field using a field name or field ID.\n\n        Args:\n            name_or_id (Union[str, int]): Either a field name or a field ID.\n            case_sensitive (bool, optional): Whether to perform a case-sensitive lookup using a field name. Defaults to True.\n\n        Raises:\n            ValueError: When the value cannot be found.\n\n        Returns:\n            NestedField: The matched NestedField.\n        \"\"\"\n        if isinstance(name_or_id, int):\n            if name_or_id not in self._lazy_id_to_field:\n                raise ValueError(f\"Could not find field with id: {name_or_id}\")\n            return self._lazy_id_to_field[name_or_id]\n\n        if case_sensitive:\n            field_id = self._name_to_id.get(name_or_id)\n        else:\n            field_id = self._lazy_name_to_id_lower.get(name_or_id.lower())\n\n        if field_id is None:\n            raise ValueError(f\"Could not find field with name {name_or_id}, case_sensitive={case_sensitive}\")\n\n        return self._lazy_id_to_field[field_id]\n\n    def find_type(self, name_or_id: str | int, case_sensitive: bool = True) -&gt; IcebergType:\n        \"\"\"Find a field type using a field name or field ID.\n\n        Args:\n            name_or_id (Union[str, int]): Either a field name or a field ID.\n            case_sensitive (bool, optional): Whether to perform a case-sensitive lookup using a field name. Defaults to True.\n\n        Returns:\n            NestedField: The type of the matched NestedField.\n        \"\"\"\n        field = self.find_field(name_or_id=name_or_id, case_sensitive=case_sensitive)\n        if not field:\n            raise ValueError(f\"Could not find field with name or id {name_or_id}, case_sensitive={case_sensitive}\")\n        return field.field_type\n\n    @property\n    def highest_field_id(self) -&gt; int:\n        return max(self._lazy_id_to_name.keys(), default=0)\n\n    @cached_property\n    def name_mapping(self) -&gt; NameMapping:\n        from pyiceberg.table.name_mapping import create_mapping_from_schema\n\n        return create_mapping_from_schema(self)\n\n    def find_column_name(self, column_id: int) -&gt; str | None:\n        \"\"\"Find a column name given a column ID.\n\n        Args:\n            column_id (int): The ID of the column.\n\n        Returns:\n            str: The column name (or None if the column ID cannot be found).\n        \"\"\"\n        return self._lazy_id_to_name.get(column_id)\n\n    @property\n    def column_names(self) -&gt; list[str]:\n        \"\"\"\n        Return a list of all the column names, including nested fields.\n\n        Excludes short names.\n\n        Returns:\n            List[str]: The column names.\n        \"\"\"\n        return list(self._lazy_id_to_name.values())\n\n    def accessor_for_field(self, field_id: int) -&gt; Accessor:\n        \"\"\"Find a schema position accessor given a field ID.\n\n        Args:\n            field_id (int): The ID of the field.\n\n        Raises:\n            ValueError: When the value cannot be found.\n\n        Returns:\n            Accessor: An accessor for the given field ID.\n        \"\"\"\n        if field_id not in self._lazy_id_to_accessor:\n            raise ValueError(f\"Could not find accessor for field with id: {field_id}\")\n\n        return self._lazy_id_to_accessor[field_id]\n\n    def identifier_field_names(self) -&gt; set[str]:\n        \"\"\"Return the names of the identifier fields.\n\n        Returns:\n            Set of names of the identifier fields\n        \"\"\"\n        ids = set()\n        for field_id in self.identifier_field_ids:\n            column_name = self.find_column_name(field_id)\n            if column_name is None:\n                raise ValueError(f\"Could not find identifier column id: {field_id}\")\n            ids.add(column_name)\n\n        return ids\n\n    def select(self, *names: str, case_sensitive: bool = True) -&gt; Schema:\n        \"\"\"Return a new schema instance pruned to a subset of columns.\n\n        Args:\n            names (List[str]): A list of column names.\n            case_sensitive (bool, optional): Whether to perform a case-sensitive lookup for each column name. Defaults to True.\n\n        Returns:\n            Schema: A new schema with pruned columns.\n\n        Raises:\n            ValueError: If a column is selected that doesn't exist.\n        \"\"\"\n        try:\n            if case_sensitive:\n                ids = {self._name_to_id[name] for name in names}\n            else:\n                ids = {self._lazy_name_to_id_lower[name.lower()] for name in names}\n        except KeyError as e:\n            raise ValueError(f\"Could not find column: {e}\") from e\n\n        return prune_columns(self, ids)\n\n    @property\n    def field_ids(self) -&gt; set[int]:\n        \"\"\"Return the IDs of the current schema.\"\"\"\n        return set(self._name_to_id.values())\n\n    def _validate_identifier_field(self, field_id: int) -&gt; None:\n        \"\"\"Validate that the field with the given ID is a valid identifier field.\n\n        Args:\n          field_id: The ID of the field to validate.\n\n        Raises:\n          ValueError: If the field is not valid.\n        \"\"\"\n        field = self.find_field(field_id)\n        if not field.field_type.is_primitive:\n            raise ValueError(f\"Identifier field {field_id} invalid: not a primitive type field\")\n\n        if not field.required:\n            raise ValueError(f\"Identifier field {field_id} invalid: not a required field\")\n\n        if isinstance(field.field_type, (DoubleType, FloatType)):\n            raise ValueError(f\"Identifier field {field_id} invalid: must not be float or double field\")\n\n        # Check whether the nested field is in a chain of required struct fields\n        # Exploring from root for better error message for list and map types\n        parent_id = self._lazy_id_to_parent.get(field.field_id)\n        fields: list[int] = []\n        while parent_id is not None:\n            fields.append(parent_id)\n            parent_id = self._lazy_id_to_parent.get(parent_id)\n\n        while fields:\n            parent = self.find_field(fields.pop())\n            if not parent.field_type.is_struct:\n                raise ValueError(f\"Cannot add field {field.name} as an identifier field: must not be nested in {parent}\")\n\n            if not parent.required:\n                raise ValueError(\n                    f\"Cannot add field {field.name} as an identifier field: must not be nested in an optional field {parent}\"\n                )\n\n    def check_format_version_compatibility(self, format_version: int) -&gt; None:\n        \"\"\"Check that the schema is compatible for the given table format version.\n\n        Args:\n          format_version: The Iceberg table format version.\n\n        Raises:\n          ValueError: If the schema is not compatible for the format version.\n        \"\"\"\n        for field in self._lazy_id_to_field.values():\n            if format_version &lt; field.field_type.minimum_format_version():\n                raise ValueError(\n                    f\"{field.field_type} is only supported in {field.field_type.minimum_format_version()} or higher. \"\n                    f\"Current format version is: {format_version}\"\n                )\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.column_names","title":"<code>column_names</code>  <code>property</code>","text":"<p>Return a list of all the column names, including nested fields.</p> <p>Excludes short names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List[str]: The column names.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.columns","title":"<code>columns</code>  <code>property</code>","text":"<p>A tuple of the top-level fields.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.field_ids","title":"<code>field_ids</code>  <code>property</code>","text":"<p>Return the IDs of the current schema.</p>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Schema class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Schema class.\"\"\"\n    if not other:\n        return False\n\n    if not isinstance(other, Schema):\n        return False\n\n    if len(self.columns) != len(other.columns):\n        return False\n\n    identifier_field_ids_is_equal = self.identifier_field_ids == other.identifier_field_ids\n    schema_is_equal = all(lhs == rhs for lhs, rhs in zip(self.columns, other.columns, strict=True))\n\n    return identifier_field_ids_is_equal and schema_is_equal\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of an instance of the Literal class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of an instance of the Literal class.\"\"\"\n    return len(self.fields)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Schema class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Schema class.\"\"\"\n    columns_repr = \", \".join(repr(column) for column in self.columns)\n    return f\"Schema({columns_repr}, schema_id={self.schema_id}, identifier_field_ids={self.identifier_field_ids})\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Schema class.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Schema class.\"\"\"\n    return \"table {\\n\" + \"\\n\".join([\"  \" + str(field) for field in self.columns]) + \"\\n}\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.accessor_for_field","title":"<code>accessor_for_field(field_id)</code>","text":"<p>Find a schema position accessor given a field ID.</p> <p>Parameters:</p> Name Type Description Default <code>field_id</code> <code>int</code> <p>The ID of the field.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>When the value cannot be found.</p> <p>Returns:</p> Name Type Description <code>Accessor</code> <code>Accessor</code> <p>An accessor for the given field ID.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def accessor_for_field(self, field_id: int) -&gt; Accessor:\n    \"\"\"Find a schema position accessor given a field ID.\n\n    Args:\n        field_id (int): The ID of the field.\n\n    Raises:\n        ValueError: When the value cannot be found.\n\n    Returns:\n        Accessor: An accessor for the given field ID.\n    \"\"\"\n    if field_id not in self._lazy_id_to_accessor:\n        raise ValueError(f\"Could not find accessor for field with id: {field_id}\")\n\n    return self._lazy_id_to_accessor[field_id]\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.as_arrow","title":"<code>as_arrow()</code>","text":"<p>Return the schema as an Arrow schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def as_arrow(self) -&gt; pa.Schema:\n    \"\"\"Return the schema as an Arrow schema.\"\"\"\n    from pyiceberg.io.pyarrow import schema_to_pyarrow\n\n    return schema_to_pyarrow(self)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.as_struct","title":"<code>as_struct()</code>","text":"<p>Return the schema as a struct.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def as_struct(self) -&gt; StructType:\n    \"\"\"Return the schema as a struct.\"\"\"\n    return StructType(*self.fields)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.check_format_version_compatibility","title":"<code>check_format_version_compatibility(format_version)</code>","text":"<p>Check that the schema is compatible for the given table format version.</p> <p>Parameters:</p> Name Type Description Default <code>format_version</code> <code>int</code> <p>The Iceberg table format version.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the schema is not compatible for the format version.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def check_format_version_compatibility(self, format_version: int) -&gt; None:\n    \"\"\"Check that the schema is compatible for the given table format version.\n\n    Args:\n      format_version: The Iceberg table format version.\n\n    Raises:\n      ValueError: If the schema is not compatible for the format version.\n    \"\"\"\n    for field in self._lazy_id_to_field.values():\n        if format_version &lt; field.field_type.minimum_format_version():\n            raise ValueError(\n                f\"{field.field_type} is only supported in {field.field_type.minimum_format_version()} or higher. \"\n                f\"Current format version is: {format_version}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.find_column_name","title":"<code>find_column_name(column_id)</code>","text":"<p>Find a column name given a column ID.</p> <p>Parameters:</p> Name Type Description Default <code>column_id</code> <code>int</code> <p>The ID of the column.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str | None</code> <p>The column name (or None if the column ID cannot be found).</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def find_column_name(self, column_id: int) -&gt; str | None:\n    \"\"\"Find a column name given a column ID.\n\n    Args:\n        column_id (int): The ID of the column.\n\n    Returns:\n        str: The column name (or None if the column ID cannot be found).\n    \"\"\"\n    return self._lazy_id_to_name.get(column_id)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.find_field","title":"<code>find_field(name_or_id, case_sensitive=True)</code>","text":"<p>Find a field using a field name or field ID.</p> <p>Parameters:</p> Name Type Description Default <code>name_or_id</code> <code>Union[str, int]</code> <p>Either a field name or a field ID.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to perform a case-sensitive lookup using a field name. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the value cannot be found.</p> <p>Returns:</p> Name Type Description <code>NestedField</code> <code>NestedField</code> <p>The matched NestedField.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def find_field(self, name_or_id: str | int, case_sensitive: bool = True) -&gt; NestedField:\n    \"\"\"Find a field using a field name or field ID.\n\n    Args:\n        name_or_id (Union[str, int]): Either a field name or a field ID.\n        case_sensitive (bool, optional): Whether to perform a case-sensitive lookup using a field name. Defaults to True.\n\n    Raises:\n        ValueError: When the value cannot be found.\n\n    Returns:\n        NestedField: The matched NestedField.\n    \"\"\"\n    if isinstance(name_or_id, int):\n        if name_or_id not in self._lazy_id_to_field:\n            raise ValueError(f\"Could not find field with id: {name_or_id}\")\n        return self._lazy_id_to_field[name_or_id]\n\n    if case_sensitive:\n        field_id = self._name_to_id.get(name_or_id)\n    else:\n        field_id = self._lazy_name_to_id_lower.get(name_or_id.lower())\n\n    if field_id is None:\n        raise ValueError(f\"Could not find field with name {name_or_id}, case_sensitive={case_sensitive}\")\n\n    return self._lazy_id_to_field[field_id]\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.find_type","title":"<code>find_type(name_or_id, case_sensitive=True)</code>","text":"<p>Find a field type using a field name or field ID.</p> <p>Parameters:</p> Name Type Description Default <code>name_or_id</code> <code>Union[str, int]</code> <p>Either a field name or a field ID.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to perform a case-sensitive lookup using a field name. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>NestedField</code> <code>IcebergType</code> <p>The type of the matched NestedField.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def find_type(self, name_or_id: str | int, case_sensitive: bool = True) -&gt; IcebergType:\n    \"\"\"Find a field type using a field name or field ID.\n\n    Args:\n        name_or_id (Union[str, int]): Either a field name or a field ID.\n        case_sensitive (bool, optional): Whether to perform a case-sensitive lookup using a field name. Defaults to True.\n\n    Returns:\n        NestedField: The type of the matched NestedField.\n    \"\"\"\n    field = self.find_field(name_or_id=name_or_id, case_sensitive=case_sensitive)\n    if not field:\n        raise ValueError(f\"Could not find field with name or id {name_or_id}, case_sensitive={case_sensitive}\")\n    return field.field_type\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.identifier_field_names","title":"<code>identifier_field_names()</code>","text":"<p>Return the names of the identifier fields.</p> <p>Returns:</p> Type Description <code>set[str]</code> <p>Set of names of the identifier fields</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def identifier_field_names(self) -&gt; set[str]:\n    \"\"\"Return the names of the identifier fields.\n\n    Returns:\n        Set of names of the identifier fields\n    \"\"\"\n    ids = set()\n    for field_id in self.identifier_field_ids:\n        column_name = self.find_column_name(field_id)\n        if column_name is None:\n            raise ValueError(f\"Could not find identifier column id: {field_id}\")\n        ids.add(column_name)\n\n    return ids\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.Schema.select","title":"<code>select(*names, case_sensitive=True)</code>","text":"<p>Return a new schema instance pruned to a subset of columns.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>List[str]</code> <p>A list of column names.</p> <code>()</code> <code>case_sensitive</code> <code>bool</code> <p>Whether to perform a case-sensitive lookup for each column name. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Schema</code> <code>Schema</code> <p>A new schema with pruned columns.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a column is selected that doesn't exist.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def select(self, *names: str, case_sensitive: bool = True) -&gt; Schema:\n    \"\"\"Return a new schema instance pruned to a subset of columns.\n\n    Args:\n        names (List[str]): A list of column names.\n        case_sensitive (bool, optional): Whether to perform a case-sensitive lookup for each column name. Defaults to True.\n\n    Returns:\n        Schema: A new schema with pruned columns.\n\n    Raises:\n        ValueError: If a column is selected that doesn't exist.\n    \"\"\"\n    try:\n        if case_sensitive:\n            ids = {self._name_to_id[name] for name in names}\n        else:\n            ids = {self._lazy_name_to_id_lower[name.lower()] for name in names}\n    except KeyError as e:\n        raise ValueError(f\"Could not find column: {e}\") from e\n\n    return prune_columns(self, ids)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor","title":"<code>SchemaVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class SchemaVisitor(Generic[T], ABC):\n    def before_field(self, field: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n\n    def after_field(self, field: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n\n    def before_list_element(self, element: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n        self.before_field(element)\n\n    def after_list_element(self, element: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n        self.after_field(element)\n\n    def before_map_key(self, key: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n        self.before_field(key)\n\n    def after_map_key(self, key: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n        self.after_field(key)\n\n    def before_map_value(self, value: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n        self.before_field(value)\n\n    def after_map_value(self, value: NestedField) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n        self.after_field(value)\n\n    @abstractmethod\n    def schema(self, schema: Schema, struct_result: T) -&gt; T:\n        \"\"\"Visit a Schema.\"\"\"\n\n    @abstractmethod\n    def struct(self, struct: StructType, field_results: builtins.list[T]) -&gt; T:\n        \"\"\"Visit a StructType.\"\"\"\n\n    @abstractmethod\n    def field(self, field: NestedField, field_result: T) -&gt; T:\n        \"\"\"Visit a NestedField.\"\"\"\n\n    @abstractmethod\n    def list(self, list_type: ListType, element_result: T) -&gt; T:\n        \"\"\"Visit a ListType.\"\"\"\n\n    @abstractmethod\n    def map(self, map_type: MapType, key_result: T, value_result: T) -&gt; T:\n        \"\"\"Visit a MapType.\"\"\"\n\n    @abstractmethod\n    def primitive(self, primitive: PrimitiveType) -&gt; T:\n        \"\"\"Visit a PrimitiveType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.after_field","title":"<code>after_field(field)</code>","text":"<p>Override this method to perform an action immediately after visiting a field.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_field(self, field: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.after_list_element","title":"<code>after_list_element(element)</code>","text":"<p>Override this method to perform an action immediately after visiting an element within a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_list_element(self, element: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n    self.after_field(element)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.after_map_key","title":"<code>after_map_key(key)</code>","text":"<p>Override this method to perform an action immediately after visiting a key within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_map_key(self, key: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n    self.after_field(key)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.after_map_value","title":"<code>after_map_value(value)</code>","text":"<p>Override this method to perform an action immediately after visiting a value within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_map_value(self, value: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n    self.after_field(value)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.before_field","title":"<code>before_field(field)</code>","text":"<p>Override this method to perform an action immediately before visiting a field.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_field(self, field: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.before_list_element","title":"<code>before_list_element(element)</code>","text":"<p>Override this method to perform an action immediately before visiting an element within a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_list_element(self, element: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n    self.before_field(element)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.before_map_key","title":"<code>before_map_key(key)</code>","text":"<p>Override this method to perform an action immediately before visiting a key within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_map_key(self, key: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n    self.before_field(key)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.before_map_value","title":"<code>before_map_value(value)</code>","text":"<p>Override this method to perform an action immediately before visiting a value within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_map_value(self, value: NestedField) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n    self.before_field(value)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.field","title":"<code>field(field, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a NestedField.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef field(self, field: NestedField, field_result: T) -&gt; T:\n    \"\"\"Visit a NestedField.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.list","title":"<code>list(list_type, element_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef list(self, list_type: ListType, element_result: T) -&gt; T:\n    \"\"\"Visit a ListType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.map","title":"<code>map(map_type, key_result, value_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map(self, map_type: MapType, key_result: T, value_result: T) -&gt; T:\n    \"\"\"Visit a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.primitive","title":"<code>primitive(primitive)</code>  <code>abstractmethod</code>","text":"<p>Visit a PrimitiveType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef primitive(self, primitive: PrimitiveType) -&gt; T:\n    \"\"\"Visit a PrimitiveType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.schema","title":"<code>schema(schema, struct_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a Schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef schema(self, schema: Schema, struct_result: T) -&gt; T:\n    \"\"\"Visit a Schema.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitor.struct","title":"<code>struct(struct, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a StructType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef struct(self, struct: StructType, field_results: builtins.list[T]) -&gt; T:\n    \"\"\"Visit a StructType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType","title":"<code>SchemaVisitorPerPrimitiveType</code>","text":"<p>               Bases: <code>SchemaVisitor[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class SchemaVisitorPerPrimitiveType(SchemaVisitor[T], ABC):\n    def primitive(self, primitive: PrimitiveType) -&gt; T:\n        \"\"\"Visit a PrimitiveType.\"\"\"\n        if isinstance(primitive, FixedType):\n            return self.visit_fixed(primitive)\n        elif isinstance(primitive, DecimalType):\n            return self.visit_decimal(primitive)\n        elif isinstance(primitive, BooleanType):\n            return self.visit_boolean(primitive)\n        elif isinstance(primitive, IntegerType):\n            return self.visit_integer(primitive)\n        elif isinstance(primitive, LongType):\n            return self.visit_long(primitive)\n        elif isinstance(primitive, FloatType):\n            return self.visit_float(primitive)\n        elif isinstance(primitive, DoubleType):\n            return self.visit_double(primitive)\n        elif isinstance(primitive, DateType):\n            return self.visit_date(primitive)\n        elif isinstance(primitive, TimeType):\n            return self.visit_time(primitive)\n        elif isinstance(primitive, TimestampType):\n            return self.visit_timestamp(primitive)\n        elif isinstance(primitive, TimestampNanoType):\n            return self.visit_timestamp_ns(primitive)\n        elif isinstance(primitive, TimestamptzType):\n            return self.visit_timestamptz(primitive)\n        elif isinstance(primitive, TimestamptzNanoType):\n            return self.visit_timestamptz_ns(primitive)\n        elif isinstance(primitive, StringType):\n            return self.visit_string(primitive)\n        elif isinstance(primitive, UUIDType):\n            return self.visit_uuid(primitive)\n        elif isinstance(primitive, BinaryType):\n            return self.visit_binary(primitive)\n        elif isinstance(primitive, UnknownType):\n            return self.visit_unknown(primitive)\n        else:\n            raise ValueError(f\"Type not recognized: {primitive}\")\n\n    @abstractmethod\n    def visit_fixed(self, fixed_type: FixedType) -&gt; T:\n        \"\"\"Visit a FixedType.\"\"\"\n\n    @abstractmethod\n    def visit_decimal(self, decimal_type: DecimalType) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_boolean(self, boolean_type: BooleanType) -&gt; T:\n        \"\"\"Visit a BooleanType.\"\"\"\n\n    @abstractmethod\n    def visit_integer(self, integer_type: IntegerType) -&gt; T:\n        \"\"\"Visit a IntegerType.\"\"\"\n\n    @abstractmethod\n    def visit_long(self, long_type: LongType) -&gt; T:\n        \"\"\"Visit a LongType.\"\"\"\n\n    @abstractmethod\n    def visit_float(self, float_type: FloatType) -&gt; T:\n        \"\"\"Visit a FloatType.\"\"\"\n\n    @abstractmethod\n    def visit_double(self, double_type: DoubleType) -&gt; T:\n        \"\"\"Visit a DoubleType.\"\"\"\n\n    @abstractmethod\n    def visit_date(self, date_type: DateType) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_time(self, time_type: TimeType) -&gt; T:\n        \"\"\"Visit a DecimalType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamp(self, timestamp_type: TimestampType) -&gt; T:\n        \"\"\"Visit a TimestampType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamp_ns(self, timestamp_type: TimestampNanoType) -&gt; T:\n        \"\"\"Visit a TimestampNanoType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamptz(self, timestamptz_type: TimestamptzType) -&gt; T:\n        \"\"\"Visit a TimestamptzType.\"\"\"\n\n    @abstractmethod\n    def visit_timestamptz_ns(self, timestamptz_ns_type: TimestamptzNanoType) -&gt; T:\n        \"\"\"Visit a TimestamptzNanoType.\"\"\"\n\n    @abstractmethod\n    def visit_string(self, string_type: StringType) -&gt; T:\n        \"\"\"Visit a StringType.\"\"\"\n\n    @abstractmethod\n    def visit_uuid(self, uuid_type: UUIDType) -&gt; T:\n        \"\"\"Visit a UUIDType.\"\"\"\n\n    @abstractmethod\n    def visit_binary(self, binary_type: BinaryType) -&gt; T:\n        \"\"\"Visit a BinaryType.\"\"\"\n\n    @abstractmethod\n    def visit_unknown(self, unknown_type: UnknownType) -&gt; T:\n        \"\"\"Visit a UnknownType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.primitive","title":"<code>primitive(primitive)</code>","text":"<p>Visit a PrimitiveType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def primitive(self, primitive: PrimitiveType) -&gt; T:\n    \"\"\"Visit a PrimitiveType.\"\"\"\n    if isinstance(primitive, FixedType):\n        return self.visit_fixed(primitive)\n    elif isinstance(primitive, DecimalType):\n        return self.visit_decimal(primitive)\n    elif isinstance(primitive, BooleanType):\n        return self.visit_boolean(primitive)\n    elif isinstance(primitive, IntegerType):\n        return self.visit_integer(primitive)\n    elif isinstance(primitive, LongType):\n        return self.visit_long(primitive)\n    elif isinstance(primitive, FloatType):\n        return self.visit_float(primitive)\n    elif isinstance(primitive, DoubleType):\n        return self.visit_double(primitive)\n    elif isinstance(primitive, DateType):\n        return self.visit_date(primitive)\n    elif isinstance(primitive, TimeType):\n        return self.visit_time(primitive)\n    elif isinstance(primitive, TimestampType):\n        return self.visit_timestamp(primitive)\n    elif isinstance(primitive, TimestampNanoType):\n        return self.visit_timestamp_ns(primitive)\n    elif isinstance(primitive, TimestamptzType):\n        return self.visit_timestamptz(primitive)\n    elif isinstance(primitive, TimestamptzNanoType):\n        return self.visit_timestamptz_ns(primitive)\n    elif isinstance(primitive, StringType):\n        return self.visit_string(primitive)\n    elif isinstance(primitive, UUIDType):\n        return self.visit_uuid(primitive)\n    elif isinstance(primitive, BinaryType):\n        return self.visit_binary(primitive)\n    elif isinstance(primitive, UnknownType):\n        return self.visit_unknown(primitive)\n    else:\n        raise ValueError(f\"Type not recognized: {primitive}\")\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_binary","title":"<code>visit_binary(binary_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a BinaryType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_binary(self, binary_type: BinaryType) -&gt; T:\n    \"\"\"Visit a BinaryType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_boolean","title":"<code>visit_boolean(boolean_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a BooleanType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_boolean(self, boolean_type: BooleanType) -&gt; T:\n    \"\"\"Visit a BooleanType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_date","title":"<code>visit_date(date_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_date(self, date_type: DateType) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_decimal","title":"<code>visit_decimal(decimal_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_decimal(self, decimal_type: DecimalType) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_double","title":"<code>visit_double(double_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a DoubleType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_double(self, double_type: DoubleType) -&gt; T:\n    \"\"\"Visit a DoubleType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_fixed","title":"<code>visit_fixed(fixed_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a FixedType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_fixed(self, fixed_type: FixedType) -&gt; T:\n    \"\"\"Visit a FixedType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_float","title":"<code>visit_float(float_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a FloatType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_float(self, float_type: FloatType) -&gt; T:\n    \"\"\"Visit a FloatType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_integer","title":"<code>visit_integer(integer_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a IntegerType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_integer(self, integer_type: IntegerType) -&gt; T:\n    \"\"\"Visit a IntegerType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_long","title":"<code>visit_long(long_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a LongType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_long(self, long_type: LongType) -&gt; T:\n    \"\"\"Visit a LongType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_string","title":"<code>visit_string(string_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a StringType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_string(self, string_type: StringType) -&gt; T:\n    \"\"\"Visit a StringType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_time","title":"<code>visit_time(time_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a DecimalType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_time(self, time_type: TimeType) -&gt; T:\n    \"\"\"Visit a DecimalType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_timestamp","title":"<code>visit_timestamp(timestamp_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestampType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamp(self, timestamp_type: TimestampType) -&gt; T:\n    \"\"\"Visit a TimestampType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_timestamp_ns","title":"<code>visit_timestamp_ns(timestamp_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestampNanoType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamp_ns(self, timestamp_type: TimestampNanoType) -&gt; T:\n    \"\"\"Visit a TimestampNanoType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_timestamptz","title":"<code>visit_timestamptz(timestamptz_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestamptzType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamptz(self, timestamptz_type: TimestamptzType) -&gt; T:\n    \"\"\"Visit a TimestamptzType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_timestamptz_ns","title":"<code>visit_timestamptz_ns(timestamptz_ns_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a TimestamptzNanoType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_timestamptz_ns(self, timestamptz_ns_type: TimestamptzNanoType) -&gt; T:\n    \"\"\"Visit a TimestamptzNanoType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_unknown","title":"<code>visit_unknown(unknown_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a UnknownType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_unknown(self, unknown_type: UnknownType) -&gt; T:\n    \"\"\"Visit a UnknownType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaVisitorPerPrimitiveType.visit_uuid","title":"<code>visit_uuid(uuid_type)</code>  <code>abstractmethod</code>","text":"<p>Visit a UUIDType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef visit_uuid(self, uuid_type: UUIDType) -&gt; T:\n    \"\"\"Visit a UUIDType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor","title":"<code>SchemaWithPartnerVisitor</code>","text":"<p>               Bases: <code>Generic[P, T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/schema.py</code> <pre><code>class SchemaWithPartnerVisitor(Generic[P, T], ABC):\n    def before_field(self, field: NestedField, field_partner: P | None) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n\n    def after_field(self, field: NestedField, field_partner: P | None) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n\n    def before_list_element(self, element: NestedField, element_partner: P | None) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n        self.before_field(element, element_partner)\n\n    def after_list_element(self, element: NestedField, element_partner: P | None) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n        self.after_field(element, element_partner)\n\n    def before_map_key(self, key: NestedField, key_partner: P | None) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n        self.before_field(key, key_partner)\n\n    def after_map_key(self, key: NestedField, key_partner: P | None) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n        self.after_field(key, key_partner)\n\n    def before_map_value(self, value: NestedField, value_partner: P | None) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n        self.before_field(value, value_partner)\n\n    def after_map_value(self, value: NestedField, value_partner: P | None) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n        self.after_field(value, value_partner)\n\n    @abstractmethod\n    def schema(self, schema: Schema, schema_partner: P | None, struct_result: T) -&gt; T:\n        \"\"\"Visit a schema with a partner.\"\"\"\n\n    @abstractmethod\n    def struct(self, struct: StructType, struct_partner: P | None, field_results: builtins.list[T]) -&gt; T:\n        \"\"\"Visit a struct type with a partner.\"\"\"\n\n    @abstractmethod\n    def field(self, field: NestedField, field_partner: P | None, field_result: T) -&gt; T:\n        \"\"\"Visit a nested field with a partner.\"\"\"\n\n    @abstractmethod\n    def list(self, list_type: ListType, list_partner: P | None, element_result: T) -&gt; T:\n        \"\"\"Visit a list type with a partner.\"\"\"\n\n    @abstractmethod\n    def map(self, map_type: MapType, map_partner: P | None, key_result: T, value_result: T) -&gt; T:\n        \"\"\"Visit a map type with a partner.\"\"\"\n\n    @abstractmethod\n    def primitive(self, primitive: PrimitiveType, primitive_partner: P | None) -&gt; T:\n        \"\"\"Visit a primitive type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.after_field","title":"<code>after_field(field, field_partner)</code>","text":"<p>Override this method to perform an action immediately after visiting a field.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_field(self, field: NestedField, field_partner: P | None) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.after_list_element","title":"<code>after_list_element(element, element_partner)</code>","text":"<p>Override this method to perform an action immediately after visiting an element within a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_list_element(self, element: NestedField, element_partner: P | None) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n    self.after_field(element, element_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.after_map_key","title":"<code>after_map_key(key, key_partner)</code>","text":"<p>Override this method to perform an action immediately after visiting a key within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_map_key(self, key: NestedField, key_partner: P | None) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n    self.after_field(key, key_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.after_map_value","title":"<code>after_map_value(value, value_partner)</code>","text":"<p>Override this method to perform an action immediately after visiting a value within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def after_map_value(self, value: NestedField, value_partner: P | None) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n    self.after_field(value, value_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.before_field","title":"<code>before_field(field, field_partner)</code>","text":"<p>Override this method to perform an action immediately before visiting a field.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_field(self, field: NestedField, field_partner: P | None) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.before_list_element","title":"<code>before_list_element(element, element_partner)</code>","text":"<p>Override this method to perform an action immediately before visiting an element within a ListType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_list_element(self, element: NestedField, element_partner: P | None) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n    self.before_field(element, element_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.before_map_key","title":"<code>before_map_key(key, key_partner)</code>","text":"<p>Override this method to perform an action immediately before visiting a key within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_map_key(self, key: NestedField, key_partner: P | None) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n    self.before_field(key, key_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.before_map_value","title":"<code>before_map_value(value, value_partner)</code>","text":"<p>Override this method to perform an action immediately before visiting a value within a MapType.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def before_map_value(self, value: NestedField, value_partner: P | None) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n    self.before_field(value, value_partner)\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.field","title":"<code>field(field, field_partner, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a nested field with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef field(self, field: NestedField, field_partner: P | None, field_result: T) -&gt; T:\n    \"\"\"Visit a nested field with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.list","title":"<code>list(list_type, list_partner, element_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a list type with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef list(self, list_type: ListType, list_partner: P | None, element_result: T) -&gt; T:\n    \"\"\"Visit a list type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.map","title":"<code>map(map_type, map_partner, key_result, value_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a map type with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef map(self, map_type: MapType, map_partner: P | None, key_result: T, value_result: T) -&gt; T:\n    \"\"\"Visit a map type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.primitive","title":"<code>primitive(primitive, primitive_partner)</code>  <code>abstractmethod</code>","text":"<p>Visit a primitive type with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef primitive(self, primitive: PrimitiveType, primitive_partner: P | None) -&gt; T:\n    \"\"\"Visit a primitive type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.schema","title":"<code>schema(schema, schema_partner, struct_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a schema with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef schema(self, schema: Schema, schema_partner: P | None, struct_result: T) -&gt; T:\n    \"\"\"Visit a schema with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.SchemaWithPartnerVisitor.struct","title":"<code>struct(struct, struct_partner, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a struct type with a partner.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@abstractmethod\ndef struct(self, struct: StructType, struct_partner: P | None, field_results: builtins.list[T]) -&gt; T:\n    \"\"\"Visit a struct type with a partner.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.assign_fresh_schema_ids","title":"<code>assign_fresh_schema_ids(schema_or_type, next_id=None)</code>","text":"<p>Traverses the schema, and sets new IDs.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def assign_fresh_schema_ids(schema_or_type: Schema | IcebergType, next_id: Callable[[], int] | None = None) -&gt; Schema:\n    \"\"\"Traverses the schema, and sets new IDs.\"\"\"\n    return pre_order_visit(schema_or_type, _SetFreshIDs(next_id_func=next_id))\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.build_position_accessors","title":"<code>build_position_accessors(schema_or_type)</code>","text":"<p>Generate an index of field IDs to schema position accessors.</p> <p>Parameters:</p> Name Type Description Default <code>schema_or_type</code> <code>Union[Schema, IcebergType]</code> <p>A schema or type to index.</p> required <p>Returns:</p> Type Description <code>dict[int, Accessor]</code> <p>Dict[int, Accessor]: An index of field IDs to accessors.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def build_position_accessors(schema_or_type: Schema | IcebergType) -&gt; dict[int, Accessor]:\n    \"\"\"Generate an index of field IDs to schema position accessors.\n\n    Args:\n        schema_or_type (Union[Schema, IcebergType]): A schema or type to index.\n\n    Returns:\n        Dict[int, Accessor]: An index of field IDs to accessors.\n    \"\"\"\n    return visit(schema_or_type, _BuildPositionAccessors())\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.index_by_id","title":"<code>index_by_id(schema_or_type)</code>","text":"<p>Generate an index of field IDs to NestedField instances.</p> <p>Parameters:</p> Name Type Description Default <code>schema_or_type</code> <code>Union[Schema, IcebergType]</code> <p>A schema or type to index.</p> required <p>Returns:</p> Type Description <code>dict[int, NestedField]</code> <p>Dict[int, NestedField]: An index of field IDs to NestedField instances.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def index_by_id(schema_or_type: Schema | IcebergType) -&gt; dict[int, NestedField]:\n    \"\"\"Generate an index of field IDs to NestedField instances.\n\n    Args:\n        schema_or_type (Union[Schema, IcebergType]): A schema or type to index.\n\n    Returns:\n        Dict[int, NestedField]: An index of field IDs to NestedField instances.\n    \"\"\"\n    return visit(schema_or_type, _IndexById())\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.index_by_name","title":"<code>index_by_name(schema_or_type)</code>","text":"<p>Generate an index of field names to field IDs.</p> <p>Parameters:</p> Name Type Description Default <code>schema_or_type</code> <code>Union[Schema, IcebergType]</code> <p>A schema or type to index.</p> required <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dict[str, int]: An index of field names to field IDs.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def index_by_name(schema_or_type: Schema | IcebergType) -&gt; dict[str, int]:\n    \"\"\"Generate an index of field names to field IDs.\n\n    Args:\n        schema_or_type (Union[Schema, IcebergType]): A schema or type to index.\n\n    Returns:\n        Dict[str, int]: An index of field names to field IDs.\n    \"\"\"\n    if len(schema_or_type.fields) &gt; 0:\n        indexer = _IndexByName()\n        visit(schema_or_type, indexer)\n        return indexer.by_name()\n    else:\n        return EMPTY_DICT\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.index_name_by_id","title":"<code>index_name_by_id(schema_or_type)</code>","text":"<p>Generate an index of field IDs full field names.</p> <p>Parameters:</p> Name Type Description Default <code>schema_or_type</code> <code>Union[Schema, IcebergType]</code> <p>A schema or type to index.</p> required <p>Returns:</p> Type Description <code>dict[int, str]</code> <p>Dict[str, int]: An index of field IDs to full names.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def index_name_by_id(schema_or_type: Schema | IcebergType) -&gt; dict[int, str]:\n    \"\"\"Generate an index of field IDs full field names.\n\n    Args:\n        schema_or_type (Union[Schema, IcebergType]): A schema or type to index.\n\n    Returns:\n        Dict[str, int]: An index of field IDs to full names.\n    \"\"\"\n    indexer = _IndexByName()\n    visit(schema_or_type, indexer)\n    return indexer.by_id()\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.make_compatible_name","title":"<code>make_compatible_name(name)</code>","text":"<p>Make a field name compatible with Avro specification.</p> <p>This function sanitizes field names to comply with Avro naming rules: - Names must start with [A-Za-z_] - Subsequent characters must be [A-Za-z0-9_]</p> <p>Invalid characters are replaced with _xHHHH where HHHH is the hex code. Names starting with digits get a leading underscore.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The original field name</p> required <p>Returns:</p> Type Description <code>str</code> <p>A sanitized name that complies with Avro specification</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def make_compatible_name(name: str) -&gt; str:\n    \"\"\"Make a field name compatible with Avro specification.\n\n    This function sanitizes field names to comply with Avro naming rules:\n    - Names must start with [A-Za-z_]\n    - Subsequent characters must be [A-Za-z0-9_]\n\n    Invalid characters are replaced with _xHHHH where HHHH is the hex code.\n    Names starting with digits get a leading underscore.\n\n    Args:\n        name: The original field name\n\n    Returns:\n        A sanitized name that complies with Avro specification\n    \"\"\"\n    if not _valid_avro_name(name):\n        return _sanitize_name(name)\n    return name\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.pre_order_visit","title":"<code>pre_order_visit(obj, visitor)</code>","text":"<p>Apply a schema visitor to any point within a schema.</p> <p>The function traverses the schema in pre-order fashion. This is a slimmed down version compared to the post-order traversal (missing before and after methods), mostly because we don't use the pre-order traversal much.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Schema, IcebergType]</code> <p>An instance of a Schema or an IcebergType.</p> required <code>visitor</code> <code>PreOrderSchemaVisitor[T]</code> <p>An instance of an implementation of the generic PreOrderSchemaVisitor base class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to visit an unrecognized object type.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@singledispatch\ndef pre_order_visit(obj: Schema | IcebergType, visitor: PreOrderSchemaVisitor[T]) -&gt; T:\n    \"\"\"Apply a schema visitor to any point within a schema.\n\n    The function traverses the schema in pre-order fashion. This is a slimmed down version\n    compared to the post-order traversal (missing before and after methods), mostly\n    because we don't use the pre-order traversal much.\n\n    Args:\n        obj (Union[Schema, IcebergType]): An instance of a Schema or an IcebergType.\n        visitor (PreOrderSchemaVisitor[T]): An instance of an implementation of the generic PreOrderSchemaVisitor base class.\n\n    Raises:\n        NotImplementedError: If attempting to visit an unrecognized object type.\n    \"\"\"\n    raise NotImplementedError(f\"Cannot visit non-type: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.promote","title":"<code>promote(file_type, read_type)</code>","text":"<p>Promotes reading a file type to a read type.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>IcebergType</code> <p>The type of the Avro file.</p> required <code>read_type</code> <code>IcebergType</code> <p>The requested read type.</p> required <p>Raises:</p> Type Description <code>ResolveError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@singledispatch\ndef promote(file_type: IcebergType, read_type: IcebergType) -&gt; IcebergType:\n    \"\"\"Promotes reading a file type to a read type.\n\n    Args:\n        file_type (IcebergType): The type of the Avro file.\n        read_type (IcebergType): The requested read type.\n\n    Raises:\n        ResolveError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    if file_type == read_type:\n        return file_type\n    else:\n        raise ResolveError(f\"Cannot promote {file_type} to {read_type}\")\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.prune_columns","title":"<code>prune_columns(schema, selected, select_full_types=True)</code>","text":"<p>Prunes a column by only selecting a set of field-ids.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The schema to be pruned.</p> required <code>selected</code> <code>set[int]</code> <p>The field-ids to be included.</p> required <code>select_full_types</code> <code>bool</code> <p>Return the full struct when a subset is recorded</p> <code>True</code> <p>Returns:</p> Type Description <code>Schema</code> <p>The pruned schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def prune_columns(schema: Schema, selected: set[int], select_full_types: bool = True) -&gt; Schema:\n    \"\"\"Prunes a column by only selecting a set of field-ids.\n\n    Args:\n        schema: The schema to be pruned.\n        selected: The field-ids to be included.\n        select_full_types: Return the full struct when a subset is recorded\n\n    Returns:\n        The pruned schema.\n    \"\"\"\n    result = visit(schema.as_struct(), _PruneColumnsVisitor(selected, select_full_types))\n    return Schema(\n        *(result or StructType()).fields,\n        schema_id=schema.schema_id,\n        identifier_field_ids=list(selected.intersection(schema.identifier_field_ids)),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.sanitize_column_names","title":"<code>sanitize_column_names(schema)</code>","text":"<p>Sanitize column names to make them compatible with Avro.</p> <p>The column name should be starting with '' or digit followed by a string only contains '', digit or alphabet, otherwise it will be sanitized to conform the avro naming convention.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The schema to be sanitized.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>The sanitized schema.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>def sanitize_column_names(schema: Schema) -&gt; Schema:\n    \"\"\"Sanitize column names to make them compatible with Avro.\n\n    The column name should be starting with '_' or digit followed by a string only contains '_', digit or alphabet,\n    otherwise it will be sanitized to conform the avro naming convention.\n\n    Args:\n        schema: The schema to be sanitized.\n\n    Returns:\n        The sanitized schema.\n    \"\"\"\n    result = visit(schema.as_struct(), _SanitizeColumnsVisitor())\n    return Schema(\n        *(result or StructType()).fields,\n        schema_id=schema.schema_id,\n        identifier_field_ids=schema.identifier_field_ids,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/schema/#pyiceberg.schema.visit","title":"<code>visit(obj, visitor)</code>","text":"<p>Apply a schema visitor to any point within a schema.</p> <p>The function traverses the schema in post-order fashion.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Schema, IcebergType]</code> <p>An instance of a Schema or an IcebergType.</p> required <code>visitor</code> <code>SchemaVisitor[T]</code> <p>An instance of an implementation of the generic SchemaVisitor base class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to visit an unrecognized object type.</p> Source code in <code>pyiceberg/schema.py</code> <pre><code>@singledispatch\ndef visit(obj: Schema | IcebergType, visitor: SchemaVisitor[T]) -&gt; T:\n    \"\"\"Apply a schema visitor to any point within a schema.\n\n    The function traverses the schema in post-order fashion.\n\n    Args:\n        obj (Union[Schema, IcebergType]): An instance of a Schema or an IcebergType.\n        visitor (SchemaVisitor[T]): An instance of an implementation of the generic SchemaVisitor base class.\n\n    Raises:\n        NotImplementedError: If attempting to visit an unrecognized object type.\n    \"\"\"\n    raise NotImplementedError(f\"Cannot visit non-type: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/serializers/","title":"serializers","text":""},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.Compressor","title":"<code>Compressor</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>class Compressor(ABC):\n    @staticmethod\n    def get_compressor(location: str) -&gt; Compressor:\n        return GzipCompressor() if location.endswith(\".gz.metadata.json\") else NOOP_COMPRESSOR\n\n    @abstractmethod\n    def stream_decompressor(self, inp: InputStream) -&gt; InputStream:\n        \"\"\"Return a stream decompressor.\n\n        Args:\n            inp: The input stream that needs decompressing.\n\n        Returns:\n            The wrapped stream\n        \"\"\"\n\n    @abstractmethod\n    def bytes_compressor(self) -&gt; Callable[[bytes], bytes]:\n        \"\"\"Return a function to compress bytes.\n\n        Returns:\n            A function that can be used to compress bytes.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.Compressor.bytes_compressor","title":"<code>bytes_compressor()</code>  <code>abstractmethod</code>","text":"<p>Return a function to compress bytes.</p> <p>Returns:</p> Type Description <code>Callable[[bytes], bytes]</code> <p>A function that can be used to compress bytes.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@abstractmethod\ndef bytes_compressor(self) -&gt; Callable[[bytes], bytes]:\n    \"\"\"Return a function to compress bytes.\n\n    Returns:\n        A function that can be used to compress bytes.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.Compressor.stream_decompressor","title":"<code>stream_decompressor(inp)</code>  <code>abstractmethod</code>","text":"<p>Return a stream decompressor.</p> <p>Parameters:</p> Name Type Description Default <code>inp</code> <code>InputStream</code> <p>The input stream that needs decompressing.</p> required <p>Returns:</p> Type Description <code>InputStream</code> <p>The wrapped stream</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@abstractmethod\ndef stream_decompressor(self, inp: InputStream) -&gt; InputStream:\n    \"\"\"Return a stream decompressor.\n\n    Args:\n        inp: The input stream that needs decompressing.\n\n    Returns:\n        The wrapped stream\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.FromByteStream","title":"<code>FromByteStream</code>","text":"<p>A collection of methods that deserialize dictionaries into Iceberg objects.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>class FromByteStream:\n    \"\"\"A collection of methods that deserialize dictionaries into Iceberg objects.\"\"\"\n\n    @staticmethod\n    def table_metadata(\n        byte_stream: InputStream, encoding: str = UTF8, compression: Compressor = NOOP_COMPRESSOR\n    ) -&gt; TableMetadata:\n        \"\"\"Instantiate a TableMetadata object from a byte stream.\n\n        Args:\n            byte_stream: A file-like byte stream object.\n            encoding (default \"utf-8\"): The byte encoder to use for the reader.\n            compression: Optional compression method\n        \"\"\"\n        with compression.stream_decompressor(byte_stream) as byte_stream:\n            reader = codecs.getreader(encoding)\n            json_bytes = reader(byte_stream)\n            metadata = json_bytes.read()\n\n        return TableMetadataUtil.parse_raw(metadata)\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.FromByteStream.table_metadata","title":"<code>table_metadata(byte_stream, encoding=UTF8, compression=NOOP_COMPRESSOR)</code>  <code>staticmethod</code>","text":"<p>Instantiate a TableMetadata object from a byte stream.</p> <p>Parameters:</p> Name Type Description Default <code>byte_stream</code> <code>InputStream</code> <p>A file-like byte stream object.</p> required <code>encoding</code> <code>default \"utf-8\"</code> <p>The byte encoder to use for the reader.</p> <code>UTF8</code> <code>compression</code> <code>Compressor</code> <p>Optional compression method</p> <code>NOOP_COMPRESSOR</code> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@staticmethod\ndef table_metadata(\n    byte_stream: InputStream, encoding: str = UTF8, compression: Compressor = NOOP_COMPRESSOR\n) -&gt; TableMetadata:\n    \"\"\"Instantiate a TableMetadata object from a byte stream.\n\n    Args:\n        byte_stream: A file-like byte stream object.\n        encoding (default \"utf-8\"): The byte encoder to use for the reader.\n        compression: Optional compression method\n    \"\"\"\n    with compression.stream_decompressor(byte_stream) as byte_stream:\n        reader = codecs.getreader(encoding)\n        json_bytes = reader(byte_stream)\n        metadata = json_bytes.read()\n\n    return TableMetadataUtil.parse_raw(metadata)\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.FromInputFile","title":"<code>FromInputFile</code>","text":"<p>A collection of methods that deserialize InputFiles into Iceberg objects.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>class FromInputFile:\n    \"\"\"A collection of methods that deserialize InputFiles into Iceberg objects.\"\"\"\n\n    @staticmethod\n    def table_metadata(input_file: InputFile, encoding: str = UTF8) -&gt; TableMetadata:\n        \"\"\"Create a TableMetadata instance from an input file.\n\n        Args:\n            input_file (InputFile): A custom implementation of the iceberg.io.file.InputFile abstract base class.\n            encoding (str): Encoding to use when loading bytestream.\n\n        Returns:\n            TableMetadata: A table metadata instance.\n\n        \"\"\"\n        with input_file.open() as input_stream:\n            return FromByteStream.table_metadata(\n                byte_stream=input_stream, encoding=encoding, compression=Compressor.get_compressor(location=input_file.location)\n            )\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.FromInputFile.table_metadata","title":"<code>table_metadata(input_file, encoding=UTF8)</code>  <code>staticmethod</code>","text":"<p>Create a TableMetadata instance from an input file.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>InputFile</code> <p>A custom implementation of the iceberg.io.file.InputFile abstract base class.</p> required <code>encoding</code> <code>str</code> <p>Encoding to use when loading bytestream.</p> <code>UTF8</code> <p>Returns:</p> Name Type Description <code>TableMetadata</code> <code>TableMetadata</code> <p>A table metadata instance.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@staticmethod\ndef table_metadata(input_file: InputFile, encoding: str = UTF8) -&gt; TableMetadata:\n    \"\"\"Create a TableMetadata instance from an input file.\n\n    Args:\n        input_file (InputFile): A custom implementation of the iceberg.io.file.InputFile abstract base class.\n        encoding (str): Encoding to use when loading bytestream.\n\n    Returns:\n        TableMetadata: A table metadata instance.\n\n    \"\"\"\n    with input_file.open() as input_stream:\n        return FromByteStream.table_metadata(\n            byte_stream=input_stream, encoding=encoding, compression=Compressor.get_compressor(location=input_file.location)\n        )\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.ToOutputFile","title":"<code>ToOutputFile</code>","text":"<p>A collection of methods that serialize Iceberg objects into files given an OutputFile instance.</p> Source code in <code>pyiceberg/serializers.py</code> <pre><code>class ToOutputFile:\n    \"\"\"A collection of methods that serialize Iceberg objects into files given an OutputFile instance.\"\"\"\n\n    @staticmethod\n    def table_metadata(metadata: TableMetadata, output_file: OutputFile, overwrite: bool = False) -&gt; None:\n        \"\"\"Write a TableMetadata instance to an output file.\n\n        Args:\n            output_file (OutputFile): A custom implementation of the iceberg.io.file.OutputFile abstract base class.\n            overwrite (bool): Where to overwrite the file if it already exists. Defaults to `False`.\n        \"\"\"\n        with output_file.create(overwrite=overwrite) as output_stream:\n            # We need to serialize None values, in order to dump `None` current-snapshot-id as `-1`\n            exclude_none = False if Config().get_bool(\"legacy-current-snapshot-id\") else True\n\n            json_bytes = metadata.model_dump_json(exclude_none=exclude_none).encode(UTF8)\n            json_bytes = Compressor.get_compressor(output_file.location).bytes_compressor()(json_bytes)\n            output_stream.write(json_bytes)\n</code></pre>"},{"location":"reference/pyiceberg/serializers/#pyiceberg.serializers.ToOutputFile.table_metadata","title":"<code>table_metadata(metadata, output_file, overwrite=False)</code>  <code>staticmethod</code>","text":"<p>Write a TableMetadata instance to an output file.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>OutputFile</code> <p>A custom implementation of the iceberg.io.file.OutputFile abstract base class.</p> required <code>overwrite</code> <code>bool</code> <p>Where to overwrite the file if it already exists. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>pyiceberg/serializers.py</code> <pre><code>@staticmethod\ndef table_metadata(metadata: TableMetadata, output_file: OutputFile, overwrite: bool = False) -&gt; None:\n    \"\"\"Write a TableMetadata instance to an output file.\n\n    Args:\n        output_file (OutputFile): A custom implementation of the iceberg.io.file.OutputFile abstract base class.\n        overwrite (bool): Where to overwrite the file if it already exists. Defaults to `False`.\n    \"\"\"\n    with output_file.create(overwrite=overwrite) as output_stream:\n        # We need to serialize None values, in order to dump `None` current-snapshot-id as `-1`\n        exclude_none = False if Config().get_bool(\"legacy-current-snapshot-id\") else True\n\n        json_bytes = metadata.model_dump_json(exclude_none=exclude_none).encode(UTF8)\n        json_bytes = Compressor.get_compressor(output_file.location).bytes_compressor()(json_bytes)\n        output_stream.write(json_bytes)\n</code></pre>"},{"location":"reference/pyiceberg/transforms/","title":"transforms","text":""},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.BoundTransform","title":"<code>BoundTransform</code>","text":"<p>               Bases: <code>BoundTerm</code></p> <p>A transform expression.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class BoundTransform(BoundTerm):\n    \"\"\"A transform expression.\"\"\"\n\n    transform: Transform[Any, Any]\n\n    def __init__(self, term: BoundTerm, transform: Transform[Any, Any]):\n        self.term: BoundTerm = term\n        self.transform = transform\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.BucketTransform","title":"<code>BucketTransform</code>","text":"<p>               Bases: <code>Transform[S, int]</code></p> <p>Base Transform class to transform a value into a bucket partition value.</p> <p>Transforms are parameterized by a number of buckets. Bucket partition transforms use a 32-bit hash of the source value to produce a positive value by mod the bucket number.</p> <p>Parameters:</p> Name Type Description Default <code>num_buckets</code> <code>int</code> <p>The number of buckets.</p> required Source code in <code>pyiceberg/transforms.py</code> <pre><code>class BucketTransform(Transform[S, int]):\n    \"\"\"Base Transform class to transform a value into a bucket partition value.\n\n    Transforms are parameterized by a number of buckets. Bucket partition transforms use a 32-bit\n    hash of the source value to produce a positive value by mod the bucket number.\n\n    Args:\n      num_buckets (int): The number of buckets.\n    \"\"\"\n\n    root: str = Field()\n    _num_buckets: PositiveInt = PrivateAttr()\n\n    def __init__(self, num_buckets: int, **data: Any) -&gt; None:\n        super().__init__(f\"bucket[{num_buckets}]\", **data)\n        self._num_buckets = num_buckets\n\n    @property\n    def num_buckets(self) -&gt; int:\n        return self._num_buckets\n\n    def hash(self, value: S) -&gt; int:\n        raise NotImplementedError()\n\n    def apply(self, value: S | None) -&gt; int | None:\n        return (self.hash(value) &amp; IntegerType.max) % self._num_buckets if value else None\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        return IntegerType()\n\n    def project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        transformer = self.transform(pred.term.ref().field.field_type)\n\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n        elif isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundEqualTo):\n            return pred.as_unbound(Reference(name), _transform_literal(transformer, pred.literal))\n        elif isinstance(pred, BoundIn):  # NotIn can't be projected\n            return pred.as_unbound(Reference(name), {_transform_literal(transformer, literal) for literal in pred.literals})  # type: ignore\n        else:\n            # - Comparison predicates can't be projected, notEq can't be projected\n            # - Small ranges can be projected:\n            #   For example, (x &gt; 0) and (x &lt; 3) can be turned into in({1, 2}) and projected.\n            return None  # type: ignore\n\n    def strict_project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        transformer = self.transform(pred.term.ref().field.field_type)\n\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n        elif isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundNotEqualTo):\n            return pred.as_unbound(Reference(name), _transform_literal(transformer, pred.literal))\n        elif isinstance(pred, BoundNotIn):\n            return pred.as_unbound(Reference(name), {_transform_literal(transformer, literal) for literal in pred.literals})  # type: ignore\n        else:\n            # no strict projection for comparison or equality\n            return None  # type: ignore\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(\n            source,\n            (\n                IntegerType,\n                DateType,\n                LongType,\n                TimeType,\n                TimestampType,\n                TimestamptzType,\n                TimestampNanoType,\n                TimestamptzNanoType,\n                DecimalType,\n                StringType,\n                FixedType,\n                BinaryType,\n                UUIDType,\n            ),\n        )\n\n    def transform(self, source: IcebergType, bucket: bool = True) -&gt; Callable[[Any | None], int | None]:\n        if isinstance(source, TimeType):\n\n            def hash_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.time):\n                    v = datetime.time_to_micros(v)\n\n                return mmh3.hash(struct.pack(\"&lt;q\", v))\n\n        elif isinstance(source, DateType):\n\n            def hash_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.date):\n                    v = datetime.date_to_days(v)\n\n                return mmh3.hash(struct.pack(\"&lt;q\", v))\n\n        elif isinstance(source, (TimestampType, TimestamptzType)):\n\n            def hash_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return mmh3.hash(struct.pack(\"&lt;q\", v))\n\n        elif isinstance(source, (TimestampNanoType, TimestamptzNanoType)):\n\n            def hash_func(v: Any) -&gt; int:\n                # In order to bucket TimestampNano the same as Timestamp\n                # convert to micros before hashing.\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n                else:\n                    v = datetime.nanos_to_micros(v)\n\n                return mmh3.hash(struct.pack(\"&lt;q\", v))\n\n        elif isinstance(source, (IntegerType, LongType)):\n\n            def hash_func(v: Any) -&gt; int:\n                return mmh3.hash(struct.pack(\"&lt;q\", v))\n\n        elif isinstance(source, DecimalType):\n\n            def hash_func(v: Any) -&gt; int:\n                return mmh3.hash(decimal_to_bytes(v))\n\n        elif isinstance(source, (StringType, FixedType, BinaryType)):\n\n            def hash_func(v: Any) -&gt; int:\n                return mmh3.hash(v)\n\n        elif isinstance(source, UUIDType):\n\n            def hash_func(v: Any) -&gt; int:\n                if isinstance(v, UUID):\n                    return mmh3.hash(v.bytes)\n                return mmh3.hash(v)\n\n        else:\n            raise ValueError(f\"Unknown type {source}\")\n\n        if bucket:\n            return lambda v: (hash_func(v) &amp; IntegerType.max) % self._num_buckets if v is not None else None\n        return hash_func\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BucketTransform class.\"\"\"\n        return f\"BucketTransform(num_buckets={self._num_buckets})\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        pyiceberg_core_transform = _try_import(\"pyiceberg_core\", extras_name=\"pyiceberg-core\").transform\n        return _pyiceberg_transform_wrapper(pyiceberg_core_transform.bucket, self._num_buckets)\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.BucketTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BucketTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BucketTransform class.\"\"\"\n    return f\"BucketTransform(num_buckets={self._num_buckets})\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.DayTransform","title":"<code>DayTransform</code>","text":"<p>               Bases: <code>TimeTransform[S]</code></p> <p>Transforms a datetime value into a day value.</p> Example <p>transform = DayTransform() transform.transform(DateType())(17501) 17501</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class DayTransform(TimeTransform[S]):\n    \"\"\"Transforms a datetime value into a day value.\n\n    Example:\n        &gt;&gt;&gt; transform = DayTransform()\n        &gt;&gt;&gt; transform.transform(DateType())(17501)\n        17501\n    \"\"\"\n\n    root: LiteralType[\"day\"] = Field(default=\"day\")  # noqa: F821\n\n    def transform(self, source: IcebergType) -&gt; Callable[[S | None], int | None]:\n        if isinstance(source, DateType):\n\n            def day_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.date):\n                    v = datetime.date_to_days(v)\n\n                return v\n\n        elif isinstance(source, (TimestampType, TimestamptzType)):\n\n            def day_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return datetime.micros_to_days(v)\n\n        elif isinstance(source, (TimestampNanoType, TimestamptzNanoType)):\n\n            def day_func(v: Any) -&gt; int:\n                # python datetime has no nanoseconds support.\n                # nanosecond datetimes will be expressed as int as a workaround\n                return datetime.nanos_to_days(v)\n\n        else:\n            raise ValueError(f\"Cannot apply day transform for type: {source}\")\n\n        return lambda v: day_func(v) if v is not None else None\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (DateType, TimestampType, TimestamptzType, TimestampNanoType, TimestamptzNanoType))\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        \"\"\"Return the result type of a day transform.\n\n        The physical representation conforms to the Iceberg spec as DateType is internally converted to int.\n        The DateType returned here provides a more human-readable way to display the partition field.\n        \"\"\"\n        return DateType()\n\n    @property\n    def granularity(self) -&gt; TimeResolution:\n        return TimeResolution.DAY\n\n    def to_human_string(self, _: IcebergType, value: S | None) -&gt; str:\n        return datetime.to_human_day(value) if isinstance(value, int) else \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the DayTransform class.\"\"\"\n        return \"DayTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        pa = _try_import(\"pyarrow\", extras_name=\"pyarrow\")\n        pyiceberg_core_transform = _try_import(\"pyiceberg_core\", extras_name=\"pyiceberg-core\").transform\n\n        return _pyiceberg_transform_wrapper(pyiceberg_core_transform.day, expected_type=pa.int32())\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.DayTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the DayTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the DayTransform class.\"\"\"\n    return \"DayTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.DayTransform.result_type","title":"<code>result_type(source)</code>","text":"<p>Return the result type of a day transform.</p> <p>The physical representation conforms to the Iceberg spec as DateType is internally converted to int. The DateType returned here provides a more human-readable way to display the partition field.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def result_type(self, source: IcebergType) -&gt; IcebergType:\n    \"\"\"Return the result type of a day transform.\n\n    The physical representation conforms to the Iceberg spec as DateType is internally converted to int.\n    The DateType returned here provides a more human-readable way to display the partition field.\n    \"\"\"\n    return DateType()\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.HourTransform","title":"<code>HourTransform</code>","text":"<p>               Bases: <code>TimeTransform[S]</code></p> <p>Transforms a datetime value into a hour value.</p> Example <p>transform = HourTransform() transform.transform(TimestampType())(1512151975038194) 420042</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class HourTransform(TimeTransform[S]):\n    \"\"\"Transforms a datetime value into a hour value.\n\n    Example:\n        &gt;&gt;&gt; transform = HourTransform()\n        &gt;&gt;&gt; transform.transform(TimestampType())(1512151975038194)\n        420042\n    \"\"\"\n\n    root: LiteralType[\"hour\"] = Field(default=\"hour\")  # noqa: F821\n\n    def transform(self, source: IcebergType) -&gt; Callable[[S | None], int | None]:\n        if isinstance(source, (TimestampType, TimestamptzType)):\n\n            def hour_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return datetime.micros_to_hours(v)\n\n        elif isinstance(source, (TimestampNanoType, TimestamptzNanoType)):\n\n            def hour_func(v: Any) -&gt; int:\n                # python datetime has no nanoseconds support.\n                # nanosecond datetimes will be expressed as int as a workaround\n                return datetime.nanos_to_hours(v)\n\n        else:\n            raise ValueError(f\"Cannot apply hour transform for type: {source}\")\n\n        return lambda v: hour_func(v) if v is not None else None\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (TimestampType, TimestamptzType, TimestampNanoType, TimestamptzNanoType))\n\n    @property\n    def granularity(self) -&gt; TimeResolution:\n        return TimeResolution.HOUR\n\n    def to_human_string(self, _: IcebergType, value: S | None) -&gt; str:\n        return datetime.to_human_hour(value) if isinstance(value, int) else \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the HourTransform class.\"\"\"\n        return \"HourTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        pyiceberg_core_transform = _try_import(\"pyiceberg_core\", extras_name=\"pyiceberg-core\").transform\n\n        return _pyiceberg_transform_wrapper(pyiceberg_core_transform.hour)\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.HourTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the HourTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the HourTransform class.\"\"\"\n    return \"HourTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.IdentityTransform","title":"<code>IdentityTransform</code>","text":"<p>               Bases: <code>Transform[S, S]</code></p> <p>Transforms a value into itself.</p> Example <p>transform = IdentityTransform() transform.transform(StringType())('hello-world') 'hello-world'</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class IdentityTransform(Transform[S, S]):\n    \"\"\"Transforms a value into itself.\n\n    Example:\n        &gt;&gt;&gt; transform = IdentityTransform()\n        &gt;&gt;&gt; transform.transform(StringType())('hello-world')\n        'hello-world'\n    \"\"\"\n\n    root: LiteralType[\"identity\"] = Field(default=\"identity\")  # noqa: F821\n\n    def __init__(self) -&gt; None:\n        super().__init__(\"identity\")\n\n    def transform(self, source: IcebergType) -&gt; Callable[[S | None], S | None]:\n        return lambda v: v\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return source.is_primitive\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        return source\n\n    def project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n        elif isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundLiteralPredicate):\n            return pred.as_unbound(Reference(name), pred.literal)\n        elif isinstance(pred, BoundSetPredicate):\n            return pred.as_unbound(Reference(name), pred.literals)\n        else:\n            return None\n\n    def strict_project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        if isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundLiteralPredicate):\n            return pred.as_unbound(Reference(name), pred.literal)\n        elif isinstance(pred, BoundSetPredicate):\n            return pred.as_unbound(Reference(name), pred.literals)\n        else:\n            return None\n\n    @property\n    def preserves_order(self) -&gt; bool:\n        return True\n\n    def satisfies_order_of(self, other: Transform[S, T]) -&gt; bool:\n        \"\"\"Ordering by value is the same as long as the other preserves order.\"\"\"\n        return other.preserves_order\n\n    def to_human_string(self, source_type: IcebergType, value: S | None) -&gt; str:\n        return _human_string(value, source_type) if value is not None else \"null\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the IdentityTransform class.\"\"\"\n        return \"identity\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the IdentityTransform class.\"\"\"\n        return \"IdentityTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        return lambda v: v\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.IdentityTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the IdentityTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the IdentityTransform class.\"\"\"\n    return \"IdentityTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.IdentityTransform.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the IdentityTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the IdentityTransform class.\"\"\"\n    return \"identity\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.IdentityTransform.satisfies_order_of","title":"<code>satisfies_order_of(other)</code>","text":"<p>Ordering by value is the same as long as the other preserves order.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def satisfies_order_of(self, other: Transform[S, T]) -&gt; bool:\n    \"\"\"Ordering by value is the same as long as the other preserves order.\"\"\"\n    return other.preserves_order\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.MonthTransform","title":"<code>MonthTransform</code>","text":"<p>               Bases: <code>TimeTransform[S]</code></p> <p>Transforms a datetime value into a month value.</p> Example <p>transform = MonthTransform() transform.transform(DateType())(17501) 575</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class MonthTransform(TimeTransform[S]):\n    \"\"\"Transforms a datetime value into a month value.\n\n    Example:\n        &gt;&gt;&gt; transform = MonthTransform()\n        &gt;&gt;&gt; transform.transform(DateType())(17501)\n        575\n    \"\"\"\n\n    root: LiteralType[\"month\"] = Field(default=\"month\")  # noqa: F821\n\n    def transform(self, source: IcebergType) -&gt; Callable[[S | None], int | None]:\n        if isinstance(source, DateType):\n\n            def month_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.date):\n                    v = datetime.date_to_days(v)\n\n                return datetime.days_to_months(v)\n\n        elif isinstance(source, (TimestampType, TimestamptzType)):\n\n            def month_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return datetime.micros_to_months(v)\n\n        elif isinstance(source, (TimestampNanoType, TimestamptzNanoType)):\n\n            def month_func(v: Any) -&gt; int:\n                # python datetime has no nanoseconds support.\n                # nanosecond datetimes will be expressed as int as a workaround\n                return datetime.nanos_to_months(v)\n\n        else:\n            raise ValueError(f\"Cannot apply month transform for type: {source}\")\n\n        return lambda v: month_func(v) if v is not None else None\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (DateType, TimestampType, TimestamptzType, TimestampNanoType, TimestamptzNanoType))\n\n    @property\n    def granularity(self) -&gt; TimeResolution:\n        return TimeResolution.MONTH\n\n    def to_human_string(self, _: IcebergType, value: S | None) -&gt; str:\n        return datetime.to_human_month(value) if isinstance(value, int) else \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the MonthTransform class.\"\"\"\n        return \"MonthTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        pa = _try_import(\"pyarrow\")\n        pyiceberg_core_transform = _try_import(\"pyiceberg_core\", extras_name=\"pyiceberg-core\").transform\n\n        return _pyiceberg_transform_wrapper(pyiceberg_core_transform.month, expected_type=pa.int32())\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.MonthTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the MonthTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the MonthTransform class.\"\"\"\n    return \"MonthTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.Transform","title":"<code>Transform</code>","text":"<p>               Bases: <code>IcebergRootModel[str]</code>, <code>ABC</code>, <code>Generic[S, T]</code></p> <p>Transform base class for concrete transforms.</p> <p>A base class to transform values and project predicates on partition values. This class is not used directly. Instead, use one of module method to create the child classes.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class Transform(IcebergRootModel[str], ABC, Generic[S, T]):\n    \"\"\"Transform base class for concrete transforms.\n\n    A base class to transform values and project predicates on partition values.\n    This class is not used directly. Instead, use one of module method to create the child classes.\n    \"\"\"\n\n    root: str = Field()\n\n    @abstractmethod\n    def transform(self, source: IcebergType) -&gt; Callable[[S | None], T | None]: ...\n\n    @abstractmethod\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return False\n\n    @abstractmethod\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        \"\"\"Return the `IcebergType` produced by this transform given a source type.\n\n        This method defines both the physical and display representation of the partition field.\n\n        The physical representation must conform to the Iceberg spec. The display representation\n        can deviate from the spec, such as by transforming the value into a more human-readable format.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None: ...\n\n    @abstractmethod\n    def strict_project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None: ...\n\n    @property\n    def preserves_order(self) -&gt; bool:\n        return False\n\n    def satisfies_order_of(self, other: Any) -&gt; bool:\n        return self == other\n\n    def to_human_string(self, _: IcebergType, value: S | None) -&gt; str:\n        return str(value) if value is not None else \"null\"\n\n    @property\n    def dedup_name(self) -&gt; str:\n        return self.__str__()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Transform class.\"\"\"\n        return self.root\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Transform class.\"\"\"\n        if isinstance(other, Transform):\n            return self.root == other.root\n        return False\n\n    @abstractmethod\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\": ...\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.Transform.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Transform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Transform class.\"\"\"\n    if isinstance(other, Transform):\n        return self.root == other.root\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.Transform.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Transform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Transform class.\"\"\"\n    return self.root\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.Transform.result_type","title":"<code>result_type(source)</code>  <code>abstractmethod</code>","text":"<p>Return the <code>IcebergType</code> produced by this transform given a source type.</p> <p>This method defines both the physical and display representation of the partition field.</p> <p>The physical representation must conform to the Iceberg spec. The display representation can deviate from the spec, such as by transforming the value into a more human-readable format.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>@abstractmethod\ndef result_type(self, source: IcebergType) -&gt; IcebergType:\n    \"\"\"Return the `IcebergType` produced by this transform given a source type.\n\n    This method defines both the physical and display representation of the partition field.\n\n    The physical representation must conform to the Iceberg spec. The display representation\n    can deviate from the spec, such as by transforming the value into a more human-readable format.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.TruncateTransform","title":"<code>TruncateTransform</code>","text":"<p>               Bases: <code>Transform[S, S]</code></p> <p>A transform for truncating a value to a specified width.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>The truncate width, should be positive.</p> required <p>Raises:   ValueError: If a type is provided that is incompatible with a Truncate transform.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class TruncateTransform(Transform[S, S]):\n    \"\"\"A transform for truncating a value to a specified width.\n\n    Args:\n      width (int): The truncate width, should be positive.\n    Raises:\n      ValueError: If a type is provided that is incompatible with a Truncate transform.\n    \"\"\"\n\n    root: str = Field()\n    _source_type: IcebergType = PrivateAttr()\n    _width: PositiveInt = PrivateAttr()\n\n    def __init__(self, width: int, **data: Any):\n        super().__init__(root=f\"truncate[{width}]\", **data)\n        self._width = width\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (IntegerType, LongType, StringType, BinaryType, DecimalType))\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        return source\n\n    @property\n    def preserves_order(self) -&gt; bool:\n        return True\n\n    @property\n    def source_type(self) -&gt; IcebergType:\n        return self._source_type\n\n    def project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        field_type = pred.term.ref().field.field_type\n\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n\n        if isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n        elif isinstance(pred, BoundIn):\n            return _set_apply_transform(name, pred, self.transform(field_type))\n        elif isinstance(field_type, (IntegerType, LongType, DecimalType)):  # type: ignore\n            if isinstance(pred, BoundLiteralPredicate):\n                return _truncate_number(name, pred, self.transform(field_type))\n        elif isinstance(field_type, (BinaryType, StringType)):\n            if isinstance(pred, BoundLiteralPredicate):\n                return _truncate_array(name, pred, self.transform(field_type))\n\n    def strict_project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        field_type = pred.term.ref().field.field_type\n\n        if isinstance(pred.term, BoundTransform):\n            return _project_transform_predicate(self, name, pred)\n\n        if isinstance(pred, BoundUnaryPredicate):\n            return pred.as_unbound(Reference(name))\n\n        if isinstance(field_type, (IntegerType, LongType, DecimalType)):\n            if isinstance(pred, BoundLiteralPredicate):\n                return _truncate_number_strict(name, pred, self.transform(field_type))\n            elif isinstance(pred, BoundNotIn):\n                return _set_apply_transform(name, pred, self.transform(field_type))\n            else:\n                return None  # type: ignore\n\n        if isinstance(pred, BoundLiteralPredicate):\n            if isinstance(pred, BoundStartsWith):\n                literal_width = len(pred.literal.value)\n                if literal_width &lt; self.width:\n                    return pred.as_unbound(name, pred.literal.value)\n                elif literal_width == self.width:\n                    return EqualTo(name, pred.literal.value)\n                else:\n                    return None\n            elif isinstance(pred, BoundNotStartsWith):\n                literal_width = len(pred.literal.value)\n                if literal_width &lt; self.width:\n                    return pred.as_unbound(name, pred.literal.value)\n                elif literal_width == self.width:\n                    return NotEqualTo(name, pred.literal.value)\n                else:\n                    return pred.as_unbound(name, self.transform(field_type)(pred.literal.value))\n            else:\n                # ProjectionUtil.truncateArrayStrict(name, pred, this);\n                return _truncate_array_strict(name, pred, self.transform(field_type))\n        elif isinstance(pred, BoundNotIn):\n            return _set_apply_transform(name, pred, self.transform(field_type))\n        else:\n            return None  # type: ignore\n\n    @property\n    def width(self) -&gt; int:\n        return self._width\n\n    def transform(self, source: IcebergType) -&gt; Callable[[S | None], S | None]:\n        if isinstance(source, (IntegerType, LongType)):\n\n            def truncate_func(v: Any) -&gt; Any:\n                return v - v % self._width\n\n        elif isinstance(source, (StringType, BinaryType)):\n\n            def truncate_func(v: Any) -&gt; Any:\n                return v[0 : min(self._width, len(v))]\n\n        elif isinstance(source, DecimalType):\n\n            def truncate_func(v: Any) -&gt; Any:\n                return truncate_decimal(v, self._width)\n\n        else:\n            raise ValueError(f\"Cannot truncate for type: {source}\")\n\n        return lambda v: truncate_func(v) if v is not None else None\n\n    def satisfies_order_of(self, other: Transform[S, T]) -&gt; bool:\n        if self == other:\n            return True\n        elif (\n            isinstance(self.source_type, StringType)\n            and isinstance(other, TruncateTransform)\n            and isinstance(other.source_type, StringType)\n        ):\n            return self.width &gt;= other.width\n\n        return False\n\n    def to_human_string(self, _: IcebergType, value: S | None) -&gt; str:\n        if value is None:\n            return \"null\"\n        elif isinstance(value, bytes):\n            return _base64encode(value)\n        else:\n            return str(value)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the TruncateTransform class.\"\"\"\n        return f\"TruncateTransform(width={self._width})\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        pyiceberg_core_transform = _try_import(\"pyiceberg_core\", extras_name=\"pyiceberg-core\").transform\n\n        return _pyiceberg_transform_wrapper(pyiceberg_core_transform.truncate, self._width)\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.TruncateTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the TruncateTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the TruncateTransform class.\"\"\"\n    return f\"TruncateTransform(width={self._width})\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.UnknownTransform","title":"<code>UnknownTransform</code>","text":"<p>               Bases: <code>Transform[S, T]</code></p> <p>A transform that represents when an unknown transform is provided.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>str</code> <p>A string name of a transform.</p> required <p>Other Parameters:</p> Name Type Description <code>source_type</code> <code>IcebergType</code> <p>An Iceberg <code>Type</code>.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class UnknownTransform(Transform[S, T]):\n    \"\"\"A transform that represents when an unknown transform is provided.\n\n    Args:\n      transform (str): A string name of a transform.\n\n    Keyword Args:\n      source_type (IcebergType): An Iceberg `Type`.\n    \"\"\"\n\n    root: LiteralType[\"unknown\"] = Field(default=\"unknown\")  # noqa: F821\n    _transform: str = PrivateAttr()\n\n    def __init__(self, transform: str, **data: Any):\n        super().__init__(**data)\n        self._transform = transform\n\n    def transform(self, source: IcebergType) -&gt; Callable[[S | None], T | None]:\n        raise AttributeError(f\"Cannot apply unsupported transform: {self}\")\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return False\n\n    def result_type(self, source: IcebergType) -&gt; StringType:\n        return StringType()\n\n    def project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        return None\n\n    def strict_project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        return None\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the UnknownTransform class.\"\"\"\n        return f\"UnknownTransform(transform={repr(self._transform)})\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        raise NotImplementedError()\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.UnknownTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the UnknownTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the UnknownTransform class.\"\"\"\n    return f\"UnknownTransform(transform={repr(self._transform)})\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.VoidTransform","title":"<code>VoidTransform</code>","text":"<p>               Bases: <code>Transform[S, None]</code>, <code>Singleton</code></p> <p>A transform that always returns None.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class VoidTransform(Transform[S, None], Singleton):\n    \"\"\"A transform that always returns None.\"\"\"\n\n    root: str = \"void\"\n\n    def transform(self, source: IcebergType) -&gt; Callable[[S | None], T | None]:\n        return lambda v: None\n\n    def can_transform(self, _: IcebergType) -&gt; bool:\n        return True\n\n    def result_type(self, source: IcebergType) -&gt; IcebergType:\n        return source\n\n    def project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        return None\n\n    def strict_project(self, name: str, pred: BoundPredicate) -&gt; UnboundPredicate | None:\n        return None\n\n    def to_human_string(self, _: IcebergType, value: S | None) -&gt; str:\n        return \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the VoidTransform class.\"\"\"\n        return \"VoidTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        try:\n            import pyarrow as pa\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For partition transforms, PyArrow needs to be installed\") from e\n\n        return lambda arr: pa.nulls(len(arr), type=arr.type)\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.VoidTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the VoidTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the VoidTransform class.\"\"\"\n    return \"VoidTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.YearTransform","title":"<code>YearTransform</code>","text":"<p>               Bases: <code>TimeTransform[S]</code></p> <p>Transforms a datetime value into a year value.</p> Example <p>transform = YearTransform() transform.transform(TimestampType())(1512151975038194) 47</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>class YearTransform(TimeTransform[S]):\n    \"\"\"Transforms a datetime value into a year value.\n\n    Example:\n        &gt;&gt;&gt; transform = YearTransform()\n        &gt;&gt;&gt; transform.transform(TimestampType())(1512151975038194)\n        47\n    \"\"\"\n\n    root: LiteralType[\"year\"] = Field(default=\"year\")  # noqa: F821\n\n    def transform(self, source: IcebergType) -&gt; Callable[[S | None], int | None]:\n        if isinstance(source, DateType):\n\n            def year_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.date):\n                    v = datetime.date_to_days(v)\n\n                return datetime.days_to_years(v)\n\n        elif isinstance(source, (TimestampType, TimestamptzType)):\n\n            def year_func(v: Any) -&gt; int:\n                if isinstance(v, py_datetime.datetime):\n                    v = datetime.datetime_to_micros(v)\n\n                return datetime.micros_to_years(v)\n\n        elif isinstance(source, (TimestampNanoType, TimestamptzNanoType)):\n\n            def year_func(v: Any) -&gt; int:\n                # python datetime has no nanoseconds support.\n                # nanosecond datetimes will be expressed as int as a workaround\n                return datetime.nanos_to_years(v)\n\n        else:\n            raise ValueError(f\"Cannot apply year transform for type: {source}\")\n\n        return lambda v: year_func(v) if v is not None else None\n\n    def can_transform(self, source: IcebergType) -&gt; bool:\n        return isinstance(source, (DateType, TimestampType, TimestamptzType, TimestampNanoType, TimestamptzNanoType))\n\n    @property\n    def granularity(self) -&gt; TimeResolution:\n        return TimeResolution.YEAR\n\n    def to_human_string(self, _: IcebergType, value: S | None) -&gt; str:\n        return datetime.to_human_year(value) if isinstance(value, int) else \"null\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the YearTransform class.\"\"\"\n        return \"YearTransform()\"\n\n    def pyarrow_transform(self, source: IcebergType) -&gt; \"Callable[[pa.Array], pa.Array]\":\n        pa = _try_import(\"pyarrow\")\n        pyiceberg_core_transform = _try_import(\"pyiceberg_core\", extras_name=\"pyiceberg-core\").transform\n        return _pyiceberg_transform_wrapper(pyiceberg_core_transform.year, expected_type=pa.int32())\n</code></pre>"},{"location":"reference/pyiceberg/transforms/#pyiceberg.transforms.YearTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the YearTransform class.</p> Source code in <code>pyiceberg/transforms.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the YearTransform class.\"\"\"\n    return \"YearTransform()\"\n</code></pre>"},{"location":"reference/pyiceberg/typedef/","title":"typedef","text":""},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Identifier","title":"<code>Identifier = tuple[str, ...]</code>  <code>module-attribute</code>","text":"<p>A tuple of strings representing a table identifier.</p> <p>Each string in the tuple represents a part of the table's unique path. For example, a table in a namespace might be identified as:</p> <pre><code>(\"namespace\", \"table_name\")\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; identifier: Identifier = (\"namespace\", \"table_name\")\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Properties","title":"<code>Properties = dict[str, Any]</code>  <code>module-attribute</code>","text":"<p>A dictionary type for properties in PyIceberg.</p>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.RecursiveDict","title":"<code>RecursiveDict = dict[str, Union[str, 'RecursiveDict']]</code>  <code>module-attribute</code>","text":"<p>A recursive dictionary type for nested structures in PyIceberg.</p>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.FrozenDict","title":"<code>FrozenDict</code>","text":"<p>               Bases: <code>dict[Any, Any]</code></p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class FrozenDict(dict[Any, Any]):\n    def __setitem__(self, instance: Any, value: Any) -&gt; None:\n        \"\"\"Assign a value to a FrozenDict.\"\"\"\n        raise AttributeError(\"FrozenDict does not support assignment\")\n\n    def update(self, *args: Any, **kwargs: Any) -&gt; None:\n        raise AttributeError(\"FrozenDict does not support .update()\")\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.FrozenDict.__setitem__","title":"<code>__setitem__(instance, value)</code>","text":"<p>Assign a value to a FrozenDict.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __setitem__(self, instance: Any, value: Any) -&gt; None:\n    \"\"\"Assign a value to a FrozenDict.\"\"\"\n    raise AttributeError(\"FrozenDict does not support assignment\")\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.IcebergBaseModel","title":"<code>IcebergBaseModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>This class extends the Pydantic BaseModel to set default values by overriding them.</p> <p>This is because we always want to set by_alias to True. In Python, the dash can't be used in variable names, and this is used throughout the Iceberg spec.</p> <p>The same goes for exclude_none, if a field is None we want to omit it from serialization, for example, the doc attribute on the NestedField object. Default non-null values will be serialized.</p> <p>This is recommended by Pydantic: https://pydantic-docs.helpmanual.io/usage/model_config/#change-behaviour-globally</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class IcebergBaseModel(BaseModel):\n    \"\"\"\n    This class extends the Pydantic BaseModel to set default values by overriding them.\n\n    This is because we always want to set by_alias to True. In Python, the dash can't\n    be used in variable names, and this is used throughout the Iceberg spec.\n\n    The same goes for exclude_none, if a field is None we want to omit it from\n    serialization, for example, the doc attribute on the NestedField object.\n    Default non-null values will be serialized.\n\n    This is recommended by Pydantic:\n    https://pydantic-docs.helpmanual.io/usage/model_config/#change-behaviour-globally\n    \"\"\"\n\n    model_config = ConfigDict(populate_by_name=True, frozen=True)\n\n    def _exclude_private_properties(self, exclude: set[str] | None = None) -&gt; set[str]:\n        # A small trick to exclude private properties. Properties are serialized by pydantic,\n        # regardless if they start with an underscore.\n        # This will look at the dict, and find the fields and exclude them\n        return set.union(\n            {field for field in self.__dict__ if field.startswith(\"_\") and not field == \"__root__\"}, exclude or set()\n        )\n\n    def model_dump(\n        self, exclude_none: bool = True, exclude: set[str] | None = None, by_alias: bool = True, **kwargs: Any\n    ) -&gt; dict[str, Any]:\n        return super().model_dump(\n            exclude_none=exclude_none, exclude=self._exclude_private_properties(exclude), by_alias=by_alias, **kwargs\n        )\n\n    def model_dump_json(\n        self, exclude_none: bool = True, exclude: set[str] | None = None, by_alias: bool = True, **kwargs: Any\n    ) -&gt; str:\n        return super().model_dump_json(\n            exclude_none=exclude_none, exclude=self._exclude_private_properties(exclude), by_alias=by_alias, **kwargs\n        )\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.IcebergRootModel","title":"<code>IcebergRootModel</code>","text":"<p>               Bases: <code>RootModel[T]</code>, <code>Generic[T]</code></p> <p>This class extends the Pydantic BaseModel to set default values by overriding them.</p> <p>This is because we always want to set by_alias to True. In Python, the dash can't be used in variable names, and this is used throughout the Iceberg spec.</p> <p>The same goes for exclude_none, if a field is None we want to omit it from serialization, for example, the doc attribute on the NestedField object. Default non-null values will be serialized.</p> <p>This is recommended by Pydantic: https://pydantic-docs.helpmanual.io/usage/model_config/#change-behaviour-globally</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class IcebergRootModel(RootModel[T], Generic[T]):\n    \"\"\"\n    This class extends the Pydantic BaseModel to set default values by overriding them.\n\n    This is because we always want to set by_alias to True. In Python, the dash can't\n    be used in variable names, and this is used throughout the Iceberg spec.\n\n    The same goes for exclude_none, if a field is None we want to omit it from\n    serialization, for example, the doc attribute on the NestedField object.\n    Default non-null values will be serialized.\n\n    This is recommended by Pydantic:\n    https://pydantic-docs.helpmanual.io/usage/model_config/#change-behaviour-globally\n    \"\"\"\n\n    model_config = ConfigDict(frozen=True)\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.KeyDefaultDict","title":"<code>KeyDefaultDict</code>","text":"<p>               Bases: <code>dict[K, V]</code></p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class KeyDefaultDict(dict[K, V]):\n    def __init__(self, default_factory: Callable[[K], V]):\n        super().__init__()\n        self.default_factory = default_factory\n\n    def __missing__(self, key: K) -&gt; V:\n        \"\"\"Define behavior if you access a non-existent key in a KeyDefaultDict.\"\"\"\n        val = self.default_factory(key)\n        self[key] = val\n        return val\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.KeyDefaultDict.__missing__","title":"<code>__missing__(key)</code>","text":"<p>Define behavior if you access a non-existent key in a KeyDefaultDict.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __missing__(self, key: K) -&gt; V:\n    \"\"\"Define behavior if you access a non-existent key in a KeyDefaultDict.\"\"\"\n    val = self.default_factory(key)\n    self[key] = val\n    return val\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record","title":"<code>Record</code>","text":"<p>               Bases: <code>StructProtocol</code></p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>class Record(StructProtocol):\n    __slots__ = (\"_data\",)\n    _data: list[Any]\n\n    @classmethod\n    def _bind(cls, struct: StructType, **arguments: Any) -&gt; Self:\n        return cls(*[arguments[field.name] if field.name in arguments else field.initial_default for field in struct.fields])\n\n    def __init__(self, *data: Any) -&gt; None:\n        self._data = list(data)\n\n    def __setitem__(self, pos: int, value: Any) -&gt; None:\n        \"\"\"Assign a value to a Record.\"\"\"\n        self._data[pos] = value\n\n    def __getitem__(self, pos: int) -&gt; Any:\n        \"\"\"Fetch a value from a Record.\"\"\"\n        return self._data[pos]\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Record class.\"\"\"\n        return self._data == other._data if isinstance(other, Record) else False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Record class.\"\"\"\n        return f\"{self.__class__.__name__}[{', '.join(str(v) for v in self._data)}]\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of fields in the Record class.\"\"\"\n        return len(self._data)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return hash value of the Record class.\"\"\"\n        return hash(str(self))\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Record class.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Record class.\"\"\"\n    return self._data == other._data if isinstance(other, Record) else False\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__getitem__","title":"<code>__getitem__(pos)</code>","text":"<p>Fetch a value from a Record.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __getitem__(self, pos: int) -&gt; Any:\n    \"\"\"Fetch a value from a Record.\"\"\"\n    return self._data[pos]\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__hash__","title":"<code>__hash__()</code>","text":"<p>Return hash value of the Record class.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash value of the Record class.\"\"\"\n    return hash(str(self))\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of fields in the Record class.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of fields in the Record class.\"\"\"\n    return len(self._data)\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Record class.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Record class.\"\"\"\n    return f\"{self.__class__.__name__}[{', '.join(str(v) for v in self._data)}]\"\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.Record.__setitem__","title":"<code>__setitem__(pos, value)</code>","text":"<p>Assign a value to a Record.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>def __setitem__(self, pos: int, value: Any) -&gt; None:\n    \"\"\"Assign a value to a Record.\"\"\"\n    self._data[pos] = value\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.StructProtocol","title":"<code>StructProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A generic protocol used by accessors to get and set at positions of an object.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>@runtime_checkable\nclass StructProtocol(Protocol):  # pragma: no cover\n    \"\"\"A generic protocol used by accessors to get and set at positions of an object.\"\"\"\n\n    @abstractmethod\n    def __getitem__(self, pos: int) -&gt; Any:\n        \"\"\"Fetch a value from a StructProtocol.\"\"\"\n\n    @abstractmethod\n    def __setitem__(self, pos: int, value: Any) -&gt; None:\n        \"\"\"Assign a value to a StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.StructProtocol.__getitem__","title":"<code>__getitem__(pos)</code>  <code>abstractmethod</code>","text":"<p>Fetch a value from a StructProtocol.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, pos: int) -&gt; Any:\n    \"\"\"Fetch a value from a StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/typedef/#pyiceberg.typedef.StructProtocol.__setitem__","title":"<code>__setitem__(pos, value)</code>  <code>abstractmethod</code>","text":"<p>Assign a value to a StructProtocol.</p> Source code in <code>pyiceberg/typedef.py</code> <pre><code>@abstractmethod\ndef __setitem__(self, pos: int, value: Any) -&gt; None:\n    \"\"\"Assign a value to a StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/types/","title":"types","text":"<p>Data types used in describing Iceberg schemas.</p> <p>This module implements the data types described in the Iceberg specification for Iceberg schemas. To describe an Iceberg table schema, these classes can be used in the construction of a StructType instance.</p> Example <p>str(StructType( ...     NestedField(1, \"required_field\", StringType(), True), ...     NestedField(2, \"optional_field\", IntegerType()) ... )) 'struct&lt;1: required_field: required string, 2: optional_field: optional int&gt;'</p> Notes <ul> <li>https://iceberg.apache.org/spec/#primitive-types</li> </ul>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.BinaryType","title":"<code>BinaryType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Binary data type in Iceberg can be represented using an instance of this class.</p> <p>Binaries in Iceberg are arbitrary-length byte arrays.</p> Example <p>column_foo = BinaryType() isinstance(column_foo, BinaryType) True column_foo BinaryType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class BinaryType(PrimitiveType):\n    \"\"\"A Binary data type in Iceberg can be represented using an instance of this class.\n\n    Binaries in Iceberg are arbitrary-length byte arrays.\n\n    Example:\n        &gt;&gt;&gt; column_foo = BinaryType()\n        &gt;&gt;&gt; isinstance(column_foo, BinaryType)\n        True\n        &gt;&gt;&gt; column_foo\n        BinaryType()\n    \"\"\"\n\n    root: Literal[\"binary\"] = Field(default=\"binary\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.BooleanType","title":"<code>BooleanType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A boolean data type in Iceberg can be represented using an instance of this class.</p> Example <p>column_foo = BooleanType() isinstance(column_foo, BooleanType) True column_foo BooleanType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class BooleanType(PrimitiveType):\n    \"\"\"A boolean data type in Iceberg can be represented using an instance of this class.\n\n    Example:\n        &gt;&gt;&gt; column_foo = BooleanType()\n        &gt;&gt;&gt; isinstance(column_foo, BooleanType)\n        True\n        &gt;&gt;&gt; column_foo\n        BooleanType()\n    \"\"\"\n\n    root: Literal[\"boolean\"] = Field(default=\"boolean\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DateType","title":"<code>DateType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Date data type in Iceberg can be represented using an instance of this class.</p> <p>Dates in Iceberg are calendar dates without a timezone or time.</p> Example <p>column_foo = DateType() isinstance(column_foo, DateType) True column_foo DateType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class DateType(PrimitiveType):\n    \"\"\"A Date data type in Iceberg can be represented using an instance of this class.\n\n    Dates in Iceberg are calendar dates without a timezone or time.\n\n    Example:\n        &gt;&gt;&gt; column_foo = DateType()\n        &gt;&gt;&gt; isinstance(column_foo, DateType)\n        True\n        &gt;&gt;&gt; column_foo\n        DateType()\n    \"\"\"\n\n    root: Literal[\"date\"] = Field(default=\"date\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType","title":"<code>DecimalType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A decimal data type in Iceberg.</p> Example <p>DecimalType(32, 3) DecimalType(precision=32, scale=3) DecimalType(8, 3) == DecimalType(8, 3) True</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class DecimalType(PrimitiveType):\n    \"\"\"A decimal data type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; DecimalType(32, 3)\n        DecimalType(precision=32, scale=3)\n        &gt;&gt;&gt; DecimalType(8, 3) == DecimalType(8, 3)\n        True\n    \"\"\"\n\n    root: tuple[int, int]\n\n    def __init__(self, precision: int, scale: int) -&gt; None:\n        super().__init__(root=(precision, scale))\n\n    @model_serializer\n    def ser_model(self) -&gt; str:\n        \"\"\"Serialize the model to a string.\"\"\"\n        return f\"decimal({self.precision}, {self.scale})\"\n\n    @property\n    def precision(self) -&gt; int:\n        \"\"\"Return the precision of the decimal.\"\"\"\n        return self.root[0]\n\n    @property\n    def scale(self) -&gt; int:\n        \"\"\"Return the scale of the decimal.\"\"\"\n        return self.root[1]\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the DecimalType class.\"\"\"\n        return f\"DecimalType(precision={self.precision}, scale={self.scale})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation.\"\"\"\n        return f\"decimal({self.precision}, {self.scale})\"\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of the tuple.\"\"\"\n        return hash(self.root)\n\n    def __getnewargs__(self) -&gt; tuple[int, int]:\n        \"\"\"Pickle the DecimalType class.\"\"\"\n        return self.precision, self.scale\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare to root to another object.\"\"\"\n        return self.root == other.root if isinstance(other, DecimalType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.precision","title":"<code>precision</code>  <code>property</code>","text":"<p>Return the precision of the decimal.</p>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.scale","title":"<code>scale</code>  <code>property</code>","text":"<p>Return the scale of the decimal.</p>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare to root to another object.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare to root to another object.\"\"\"\n    return self.root == other.root if isinstance(other, DecimalType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the DecimalType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[int, int]:\n    \"\"\"Pickle the DecimalType class.\"\"\"\n    return self.precision, self.scale\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__hash__","title":"<code>__hash__()</code>","text":"<p>Return the hash of the tuple.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of the tuple.\"\"\"\n    return hash(self.root)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the DecimalType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the DecimalType class.\"\"\"\n    return f\"DecimalType(precision={self.precision}, scale={self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation.\"\"\"\n    return f\"decimal({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DecimalType.ser_model","title":"<code>ser_model()</code>","text":"<p>Serialize the model to a string.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>@model_serializer\ndef ser_model(self) -&gt; str:\n    \"\"\"Serialize the model to a string.\"\"\"\n    return f\"decimal({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.DoubleType","title":"<code>DoubleType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Double data type in Iceberg can be represented using an instance of this class.</p> <p>Doubles in Iceberg are 64-bit IEEE 754 floating points.</p> Example <p>column_foo = DoubleType() isinstance(column_foo, DoubleType) True column_foo DoubleType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class DoubleType(PrimitiveType):\n    \"\"\"A Double data type in Iceberg can be represented using an instance of this class.\n\n    Doubles in Iceberg are 64-bit IEEE 754 floating points.\n\n    Example:\n        &gt;&gt;&gt; column_foo = DoubleType()\n        &gt;&gt;&gt; isinstance(column_foo, DoubleType)\n        True\n        &gt;&gt;&gt; column_foo\n        DoubleType()\n    \"\"\"\n\n    root: Literal[\"double\"] = Field(default=\"double\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType","title":"<code>FixedType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A fixed data type in Iceberg.</p> Example <p>FixedType(8) FixedType(length=8) FixedType(8) == FixedType(8) True FixedType(19) == FixedType(25) False</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class FixedType(PrimitiveType):\n    \"\"\"A fixed data type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; FixedType(8)\n        FixedType(length=8)\n        &gt;&gt;&gt; FixedType(8) == FixedType(8)\n        True\n        &gt;&gt;&gt; FixedType(19) == FixedType(25)\n        False\n    \"\"\"\n\n    root: int = Field()\n\n    def __init__(self, length: int) -&gt; None:\n        super().__init__(root=length)\n\n    @model_serializer\n    def ser_model(self) -&gt; str:\n        return f\"fixed[{self.root}]\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of an instance of the FixedType class.\"\"\"\n        return self.root\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation.\"\"\"\n        return f\"fixed[{self.root}]\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the FixedType class.\"\"\"\n        return f\"FixedType(length={self.root})\"\n\n    def __getnewargs__(self) -&gt; tuple[int]:\n        \"\"\"Pickle the FixedType class.\"\"\"\n        return (self.root,)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the FixedType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[int]:\n    \"\"\"Pickle the FixedType class.\"\"\"\n    return (self.root,)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of an instance of the FixedType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of an instance of the FixedType class.\"\"\"\n    return self.root\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the FixedType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the FixedType class.\"\"\"\n    return f\"FixedType(length={self.root})\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FixedType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation.\"\"\"\n    return f\"fixed[{self.root}]\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.FloatType","title":"<code>FloatType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Float data type in Iceberg can be represented using an instance of this class.</p> <p>Floats in Iceberg are 32-bit IEEE 754 floating points and can be promoted to Doubles.</p> Example <p>column_foo = FloatType() isinstance(column_foo, FloatType) True column_foo FloatType()</p> <p>Attributes:</p> Name Type Description <code>max</code> <code>float</code> <p>The maximum allowed value for Floats, inherited from the canonical Iceberg implementation in Java. (returns <code>3.4028235e38</code>)</p> <code>min</code> <code>float</code> <p>The minimum allowed value for Floats, inherited from the canonical Iceberg implementation in Java (returns <code>-3.4028235e38</code>)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class FloatType(PrimitiveType):\n    \"\"\"A Float data type in Iceberg can be represented using an instance of this class.\n\n    Floats in Iceberg are 32-bit IEEE 754 floating points and can be promoted to Doubles.\n\n    Example:\n        &gt;&gt;&gt; column_foo = FloatType()\n        &gt;&gt;&gt; isinstance(column_foo, FloatType)\n        True\n        &gt;&gt;&gt; column_foo\n        FloatType()\n\n    Attributes:\n        max (float): The maximum allowed value for Floats, inherited from the canonical Iceberg implementation\n            in Java. (returns `3.4028235e38`)\n        min (float): The minimum allowed value for Floats, inherited from the canonical Iceberg implementation\n            in Java (returns `-3.4028235e38`)\n    \"\"\"\n\n    max: ClassVar[float] = 3.4028235e38\n    min: ClassVar[float] = -3.4028235e38\n\n    root: Literal[\"float\"] = Field(default=\"float\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.IcebergType","title":"<code>IcebergType</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Base type for all Iceberg Types.</p> Example <p>str(IcebergType()) 'IcebergType()' repr(IcebergType()) 'IcebergType()'</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class IcebergType(IcebergBaseModel):\n    \"\"\"Base type for all Iceberg Types.\n\n    Example:\n        &gt;&gt;&gt; str(IcebergType())\n        'IcebergType()'\n        &gt;&gt;&gt; repr(IcebergType())\n        'IcebergType()'\n    \"\"\"\n\n    @model_validator(mode=\"wrap\")\n    @classmethod\n    def handle_primitive_type(cls, v: Any, handler: ValidatorFunctionWrapHandler) -&gt; IcebergType:\n        # Pydantic works mostly around dicts, and there seems to be something\n        # by not serializing into a RootModel, might revisit this.\n        if isinstance(v, str):\n            if v == \"boolean\":\n                return BooleanType()\n            elif v == \"string\":\n                return StringType()\n            elif v == \"int\":\n                return IntegerType()\n            elif v == \"long\":\n                return LongType()\n            if v == \"float\":\n                return FloatType()\n            if v == \"double\":\n                return DoubleType()\n            if v == \"timestamp\":\n                return TimestampType()\n            if v == \"timestamptz\":\n                return TimestamptzType()\n            if v == \"timestamp_ns\":\n                return TimestampNanoType()\n            if v == \"timestamptz_ns\":\n                return TimestamptzNanoType()\n            if v == \"date\":\n                return DateType()\n            if v == \"time\":\n                return TimeType()\n            if v == \"uuid\":\n                return UUIDType()\n            if v == \"binary\":\n                return BinaryType()\n            if v == \"unknown\":\n                return UnknownType()\n            if v.startswith(\"fixed\"):\n                return FixedType(_parse_fixed_type(v))\n            if v.startswith(\"decimal\"):\n                precision, scale = _parse_decimal_type(v)\n                return DecimalType(precision, scale)\n            else:\n                raise ValueError(f\"Type not recognized: {v}\")\n        if isinstance(v, dict) and cls == IcebergType:\n            complex_type = v.get(\"type\")\n            if complex_type == \"list\":\n                return ListType(**v)\n            elif complex_type == \"map\":\n                return MapType(**v)\n            elif complex_type == \"struct\":\n                return StructType(**v)\n            else:\n                return NestedField(**v)\n        return handler(v)\n\n    @property\n    def is_primitive(self) -&gt; bool:\n        return isinstance(self, PrimitiveType)\n\n    @property\n    def is_struct(self) -&gt; bool:\n        return isinstance(self, StructType)\n\n    def minimum_format_version(self) -&gt; TableVersion:\n        \"\"\"Minimum Iceberg format version after which this type is supported.\"\"\"\n        return 1\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.IcebergType.minimum_format_version","title":"<code>minimum_format_version()</code>","text":"<p>Minimum Iceberg format version after which this type is supported.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def minimum_format_version(self) -&gt; TableVersion:\n    \"\"\"Minimum Iceberg format version after which this type is supported.\"\"\"\n    return 1\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.IntegerType","title":"<code>IntegerType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>An Integer data type in Iceberg can be represented using an instance of this class.</p> <p>Integers in Iceberg are 32-bit signed and can be promoted to Longs.</p> Example <p>column_foo = IntegerType() isinstance(column_foo, IntegerType) True</p> <p>Attributes:</p> Name Type Description <code>max</code> <code>int</code> <p>The maximum allowed value for Integers, inherited from the canonical Iceberg implementation in Java (returns <code>2147483647</code>)</p> <code>min</code> <code>int</code> <p>The minimum allowed value for Integers, inherited from the canonical Iceberg implementation in Java (returns <code>-2147483648</code>)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class IntegerType(PrimitiveType):\n    \"\"\"An Integer data type in Iceberg can be represented using an instance of this class.\n\n    Integers in Iceberg are 32-bit signed and can be promoted to Longs.\n\n    Example:\n        &gt;&gt;&gt; column_foo = IntegerType()\n        &gt;&gt;&gt; isinstance(column_foo, IntegerType)\n        True\n\n    Attributes:\n        max (int): The maximum allowed value for Integers, inherited from the canonical Iceberg implementation\n            in Java (returns `2147483647`)\n        min (int): The minimum allowed value for Integers, inherited from the canonical Iceberg implementation\n            in Java (returns `-2147483648`)\n    \"\"\"\n\n    root: Literal[\"int\"] = Field(default=\"int\")\n\n    max: ClassVar[int] = 2147483647\n    min: ClassVar[int] = -2147483648\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType","title":"<code>ListType</code>","text":"<p>               Bases: <code>IcebergType</code></p> <p>A list type in Iceberg.</p> Example <p>ListType(element_id=3, element_type=StringType(), element_required=True) ListType(element_id=3, element_type=StringType(), element_required=True)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class ListType(IcebergType):\n    \"\"\"A list type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; ListType(element_id=3, element_type=StringType(), element_required=True)\n        ListType(element_id=3, element_type=StringType(), element_required=True)\n    \"\"\"\n\n    type: Literal[\"list\"] = Field(default=\"list\")\n    element_id: int = Field(alias=\"element-id\")\n    element_type: SerializeAsAny[IcebergType] = Field(alias=\"element\")\n    element_required: bool = Field(alias=\"element-required\", default=True)\n    _element_field: NestedField = PrivateAttr()\n    _hash: int = PrivateAttr()\n\n    def __init__(\n        self, element_id: int | None = None, element: IcebergType | None = None, element_required: bool = True, **data: Any\n    ):\n        data[\"element-id\"] = data[\"element-id\"] if \"element-id\" in data else element_id\n        data[\"element\"] = element or data[\"element_type\"]\n        data[\"element-required\"] = data[\"element-required\"] if \"element-required\" in data else element_required\n        super().__init__(**data)\n        self._hash = hash(data.values())\n\n    @cached_property\n    def element_field(self) -&gt; NestedField:\n        return NestedField(\n            name=\"element\",\n            field_id=self.element_id,\n            field_type=self.element_type,\n            required=self.element_required,\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the ListType class.\"\"\"\n        return f\"list&lt;{self.element_type}&gt;\"\n\n    def __getnewargs__(self) -&gt; tuple[int, IcebergType, bool]:\n        \"\"\"Pickle the ListType class.\"\"\"\n        return (self.element_id, self.element_type, self.element_required)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Use the cache hash value of the StructType class.\"\"\"\n        return self._hash\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare the list type to another list type.\"\"\"\n        return self.element_field == other.element_field if isinstance(other, ListType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare the list type to another list type.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare the list type to another list type.\"\"\"\n    return self.element_field == other.element_field if isinstance(other, ListType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the ListType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[int, IcebergType, bool]:\n    \"\"\"Pickle the ListType class.\"\"\"\n    return (self.element_id, self.element_type, self.element_required)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType.__hash__","title":"<code>__hash__()</code>","text":"<p>Use the cache hash value of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Use the cache hash value of the StructType class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.ListType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the ListType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the ListType class.\"\"\"\n    return f\"list&lt;{self.element_type}&gt;\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.LongType","title":"<code>LongType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Long data type in Iceberg can be represented using an instance of this class.</p> <p>Longs in Iceberg are 64-bit signed integers.</p> Example <p>column_foo = LongType() isinstance(column_foo, LongType) True column_foo LongType() str(column_foo) 'long'</p> <p>Attributes:</p> Name Type Description <code>max</code> <code>int</code> <p>The maximum allowed value for Longs, inherited from the canonical Iceberg implementation in Java. (returns <code>9223372036854775807</code>)</p> <code>min</code> <code>int</code> <p>The minimum allowed value for Longs, inherited from the canonical Iceberg implementation in Java (returns <code>-9223372036854775808</code>)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class LongType(PrimitiveType):\n    \"\"\"A Long data type in Iceberg can be represented using an instance of this class.\n\n    Longs in Iceberg are 64-bit signed integers.\n\n    Example:\n        &gt;&gt;&gt; column_foo = LongType()\n        &gt;&gt;&gt; isinstance(column_foo, LongType)\n        True\n        &gt;&gt;&gt; column_foo\n        LongType()\n        &gt;&gt;&gt; str(column_foo)\n        'long'\n\n    Attributes:\n        max (int): The maximum allowed value for Longs, inherited from the canonical Iceberg implementation\n            in Java. (returns `9223372036854775807`)\n        min (int): The minimum allowed value for Longs, inherited from the canonical Iceberg implementation\n            in Java (returns `-9223372036854775808`)\n    \"\"\"\n\n    root: Literal[\"long\"] = Field(default=\"long\")\n\n    max: ClassVar[int] = 9223372036854775807\n    min: ClassVar[int] = -9223372036854775808\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType","title":"<code>MapType</code>","text":"<p>               Bases: <code>IcebergType</code></p> <p>A map type in Iceberg.</p> Example <p>MapType(key_id=1, key_type=StringType(), value_id=2, value_type=IntegerType(), value_required=True) MapType(key_id=1, key_type=StringType(), value_id=2, value_type=IntegerType(), value_required=True)</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class MapType(IcebergType):\n    \"\"\"A map type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; MapType(key_id=1, key_type=StringType(), value_id=2, value_type=IntegerType(), value_required=True)\n        MapType(key_id=1, key_type=StringType(), value_id=2, value_type=IntegerType(), value_required=True)\n    \"\"\"\n\n    type: Literal[\"map\"] = Field(default=\"map\")\n    key_id: int = Field(alias=\"key-id\")\n    key_type: SerializeAsAny[IcebergType] = Field(alias=\"key\")\n    value_id: int = Field(alias=\"value-id\")\n    value_type: SerializeAsAny[IcebergType] = Field(alias=\"value\")\n    value_required: bool = Field(alias=\"value-required\", default=True)\n    _hash: int = PrivateAttr()\n\n    def __init__(\n        self,\n        key_id: int | None = None,\n        key_type: IcebergType | None = None,\n        value_id: int | None = None,\n        value_type: IcebergType | None = None,\n        value_required: bool = True,\n        **data: Any,\n    ):\n        data[\"key-id\"] = data[\"key-id\"] if \"key-id\" in data else key_id\n        data[\"key\"] = data[\"key\"] if \"key\" in data else key_type\n        data[\"value-id\"] = data[\"value-id\"] if \"value-id\" in data else value_id\n        data[\"value\"] = data[\"value\"] if \"value\" in data else value_type\n        data[\"value-required\"] = data[\"value-required\"] if \"value-required\" in data else value_required\n        super().__init__(**data)\n        self._hash = hash(self.__getnewargs__())\n\n    @cached_property\n    def key_field(self) -&gt; NestedField:\n        return NestedField(\n            name=\"key\",\n            field_id=self.key_id,\n            field_type=self.key_type,\n            required=True,\n        )\n\n    @cached_property\n    def value_field(self) -&gt; NestedField:\n        return NestedField(\n            name=\"value\",\n            field_id=self.value_id,\n            field_type=self.value_type,\n            required=self.value_required,\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the MapType class.\"\"\"\n        return f\"map&lt;{self.key_type}, {self.value_type}&gt;\"\n\n    def __getnewargs__(self) -&gt; tuple[int, IcebergType, int, IcebergType, bool]:\n        \"\"\"Pickle the MapType class.\"\"\"\n        return (self.key_id, self.key_type, self.value_id, self.value_type, self.value_required)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of the MapType.\"\"\"\n        return self._hash\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare the MapType to another object.\"\"\"\n        return (\n            self.key_field == other.key_field and self.value_field == other.value_field if isinstance(other, MapType) else False\n        )\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare the MapType to another object.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare the MapType to another object.\"\"\"\n    return (\n        self.key_field == other.key_field and self.value_field == other.value_field if isinstance(other, MapType) else False\n    )\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the MapType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[int, IcebergType, int, IcebergType, bool]:\n    \"\"\"Pickle the MapType class.\"\"\"\n    return (self.key_id, self.key_type, self.value_id, self.value_type, self.value_required)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType.__hash__","title":"<code>__hash__()</code>","text":"<p>Return the hash of the MapType.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of the MapType.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.MapType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the MapType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the MapType class.\"\"\"\n    return f\"map&lt;{self.key_type}, {self.value_type}&gt;\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.NestedField","title":"<code>NestedField</code>","text":"<p>               Bases: <code>IcebergType</code></p> <p>Represents a field of a struct, a map key, a map value, or a list element.</p> <p>This is where field IDs, names, docs, and nullability are tracked.</p> Example <p>str(NestedField( ...     field_id=1, ...     name='foo', ...     field_type=FixedType(22), ...     required=False, ... )) '1: foo: optional fixed[22]' str(NestedField( ...     field_id=2, ...     name='bar', ...     field_type=LongType(), ...     is_optional=False, ...     doc=\"Just a long\" ... )) '2: bar: required long (Just a long)' str(NestedField( ...     field_id=3, ...     name='baz', ...     field_type=\"string\", ...     required=True, ...     doc=\"A string field\" ... )) '3: baz: required string (A string field)'</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class NestedField(IcebergType):\n    \"\"\"Represents a field of a struct, a map key, a map value, or a list element.\n\n    This is where field IDs, names, docs, and nullability are tracked.\n\n    Example:\n        &gt;&gt;&gt; str(NestedField(\n        ...     field_id=1,\n        ...     name='foo',\n        ...     field_type=FixedType(22),\n        ...     required=False,\n        ... ))\n        '1: foo: optional fixed[22]'\n        &gt;&gt;&gt; str(NestedField(\n        ...     field_id=2,\n        ...     name='bar',\n        ...     field_type=LongType(),\n        ...     is_optional=False,\n        ...     doc=\"Just a long\"\n        ... ))\n        '2: bar: required long (Just a long)'\n        &gt;&gt;&gt; str(NestedField(\n        ...     field_id=3,\n        ...     name='baz',\n        ...     field_type=\"string\",\n        ...     required=True,\n        ...     doc=\"A string field\"\n        ... ))\n        '3: baz: required string (A string field)'\n    \"\"\"\n\n    field_id: int = Field(alias=\"id\")\n    name: str = Field()\n    field_type: SerializeAsAny[IcebergType] = Field(alias=\"type\")\n    required: bool = Field(default=False)\n    doc: str | None = Field(default=None, repr=False)\n    initial_default: DefaultValue | None = Field(alias=\"initial-default\", default=None, repr=True)  # type: ignore\n    write_default: DefaultValue | None = Field(alias=\"write-default\", default=None, repr=True)  # type: ignore\n\n    @field_validator(\"field_type\", mode=\"before\")\n    def convert_field_type(cls, v: Any) -&gt; IcebergType:\n        \"\"\"Convert string values into IcebergType instances.\"\"\"\n        if isinstance(v, str):\n            try:\n                return IcebergType.handle_primitive_type(v, None)\n            except ValueError as e:\n                raise ValueError(f\"Unsupported field type: '{v}'\") from e\n        return v\n\n    def __init__(\n        self,\n        field_id: int | None = None,\n        name: str | None = None,\n        field_type: IcebergType | str | None = None,\n        required: bool = False,\n        doc: str | None = None,\n        initial_default: Any | None = None,\n        write_default: L | None = None,\n        **data: Any,\n    ):\n        # We need an init when we want to use positional arguments, but\n        # need also to support the aliases.\n        data[\"id\"] = data[\"id\"] if \"id\" in data else field_id\n        data[\"name\"] = name\n        data[\"type\"] = data[\"type\"] if \"type\" in data else field_type\n        data[\"required\"] = required\n        data[\"doc\"] = doc\n        data[\"initial-default\"] = data[\"initial-default\"] if \"initial-default\" in data else initial_default\n        data[\"write-default\"] = data[\"write-default\"] if \"write-default\" in data else write_default\n        super().__init__(**data)\n\n    @model_serializer()\n    def serialize_model(self) -&gt; dict[str, Any]:\n        from pyiceberg.conversions import to_json\n\n        fields = {\n            \"id\": self.field_id,\n            \"name\": self.name,\n            \"type\": self.field_type,\n            \"required\": self.required,\n        }\n\n        if self.doc is not None:\n            fields[\"doc\"] = self.doc\n        if self.initial_default is not None:\n            fields[\"initial-default\"] = to_json(self.field_type, self.initial_default)\n        if self.write_default is not None:\n            fields[\"write-default\"] = to_json(self.field_type, self.write_default)\n\n        return fields\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the NestedField class.\"\"\"\n        doc = \"\" if not self.doc else f\" ({self.doc})\"\n        req = \"required\" if self.required else \"optional\"\n        return f\"{self.field_id}: {self.name}: {req} {self.field_type}{doc}\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the NestedField class.\"\"\"\n        parts = [\n            f\"field_id={self.field_id}\",\n            f\"name={self.name!r}\",\n            f\"field_type={self.field_type!r}\",\n            f\"required={self.required}\",\n        ]\n        if self.initial_default is not None:\n            parts.append(f\"initial_default={self.initial_default!r}\")\n        if self.write_default is not None:\n            parts.append(f\"write_default={self.write_default!r}\")\n\n        return f\"NestedField({', '.join(parts)})\"\n\n    def __getnewargs__(self) -&gt; tuple[int, str, IcebergType, bool, str | None]:\n        \"\"\"Pickle the NestedField class.\"\"\"\n        return (self.field_id, self.name, self.field_type, self.required, self.doc)\n\n    @property\n    def optional(self) -&gt; bool:\n        return not self.required\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.NestedField.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the NestedField class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[int, str, IcebergType, bool, str | None]:\n    \"\"\"Pickle the NestedField class.\"\"\"\n    return (self.field_id, self.name, self.field_type, self.required, self.doc)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.NestedField.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the NestedField class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the NestedField class.\"\"\"\n    parts = [\n        f\"field_id={self.field_id}\",\n        f\"name={self.name!r}\",\n        f\"field_type={self.field_type!r}\",\n        f\"required={self.required}\",\n    ]\n    if self.initial_default is not None:\n        parts.append(f\"initial_default={self.initial_default!r}\")\n    if self.write_default is not None:\n        parts.append(f\"write_default={self.write_default!r}\")\n\n    return f\"NestedField({', '.join(parts)})\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.NestedField.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the NestedField class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the NestedField class.\"\"\"\n    doc = \"\" if not self.doc else f\" ({self.doc})\"\n    req = \"required\" if self.required else \"optional\"\n    return f\"{self.field_id}: {self.name}: {req} {self.field_type}{doc}\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.NestedField.convert_field_type","title":"<code>convert_field_type(v)</code>","text":"<p>Convert string values into IcebergType instances.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>@field_validator(\"field_type\", mode=\"before\")\ndef convert_field_type(cls, v: Any) -&gt; IcebergType:\n    \"\"\"Convert string values into IcebergType instances.\"\"\"\n    if isinstance(v, str):\n        try:\n            return IcebergType.handle_primitive_type(v, None)\n        except ValueError as e:\n            raise ValueError(f\"Unsupported field type: '{v}'\") from e\n    return v\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.PrimitiveType","title":"<code>PrimitiveType</code>","text":"<p>               Bases: <code>Singleton</code>, <code>IcebergRootModel[str]</code>, <code>IcebergType</code></p> <p>Base class for all Iceberg Primitive Types.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class PrimitiveType(Singleton, IcebergRootModel[str], IcebergType):\n    \"\"\"Base class for all Iceberg Primitive Types.\"\"\"\n\n    root: Any = Field()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the PrimitiveType class.\"\"\"\n        return f\"{type(self).__name__}()\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the PrimitiveType class.\"\"\"\n        return self.root\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.PrimitiveType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the PrimitiveType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the PrimitiveType class.\"\"\"\n    return f\"{type(self).__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.PrimitiveType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the PrimitiveType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the PrimitiveType class.\"\"\"\n    return self.root\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StringType","title":"<code>StringType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A String data type in Iceberg can be represented using an instance of this class.</p> <p>Strings in Iceberg are arbitrary-length character sequences and are encoded with UTF-8.</p> Example <p>column_foo = StringType() isinstance(column_foo, StringType) True column_foo StringType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class StringType(PrimitiveType):\n    \"\"\"A String data type in Iceberg can be represented using an instance of this class.\n\n    Strings in Iceberg are arbitrary-length character sequences and are encoded with UTF-8.\n\n    Example:\n        &gt;&gt;&gt; column_foo = StringType()\n        &gt;&gt;&gt; isinstance(column_foo, StringType)\n        True\n        &gt;&gt;&gt; column_foo\n        StringType()\n    \"\"\"\n\n    root: Literal[\"string\"] = Field(default=\"string\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType","title":"<code>StructType</code>","text":"<p>               Bases: <code>IcebergType</code></p> <p>A struct type in Iceberg.</p> Example <p>str(StructType( ...     NestedField(1, \"required_field\", StringType(), True), ...     NestedField(2, \"optional_field\", IntegerType()) ... )) 'struct&lt;1: required_field: optional string, 2: optional_field: optional int&gt;'</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class StructType(IcebergType):\n    \"\"\"A struct type in Iceberg.\n\n    Example:\n        &gt;&gt;&gt; str(StructType(\n        ...     NestedField(1, \"required_field\", StringType(), True),\n        ...     NestedField(2, \"optional_field\", IntegerType())\n        ... ))\n        'struct&lt;1: required_field: optional string, 2: optional_field: optional int&gt;'\n    \"\"\"\n\n    type: Literal[\"struct\"] = Field(default=\"struct\")\n    fields: tuple[NestedField, ...] = Field(default_factory=tuple)\n    _hash: int = PrivateAttr()\n\n    def __init__(self, *fields: NestedField, **data: Any):\n        # In case we use positional arguments, instead of keyword args\n        if fields:\n            data[\"fields\"] = fields\n        super().__init__(**data)\n        self._hash = hash(self.fields)\n\n    def field(self, field_id: int) -&gt; NestedField | None:\n        for field in self.fields:\n            if field.field_id == field_id:\n                return field\n        return None\n\n    def field_by_name(self, name: str, case_sensitive: bool = True) -&gt; NestedField | None:\n        if case_sensitive:\n            for field in self.fields:\n                if field.name == name:\n                    return field\n        else:\n            name_lower = name.lower()\n            for field in self.fields:\n                if field.name.lower() == name_lower:\n                    return field\n        return None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the StructType class.\"\"\"\n        return f\"struct&lt;{', '.join(map(str, self.fields))}&gt;\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the StructType class.\"\"\"\n        return f\"StructType(fields=({', '.join(map(repr, self.fields))},))\"\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of an instance of the StructType class.\"\"\"\n        return len(self.fields)\n\n    def __getnewargs__(self) -&gt; tuple[NestedField, ...]:\n        \"\"\"Pickle the StructType class.\"\"\"\n        return self.fields\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Use the cache hash value of the StructType class.\"\"\"\n        return self._hash\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare the object if it is equal to another object.\"\"\"\n        return self.fields == other.fields if isinstance(other, StructType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare the object if it is equal to another object.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare the object if it is equal to another object.\"\"\"\n    return self.fields == other.fields if isinstance(other, StructType) else False\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[NestedField, ...]:\n    \"\"\"Pickle the StructType class.\"\"\"\n    return self.fields\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__hash__","title":"<code>__hash__()</code>","text":"<p>Use the cache hash value of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Use the cache hash value of the StructType class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of an instance of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of an instance of the StructType class.\"\"\"\n    return len(self.fields)\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the StructType class.\"\"\"\n    return f\"StructType(fields=({', '.join(map(repr, self.fields))},))\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.StructType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the StructType class.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the StructType class.\"\"\"\n    return f\"struct&lt;{', '.join(map(str, self.fields))}&gt;\"\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.TimeType","title":"<code>TimeType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Time data type in Iceberg can be represented using an instance of this class.</p> <p>Times in Iceberg have microsecond precision and are a time of day without a date or timezone.</p> Example <p>column_foo = TimeType() isinstance(column_foo, TimeType) True column_foo TimeType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class TimeType(PrimitiveType):\n    \"\"\"A Time data type in Iceberg can be represented using an instance of this class.\n\n    Times in Iceberg have microsecond precision and are a time of day without a date or timezone.\n\n    Example:\n        &gt;&gt;&gt; column_foo = TimeType()\n        &gt;&gt;&gt; isinstance(column_foo, TimeType)\n        True\n        &gt;&gt;&gt; column_foo\n        TimeType()\n    \"\"\"\n\n    root: Literal[\"time\"] = Field(default=\"time\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.TimestampNanoType","title":"<code>TimestampNanoType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A TimestampNano data type in Iceberg can be represented using an instance of this class.</p> <p>TimestampNanos in Iceberg have nanosecond precision and include a date and a time of day without a timezone.</p> Example <p>column_foo = TimestampNanoType() isinstance(column_foo, TimestampNanoType) True column_foo TimestampNanoType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class TimestampNanoType(PrimitiveType):\n    \"\"\"A TimestampNano data type in Iceberg can be represented using an instance of this class.\n\n    TimestampNanos in Iceberg have nanosecond precision and include a date and a time of day without a timezone.\n\n    Example:\n        &gt;&gt;&gt; column_foo = TimestampNanoType()\n        &gt;&gt;&gt; isinstance(column_foo, TimestampNanoType)\n        True\n        &gt;&gt;&gt; column_foo\n        TimestampNanoType()\n    \"\"\"\n\n    root: Literal[\"timestamp_ns\"] = Field(default=\"timestamp_ns\")\n\n    def minimum_format_version(self) -&gt; TableVersion:\n        return 3\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.TimestampType","title":"<code>TimestampType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Timestamp data type in Iceberg can be represented using an instance of this class.</p> <p>Timestamps in Iceberg have microsecond precision and include a date and a time of day without a timezone.</p> Example <p>column_foo = TimestampType() isinstance(column_foo, TimestampType) True column_foo TimestampType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class TimestampType(PrimitiveType):\n    \"\"\"A Timestamp data type in Iceberg can be represented using an instance of this class.\n\n    Timestamps in Iceberg have microsecond precision and include a date and a time of day without a timezone.\n\n    Example:\n        &gt;&gt;&gt; column_foo = TimestampType()\n        &gt;&gt;&gt; isinstance(column_foo, TimestampType)\n        True\n        &gt;&gt;&gt; column_foo\n        TimestampType()\n    \"\"\"\n\n    root: Literal[\"timestamp\"] = Field(default=\"timestamp\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.TimestamptzNanoType","title":"<code>TimestamptzNanoType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A TimestamptzNano data type in Iceberg can be represented using an instance of this class.</p> <p>TimestamptzNanos in Iceberg are stored as UTC and include a date and a time of day with a timezone.</p> Example <p>column_foo = TimestamptzNanoType() isinstance(column_foo, TimestamptzNanoType) True column_foo TimestamptzNanoType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class TimestamptzNanoType(PrimitiveType):\n    \"\"\"A TimestamptzNano data type in Iceberg can be represented using an instance of this class.\n\n    TimestamptzNanos in Iceberg are stored as UTC and include a date and a time of day with a timezone.\n\n    Example:\n        &gt;&gt;&gt; column_foo = TimestamptzNanoType()\n        &gt;&gt;&gt; isinstance(column_foo, TimestamptzNanoType)\n        True\n        &gt;&gt;&gt; column_foo\n        TimestamptzNanoType()\n    \"\"\"\n\n    root: Literal[\"timestamptz_ns\"] = Field(default=\"timestamptz_ns\")\n\n    def minimum_format_version(self) -&gt; TableVersion:\n        return 3\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.TimestamptzType","title":"<code>TimestamptzType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A Timestamptz data type in Iceberg can be represented using an instance of this class.</p> <p>Timestamptzs in Iceberg are stored as UTC and include a date and a time of day with a timezone.</p> Example <p>column_foo = TimestamptzType() isinstance(column_foo, TimestamptzType) True column_foo TimestamptzType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class TimestamptzType(PrimitiveType):\n    \"\"\"A Timestamptz data type in Iceberg can be represented using an instance of this class.\n\n    Timestamptzs in Iceberg are stored as UTC and include a date and a time of day with a timezone.\n\n    Example:\n        &gt;&gt;&gt; column_foo = TimestamptzType()\n        &gt;&gt;&gt; isinstance(column_foo, TimestamptzType)\n        True\n        &gt;&gt;&gt; column_foo\n        TimestamptzType()\n    \"\"\"\n\n    root: Literal[\"timestamptz\"] = Field(default=\"timestamptz\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.UUIDType","title":"<code>UUIDType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>A UUID data type in Iceberg can be represented using an instance of this class.</p> <p>UUIDs in Iceberg are universally unique identifiers.</p> Example <p>column_foo = UUIDType() isinstance(column_foo, UUIDType) True column_foo UUIDType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class UUIDType(PrimitiveType):\n    \"\"\"A UUID data type in Iceberg can be represented using an instance of this class.\n\n    UUIDs in Iceberg are universally unique identifiers.\n\n    Example:\n        &gt;&gt;&gt; column_foo = UUIDType()\n        &gt;&gt;&gt; isinstance(column_foo, UUIDType)\n        True\n        &gt;&gt;&gt; column_foo\n        UUIDType()\n    \"\"\"\n\n    root: Literal[\"uuid\"] = Field(default=\"uuid\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.UnknownType","title":"<code>UnknownType</code>","text":"<p>               Bases: <code>PrimitiveType</code></p> <p>An unknown data type in Iceberg can be represented using an instance of this class.</p> <p>Unknowns in Iceberg are used to represent data types that are not known at the time of writing.</p> Example <p>column_foo = UnknownType() isinstance(column_foo, UnknownType) True column_foo UnknownType()</p> Source code in <code>pyiceberg/types.py</code> <pre><code>class UnknownType(PrimitiveType):\n    \"\"\"An unknown data type in Iceberg can be represented using an instance of this class.\n\n    Unknowns in Iceberg are used to represent data types that are not known at the time of writing.\n\n    Example:\n        &gt;&gt;&gt; column_foo = UnknownType()\n        &gt;&gt;&gt; isinstance(column_foo, UnknownType)\n        True\n        &gt;&gt;&gt; column_foo\n        UnknownType()\n    \"\"\"\n\n    root: Literal[\"unknown\"] = Field(default=\"unknown\")\n\n    def minimum_format_version(self) -&gt; TableVersion:\n        return 3\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.strtobool","title":"<code>strtobool(val)</code>","text":"<p>Convert a string representation of truth to true (1) or false (0).</p> <p>True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if 'val' is anything else.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def strtobool(val: str) -&gt; bool:\n    \"\"\"Convert a string representation of truth to true (1) or false (0).\n\n    True values are 'y', 'yes', 't', 'true', 'on', and '1'; false values\n    are 'n', 'no', 'f', 'false', 'off', and '0'.  Raises ValueError if\n    'val' is anything else.\n    \"\"\"\n    val = val.lower()\n    if val in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n        return True\n    elif val in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n        return False\n    else:\n        raise ValueError(f\"Invalid truth value: {val!r}\")\n</code></pre>"},{"location":"reference/pyiceberg/types/#pyiceberg.types.transform_dict_value_to_str","title":"<code>transform_dict_value_to_str(dict)</code>","text":"<p>Transform all values in the dictionary to string. Raise an error if any value is None.</p> Source code in <code>pyiceberg/types.py</code> <pre><code>def transform_dict_value_to_str(dict: dict[str, Any]) -&gt; dict[str, str]:\n    \"\"\"Transform all values in the dictionary to string. Raise an error if any value is None.\"\"\"\n    for key, value in dict.items():\n        if value is None:\n            raise ValueError(f\"None type is not a supported value in properties: {key}\")\n    return {k: str(v).lower() if isinstance(v, bool) else str(v) for k, v in dict.items()}\n</code></pre>"},{"location":"reference/pyiceberg/avro/","title":"avro","text":""},{"location":"reference/pyiceberg/avro/decoder/","title":"decoder","text":""},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder","title":"<code>BinaryDecoder</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Decodes bytes into Python physical primitives.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>class BinaryDecoder(ABC):\n    \"\"\"Decodes bytes into Python physical primitives.\"\"\"\n\n    @abstractmethod\n    def tell(self) -&gt; int:\n        \"\"\"Return the current position.\"\"\"\n\n    @abstractmethod\n    def read(self, n: int) -&gt; bytes:\n        \"\"\"Read n bytes.\"\"\"\n\n    @abstractmethod\n    def skip(self, n: int) -&gt; None:\n        \"\"\"Skip n bytes.\"\"\"\n\n    def read_boolean(self) -&gt; bool:\n        \"\"\"Read a value from the stream as a boolean.\n\n        A boolean is written as a single byte\n        whose value is either 0 (false) or 1 (true).\n        \"\"\"\n        return ord(self.read(1)) == 1\n\n    def read_int(self) -&gt; int:\n        \"\"\"Read an int/long value.\n\n        int/long values are written using variable-length, zigzag coding.\n        \"\"\"\n        b = ord(self.read(1))\n        n = b &amp; 0x7F\n        shift = 7\n        while (b &amp; 0x80) != 0:\n            b = ord(self.read(1))\n            n |= (b &amp; 0x7F) &lt;&lt; shift\n            shift += 7\n        datum = (n &gt;&gt; 1) ^ -(n &amp; 1)\n        return datum\n\n    def read_ints(self, n: int) -&gt; tuple[int, ...]:\n        \"\"\"Read a list of integers.\"\"\"\n        return tuple(self.read_int() for _ in range(n))\n\n    def read_int_bytes_dict(self, n: int, dest: dict[int, bytes]) -&gt; None:\n        \"\"\"Read a dictionary of integers for keys and bytes for values into a destination dictionary.\"\"\"\n        for _ in range(n):\n            k = self.read_int()\n            v = self.read_bytes()\n            dest[k] = v\n\n    def read_float(self) -&gt; float:\n        \"\"\"Read a value from the stream as a float.\n\n        A float is written as 4 bytes.\n        The float is converted into a 32-bit integer using a method equivalent to\n        Java's floatToIntBits and then encoded in little-endian format.\n        \"\"\"\n        return float(cast(tuple[float, ...], STRUCT_FLOAT.unpack(self.read(4)))[0])\n\n    def read_double(self) -&gt; float:\n        \"\"\"Read a value from the stream as a double.\n\n        A double is written as 8 bytes.\n        The double is converted into a 64-bit integer using a method equivalent to\n        Java's doubleToLongBits and then encoded in little-endian format.\n        \"\"\"\n        return float(cast(tuple[float, ...], STRUCT_DOUBLE.unpack(self.read(8)))[0])\n\n    def read_bytes(self) -&gt; bytes:\n        \"\"\"Bytes are encoded as a long followed by that many bytes of data.\"\"\"\n        num_bytes = self.read_int()\n        return self.read(num_bytes) if num_bytes &gt; 0 else b\"\"\n\n    def read_utf8(self) -&gt; str:\n        \"\"\"Read an utf-8 encoded string from the stream.\n\n        A string is encoded as a long followed by\n        that many bytes of UTF-8 encoded character data.\n        \"\"\"\n        return self.read_bytes().decode(UTF8)\n\n    def skip_boolean(self) -&gt; None:\n        self.skip(1)\n\n    def skip_int(self) -&gt; None:\n        b = ord(self.read(1))\n        while (b &amp; 0x80) != 0:\n            b = ord(self.read(1))\n\n    def skip_float(self) -&gt; None:\n        self.skip(4)\n\n    def skip_double(self) -&gt; None:\n        self.skip(8)\n\n    def skip_bytes(self) -&gt; None:\n        self.skip(self.read_int())\n\n    def skip_utf8(self) -&gt; None:\n        self.skip_bytes()\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read","title":"<code>read(n)</code>  <code>abstractmethod</code>","text":"<p>Read n bytes.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>@abstractmethod\ndef read(self, n: int) -&gt; bytes:\n    \"\"\"Read n bytes.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_boolean","title":"<code>read_boolean()</code>","text":"<p>Read a value from the stream as a boolean.</p> <p>A boolean is written as a single byte whose value is either 0 (false) or 1 (true).</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_boolean(self) -&gt; bool:\n    \"\"\"Read a value from the stream as a boolean.\n\n    A boolean is written as a single byte\n    whose value is either 0 (false) or 1 (true).\n    \"\"\"\n    return ord(self.read(1)) == 1\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_bytes","title":"<code>read_bytes()</code>","text":"<p>Bytes are encoded as a long followed by that many bytes of data.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_bytes(self) -&gt; bytes:\n    \"\"\"Bytes are encoded as a long followed by that many bytes of data.\"\"\"\n    num_bytes = self.read_int()\n    return self.read(num_bytes) if num_bytes &gt; 0 else b\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_double","title":"<code>read_double()</code>","text":"<p>Read a value from the stream as a double.</p> <p>A double is written as 8 bytes. The double is converted into a 64-bit integer using a method equivalent to Java's doubleToLongBits and then encoded in little-endian format.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_double(self) -&gt; float:\n    \"\"\"Read a value from the stream as a double.\n\n    A double is written as 8 bytes.\n    The double is converted into a 64-bit integer using a method equivalent to\n    Java's doubleToLongBits and then encoded in little-endian format.\n    \"\"\"\n    return float(cast(tuple[float, ...], STRUCT_DOUBLE.unpack(self.read(8)))[0])\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_float","title":"<code>read_float()</code>","text":"<p>Read a value from the stream as a float.</p> <p>A float is written as 4 bytes. The float is converted into a 32-bit integer using a method equivalent to Java's floatToIntBits and then encoded in little-endian format.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_float(self) -&gt; float:\n    \"\"\"Read a value from the stream as a float.\n\n    A float is written as 4 bytes.\n    The float is converted into a 32-bit integer using a method equivalent to\n    Java's floatToIntBits and then encoded in little-endian format.\n    \"\"\"\n    return float(cast(tuple[float, ...], STRUCT_FLOAT.unpack(self.read(4)))[0])\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_int","title":"<code>read_int()</code>","text":"<p>Read an int/long value.</p> <p>int/long values are written using variable-length, zigzag coding.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_int(self) -&gt; int:\n    \"\"\"Read an int/long value.\n\n    int/long values are written using variable-length, zigzag coding.\n    \"\"\"\n    b = ord(self.read(1))\n    n = b &amp; 0x7F\n    shift = 7\n    while (b &amp; 0x80) != 0:\n        b = ord(self.read(1))\n        n |= (b &amp; 0x7F) &lt;&lt; shift\n        shift += 7\n    datum = (n &gt;&gt; 1) ^ -(n &amp; 1)\n    return datum\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_int_bytes_dict","title":"<code>read_int_bytes_dict(n, dest)</code>","text":"<p>Read a dictionary of integers for keys and bytes for values into a destination dictionary.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_int_bytes_dict(self, n: int, dest: dict[int, bytes]) -&gt; None:\n    \"\"\"Read a dictionary of integers for keys and bytes for values into a destination dictionary.\"\"\"\n    for _ in range(n):\n        k = self.read_int()\n        v = self.read_bytes()\n        dest[k] = v\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_ints","title":"<code>read_ints(n)</code>","text":"<p>Read a list of integers.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_ints(self, n: int) -&gt; tuple[int, ...]:\n    \"\"\"Read a list of integers.\"\"\"\n    return tuple(self.read_int() for _ in range(n))\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.read_utf8","title":"<code>read_utf8()</code>","text":"<p>Read an utf-8 encoded string from the stream.</p> <p>A string is encoded as a long followed by that many bytes of UTF-8 encoded character data.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read_utf8(self) -&gt; str:\n    \"\"\"Read an utf-8 encoded string from the stream.\n\n    A string is encoded as a long followed by\n    that many bytes of UTF-8 encoded character data.\n    \"\"\"\n    return self.read_bytes().decode(UTF8)\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.skip","title":"<code>skip(n)</code>  <code>abstractmethod</code>","text":"<p>Skip n bytes.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>@abstractmethod\ndef skip(self, n: int) -&gt; None:\n    \"\"\"Skip n bytes.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.BinaryDecoder.tell","title":"<code>tell()</code>  <code>abstractmethod</code>","text":"<p>Return the current position.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>@abstractmethod\ndef tell(self) -&gt; int:\n    \"\"\"Return the current position.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.StreamingBinaryDecoder","title":"<code>StreamingBinaryDecoder</code>","text":"<p>               Bases: <code>BinaryDecoder</code></p> <p>Decodes bytes into Python physical primitives.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>class StreamingBinaryDecoder(BinaryDecoder):\n    \"\"\"Decodes bytes into Python physical primitives.\"\"\"\n\n    __slots__ = \"_input_stream\"\n    _input_stream: InputStream\n\n    def __init__(self, input_stream: bytes | InputStream) -&gt; None:\n        \"\"\"Reader is a Python object on which we can call read, seek, and tell.\"\"\"\n        if isinstance(input_stream, bytes):\n            # In the case of bytes, we wrap it into a BytesIO to make it a stream\n            self._input_stream = io.BytesIO(input_stream)\n        else:\n            self._input_stream = input_stream\n\n    def tell(self) -&gt; int:\n        \"\"\"Return the current stream position.\"\"\"\n        return self._input_stream.tell()\n\n    def read(self, n: int) -&gt; bytes:\n        \"\"\"Read n bytes.\"\"\"\n        if n &lt; 0:\n            raise ValueError(f\"Requested {n} bytes to read, expected positive integer.\")\n        data: list[bytes] = []\n\n        n_remaining = n\n        while n_remaining &gt; 0:\n            data_read = self._input_stream.read(n_remaining)\n            read_len = len(data_read)\n            if read_len == n:\n                # If we read everything, we return directly\n                # otherwise we'll continue to fetch the rest\n                return data_read\n            elif read_len &lt;= 0:\n                raise EOFError(f\"EOF: read {read_len} bytes\")\n            data.append(data_read)\n            n_remaining -= read_len\n\n        return b\"\".join(data)\n\n    def skip(self, n: int) -&gt; None:\n        self._input_stream.seek(n, SEEK_CUR)\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.StreamingBinaryDecoder.__init__","title":"<code>__init__(input_stream)</code>","text":"<p>Reader is a Python object on which we can call read, seek, and tell.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def __init__(self, input_stream: bytes | InputStream) -&gt; None:\n    \"\"\"Reader is a Python object on which we can call read, seek, and tell.\"\"\"\n    if isinstance(input_stream, bytes):\n        # In the case of bytes, we wrap it into a BytesIO to make it a stream\n        self._input_stream = io.BytesIO(input_stream)\n    else:\n        self._input_stream = input_stream\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.StreamingBinaryDecoder.read","title":"<code>read(n)</code>","text":"<p>Read n bytes.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def read(self, n: int) -&gt; bytes:\n    \"\"\"Read n bytes.\"\"\"\n    if n &lt; 0:\n        raise ValueError(f\"Requested {n} bytes to read, expected positive integer.\")\n    data: list[bytes] = []\n\n    n_remaining = n\n    while n_remaining &gt; 0:\n        data_read = self._input_stream.read(n_remaining)\n        read_len = len(data_read)\n        if read_len == n:\n            # If we read everything, we return directly\n            # otherwise we'll continue to fetch the rest\n            return data_read\n        elif read_len &lt;= 0:\n            raise EOFError(f\"EOF: read {read_len} bytes\")\n        data.append(data_read)\n        n_remaining -= read_len\n\n    return b\"\".join(data)\n</code></pre>"},{"location":"reference/pyiceberg/avro/decoder/#pyiceberg.avro.decoder.StreamingBinaryDecoder.tell","title":"<code>tell()</code>","text":"<p>Return the current stream position.</p> Source code in <code>pyiceberg/avro/decoder.py</code> <pre><code>def tell(self) -&gt; int:\n    \"\"\"Return the current stream position.\"\"\"\n    return self._input_stream.tell()\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/","title":"encoder","text":""},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder","title":"<code>BinaryEncoder</code>","text":"<p>Encodes Python physical types into bytes.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>class BinaryEncoder:\n    \"\"\"Encodes Python physical types into bytes.\"\"\"\n\n    _output_stream: OutputStream\n\n    def __init__(self, output_stream: OutputStream) -&gt; None:\n        self._output_stream = output_stream\n\n    def write(self, b: bytes) -&gt; None:\n        self._output_stream.write(b)\n\n    def write_boolean(self, boolean: bool) -&gt; None:\n        \"\"\"Write a boolean as a single byte whose value is either 0 (false) or 1 (true).\n\n        Args:\n            boolean: The boolean to write.\n        \"\"\"\n        self.write(bytes([bool(boolean)]))\n\n    def write_int(self, integer: int) -&gt; None:\n        \"\"\"Integer and long values are written using variable-length zig-zag coding.\"\"\"\n        datum = (integer &lt;&lt; 1) ^ (integer &gt;&gt; 63)\n        while (datum &amp; ~0x7F) != 0:\n            self.write(bytes([(datum &amp; 0x7F) | 0x80]))\n            datum &gt;&gt;= 7\n        self.write(bytes([datum]))\n\n    def write_float(self, f: float) -&gt; None:\n        \"\"\"Write a float as 4 bytes.\"\"\"\n        self.write(STRUCT_FLOAT.pack(f))\n\n    def write_double(self, f: float) -&gt; None:\n        \"\"\"Write a double as 8 bytes.\"\"\"\n        self.write(STRUCT_DOUBLE.pack(f))\n\n    def write_bytes(self, b: bytes) -&gt; None:\n        \"\"\"Bytes are encoded as a long followed by that many bytes of data.\"\"\"\n        self.write_int(len(b))\n        self.write(b)\n\n    def write_utf8(self, s: str) -&gt; None:\n        \"\"\"Encode a string as a long followed by that many bytes of UTF-8 encoded character data.\"\"\"\n        self.write_bytes(s.encode(UTF8))\n\n    def write_uuid(self, uuid: UUID) -&gt; None:\n        \"\"\"Write UUID as a fixed[16].\n\n        The uuid logical type represents a random generated universally unique identifier (UUID).\n        An uuid logical type annotates an Avro string. The string has to conform with RFC-4122.\n        \"\"\"\n        if len(uuid.bytes) != 16:\n            raise ValueError(f\"Expected UUID to have 16 bytes, got: len({uuid.bytes!r})\")\n        return self.write(uuid.bytes)\n\n    def write_unknown(self, _: Any) -&gt; None:\n        \"\"\"Nulls are written as 0 bytes in avro, so we do nothing.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_boolean","title":"<code>write_boolean(boolean)</code>","text":"<p>Write a boolean as a single byte whose value is either 0 (false) or 1 (true).</p> <p>Parameters:</p> Name Type Description Default <code>boolean</code> <code>bool</code> <p>The boolean to write.</p> required Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_boolean(self, boolean: bool) -&gt; None:\n    \"\"\"Write a boolean as a single byte whose value is either 0 (false) or 1 (true).\n\n    Args:\n        boolean: The boolean to write.\n    \"\"\"\n    self.write(bytes([bool(boolean)]))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_bytes","title":"<code>write_bytes(b)</code>","text":"<p>Bytes are encoded as a long followed by that many bytes of data.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_bytes(self, b: bytes) -&gt; None:\n    \"\"\"Bytes are encoded as a long followed by that many bytes of data.\"\"\"\n    self.write_int(len(b))\n    self.write(b)\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_double","title":"<code>write_double(f)</code>","text":"<p>Write a double as 8 bytes.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_double(self, f: float) -&gt; None:\n    \"\"\"Write a double as 8 bytes.\"\"\"\n    self.write(STRUCT_DOUBLE.pack(f))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_float","title":"<code>write_float(f)</code>","text":"<p>Write a float as 4 bytes.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_float(self, f: float) -&gt; None:\n    \"\"\"Write a float as 4 bytes.\"\"\"\n    self.write(STRUCT_FLOAT.pack(f))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_int","title":"<code>write_int(integer)</code>","text":"<p>Integer and long values are written using variable-length zig-zag coding.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_int(self, integer: int) -&gt; None:\n    \"\"\"Integer and long values are written using variable-length zig-zag coding.\"\"\"\n    datum = (integer &lt;&lt; 1) ^ (integer &gt;&gt; 63)\n    while (datum &amp; ~0x7F) != 0:\n        self.write(bytes([(datum &amp; 0x7F) | 0x80]))\n        datum &gt;&gt;= 7\n    self.write(bytes([datum]))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_unknown","title":"<code>write_unknown(_)</code>","text":"<p>Nulls are written as 0 bytes in avro, so we do nothing.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_unknown(self, _: Any) -&gt; None:\n    \"\"\"Nulls are written as 0 bytes in avro, so we do nothing.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_utf8","title":"<code>write_utf8(s)</code>","text":"<p>Encode a string as a long followed by that many bytes of UTF-8 encoded character data.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_utf8(self, s: str) -&gt; None:\n    \"\"\"Encode a string as a long followed by that many bytes of UTF-8 encoded character data.\"\"\"\n    self.write_bytes(s.encode(UTF8))\n</code></pre>"},{"location":"reference/pyiceberg/avro/encoder/#pyiceberg.avro.encoder.BinaryEncoder.write_uuid","title":"<code>write_uuid(uuid)</code>","text":"<p>Write UUID as a fixed[16].</p> <p>The uuid logical type represents a random generated universally unique identifier (UUID). An uuid logical type annotates an Avro string. The string has to conform with RFC-4122.</p> Source code in <code>pyiceberg/avro/encoder.py</code> <pre><code>def write_uuid(self, uuid: UUID) -&gt; None:\n    \"\"\"Write UUID as a fixed[16].\n\n    The uuid logical type represents a random generated universally unique identifier (UUID).\n    An uuid logical type annotates an Avro string. The string has to conform with RFC-4122.\n    \"\"\"\n    if len(uuid.bytes) != 16:\n        raise ValueError(f\"Expected UUID to have 16 bytes, got: len({uuid.bytes!r})\")\n    return self.write(uuid.bytes)\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/","title":"file","text":"<p>Avro reader for reading Avro files.</p>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile","title":"<code>AvroFile</code>","text":"<p>               Bases: <code>Generic[D]</code></p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>class AvroFile(Generic[D]):\n    __slots__ = (\n        \"input_file\",\n        \"read_schema\",\n        \"read_types\",\n        \"read_enums\",\n        \"header\",\n        \"schema\",\n        \"reader\",\n        \"decoder\",\n        \"block\",\n    )\n    input_file: InputFile\n    read_schema: Schema | None\n    read_types: dict[int, Callable[..., StructProtocol]]\n    read_enums: dict[int, Callable[..., Enum]]\n    header: AvroFileHeader\n    schema: Schema\n    reader: Reader\n\n    decoder: BinaryDecoder\n    block: Block[D] | None\n\n    def __init__(\n        self,\n        input_file: InputFile,\n        read_schema: Schema | None = None,\n        read_types: dict[int, Callable[..., StructProtocol]] = EMPTY_DICT,\n        read_enums: dict[int, Callable[..., Enum]] = EMPTY_DICT,\n    ) -&gt; None:\n        self.input_file = input_file\n        self.read_schema = read_schema\n        self.read_types = read_types\n        self.read_enums = read_enums\n        self.block = None\n\n    def __enter__(self) -&gt; AvroFile[D]:\n        \"\"\"Generate a reader tree for the payload within an avro file.\n\n        Return:\n            A generator returning the AvroStructs.\n        \"\"\"\n        with self.input_file.open() as f:\n            self.decoder = new_decoder(f.read())\n        self.header = self._read_header()\n        self.schema = self.header.get_schema()\n        if not self.read_schema:\n            self.read_schema = self.schema\n\n        self.reader = resolve_reader(self.schema, self.read_schema, self.read_types, self.read_enums)\n\n        return self\n\n    def __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n        \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n\n    def __iter__(self) -&gt; AvroFile[D]:\n        \"\"\"Return an iterator for the AvroFile class.\"\"\"\n        return self\n\n    def _read_block(self) -&gt; int:\n        # If there is already a block, we'll have the sync bytes\n        if self.block:\n            sync_marker = self.decoder.read(SYNC_SIZE)\n            if sync_marker != self.header.sync:\n                raise ValueError(f\"Expected sync bytes {self.header.sync!r}, but got {sync_marker!r}\")\n        block_records = self.decoder.read_int()\n\n        block_bytes = self.decoder.read_bytes()\n        if codec := self.header.compression_codec():\n            block_bytes = codec.decompress(block_bytes)\n\n        self.block = Block(reader=self.reader, block_records=block_records, block_decoder=new_decoder(block_bytes))\n        return block_records\n\n    def __next__(self) -&gt; D:\n        \"\"\"Return the next item when iterating over the AvroFile class.\"\"\"\n        if self.block and self.block.has_next():\n            return next(self.block)\n\n        try:\n            new_block = self._read_block()\n        except EOFError as exc:\n            raise StopIteration from exc\n\n        if new_block &gt; 0:\n            return self.__next__()\n        raise StopIteration\n\n    def _read_header(self) -&gt; AvroFileHeader:\n        return construct_reader(META_SCHEMA, {-1: AvroFileHeader}).read(self.decoder)\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile.__enter__","title":"<code>__enter__()</code>","text":"<p>Generate a reader tree for the payload within an avro file.</p> Return <p>A generator returning the AvroStructs.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __enter__(self) -&gt; AvroFile[D]:\n    \"\"\"Generate a reader tree for the payload within an avro file.\n\n    Return:\n        A generator returning the AvroStructs.\n    \"\"\"\n    with self.input_file.open() as f:\n        self.decoder = new_decoder(f.read())\n    self.header = self._read_header()\n    self.schema = self.header.get_schema()\n    if not self.read_schema:\n        self.read_schema = self.schema\n\n    self.reader = resolve_reader(self.schema, self.read_schema, self.read_types, self.read_enums)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>","text":"<p>Perform cleanup when exiting the scope of a 'with' statement.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n    \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator for the AvroFile class.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __iter__(self) -&gt; AvroFile[D]:\n    \"\"\"Return an iterator for the AvroFile class.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFile.__next__","title":"<code>__next__()</code>","text":"<p>Return the next item when iterating over the AvroFile class.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __next__(self) -&gt; D:\n    \"\"\"Return the next item when iterating over the AvroFile class.\"\"\"\n    if self.block and self.block.has_next():\n        return next(self.block)\n\n    try:\n        new_block = self._read_block()\n    except EOFError as exc:\n        raise StopIteration from exc\n\n    if new_block &gt; 0:\n        return self.__next__()\n    raise StopIteration\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFileHeader","title":"<code>AvroFileHeader</code>","text":"<p>               Bases: <code>Record</code></p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>class AvroFileHeader(Record):\n    @property\n    def magic(self) -&gt; bytes:\n        return self._data[0]\n\n    @property\n    def meta(self) -&gt; dict[str, str]:\n        return self._data[1]\n\n    @property\n    def sync(self) -&gt; bytes:\n        return self._data[2]\n\n    def compression_codec(self) -&gt; type[Codec] | None:\n        \"\"\"Get the file's compression codec algorithm from the file's metadata.\n\n        In the case of a null codec, we return a None indicating that we\n        don't need to compress/decompress.\n        \"\"\"\n        from pyiceberg.table import TableProperties\n\n        codec_name = self.meta.get(AVRO_CODEC_KEY, TableProperties.WRITE_AVRO_COMPRESSION_DEFAULT)\n        if codec_name not in KNOWN_CODECS:\n            raise ValueError(f\"Unsupported codec: {codec_name}\")\n\n        return KNOWN_CODECS[codec_name]  # type: ignore\n\n    def get_schema(self) -&gt; Schema:\n        if _SCHEMA_KEY in self.meta:\n            avro_schema_string = self.meta[_SCHEMA_KEY]\n            avro_schema = json.loads(avro_schema_string)\n            return AvroSchemaConversion().avro_to_iceberg(avro_schema)\n        else:\n            raise ValueError(\"No schema found in Avro file headers\")\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroFileHeader.compression_codec","title":"<code>compression_codec()</code>","text":"<p>Get the file's compression codec algorithm from the file's metadata.</p> <p>In the case of a null codec, we return a None indicating that we don't need to compress/decompress.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def compression_codec(self) -&gt; type[Codec] | None:\n    \"\"\"Get the file's compression codec algorithm from the file's metadata.\n\n    In the case of a null codec, we return a None indicating that we\n    don't need to compress/decompress.\n    \"\"\"\n    from pyiceberg.table import TableProperties\n\n    codec_name = self.meta.get(AVRO_CODEC_KEY, TableProperties.WRITE_AVRO_COMPRESSION_DEFAULT)\n    if codec_name not in KNOWN_CODECS:\n        raise ValueError(f\"Unsupported codec: {codec_name}\")\n\n    return KNOWN_CODECS[codec_name]  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroOutputFile","title":"<code>AvroOutputFile</code>","text":"<p>               Bases: <code>Generic[D]</code></p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>class AvroOutputFile(Generic[D]):\n    output_file: OutputFile\n    output_stream: OutputStream\n    file_schema: Schema\n    schema_name: str\n    encoder: BinaryEncoder\n    sync_bytes: bytes\n    writer: Writer\n\n    def __init__(\n        self,\n        output_file: OutputFile,\n        file_schema: Schema,\n        schema_name: str,\n        record_schema: Schema | None = None,\n        metadata: dict[str, str] = EMPTY_DICT,\n    ) -&gt; None:\n        self.output_file = output_file\n        self.file_schema = file_schema\n        self.schema_name = schema_name\n        self.sync_bytes = os.urandom(SYNC_SIZE)\n        self.writer = (\n            construct_writer(file_schema=self.file_schema)\n            if record_schema is None\n            else resolve_writer(record_schema=record_schema, file_schema=self.file_schema)\n        )\n        self.metadata = metadata\n\n    def __enter__(self) -&gt; AvroOutputFile[D]:\n        \"\"\"\n        Open the file and writes the header.\n\n        Returns:\n            The file object to write records to\n        \"\"\"\n        self.output_stream = self.output_file.create(overwrite=True)\n        self.encoder = BinaryEncoder(self.output_stream)\n\n        self._write_header()\n\n        return self\n\n    def __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n        \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n        self.output_stream.close()\n\n    def _write_header(self) -&gt; None:\n        from pyiceberg.table import TableProperties\n\n        codec_name = self.metadata.get(AVRO_CODEC_KEY, TableProperties.WRITE_AVRO_COMPRESSION_DEFAULT)\n        if avro_codec_name := CODEC_MAPPING_ICEBERG_TO_AVRO.get(codec_name):\n            codec_name = avro_codec_name\n\n        json_schema = json.dumps(AvroSchemaConversion().iceberg_to_avro(self.file_schema, schema_name=self.schema_name))\n\n        meta = {**self.metadata, _SCHEMA_KEY: json_schema, AVRO_CODEC_KEY: codec_name}\n        header = AvroFileHeader(MAGIC, meta, self.sync_bytes)\n        construct_writer(META_SCHEMA).write(self.encoder, header)\n\n    def compression_codec(self) -&gt; type[Codec] | None:\n        \"\"\"Get the file's compression codec algorithm from the file's metadata.\n\n        In the case of a null codec, we return a None indicating that we\n        don't need to compress/decompress.\n        \"\"\"\n        from pyiceberg.table import TableProperties\n\n        codec_name = self.metadata.get(AVRO_CODEC_KEY, TableProperties.WRITE_AVRO_COMPRESSION_DEFAULT)\n\n        if avro_codec_name := CODEC_MAPPING_ICEBERG_TO_AVRO.get(codec_name):\n            codec_name = avro_codec_name\n\n        if codec_name not in KNOWN_CODECS:\n            raise ValueError(f\"Unsupported codec: {codec_name}\")\n\n        return KNOWN_CODECS[codec_name]  # type: ignore\n\n    def write_block(self, objects: list[D]) -&gt; None:\n        in_memory = io.BytesIO()\n        block_content_encoder = BinaryEncoder(output_stream=in_memory)\n        for obj in objects:\n            self.writer.write(block_content_encoder, obj)\n        block_content = in_memory.getvalue()\n\n        self.encoder.write_int(len(objects))\n\n        if codec := self.compression_codec():\n            content, content_length = codec.compress(block_content)\n            self.encoder.write_int(content_length)\n            self.encoder.write(content)\n        else:\n            self.encoder.write_int(len(block_content))\n            self.encoder.write(block_content)\n\n        self.encoder.write(self.sync_bytes)\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroOutputFile.__enter__","title":"<code>__enter__()</code>","text":"<p>Open the file and writes the header.</p> <p>Returns:</p> Type Description <code>AvroOutputFile[D]</code> <p>The file object to write records to</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __enter__(self) -&gt; AvroOutputFile[D]:\n    \"\"\"\n    Open the file and writes the header.\n\n    Returns:\n        The file object to write records to\n    \"\"\"\n    self.output_stream = self.output_file.create(overwrite=True)\n    self.encoder = BinaryEncoder(self.output_stream)\n\n    self._write_header()\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroOutputFile.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>","text":"<p>Perform cleanup when exiting the scope of a 'with' statement.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n    \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n    self.output_stream.close()\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.AvroOutputFile.compression_codec","title":"<code>compression_codec()</code>","text":"<p>Get the file's compression codec algorithm from the file's metadata.</p> <p>In the case of a null codec, we return a None indicating that we don't need to compress/decompress.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def compression_codec(self) -&gt; type[Codec] | None:\n    \"\"\"Get the file's compression codec algorithm from the file's metadata.\n\n    In the case of a null codec, we return a None indicating that we\n    don't need to compress/decompress.\n    \"\"\"\n    from pyiceberg.table import TableProperties\n\n    codec_name = self.metadata.get(AVRO_CODEC_KEY, TableProperties.WRITE_AVRO_COMPRESSION_DEFAULT)\n\n    if avro_codec_name := CODEC_MAPPING_ICEBERG_TO_AVRO.get(codec_name):\n        codec_name = avro_codec_name\n\n    if codec_name not in KNOWN_CODECS:\n        raise ValueError(f\"Unsupported codec: {codec_name}\")\n\n    return KNOWN_CODECS[codec_name]  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.Block","title":"<code>Block</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[D]</code></p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>@dataclass\nclass Block(Generic[D]):\n    reader: Reader\n    block_records: int\n    block_decoder: BinaryDecoder\n    position: int = 0\n\n    def __iter__(self) -&gt; Block[D]:\n        \"\"\"Return an iterator for the Block class.\"\"\"\n        return self\n\n    def has_next(self) -&gt; bool:\n        return self.position &lt; self.block_records\n\n    def __next__(self) -&gt; D:\n        \"\"\"Return the next item when iterating over the Block class.\"\"\"\n        if self.has_next():\n            self.position += 1\n            return self.reader.read(self.block_decoder)\n        raise StopIteration\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.Block.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator for the Block class.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __iter__(self) -&gt; Block[D]:\n    \"\"\"Return an iterator for the Block class.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/avro/file/#pyiceberg.avro.file.Block.__next__","title":"<code>__next__()</code>","text":"<p>Return the next item when iterating over the Block class.</p> Source code in <code>pyiceberg/avro/file.py</code> <pre><code>def __next__(self) -&gt; D:\n    \"\"\"Return the next item when iterating over the Block class.\"\"\"\n    if self.has_next():\n        self.position += 1\n        return self.reader.read(self.block_decoder)\n    raise StopIteration\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/","title":"reader","text":"<p>Classes for building the Reader tree.</p> <p>Constructing a reader tree from the schema makes it easy to decouple the reader implementation from the schema.</p> <p>The reader tree can be changed in such a way that the read schema is different, while respecting the read schema.</p>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.BinaryReader","title":"<code>BinaryReader</code>","text":"<p>               Bases: <code>Reader</code></p> <p>Read a binary value.</p> <p>First reads an integer, to get the length of the binary value, then reads the binary field itself.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class BinaryReader(Reader):\n    \"\"\"Read a binary value.\n\n    First reads an integer, to get the length of the binary value,\n    then reads the binary field itself.\n    \"\"\"\n\n    def read(self, decoder: BinaryDecoder) -&gt; bytes:\n        return decoder.read_bytes()\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        decoder.skip_bytes()\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.DateReader","title":"<code>DateReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a day granularity date from the stream.</p> <p>The number of days from 1 January 1970.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class DateReader(IntegerReader):\n    \"\"\"Reads a day granularity date from the stream.\n\n    The number of days from 1 January 1970.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.DecimalReader","title":"<code>DecimalReader</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Reader</code></p> <p>Reads a value as a decimal.</p> <p>Decimal bytes are decoded as signed short, int or long depending on the size of bytes.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>@dataclass(frozen=True, init=False)\nclass DecimalReader(Reader):\n    \"\"\"Reads a value as a decimal.\n\n    Decimal bytes are decoded as signed short, int or long depending on the\n    size of bytes.\n    \"\"\"\n\n    precision: int = dataclassfield()\n    scale: int = dataclassfield()\n    _length: int\n\n    def __init__(self, precision: int, scale: int):\n        object.__setattr__(self, \"precision\", precision)\n        object.__setattr__(self, \"scale\", scale)\n        object.__setattr__(self, \"_length\", decimal_required_bytes(precision))\n\n    def read(self, decoder: BinaryDecoder) -&gt; Decimal:\n        return bytes_to_decimal(decoder.read(self._length), self.scale)\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        decoder.skip_bytes()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the DecimalReader class.\"\"\"\n        return f\"DecimalReader({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.DecimalReader.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the DecimalReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the DecimalReader class.\"\"\"\n    return f\"DecimalReader({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.FixedReader","title":"<code>FixedReader</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Reader</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>@dataclass(frozen=True)\nclass FixedReader(Reader):\n    _len: int = dataclassfield()\n\n    def read(self, decoder: BinaryDecoder) -&gt; bytes:\n        return decoder.read(len(self))\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        decoder.skip(len(self))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of an instance of the FixedReader class.\"\"\"\n        return self._len\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the FixedReader class.\"\"\"\n        return f\"FixedReader({self._len})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.FixedReader.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of an instance of the FixedReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of an instance of the FixedReader class.\"\"\"\n    return self._len\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.FixedReader.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the FixedReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the FixedReader class.\"\"\"\n    return f\"FixedReader({self._len})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.IntegerReader","title":"<code>IntegerReader</code>","text":"<p>               Bases: <code>Reader</code></p> <p>Longs and ints are encoded the same way, and there is no long in Python.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class IntegerReader(Reader):\n    \"\"\"Longs and ints are encoded the same way, and there is no long in Python.\"\"\"\n\n    def read(self, decoder: BinaryDecoder) -&gt; int:\n        return decoder.read_int()\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        decoder.skip_int()\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.ListReader","title":"<code>ListReader</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Reader</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>@dataclass(frozen=False, init=False)\nclass ListReader(Reader):\n    __slots__ = (\"element\", \"_is_int_list\", \"_hash\")\n    element: Reader\n\n    def __init__(self, element: Reader) -&gt; None:\n        super().__init__()\n        self.element = element\n        self._hash = hash(self.element)\n        self._is_int_list = isinstance(self.element, IntegerReader)\n\n    def read(self, decoder: BinaryDecoder) -&gt; list[Any]:\n        read_items: list[Any] = []\n        block_count = decoder.read_int()\n        while block_count != 0:\n            if block_count &lt; 0:\n                block_count = -block_count\n                _ = decoder.read_int()\n            if self._is_int_list:\n                read_items.extend(decoder.read_ints(block_count))\n            else:\n                for _ in range(block_count):\n                    read_items.append(self.element.read(decoder))\n            block_count = decoder.read_int()\n        return read_items\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        _skip_map_array(decoder, lambda: self.element.skip(decoder))\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the ListReader class.\"\"\"\n        return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.ListReader.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the ListReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the ListReader class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.MapReader","title":"<code>MapReader</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Reader</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>@dataclass(frozen=False, init=False)\nclass MapReader(Reader):\n    __slots__ = (\"key\", \"value\", \"_is_int_int\", \"_is_int_bytes\", \"_key_reader\", \"_value_reader\", \"_hash\")\n    key: Reader\n    value: Reader\n\n    def __init__(self, key: Reader, value: Reader) -&gt; None:\n        super().__init__()\n        self.key = key\n        self.value = value\n        if isinstance(self.key, IntegerReader):\n            self._is_int_int = isinstance(self.value, IntegerReader)\n            self._is_int_bytes = isinstance(self.value, BinaryReader)\n        else:\n            self._is_int_int = False\n            self._is_int_bytes = False\n            self._key_reader = self.key.read\n            self._value_reader = self.value.read\n        self._hash = hash((self.key, self.value))\n\n    def _read_int_int(self, decoder: BinaryDecoder) -&gt; Mapping[int, int]:\n        \"\"\"Read a mapping from int to int from the decoder.\n\n        Read a map of ints to ints from the decoder, since this is such a common\n        data type, it is optimized to be faster than the generic map reader, by\n        using a lazy dict.\n\n        The time it takes to create the python dictionary is much larger than\n        the time it takes to read the data from the decoder as an array, so the\n        lazy dict defers creating the python dictionary until it is actually\n        accessed.\n\n        \"\"\"\n        block_count = decoder.read_int()\n\n        # Often times the map is empty, so we can just return an empty dict without\n        # instancing the LazyDict\n        if block_count == 0:\n            return EMPTY_DICT\n\n        contents_array: list[tuple[int, ...]] = []\n\n        while block_count != 0:\n            if block_count &lt; 0:\n                block_count = -block_count\n                # We ignore the block size for now\n                decoder.skip_int()\n\n            # Since the integers are encoding right next to each other\n            # just read them all at once.\n            contents_array.append(decoder.read_ints(block_count * 2))\n            block_count = decoder.read_int()\n\n        return LazyDict(contents_array)\n\n    def read(self, decoder: BinaryDecoder) -&gt; Mapping[Any, Any]:\n        read_items: dict[Any, Any] = {}\n\n        if self._is_int_int or self._is_int_bytes:\n            if self._is_int_int:\n                return self._read_int_int(decoder)\n\n            block_count = decoder.read_int()\n            while block_count != 0:\n                if block_count &lt; 0:\n                    block_count = -block_count\n                    # We ignore the block size for now\n                    _ = decoder.read_int()\n                decoder.read_int_bytes_dict(block_count, read_items)\n                block_count = decoder.read_int()\n        else:\n            block_count = decoder.read_int()\n            while block_count != 0:\n                if block_count &lt; 0:\n                    block_count = -block_count\n                    # We ignore the block size for now\n                    _ = decoder.read_int()\n                for _ in range(block_count):\n                    key = self._key_reader(decoder)\n                    read_items[key] = self._value_reader(decoder)\n                block_count = decoder.read_int()\n\n        return read_items\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        def skip() -&gt; None:\n            self.key.skip(decoder)\n            self.value.skip(decoder)\n\n        _skip_map_array(decoder, skip)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the MapReader class.\"\"\"\n        return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.MapReader.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the MapReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the MapReader class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.Reader","title":"<code>Reader</code>","text":"<p>               Bases: <code>Singleton</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class Reader(Singleton):\n    @abstractmethod\n    def read(self, decoder: BinaryDecoder) -&gt; Any: ...\n\n    @abstractmethod\n    def skip(self, decoder: BinaryDecoder) -&gt; None: ...\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Reader class.\"\"\"\n        return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.Reader.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Reader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Reader class.\"\"\"\n    return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.StructReader","title":"<code>StructReader</code>","text":"<p>               Bases: <code>Reader</code></p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class StructReader(Reader):\n    __slots__ = (\n        \"field_readers\",\n        \"create_struct\",\n        \"struct\",\n        \"_field_reader_functions\",\n        \"_hash\",\n        \"_max_pos\",\n    )\n    field_readers: tuple[tuple[int | None, Reader], ...]\n    create_struct: Callable[..., StructProtocol]\n    struct: StructType\n    field_reader_functions = tuple[tuple[str | None, int, Callable[[BinaryDecoder], Any] | None], ...]\n\n    def __init__(\n        self,\n        field_readers: tuple[tuple[int | None, Reader], ...],\n        create_struct: Callable[..., StructProtocol],\n        struct: StructType,\n    ) -&gt; None:\n        self.field_readers = field_readers\n        self.create_struct = create_struct\n        # TODO: Implement struct-reuse\n        self.struct = struct\n\n        if not isinstance(self.create_struct(), StructProtocol):\n            raise ValueError(f\"Incompatible with StructProtocol: {self.create_struct}\")\n\n        reading_callbacks: list[tuple[int | None, Callable[[BinaryDecoder], Any]]] = []\n        max_pos = -1\n        for pos, field in field_readers:\n            if pos is not None:\n                reading_callbacks.append((pos, field.read))\n                max_pos = max(max_pos, pos)\n            else:\n                reading_callbacks.append((None, field.skip))\n\n        self._field_reader_functions = tuple(reading_callbacks)\n        self._hash = hash(self._field_reader_functions)\n        self._max_pos = 1 + max_pos\n\n    def read(self, decoder: BinaryDecoder) -&gt; StructProtocol:\n        # TODO: Implement struct-reuse\n        struct = self.create_struct(*[None] * self._max_pos)\n        for pos, field_reader in self._field_reader_functions:\n            if pos is not None:\n                struct[pos] = field_reader(decoder)  # later: pass reuse in here\n            else:\n                field_reader(decoder)\n\n        return struct\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        for _, field in self.field_readers:\n            field.skip(decoder)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the StructReader class.\"\"\"\n        return (\n            self.field_readers == other.field_readers and self.create_struct == other.create_struct\n            if isinstance(other, StructReader)\n            else False\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the StructReader class.\"\"\"\n        return f\"StructReader(({','.join(repr(field) for field in self.field_readers)}), {repr(self.create_struct)})\"\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the StructReader class.\"\"\"\n        return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.StructReader.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the StructReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the StructReader class.\"\"\"\n    return (\n        self.field_readers == other.field_readers and self.create_struct == other.create_struct\n        if isinstance(other, StructReader)\n        else False\n    )\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.StructReader.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the StructReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the StructReader class.\"\"\"\n    return self._hash\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.StructReader.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the StructReader class.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the StructReader class.\"\"\"\n    return f\"StructReader(({','.join(repr(field) for field in self.field_readers)}), {repr(self.create_struct)})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.TimeReader","title":"<code>TimeReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a microsecond granularity timestamp from the stream.</p> <p>Long is decoded as an integer which represents the number of microseconds from the unix epoch, 1 January 1970.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class TimeReader(IntegerReader):\n    \"\"\"Reads a microsecond granularity timestamp from the stream.\n\n    Long is decoded as an integer which represents\n    the number of microseconds from the unix epoch, 1 January 1970.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.TimestampNanoReader","title":"<code>TimestampNanoReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a nanosecond granularity timestamp from the stream.</p> <p>Long is decoded as python integer which represents the number of nanoseconds from the unix epoch, 1 January 1970.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class TimestampNanoReader(IntegerReader):\n    \"\"\"Reads a nanosecond granularity timestamp from the stream.\n\n    Long is decoded as python integer which represents\n    the number of nanoseconds from the unix epoch, 1 January 1970.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.TimestampReader","title":"<code>TimestampReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a microsecond granularity timestamp from the stream.</p> <p>Long is decoded as python integer which represents the number of microseconds from the unix epoch, 1 January 1970.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class TimestampReader(IntegerReader):\n    \"\"\"Reads a microsecond granularity timestamp from the stream.\n\n    Long is decoded as python integer which represents\n    the number of microseconds from the unix epoch, 1 January 1970.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.TimestamptzNanoReader","title":"<code>TimestamptzNanoReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a microsecond granularity timestamptz from the stream.</p> <p>Long is decoded as python integer which represents the number of nanoseconds from the unix epoch, 1 January 1970.</p> <p>Adjusted to UTC.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class TimestamptzNanoReader(IntegerReader):\n    \"\"\"Reads a microsecond granularity timestamptz from the stream.\n\n    Long is decoded as python integer which represents\n    the number of nanoseconds from the unix epoch, 1 January 1970.\n\n    Adjusted to UTC.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/reader/#pyiceberg.avro.reader.TimestamptzReader","title":"<code>TimestamptzReader</code>","text":"<p>               Bases: <code>IntegerReader</code></p> <p>Reads a microsecond granularity timestamptz from the stream.</p> <p>Long is decoded as python integer which represents the number of microseconds from the unix epoch, 1 January 1970.</p> <p>Adjusted to UTC.</p> Source code in <code>pyiceberg/avro/reader.py</code> <pre><code>class TimestamptzReader(IntegerReader):\n    \"\"\"Reads a microsecond granularity timestamptz from the stream.\n\n    Long is decoded as python integer which represents\n    the number of microseconds from the unix epoch, 1 January 1970.\n\n    Adjusted to UTC.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/","title":"resolver","text":""},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.ConstructWriter","title":"<code>ConstructWriter</code>","text":"<p>               Bases: <code>SchemaVisitorPerPrimitiveType[Writer]</code></p> <p>Construct a writer tree from an Iceberg schema.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>class ConstructWriter(SchemaVisitorPerPrimitiveType[Writer]):\n    \"\"\"Construct a writer tree from an Iceberg schema.\"\"\"\n\n    def schema(self, schema: Schema, struct_result: Writer) -&gt; Writer:\n        return struct_result\n\n    def struct(self, struct: StructType, field_results: list[Writer]) -&gt; Writer:\n        return StructWriter(tuple((pos, result) for pos, result in enumerate(field_results)))\n\n    def field(self, field: NestedField, field_result: Writer) -&gt; Writer:\n        return field_result if field.required else OptionWriter(field_result)\n\n    def list(self, list_type: ListType, element_result: Writer) -&gt; Writer:\n        return ListWriter(element_result)\n\n    def map(self, map_type: MapType, key_result: Writer, value_result: Writer) -&gt; Writer:\n        return MapWriter(key_result, value_result)\n\n    def visit_fixed(self, fixed_type: FixedType) -&gt; Writer:\n        return FixedWriter(len(fixed_type))\n\n    def visit_decimal(self, decimal_type: DecimalType) -&gt; Writer:\n        return DecimalWriter(decimal_type.precision, decimal_type.scale)\n\n    def visit_boolean(self, boolean_type: BooleanType) -&gt; Writer:\n        return BooleanWriter()\n\n    def visit_integer(self, integer_type: IntegerType) -&gt; Writer:\n        return IntegerWriter()\n\n    def visit_long(self, long_type: LongType) -&gt; Writer:\n        return IntegerWriter()\n\n    def visit_float(self, float_type: FloatType) -&gt; Writer:\n        return FloatWriter()\n\n    def visit_double(self, double_type: DoubleType) -&gt; Writer:\n        return DoubleWriter()\n\n    def visit_date(self, date_type: DateType) -&gt; Writer:\n        return DateWriter()\n\n    def visit_time(self, time_type: TimeType) -&gt; Writer:\n        return TimeWriter()\n\n    def visit_timestamp(self, timestamp_type: TimestampType) -&gt; Writer:\n        return TimestampWriter()\n\n    def visit_timestamp_ns(self, timestamp_ns_type: TimestampNanoType) -&gt; Writer:\n        return TimestampNanoWriter()\n\n    def visit_timestamptz(self, timestamptz_type: TimestamptzType) -&gt; Writer:\n        return TimestamptzWriter()\n\n    def visit_timestamptz_ns(self, timestamptz_ns_type: TimestamptzNanoType) -&gt; Writer:\n        return TimestamptzNanoWriter()\n\n    def visit_string(self, string_type: StringType) -&gt; Writer:\n        return StringWriter()\n\n    def visit_uuid(self, uuid_type: UUIDType) -&gt; Writer:\n        return UUIDWriter()\n\n    def visit_binary(self, binary_type: BinaryType) -&gt; Writer:\n        return BinaryWriter()\n\n    def visit_unknown(self, unknown_type: UnknownType) -&gt; Writer:\n        return UnknownWriter()\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.EnumReader","title":"<code>EnumReader</code>","text":"<p>               Bases: <code>Reader</code></p> <p>An Enum reader to wrap primitive values into an Enum.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>class EnumReader(Reader):\n    \"\"\"An Enum reader to wrap primitive values into an Enum.\"\"\"\n\n    __slots__ = (\"enum\", \"reader\")\n\n    enum: Callable[..., Enum]\n    reader: Reader\n\n    def __init__(self, enum: Callable[..., Enum], reader: Reader) -&gt; None:\n        self.enum = enum\n        self.reader = reader\n\n    def read(self, decoder: BinaryDecoder) -&gt; Enum:\n        return self.enum(self.reader.read(decoder))\n\n    def skip(self, decoder: BinaryDecoder) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.construct_reader","title":"<code>construct_reader(file_schema, read_types=EMPTY_DICT)</code>","text":"<p>Construct a reader from a file schema.</p> <p>Parameters:</p> Name Type Description Default <code>file_schema</code> <code>Schema | IcebergType</code> <p>The schema of the Avro file.</p> required <code>read_types</code> <code>Dict[int, Callable[..., StructProtocol]]</code> <p>Constructors for structs for certain field-ids</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>def construct_reader(\n    file_schema: Schema | IcebergType, read_types: dict[int, Callable[..., StructProtocol]] = EMPTY_DICT\n) -&gt; Reader:\n    \"\"\"Construct a reader from a file schema.\n\n    Args:\n        file_schema (Schema | IcebergType): The schema of the Avro file.\n        read_types (Dict[int, Callable[..., StructProtocol]]): Constructors for structs for certain field-ids\n\n    Raises:\n        NotImplementedError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    return resolve_reader(file_schema, file_schema, read_types)\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.construct_writer","title":"<code>construct_writer(file_schema)</code>","text":"<p>Construct a writer from a file schema.</p> <p>Parameters:</p> Name Type Description Default <code>file_schema</code> <code>Schema | IcebergType</code> <p>The schema of the Avro file.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>def construct_writer(file_schema: Schema | IcebergType) -&gt; Writer:\n    \"\"\"Construct a writer from a file schema.\n\n    Args:\n        file_schema (Schema | IcebergType): The schema of the Avro file.\n\n    Raises:\n        NotImplementedError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    return visit(file_schema, CONSTRUCT_WRITER_VISITOR)\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.resolve_reader","title":"<code>resolve_reader(file_schema, read_schema, read_types=EMPTY_DICT, read_enums=EMPTY_DICT)</code>","text":"<p>Resolve the file and read schema to produce a reader.</p> <p>Parameters:</p> Name Type Description Default <code>file_schema</code> <code>Schema | IcebergType</code> <p>The schema of the Avro file.</p> required <code>read_schema</code> <code>Schema | IcebergType</code> <p>The requested read schema which is equal, subset or superset of the file schema.</p> required <code>read_types</code> <code>Dict[int, Callable[..., StructProtocol]]</code> <p>A dict of types to use for struct data.</p> <code>EMPTY_DICT</code> <code>read_enums</code> <code>Dict[int, Callable[..., Enum]]</code> <p>A dict of fields that have to be converted to an enum.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>def resolve_reader(\n    file_schema: Schema | IcebergType,\n    read_schema: Schema | IcebergType,\n    read_types: dict[int, Callable[..., StructProtocol]] = EMPTY_DICT,\n    read_enums: dict[int, Callable[..., Enum]] = EMPTY_DICT,\n) -&gt; Reader:\n    \"\"\"Resolve the file and read schema to produce a reader.\n\n    Args:\n        file_schema (Schema | IcebergType): The schema of the Avro file.\n        read_schema (Schema | IcebergType): The requested read schema which is equal, subset or superset of the file schema.\n        read_types (Dict[int, Callable[..., StructProtocol]]): A dict of types to use for struct data.\n        read_enums (Dict[int, Callable[..., Enum]]): A dict of fields that have to be converted to an enum.\n\n    Raises:\n        NotImplementedError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    return visit_with_partner(file_schema, read_schema, ReadSchemaResolver(read_types, read_enums), SchemaPartnerAccessor())  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/avro/resolver/#pyiceberg.avro.resolver.resolve_writer","title":"<code>resolve_writer(record_schema, file_schema)</code>","text":"<p>Resolve the file and read schema to produce a reader.</p> <p>Parameters:</p> Name Type Description Default <code>record_schema</code> <code>Schema | IcebergType</code> <p>The schema of the record in memory.</p> required <code>file_schema</code> <code>Schema | IcebergType</code> <p>The schema of the file that will be written</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to resolve an unrecognized object type.</p> Source code in <code>pyiceberg/avro/resolver.py</code> <pre><code>def resolve_writer(\n    record_schema: Schema | IcebergType,\n    file_schema: Schema | IcebergType,\n) -&gt; Writer:\n    \"\"\"Resolve the file and read schema to produce a reader.\n\n    Args:\n        record_schema (Schema | IcebergType): The schema of the record in memory.\n        file_schema (Schema | IcebergType): The schema of the file that will be written\n\n    Raises:\n        NotImplementedError: If attempting to resolve an unrecognized object type.\n    \"\"\"\n    if record_schema == file_schema:\n        return construct_writer(file_schema)\n    return visit_with_partner(file_schema, record_schema, WriteSchemaResolver(), SchemaPartnerAccessor())  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/","title":"writer","text":"<p>Classes for building the Writer tree.</p> <p>Constructing a writer tree from the schema makes it easy to decouple the writing implementation from the schema.</p>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.BinaryWriter","title":"<code>BinaryWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> <p>Variable byte length writer.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass BinaryWriter(Writer):\n    \"\"\"Variable byte length writer.\"\"\"\n\n    def write(self, encoder: BinaryEncoder, val: Any) -&gt; None:\n        encoder.write_bytes(val)\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.DecimalWriter","title":"<code>DecimalWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass DecimalWriter(Writer):\n    precision: int = dataclassfield()\n    scale: int = dataclassfield()\n\n    def write(self, encoder: BinaryEncoder, val: Any) -&gt; None:\n        return encoder.write(decimal_to_bytes(val, byte_length=decimal_required_bytes(self.precision)))\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of this object.\"\"\"\n        return f\"DecimalWriter({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.DecimalWriter.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of this object.\"\"\"\n    return f\"DecimalWriter({self.precision}, {self.scale})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.FixedWriter","title":"<code>FixedWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass FixedWriter(Writer):\n    _len: int = dataclassfield()\n\n    def write(self, encoder: BinaryEncoder, val: bytes) -&gt; None:\n        if len(val) != self._len:\n            raise ValueError(f\"Expected {self._len} bytes, got {len(val)}\")\n        encoder.write(val)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the length of this object.\"\"\"\n        return self._len\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of this object.\"\"\"\n        return f\"FixedWriter({self._len})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.FixedWriter.__len__","title":"<code>__len__()</code>","text":"<p>Return the length of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the length of this object.\"\"\"\n    return self._len\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.FixedWriter.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of this object.\"\"\"\n    return f\"FixedWriter({self._len})\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.IntegerWriter","title":"<code>IntegerWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> <p>Longs and ints are encoded the same way, and there is no long in Python.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass IntegerWriter(Writer):\n    \"\"\"Longs and ints are encoded the same way, and there is no long in Python.\"\"\"\n\n    def write(self, encoder: BinaryEncoder, val: int) -&gt; None:\n        encoder.write_int(val)\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.StructWriter","title":"<code>StructWriter</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Writer</code></p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass StructWriter(Writer):\n    field_writers: tuple[tuple[int | None, Writer], ...] = dataclassfield()\n\n    def write(self, encoder: BinaryEncoder, val: Record) -&gt; None:\n        for pos, writer in self.field_writers:\n            # When pos is None, then it is a default value\n            writer.write(encoder, val[pos] if pos is not None else None)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Implement the equality operator for this object.\"\"\"\n        return self.field_writers == other.field_writers if isinstance(other, StructWriter) else False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of this object.\"\"\"\n        return f\"StructWriter(tuple(({','.join(repr(field) for field in self.field_writers)})))\"\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return the hash of the writer as hash of this object.\"\"\"\n        return hash(self.field_writers)\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.StructWriter.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Implement the equality operator for this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Implement the equality operator for this object.\"\"\"\n    return self.field_writers == other.field_writers if isinstance(other, StructWriter) else False\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.StructWriter.__hash__","title":"<code>__hash__()</code>","text":"<p>Return the hash of the writer as hash of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return the hash of the writer as hash of this object.\"\"\"\n    return hash(self.field_writers)\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.StructWriter.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of this object.\"\"\"\n    return f\"StructWriter(tuple(({','.join(repr(field) for field in self.field_writers)})))\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.Writer","title":"<code>Writer</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Singleton</code></p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>@dataclass(frozen=True)\nclass Writer(Singleton):\n    @abstractmethod\n    def write(self, encoder: BinaryEncoder, val: Any) -&gt; Any: ...\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of this object.\"\"\"\n        return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/writer/#pyiceberg.avro.writer.Writer.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of this object.</p> Source code in <code>pyiceberg/avro/writer.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of this object.\"\"\"\n    return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/avro/codecs/","title":"codecs","text":"<p>Contains Codecs for Python Avro.</p> <p>Note that the word \"codecs\" means \"compression/decompression algorithms\" in the Avro world (https://avro.apache.org/docs/current/spec.html#Object+Container+Files), so don't confuse it with the Python's \"codecs\", which is a package mainly for converting character sets (https://docs.python.org/3/library/codecs.html).</p>"},{"location":"reference/pyiceberg/avro/codecs/bzip2/","title":"bzip2","text":""},{"location":"reference/pyiceberg/avro/codecs/codec/","title":"codec","text":""},{"location":"reference/pyiceberg/avro/codecs/codec/#pyiceberg.avro.codecs.codec.Codec","title":"<code>Codec</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all Avro codec classes.</p> Source code in <code>pyiceberg/avro/codecs/codec.py</code> <pre><code>class Codec(ABC):\n    \"\"\"Abstract base class for all Avro codec classes.\"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def compress(data: bytes) -&gt; tuple[bytes, int]: ...\n\n    @staticmethod\n    @abstractmethod\n    def decompress(data: bytes) -&gt; bytes: ...\n</code></pre>"},{"location":"reference/pyiceberg/avro/codecs/deflate/","title":"deflate","text":""},{"location":"reference/pyiceberg/avro/codecs/snappy_codec/","title":"snappy_codec","text":""},{"location":"reference/pyiceberg/avro/codecs/zstandard_codec/","title":"zstandard_codec","text":""},{"location":"reference/pyiceberg/catalog/","title":"catalog","text":""},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog","title":"<code>Catalog</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base Catalog for table operations like - create, drop, load, list and others.</p> <p>The catalog table APIs accept a table identifier, which is fully classified table name. The identifier can be a string or tuple of strings. If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.</p> <p>The catalog namespace APIs follow a similar convention wherein they also accept a namespace identifier that can be a string or tuple of strings.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the catalog.</p> <code>properties</code> <code>Properties</code> <p>Catalog properties.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>class Catalog(ABC):\n    \"\"\"Base Catalog for table operations like - create, drop, load, list and others.\n\n    The catalog table APIs accept a table identifier, which is fully classified table name. The identifier can be a string or\n    tuple of strings. If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.\n\n    The catalog namespace APIs follow a similar convention wherein they also accept a namespace identifier that can be a string\n    or tuple of strings.\n\n    Attributes:\n        name (str): Name of the catalog.\n        properties (Properties): Catalog properties.\n    \"\"\"\n\n    name: str\n    properties: Properties\n\n    def __init__(self, name: str, **properties: str):\n        self.name = name\n        self.properties = properties\n\n    @abstractmethod\n    def create_table(\n        self,\n        identifier: str | Identifier,\n        schema: Schema | pa.Schema,\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"Create a table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n            schema (Schema): Table's schema.\n            location (str | None): Location for the table. Optional Argument.\n            partition_spec (PartitionSpec): PartitionSpec for the table.\n            sort_order (SortOrder): SortOrder for the table.\n            properties (Properties): Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            TableAlreadyExistsError: If a table with the name already exists.\n        \"\"\"\n\n    @abstractmethod\n    def create_table_transaction(\n        self,\n        identifier: str | Identifier,\n        schema: Schema | pa.Schema,\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; CreateTableTransaction:\n        \"\"\"Create a CreateTableTransaction.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n            schema (Schema): Table's schema.\n            location (str | None): Location for the table. Optional Argument.\n            partition_spec (PartitionSpec): PartitionSpec for the table.\n            sort_order (SortOrder): SortOrder for the table.\n            properties (Properties): Table properties that can be a string based dictionary.\n\n        Returns:\n            CreateTableTransaction: createTableTransaction instance.\n        \"\"\"\n\n    def create_table_if_not_exists(\n        self,\n        identifier: str | Identifier,\n        schema: Schema | pa.Schema,\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"Create a table if it does not exist.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n            schema (Schema): Table's schema.\n            location (str | None): Location for the table. Optional Argument.\n            partition_spec (PartitionSpec): PartitionSpec for the table.\n            sort_order (SortOrder): SortOrder for the table.\n            properties (Properties): Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance if the table does not exist, else the existing\n            table instance.\n        \"\"\"\n        try:\n            return self.create_table(identifier, schema, location, partition_spec, sort_order, properties)\n        except TableAlreadyExistsError:\n            return self.load_table(identifier)\n\n    @abstractmethod\n    def load_table(self, identifier: str | Identifier) -&gt; Table:\n        \"\"\"Load the table's metadata and returns the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def table_exists(self, identifier: str | Identifier) -&gt; bool:\n        \"\"\"Check if a table exists.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Returns:\n            bool: True if the table exists, False otherwise.\n        \"\"\"\n\n    @abstractmethod\n    def view_exists(self, identifier: str | Identifier) -&gt; bool:\n        \"\"\"Check if a view exists.\n\n        Args:\n            identifier (str | Identifier): View identifier.\n\n        Returns:\n            bool: True if the view exists, False otherwise.\n        \"\"\"\n\n    @abstractmethod\n    def namespace_exists(self, namespace: str | Identifier) -&gt; bool:\n        \"\"\"Check if a namespace exists.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Returns:\n            bool: True if the namespace exists, False otherwise.\n        \"\"\"\n\n    @abstractmethod\n    def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier (Union[str, Identifier]): Table identifier for the table\n            metadata_location (str): The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n\n    @abstractmethod\n    def drop_table(self, identifier: str | Identifier) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def purge_table(self, identifier: str | Identifier) -&gt; None:\n        \"\"\"Drop a table and purge all data and metadata files.\n\n        Note: This method only logs warning rather than raise exception when encountering file deletion failure.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n\n    @abstractmethod\n    def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        Args:\n            from_identifier (str | Identifier): Existing table identifier.\n            to_identifier (str | Identifier): New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def commit_table(\n        self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n            CommitStateUnknownException: Failed due to an internal exception on the side of the catalog.\n        \"\"\"\n\n    @abstractmethod\n    def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            properties (Properties): A string dictionary of properties for the given namespace.\n\n        Raises:\n            NamespaceAlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n\n    def create_namespace_if_not_exists(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace if it does not exist.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            properties (Properties): A string dictionary of properties for the given namespace.\n        \"\"\"\n        try:\n            self.create_namespace(namespace, properties)\n        except NamespaceAlreadyExistsError:\n            pass\n\n    @abstractmethod\n    def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n\n    @abstractmethod\n    def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        \"\"\"List tables under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n        \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def list_views(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        \"\"\"List views under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n        \"\"\"Get properties for a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n\n    @abstractmethod\n    def update_namespace_properties(\n        self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"Remove provided property keys and updates properties for a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            removals (Set[str]): Set of property keys that need to be removed. Optional Argument.\n            updates (Properties): Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n\n    @abstractmethod\n    def drop_view(self, identifier: str | Identifier) -&gt; None:\n        \"\"\"Drop a view.\n\n        Args:\n            identifier (str | Identifier): View identifier.\n\n        Raises:\n            NoSuchViewError: If a view with the given name does not exist.\n        \"\"\"\n\n    @staticmethod\n    def identifier_to_tuple(identifier: str | Identifier) -&gt; Identifier:\n        \"\"\"Parse an identifier to a tuple.\n\n        If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.\n\n        Args:\n            identifier (str | Identifier): an identifier, either a string or tuple of strings.\n\n        Returns:\n            Identifier: a tuple of strings.\n        \"\"\"\n        return identifier if isinstance(identifier, tuple) else tuple(str.split(identifier, \".\"))\n\n    @staticmethod\n    def table_name_from(identifier: str | Identifier) -&gt; str:\n        \"\"\"Extract table name from a table identifier.\n\n        Args:\n            identifier (str | Identifier): a table identifier.\n\n        Returns:\n            str: Table name.\n        \"\"\"\n        return Catalog.identifier_to_tuple(identifier)[-1]\n\n    @staticmethod\n    def namespace_from(identifier: str | Identifier) -&gt; Identifier:\n        \"\"\"Extract table namespace from a table identifier.\n\n        Args:\n            identifier (Union[str, Identifier]): a table identifier.\n\n        Returns:\n            Identifier: Namespace identifier.\n        \"\"\"\n        return Catalog.identifier_to_tuple(identifier)[:-1]\n\n    @staticmethod\n    def namespace_to_string(identifier: str | Identifier, err: type[ValueError] | type[NoSuchNamespaceError] = ValueError) -&gt; str:\n        \"\"\"Transform a namespace identifier into a string.\n\n        Args:\n            identifier (Union[str, Identifier]): a namespace identifier.\n            err (Union[Type[ValueError], Type[NoSuchNamespaceError]]): the error type to raise when identifier is empty.\n\n        Returns:\n            Identifier: Namespace identifier.\n        \"\"\"\n        tuple_identifier = Catalog.identifier_to_tuple(identifier)\n        if len(tuple_identifier) &lt; 1:\n            raise err(\"Empty namespace identifier\")\n\n        # Check if any segment of the tuple is an empty string\n        if any(segment.strip() == \"\" for segment in tuple_identifier):\n            raise err(\"Namespace identifier contains an empty segment or a segment with only whitespace\")\n\n        return \".\".join(segment.strip() for segment in tuple_identifier)\n\n    def supports_server_side_planning(self) -&gt; bool:\n        \"\"\"Check if the catalog supports server-side scan planning.\"\"\"\n        return False\n\n    @staticmethod\n    def identifier_to_database(\n        identifier: str | Identifier, err: type[ValueError] | type[NoSuchNamespaceError] = ValueError\n    ) -&gt; str:\n        tuple_identifier = Catalog.identifier_to_tuple(identifier)\n        if len(tuple_identifier) != 1:\n            raise err(f\"Invalid database, hierarchical namespaces are not supported: {identifier}\")\n\n        return tuple_identifier[0]\n\n    @staticmethod\n    def identifier_to_database_and_table(\n        identifier: str | Identifier,\n        err: type[ValueError] | type[NoSuchTableError] | type[NoSuchNamespaceError] = ValueError,\n    ) -&gt; tuple[str, str]:\n        tuple_identifier = Catalog.identifier_to_tuple(identifier)\n        if len(tuple_identifier) != 2:\n            raise err(f\"Invalid path, hierarchical namespaces are not supported: {identifier}\")\n\n        return tuple_identifier[0], tuple_identifier[1]\n\n    def _load_file_io(self, properties: Properties = EMPTY_DICT, location: str | None = None) -&gt; FileIO:\n        return load_file_io({**self.properties, **properties}, location)\n\n    @staticmethod\n    def _convert_schema_if_needed(\n        schema: Schema | pa.Schema, format_version: TableVersion = TableProperties.DEFAULT_FORMAT_VERSION\n    ) -&gt; Schema:\n        if isinstance(schema, Schema):\n            return schema\n        try:\n            import pyarrow as pa\n\n            from pyiceberg.io.pyarrow import _ConvertToIcebergWithoutIDs, visit_pyarrow\n\n            downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n            if isinstance(schema, pa.Schema):\n                schema: Schema = visit_pyarrow(  # type: ignore\n                    schema,\n                    _ConvertToIcebergWithoutIDs(\n                        downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us, format_version=format_version\n                    ),\n                )\n                return schema\n        except ModuleNotFoundError:\n            pass\n        raise ValueError(f\"{type(schema)=}, but it must be pyiceberg.schema.Schema or pyarrow.Schema\")\n\n    @staticmethod\n    def _delete_old_metadata(io: FileIO, base: TableMetadata, metadata: TableMetadata) -&gt; None:\n        \"\"\"Delete oldest metadata if config is set to true.\"\"\"\n        delete_after_commit: bool = property_as_bool(\n            metadata.properties,\n            TableProperties.METADATA_DELETE_AFTER_COMMIT_ENABLED,\n            TableProperties.METADATA_DELETE_AFTER_COMMIT_ENABLED_DEFAULT,\n        )\n\n        if delete_after_commit:\n            removed_previous_metadata_files: set[str] = {log.metadata_file for log in base.metadata_log}\n            current_metadata_files: set[str] = {log.metadata_file for log in metadata.metadata_log}\n            removed_previous_metadata_files.difference_update(current_metadata_files)\n            delete_files(io, removed_previous_metadata_files, METADATA)\n\n    def close(self) -&gt; None:  # noqa: B027\n        \"\"\"Close the catalog and release any resources.\n\n        This method should be called when the catalog is no longer needed to ensure\n        proper cleanup of resources like database connections, file handles, etc.\n\n        Default implementation does nothing. Override in subclasses that need cleanup.\n        \"\"\"\n\n    def __enter__(self) -&gt; Catalog:\n        \"\"\"Enter the context manager.\n\n        Returns:\n            Catalog: The catalog instance.\n        \"\"\"\n        return self\n\n    def __exit__(self, exc_type: type | None, exc_val: BaseException | None, exc_tb: Any | None) -&gt; None:\n        \"\"\"Exit the context manager and close the catalog.\n\n        Args:\n            exc_type: Exception type if an exception occurred.\n            exc_val: Exception value if an exception occurred.\n            exc_tb: Exception traceback if an exception occurred.\n        \"\"\"\n        self.close()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Catalog class.\"\"\"\n        return f\"{self.name} ({self.__class__})\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.__enter__","title":"<code>__enter__()</code>","text":"<p>Enter the context manager.</p> <p>Returns:</p> Name Type Description <code>Catalog</code> <code>Catalog</code> <p>The catalog instance.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def __enter__(self) -&gt; Catalog:\n    \"\"\"Enter the context manager.\n\n    Returns:\n        Catalog: The catalog instance.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Exit the context manager and close the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>exc_type</code> <code>type | None</code> <p>Exception type if an exception occurred.</p> required <code>exc_val</code> <code>BaseException | None</code> <p>Exception value if an exception occurred.</p> required <code>exc_tb</code> <code>Any | None</code> <p>Exception traceback if an exception occurred.</p> required Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def __exit__(self, exc_type: type | None, exc_val: BaseException | None, exc_tb: Any | None) -&gt; None:\n    \"\"\"Exit the context manager and close the catalog.\n\n    Args:\n        exc_type: Exception type if an exception occurred.\n        exc_val: Exception value if an exception occurred.\n        exc_tb: Exception traceback if an exception occurred.\n    \"\"\"\n    self.close()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Catalog class.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Catalog class.\"\"\"\n    return f\"{self.name} ({self.__class__})\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.close","title":"<code>close()</code>","text":"<p>Close the catalog and release any resources.</p> <p>This method should be called when the catalog is no longer needed to ensure proper cleanup of resources like database connections, file handles, etc.</p> <p>Default implementation does nothing. Override in subclasses that need cleanup.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def close(self) -&gt; None:  # noqa: B027\n    \"\"\"Close the catalog and release any resources.\n\n    This method should be called when the catalog is no longer needed to ensure\n    proper cleanup of resources like database connections, file handles, etc.\n\n    Default implementation does nothing. Override in subclasses that need cleanup.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>  <code>abstractmethod</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> <code>CommitStateUnknownException</code> <p>Failed due to an internal exception on the side of the catalog.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef commit_table(\n    self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        CommitStateUnknownException: Failed due to an internal exception on the side of the catalog.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>  <code>abstractmethod</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NamespaceAlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        properties (Properties): A string dictionary of properties for the given namespace.\n\n    Raises:\n        NamespaceAlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_namespace_if_not_exists","title":"<code>create_namespace_if_not_exists(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def create_namespace_if_not_exists(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace if it does not exist.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        properties (Properties): A string dictionary of properties for the given namespace.\n    \"\"\"\n    try:\n        self.create_namespace(namespace, properties)\n    except NamespaceAlreadyExistsError:\n        pass\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>  <code>abstractmethod</code>","text":"<p>Create a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Schema</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If a table with the name already exists.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef create_table(\n    self,\n    identifier: str | Identifier,\n    schema: Schema | pa.Schema,\n    location: str | None = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"Create a table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n        schema (Schema): Table's schema.\n        location (str | None): Location for the table. Optional Argument.\n        partition_spec (PartitionSpec): PartitionSpec for the table.\n        sort_order (SortOrder): SortOrder for the table.\n        properties (Properties): Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        TableAlreadyExistsError: If a table with the name already exists.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_table_if_not_exists","title":"<code>create_table_if_not_exists(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create a table if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Schema</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance if the table does not exist, else the existing</p> <code>Table</code> <p>table instance.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def create_table_if_not_exists(\n    self,\n    identifier: str | Identifier,\n    schema: Schema | pa.Schema,\n    location: str | None = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"Create a table if it does not exist.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n        schema (Schema): Table's schema.\n        location (str | None): Location for the table. Optional Argument.\n        partition_spec (PartitionSpec): PartitionSpec for the table.\n        sort_order (SortOrder): SortOrder for the table.\n        properties (Properties): Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance if the table does not exist, else the existing\n        table instance.\n    \"\"\"\n    try:\n        return self.create_table(identifier, schema, location, partition_spec, sort_order, properties)\n    except TableAlreadyExistsError:\n        return self.load_table(identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.create_table_transaction","title":"<code>create_table_transaction(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>  <code>abstractmethod</code>","text":"<p>Create a CreateTableTransaction.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Schema</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>CreateTableTransaction</code> <code>CreateTableTransaction</code> <p>createTableTransaction instance.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef create_table_transaction(\n    self,\n    identifier: str | Identifier,\n    schema: Schema | pa.Schema,\n    location: str | None = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; CreateTableTransaction:\n    \"\"\"Create a CreateTableTransaction.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n        schema (Schema): Table's schema.\n        location (str | None): Location for the table. Optional Argument.\n        partition_spec (PartitionSpec): PartitionSpec for the table.\n        sort_order (SortOrder): SortOrder for the table.\n        properties (Properties): Table properties that can be a string based dictionary.\n\n    Returns:\n        CreateTableTransaction: createTableTransaction instance.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>  <code>abstractmethod</code>","text":"<p>Drop a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef drop_namespace(self, namespace: str | Identifier) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.drop_table","title":"<code>drop_table(identifier)</code>  <code>abstractmethod</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef drop_table(self, identifier: str | Identifier) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.drop_view","title":"<code>drop_view(identifier)</code>  <code>abstractmethod</code>","text":"<p>Drop a view.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>View identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchViewError</code> <p>If a view with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef drop_view(self, identifier: str | Identifier) -&gt; None:\n    \"\"\"Drop a view.\n\n    Args:\n        identifier (str | Identifier): View identifier.\n\n    Raises:\n        NoSuchViewError: If a view with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.identifier_to_tuple","title":"<code>identifier_to_tuple(identifier)</code>  <code>staticmethod</code>","text":"<p>Parse an identifier to a tuple.</p> <p>If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>an identifier, either a string or tuple of strings.</p> required <p>Returns:</p> Name Type Description <code>Identifier</code> <code>Identifier</code> <p>a tuple of strings.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef identifier_to_tuple(identifier: str | Identifier) -&gt; Identifier:\n    \"\"\"Parse an identifier to a tuple.\n\n    If the identifier is a string, it is split into a tuple on '.'. If it is a tuple, it is used as-is.\n\n    Args:\n        identifier (str | Identifier): an identifier, either a string or tuple of strings.\n\n    Returns:\n        Identifier: a tuple of strings.\n    \"\"\"\n    return identifier if isinstance(identifier, tuple) else tuple(str.split(identifier, \".\"))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>  <code>abstractmethod</code>","text":"<p>List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> <code>()</code> <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n    \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.list_tables","title":"<code>list_tables(namespace)</code>  <code>abstractmethod</code>","text":"<p>List tables under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n    \"\"\"List tables under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.list_views","title":"<code>list_views(namespace)</code>  <code>abstractmethod</code>","text":"<p>List views under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef list_views(self, namespace: str | Identifier) -&gt; list[Identifier]:\n    \"\"\"List views under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>  <code>abstractmethod</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n    \"\"\"Get properties for a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.load_table","title":"<code>load_table(identifier)</code>  <code>abstractmethod</code>","text":"<p>Load the table's metadata and returns the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef load_table(self, identifier: str | Identifier) -&gt; Table:\n    \"\"\"Load the table's metadata and returns the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.namespace_exists","title":"<code>namespace_exists(namespace)</code>  <code>abstractmethod</code>","text":"<p>Check if a namespace exists.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the namespace exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef namespace_exists(self, namespace: str | Identifier) -&gt; bool:\n    \"\"\"Check if a namespace exists.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Returns:\n        bool: True if the namespace exists, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.namespace_from","title":"<code>namespace_from(identifier)</code>  <code>staticmethod</code>","text":"<p>Extract table namespace from a table identifier.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>a table identifier.</p> required <p>Returns:</p> Name Type Description <code>Identifier</code> <code>Identifier</code> <p>Namespace identifier.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef namespace_from(identifier: str | Identifier) -&gt; Identifier:\n    \"\"\"Extract table namespace from a table identifier.\n\n    Args:\n        identifier (Union[str, Identifier]): a table identifier.\n\n    Returns:\n        Identifier: Namespace identifier.\n    \"\"\"\n    return Catalog.identifier_to_tuple(identifier)[:-1]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.namespace_to_string","title":"<code>namespace_to_string(identifier, err=ValueError)</code>  <code>staticmethod</code>","text":"<p>Transform a namespace identifier into a string.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>a namespace identifier.</p> required <code>err</code> <code>Union[Type[ValueError], Type[NoSuchNamespaceError]]</code> <p>the error type to raise when identifier is empty.</p> <code>ValueError</code> <p>Returns:</p> Name Type Description <code>Identifier</code> <code>str</code> <p>Namespace identifier.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef namespace_to_string(identifier: str | Identifier, err: type[ValueError] | type[NoSuchNamespaceError] = ValueError) -&gt; str:\n    \"\"\"Transform a namespace identifier into a string.\n\n    Args:\n        identifier (Union[str, Identifier]): a namespace identifier.\n        err (Union[Type[ValueError], Type[NoSuchNamespaceError]]): the error type to raise when identifier is empty.\n\n    Returns:\n        Identifier: Namespace identifier.\n    \"\"\"\n    tuple_identifier = Catalog.identifier_to_tuple(identifier)\n    if len(tuple_identifier) &lt; 1:\n        raise err(\"Empty namespace identifier\")\n\n    # Check if any segment of the tuple is an empty string\n    if any(segment.strip() == \"\" for segment in tuple_identifier):\n        raise err(\"Namespace identifier contains an empty segment or a segment with only whitespace\")\n\n    return \".\".join(segment.strip() for segment in tuple_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.purge_table","title":"<code>purge_table(identifier)</code>  <code>abstractmethod</code>","text":"<p>Drop a table and purge all data and metadata files.</p> <p>Note: This method only logs warning rather than raise exception when encountering file deletion failure.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef purge_table(self, identifier: str | Identifier) -&gt; None:\n    \"\"\"Drop a table and purge all data and metadata files.\n\n    Note: This method only logs warning rather than raise exception when encountering file deletion failure.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>  <code>abstractmethod</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier (Union[str, Identifier]): Table identifier for the table\n        metadata_location (str): The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>  <code>abstractmethod</code>","text":"<p>Rename a fully classified table name.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str | Identifier</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>str | Identifier</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    Args:\n        from_identifier (str | Identifier): Existing table identifier.\n        to_identifier (str | Identifier): New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.supports_server_side_planning","title":"<code>supports_server_side_planning()</code>","text":"<p>Check if the catalog supports server-side scan planning.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def supports_server_side_planning(self) -&gt; bool:\n    \"\"\"Check if the catalog supports server-side scan planning.\"\"\"\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.table_exists","title":"<code>table_exists(identifier)</code>  <code>abstractmethod</code>","text":"<p>Check if a table exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the table exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef table_exists(self, identifier: str | Identifier) -&gt; bool:\n    \"\"\"Check if a table exists.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Returns:\n        bool: True if the table exists, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.table_name_from","title":"<code>table_name_from(identifier)</code>  <code>staticmethod</code>","text":"<p>Extract table name from a table identifier.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>a table identifier.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Table name.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@staticmethod\ndef table_name_from(identifier: str | Identifier) -&gt; str:\n    \"\"\"Extract table name from a table identifier.\n\n    Args:\n        identifier (str | Identifier): a table identifier.\n\n    Returns:\n        str: Table name.\n    \"\"\"\n    return Catalog.identifier_to_tuple(identifier)[-1]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>  <code>abstractmethod</code>","text":"<p>Remove provided property keys and updates properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>removals</code> <code>Set[str]</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef update_namespace_properties(\n    self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"Remove provided property keys and updates properties for a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        removals (Set[str]): Set of property keys that need to be removed. Optional Argument.\n        updates (Properties): Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.Catalog.view_exists","title":"<code>view_exists(identifier)</code>  <code>abstractmethod</code>","text":"<p>Check if a view exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>View identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the view exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>@abstractmethod\ndef view_exists(self, identifier: str | Identifier) -&gt; bool:\n    \"\"\"Check if a view exists.\n\n    Args:\n        identifier (str | Identifier): View identifier.\n\n    Returns:\n        bool: True if the view exists, False otherwise.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.MetastoreCatalog","title":"<code>MetastoreCatalog</code>","text":"<p>               Bases: <code>Catalog</code>, <code>ABC</code></p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>class MetastoreCatalog(Catalog, ABC):\n    def __init__(self, name: str, **properties: str):\n        super().__init__(name, **properties)\n\n    def create_table_transaction(\n        self,\n        identifier: str | Identifier,\n        schema: Schema | pa.Schema,\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; CreateTableTransaction:\n        return CreateTableTransaction(\n            self._create_staged_table(identifier, schema, location, partition_spec, sort_order, properties)\n        )\n\n    def table_exists(self, identifier: str | Identifier) -&gt; bool:\n        try:\n            self.load_table(identifier)\n            return True\n        except NoSuchTableError:\n            return False\n\n    def namespace_exists(self, namespace: str | Identifier) -&gt; bool:\n        \"\"\"Check if a namespace exists.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Returns:\n            bool: True if the namespace exists, False otherwise.\n        \"\"\"\n        try:\n            self.load_namespace_properties(namespace)\n            return True\n        except NoSuchNamespaceError:\n            return False\n\n    def purge_table(self, identifier: str | Identifier) -&gt; None:\n        table = self.load_table(identifier)\n        self.drop_table(identifier)\n        io = load_file_io(self.properties, table.metadata_location)\n        metadata = table.metadata\n        manifest_lists_to_delete = set()\n        manifests_to_delete: list[ManifestFile] = []\n        for snapshot in metadata.snapshots:\n            manifests_to_delete += snapshot.manifests(io)\n            manifest_lists_to_delete.add(snapshot.manifest_list)\n\n        manifest_paths_to_delete = {manifest.manifest_path for manifest in manifests_to_delete}\n        prev_metadata_files = {log.metadata_file for log in metadata.metadata_log}\n\n        delete_data_files(io, manifests_to_delete)\n        delete_files(io, manifest_paths_to_delete, MANIFEST)\n        delete_files(io, manifest_lists_to_delete, MANIFEST_LIST)\n        delete_files(io, prev_metadata_files, PREVIOUS_METADATA)\n        delete_files(io, {table.metadata_location}, METADATA)\n\n    def _create_staged_table(\n        self,\n        identifier: str | Identifier,\n        schema: Schema | pa.Schema,\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; StagedTable:\n        \"\"\"Create a table and return the table instance without committing the changes.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n            schema (Schema): Table's schema.\n            location (str | None): Location for the table. Optional Argument.\n            partition_spec (PartitionSpec): PartitionSpec for the table.\n            sort_order (SortOrder): SortOrder for the table.\n            properties (Properties): Table properties that can be a string based dictionary.\n\n        Returns:\n            StagedTable: the created staged table instance.\n        \"\"\"\n        schema: Schema = self._convert_schema_if_needed(  # type: ignore\n            schema,\n            int(properties.get(TableProperties.FORMAT_VERSION, TableProperties.DEFAULT_FORMAT_VERSION)),  # type: ignore\n        )\n\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        location = self._resolve_table_location(location, database_name, table_name)\n        provider = load_location_provider(location, properties)\n        metadata_location = provider.new_table_metadata_file_location()\n        metadata = new_table_metadata(\n            location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n        )\n        io = self._load_file_io(properties=properties, location=metadata_location)\n        return StagedTable(\n            identifier=(database_name, table_name),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=io,\n            catalog=self,\n        )\n\n    def _update_and_stage_table(\n        self,\n        current_table: Table | None,\n        table_identifier: Identifier,\n        requirements: tuple[TableRequirement, ...],\n        updates: tuple[TableUpdate, ...],\n    ) -&gt; StagedTable:\n        for requirement in requirements:\n            requirement.validate(current_table.metadata if current_table else None)\n\n        updated_metadata = update_table_metadata(\n            base_metadata=current_table.metadata if current_table else self._empty_table_metadata(),\n            updates=updates,\n            enforce_validation=current_table is None,\n            metadata_location=current_table.metadata_location if current_table else None,\n        )\n\n        new_metadata_version = self._parse_metadata_version(current_table.metadata_location) + 1 if current_table else 0\n        provider = load_location_provider(updated_metadata.location, updated_metadata.properties)\n        new_metadata_location = provider.new_table_metadata_file_location(new_metadata_version)\n\n        return StagedTable(\n            identifier=table_identifier,\n            metadata=updated_metadata,\n            metadata_location=new_metadata_location,\n            io=self._load_file_io(properties=updated_metadata.properties, location=new_metadata_location),\n            catalog=self,\n        )\n\n    def _get_updated_props_and_update_summary(\n        self, current_properties: Properties, removals: set[str] | None, updates: Properties\n    ) -&gt; tuple[PropertiesUpdateSummary, Properties]:\n        self._check_for_overlap(updates=updates, removals=removals)\n        updated_properties = dict(current_properties)\n\n        removed: set[str] = set()\n        updated: set[str] = set()\n\n        if removals:\n            for key in removals:\n                if key in updated_properties:\n                    updated_properties.pop(key)\n                    removed.add(key)\n        if updates:\n            for key, value in updates.items():\n                updated_properties[key] = value\n                updated.add(key)\n\n        expected_to_change = (removals or set()).difference(removed)\n        properties_update_summary = PropertiesUpdateSummary(\n            removed=list(removed or []), updated=list(updated or []), missing=list(expected_to_change)\n        )\n\n        return properties_update_summary, updated_properties\n\n    def _resolve_table_location(self, location: str | None, database_name: str, table_name: str) -&gt; str:\n        if not location:\n            return self._get_default_warehouse_location(database_name, table_name)\n        return location.rstrip(\"/\")\n\n    def _get_default_warehouse_location(self, database_name: str, table_name: str) -&gt; str:\n        \"\"\"Return the default warehouse location using the convention of `warehousePath/databaseName/tableName`.\"\"\"\n        database_properties = self.load_namespace_properties(database_name)\n        if database_location := database_properties.get(LOCATION):\n            database_location = database_location.rstrip(\"/\")\n            return f\"{database_location}/{table_name}\"\n\n        if warehouse_path := self.properties.get(WAREHOUSE_LOCATION):\n            warehouse_path = warehouse_path.rstrip(\"/\")\n            return f\"{warehouse_path}/{database_name}/{table_name}\"\n\n        raise ValueError(\"No default path is set, please specify a location when creating a table\")\n\n    def _get_hive_style_warehouse_location(self, database_name: str, table_name: str) -&gt; str:\n        \"\"\"Return the default warehouse location following the Hive convention of `warehousePath/databaseName.db/tableName`.\"\"\"\n        database_properties = self.load_namespace_properties(database_name)\n        if database_location := database_properties.get(LOCATION):\n            database_location = database_location.rstrip(\"/\")\n            return f\"{database_location}/{table_name}\"\n\n        if warehouse_path := self.properties.get(WAREHOUSE_LOCATION):\n            warehouse_path = warehouse_path.rstrip(\"/\")\n            return f\"{warehouse_path}/{database_name}.db/{table_name}\"\n\n        raise ValueError(\"No default path is set, please specify a location when creating a table\")\n\n    @staticmethod\n    def _write_metadata(metadata: TableMetadata, io: FileIO, metadata_path: str) -&gt; None:\n        ToOutputFile.table_metadata(metadata, io.new_output(metadata_path))\n\n    @staticmethod\n    def _parse_metadata_version(metadata_location: str) -&gt; int:\n        \"\"\"Parse the version from the metadata location.\n\n        The version is the first part of the file name, before the first dash.\n        For example, the version of the metadata file\n        `s3://bucket/db/tb/metadata/00001-6c97e413-d51b-4538-ac70-12fe2a85cb83.metadata.json`\n        is 1.\n        If the path does not comply with the pattern, the version is defaulted to be -1, ensuring\n        that the next metadata file is treated as having version 0.\n\n        Args:\n            metadata_location (str): The location of the metadata file.\n\n        Returns:\n            int: The version of the metadata file. -1 if the file name does not have valid version string\n        \"\"\"\n        file_name = metadata_location.split(\"/\")[-1]\n        if file_name_match := TABLE_METADATA_FILE_NAME_REGEX.fullmatch(file_name):\n            try:\n                uuid.UUID(file_name_match.group(2))\n            except ValueError:\n                return -1\n            return int(file_name_match.group(1))\n        else:\n            return -1\n\n    @staticmethod\n    def _check_for_overlap(removals: set[str] | None, updates: Properties) -&gt; None:\n        if updates and removals:\n            overlap = set(removals) &amp; set(updates.keys())\n            if overlap:\n                raise ValueError(f\"Updates and deletes have an overlap: {overlap}\")\n\n    @staticmethod\n    def _empty_table_metadata() -&gt; TableMetadata:\n        \"\"\"Return an empty TableMetadata instance.\n\n        It is used to build a TableMetadata from a sequence of initial TableUpdates.\n        It is a V1 TableMetadata because there will be a UpgradeFormatVersionUpdate in\n        initial changes to bump the metadata to the target version.\n\n        Returns:\n            TableMetadata: An empty TableMetadata instance.\n        \"\"\"\n        return TableMetadataV1.model_construct(last_column_id=-1, schema=Schema())\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.MetastoreCatalog.namespace_exists","title":"<code>namespace_exists(namespace)</code>","text":"<p>Check if a namespace exists.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the namespace exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def namespace_exists(self, namespace: str | Identifier) -&gt; bool:\n    \"\"\"Check if a namespace exists.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Returns:\n        bool: True if the namespace exists, False otherwise.\n    \"\"\"\n    try:\n        self.load_namespace_properties(namespace)\n        return True\n    except NoSuchNamespaceError:\n        return False\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.delete_data_files","title":"<code>delete_data_files(io, manifests_to_delete)</code>","text":"<p>Delete data files linked to given manifests.</p> <p>Log warnings if failing to delete any file.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>FileIO</code> <p>The FileIO used to delete the object.</p> required <code>manifests_to_delete</code> <code>list[ManifestFile]</code> <p>A list of manifest contains paths of data files to be deleted.</p> required Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def delete_data_files(io: FileIO, manifests_to_delete: list[ManifestFile]) -&gt; None:\n    \"\"\"Delete data files linked to given manifests.\n\n    Log warnings if failing to delete any file.\n\n    Args:\n        io: The FileIO used to delete the object.\n        manifests_to_delete: A list of manifest contains paths of data files to be deleted.\n    \"\"\"\n    deleted_files: dict[str, bool] = {}\n    for manifest_file in manifests_to_delete:\n        for entry in manifest_file.fetch_manifest_entry(io, discard_deleted=False):\n            path = entry.data_file.file_path\n            if not deleted_files.get(path, False):\n                try:\n                    io.delete(path)\n                except OSError:\n                    logger.warning(f\"Failed to delete data file {path}\", exc_info=logger.isEnabledFor(logging.DEBUG))\n                deleted_files[path] = True\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.delete_files","title":"<code>delete_files(io, files_to_delete, file_type)</code>","text":"<p>Delete files.</p> <p>Log warnings if failing to delete any file.</p> <p>Parameters:</p> Name Type Description Default <code>io</code> <code>FileIO</code> <p>The FileIO used to delete the object.</p> required <code>files_to_delete</code> <code>set[str]</code> <p>A set of file paths to be deleted.</p> required <code>file_type</code> <code>str</code> <p>The type of the file.</p> required Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def delete_files(io: FileIO, files_to_delete: set[str], file_type: str) -&gt; None:\n    \"\"\"Delete files.\n\n    Log warnings if failing to delete any file.\n\n    Args:\n        io: The FileIO used to delete the object.\n        files_to_delete: A set of file paths to be deleted.\n        file_type: The type of the file.\n    \"\"\"\n    for file in files_to_delete:\n        try:\n            io.delete(file)\n        except OSError:\n            logger.warning(f\"Failed to delete {file_type} file {file}\", exc_info=logger.isEnabledFor(logging.DEBUG))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.infer_catalog_type","title":"<code>infer_catalog_type(name, catalog_properties)</code>","text":"<p>Try to infer the type based on the dict.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the catalog.</p> required <code>catalog_properties</code> <code>RecursiveDict</code> <p>Catalog properties.</p> required <p>Returns:</p> Type Description <code>CatalogType | None</code> <p>The inferred type based on the provided properties.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises a ValueError in case properties are missing, or the wrong type.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def infer_catalog_type(name: str, catalog_properties: RecursiveDict) -&gt; CatalogType | None:\n    \"\"\"Try to infer the type based on the dict.\n\n    Args:\n        name: Name of the catalog.\n        catalog_properties: Catalog properties.\n\n    Returns:\n        The inferred type based on the provided properties.\n\n    Raises:\n        ValueError: Raises a ValueError in case properties are missing, or the wrong type.\n    \"\"\"\n    if uri := catalog_properties.get(URI):\n        if isinstance(uri, str):\n            if uri.startswith(\"http\"):\n                return CatalogType.REST\n            elif uri.startswith(\"thrift\"):\n                return CatalogType.HIVE\n            elif uri.startswith((\"sqlite\", \"postgresql\")):\n                return CatalogType.SQL\n            else:\n                raise ValueError(f\"Could not infer the catalog type from the uri: {uri}\")\n        else:\n            raise ValueError(f\"Expects the URI to be a string, got: {type(uri)}\")\n    raise ValueError(\n        f\"URI missing, please provide using --uri, the config or environment variable PYICEBERG_CATALOG__{name.upper()}__URI\"\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/#pyiceberg.catalog.load_catalog","title":"<code>load_catalog(name=None, **properties)</code>","text":"<p>Load the catalog based on the properties.</p> <p>Will look up the properties from the config, based on the name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>The name of the catalog.</p> <code>None</code> <code>properties</code> <code>str | None</code> <p>The properties that are used next to the configuration.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Catalog</code> <p>An initialized Catalog.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises a ValueError in case properties are missing or malformed, or if it could not determine the catalog based on the properties.</p> Source code in <code>pyiceberg/catalog/__init__.py</code> <pre><code>def load_catalog(name: str | None = None, **properties: str | None) -&gt; Catalog:\n    \"\"\"Load the catalog based on the properties.\n\n    Will look up the properties from the config, based on the name.\n\n    Args:\n        name: The name of the catalog.\n        properties: The properties that are used next to the configuration.\n\n    Returns:\n        An initialized Catalog.\n\n    Raises:\n        ValueError: Raises a ValueError in case properties are missing or malformed,\n            or if it could not determine the catalog based on the properties.\n    \"\"\"\n    if name is None:\n        name = _ENV_CONFIG.get_default_catalog_name()\n\n    env = _ENV_CONFIG.get_catalog_config(name)\n    conf: RecursiveDict = merge_config(env or {}, cast(RecursiveDict, properties))\n\n    catalog_type: CatalogType | None\n    provided_catalog_type = conf.get(TYPE)\n\n    if catalog_impl := properties.get(PY_CATALOG_IMPL):\n        if provided_catalog_type:\n            raise ValueError(\n                \"Must not set both catalog type and py-catalog-impl configurations, \"\n                f\"but found type {provided_catalog_type} and py-catalog-impl {catalog_impl}\"\n            )\n\n        if catalog := _import_catalog(name, catalog_impl, properties):\n            logger.info(\"Loaded Catalog: %s\", catalog_impl)\n            return catalog\n        else:\n            raise ValueError(f\"Could not initialize Catalog: {catalog_impl}\")\n\n    catalog_type = None\n    if provided_catalog_type and isinstance(provided_catalog_type, str):\n        catalog_type = CatalogType(provided_catalog_type.lower())\n    elif not provided_catalog_type:\n        catalog_type = infer_catalog_type(name, conf)\n\n    if catalog_type:\n        return AVAILABLE_CATALOGS[catalog_type](name, cast(dict[str, str], conf))\n\n    raise ValueError(f\"Could not initialize catalog with the following properties: {properties}\")\n</code></pre>"},{"location":"reference/pyiceberg/catalog/bigquery_metastore/","title":"bigquery_metastore","text":""},{"location":"reference/pyiceberg/catalog/bigquery_metastore/#pyiceberg.catalog.bigquery_metastore.BigQueryMetastoreCatalog","title":"<code>BigQueryMetastoreCatalog</code>","text":"<p>               Bases: <code>MetastoreCatalog</code></p> Source code in <code>pyiceberg/catalog/bigquery_metastore.py</code> <pre><code>class BigQueryMetastoreCatalog(MetastoreCatalog):\n    def __init__(self, name: str, **properties: str):\n        super().__init__(name, **properties)\n\n        project_id: str | None = self.properties.get(GCP_PROJECT_ID)\n        location: str | None = self.properties.get(GCP_LOCATION)\n        credentials_file: str | None = self.properties.get(GCP_CREDENTIALS_FILE)\n        credentials_info_str: str | None = self.properties.get(GCP_CREDENTIALS_INFO)\n\n        if not project_id:\n            raise ValueError(f\"Missing property: {GCP_PROJECT_ID}\")\n\n        # BigQuery requires current-snapshot-id to be present for tables to be created.\n        if not Config().get_bool(\"legacy-current-snapshot-id\"):\n            raise ValueError(\"legacy-current-snapshot-id must be enabled to work with BigQuery.\")\n\n        if credentials_file and credentials_info_str:\n            raise ValueError(\"Cannot specify both `gcp.bigquery.credentials-file` and `gcp.bigquery.credentials-info`\")\n\n        gcp_credentials = None\n        if credentials_file:\n            gcp_credentials = service_account.Credentials.from_service_account_file(credentials_file)\n        elif credentials_info_str:\n            try:\n                credentials_info_dict = json.loads(credentials_info_str)\n                gcp_credentials = service_account.Credentials.from_service_account_info(credentials_info_dict)\n            except json.JSONDecodeError as e:\n                raise ValueError(f\"Invalid JSON string for {GCP_CREDENTIALS_INFO}: {e}\") from e\n            except TypeError as e:  # from_service_account_info can raise TypeError for bad structure\n                raise ValueError(f\"Invalid credentials structure for {GCP_CREDENTIALS_INFO}: {e}\") from e\n\n        self.client: Client = Client(\n            project=project_id,\n            credentials=gcp_credentials,\n            location=location,\n        )\n\n        self.location = location\n        self.project_id = project_id\n\n    def create_table(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"\n        Create an Iceberg table.\n\n        Args:\n            identifier: Table identifier.\n            schema: Table's schema.\n            location: Location for the table. Optional Argument.\n            partition_spec: PartitionSpec for the table.\n            sort_order: SortOrder for the table.\n            properties: Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            AlreadyExistsError: If a table with the name already exists.\n            ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n        \"\"\"\n        schema: Schema = self._convert_schema_if_needed(schema)  # type: ignore\n\n        dataset_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        dataset_ref = DatasetReference(project=self.project_id, dataset_id=dataset_name)\n        location = self._resolve_table_location(location, dataset_name, table_name)\n        provider = load_location_provider(table_location=location, table_properties=properties)\n        metadata_location = provider.new_table_metadata_file_location()\n\n        metadata = new_table_metadata(\n            location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n        )\n\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        self._write_metadata(metadata, io, metadata_location)\n\n        dataset_ref = DatasetReference(project=self.project_id, dataset_id=dataset_name)\n\n        try:\n            table = self._make_new_table(\n                metadata, metadata_location, TableReference(dataset_ref=dataset_ref, table_id=table_name)\n            )\n            self.client.create_table(table)\n        except Conflict as e:\n            raise TableAlreadyExistsError(f\"Table {table_name} already exists\") from e\n\n        return self.load_table(identifier=identifier)\n\n    def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace: Namespace identifier.\n            properties: A string dictionary of properties for the given namespace.\n\n        Raises:\n            ValueError: If the identifier is invalid.\n            AlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace)\n\n        try:\n            dataset_ref = DatasetReference(project=self.project_id, dataset_id=database_name)\n            dataset = Dataset(dataset_ref=dataset_ref)\n            dataset.external_catalog_dataset_options = self._create_external_catalog_dataset_options(\n                self._get_default_warehouse_location_for_dataset(database_name), properties, dataset_ref\n            )\n            self.client.create_dataset(dataset)\n        except Conflict as e:\n            raise NamespaceAlreadyExistsError(\"Namespace {database_name} already exists\") from e\n\n    def load_table(self, identifier: str | Identifier) -&gt; Table:\n        \"\"\"\n        Load the table's metadata and returns the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier: Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n        dataset_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        try:\n            table_ref = TableReference(\n                dataset_ref=DatasetReference(project=self.project_id, dataset_id=dataset_name),\n                table_id=table_name,\n            )\n            table = self.client.get_table(table_ref)\n            return self._convert_bigquery_table_to_iceberg_table(identifier, table)\n        except NotFound as e:\n            raise NoSuchTableError(f\"Table does not exist: {dataset_name}.{table_name}\") from e\n\n    def drop_table(self, identifier: str | Identifier) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier: Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        dataset_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        try:\n            table_ref = TableReference(\n                dataset_ref=DatasetReference(project=self.project_id, dataset_id=dataset_name),\n                table_id=table_name,\n            )\n            self.client.delete_table(table_ref)\n        except NoSuchTableError as e:\n            raise NoSuchTableError(f\"Table does not exist: {dataset_name}.{table_name}\") from e\n\n    def commit_table(\n        self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        raise NotImplementedError\n\n    def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n        raise NotImplementedError\n\n    def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n        database_name = self.identifier_to_database(namespace)\n\n        try:\n            dataset_ref = DatasetReference(project=self.project_id, dataset_id=database_name)\n            dataset = Dataset(dataset_ref=dataset_ref)\n            self.client.delete_dataset(dataset)\n        except NotFound as e:\n            raise NoSuchNamespaceError(f\"Namespace {namespace} does not exist.\") from e\n\n    def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        database_name = self.identifier_to_database(namespace)\n        iceberg_tables: list[Identifier] = []\n        try:\n            dataset_ref = DatasetReference(project=self.project_id, dataset_id=database_name)\n            # The list_tables method returns an iterator of TableListItem\n            bq_tables_iterator = self.client.list_tables(dataset=dataset_ref)\n\n            for bq_table_list_item in bq_tables_iterator:\n                iceberg_tables.append((database_name, bq_table_list_item.table_id))\n        except NotFound:\n            raise NoSuchNamespaceError(f\"Namespace (dataset) '{database_name}' not found.\") from None\n        return iceberg_tables\n\n    def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n        # Since this catalog only supports one-level namespaces, it always returns an empty list unless\n        # passed an empty namespace to list all namespaces within the catalog.\n        if namespace:\n            raise NoSuchNamespaceError(f\"Namespace (dataset) '{namespace}' not found.\") from None\n\n        # List top-level datasets\n        datasets_iterator = self.client.list_datasets()\n        return [(dataset.dataset_id,) for dataset in datasets_iterator]\n\n    def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier (Union[str, Identifier]): Table identifier for the table\n            metadata_location (str): The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        dataset_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        dataset_ref = DatasetReference(project=self.project_id, dataset_id=dataset_name)\n\n        io = self._load_file_io(location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n\n        try:\n            table = self._make_new_table(\n                metadata, metadata_location, TableReference(dataset_ref=dataset_ref, table_id=table_name)\n            )\n            self.client.create_table(table)\n        except Conflict as e:\n            raise TableAlreadyExistsError(f\"Table {table_name} already exists\") from e\n\n        return self.load_table(identifier=identifier)\n\n    def list_views(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        raise NotImplementedError\n\n    def drop_view(self, identifier: str | Identifier) -&gt; None:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: str | Identifier) -&gt; bool:\n        raise NotImplementedError\n\n    def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n        dataset_name = self.identifier_to_database(namespace)\n\n        try:\n            dataset = self.client.get_dataset(DatasetReference(project=self.project_id, dataset_id=dataset_name))\n\n            if dataset and dataset.external_catalog_dataset_options:\n                return dataset.external_catalog_dataset_options.to_api_repr()\n        except NotFound as e:\n            raise NoSuchNamespaceError(f\"Namespace {namespace} not found\") from e\n        return {}\n\n    def update_namespace_properties(\n        self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        raise NotImplementedError\n\n    def _make_new_table(self, metadata: TableMetadata, metadata_file_location: str, table_ref: TableReference) -&gt; BQTable:\n        \"\"\"To make the table queryable from Hive, the user would likely be setting the HIVE_ENGINE_ENABLED parameter.\"\"\"\n        table = BQTable(table_ref)\n\n        # In Python, you typically set the external data configuration directly.\n        # BigQueryMetastoreUtils.create_external_catalog_table_options is mapped to\n        # constructing the external_data_configuration for the Table object.\n        external_config_options = self._create_external_catalog_table_options(\n            metadata.location,\n            self._create_table_parameters(metadata_file_location=metadata_file_location, table_metadata=metadata),\n        )\n\n        # Apply the external configuration to the Table object.\n        # This will depend on the exact structure returned by create_external_catalog_table_options.\n        # A common way to set up an external table in BigQuery Python client is:\n        table.external_catalog_table_options = external_config_options\n\n        return table\n\n    def _create_external_catalog_table_options(self, location: str, parameters: dict[str, Any]) -&gt; ExternalCatalogTableOptions:\n        # This structure directly maps to what BigQuery's ExternalConfig expects for Hive.\n        return ExternalCatalogTableOptions(\n            storage_descriptor=StorageDescriptor(\n                location_uri=location,\n                input_format=HIVE_FILE_INPUT_FORMAT,\n                output_format=HIVE_FILE_OUTPUT_FORMAT,\n                serde_info=SerDeInfo(serialization_library=HIVE_SERIALIZATION_LIBRARY),\n            ),\n            parameters=parameters,\n        )\n\n    def _create_external_catalog_dataset_options(\n        self, default_storage_location: str, metadataParameters: dict[str, Any], dataset_ref: DatasetReference\n    ) -&gt; ExternalCatalogDatasetOptions:\n        return ExternalCatalogDatasetOptions(\n            default_storage_location_uri=self._get_default_warehouse_location_for_dataset(dataset_ref.dataset_id),\n            parameters=metadataParameters,\n        )\n\n    def _convert_bigquery_table_to_iceberg_table(self, identifier: str | Identifier, table: BQTable) -&gt; Table:\n        dataset_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n        metadata_location = \"\"\n        if table.external_catalog_table_options and table.external_catalog_table_options.parameters:\n            metadata_location = table.external_catalog_table_options.parameters[METADATA_LOCATION_PROP]\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n\n        return Table(\n            identifier=(dataset_name, table_name),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=self._load_file_io(metadata.properties, metadata_location),\n            catalog=self,\n        )\n\n    def _create_table_parameters(self, metadata_file_location: str, table_metadata: TableMetadata) -&gt; dict[str, Any]:\n        parameters: dict[str, Any] = table_metadata.properties\n        if table_metadata.table_uuid:\n            parameters[\"uuid\"] = str(table_metadata.table_uuid)\n        parameters[METADATA_LOCATION_PROP] = metadata_file_location\n        parameters[TABLE_TYPE_PROP] = ICEBERG_TABLE_TYPE_VALUE\n        parameters[\"EXTERNAL\"] = True\n\n        # Add Hive-style basic statistics from snapshot metadata if it exists.\n        snapshot = table_metadata.current_snapshot()\n        if snapshot:\n            summary = snapshot.summary\n            if summary:\n                if summary.get(TOTAL_DATA_FILES):\n                    parameters[\"numFiles\"] = summary.get(TOTAL_DATA_FILES)\n\n                if summary.get(TOTAL_RECORDS):\n                    parameters[\"numRows\"] = summary.get(TOTAL_RECORDS)\n\n                if summary.get(TOTAL_FILE_SIZE):\n                    parameters[\"totalSize\"] = summary.get(TOTAL_FILE_SIZE)\n\n        return parameters\n\n    def _default_storage_location(self, location: str | None, dataset_ref: DatasetReference) -&gt; str | None:\n        if location:\n            return location\n        dataset = self.client.get_dataset(dataset_ref)\n        if dataset and dataset.external_catalog_dataset_options:\n            return dataset.external_catalog_dataset_options.default_storage_location_uri\n\n        raise ValueError(\"Could not find default storage location\")\n\n    def _get_default_warehouse_location_for_dataset(self, database_name: str) -&gt; str:\n        if warehouse_path := self.properties.get(WAREHOUSE_LOCATION):\n            warehouse_path = warehouse_path.rstrip(\"/\")\n            return f\"{warehouse_path}/{database_name}.db\"\n\n        raise ValueError(\"No default path is set, please specify a location when creating a table\")\n</code></pre>"},{"location":"reference/pyiceberg/catalog/bigquery_metastore/#pyiceberg.catalog.bigquery_metastore.BigQueryMetastoreCatalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the identifier is invalid.</p> <code>AlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/bigquery_metastore.py</code> <pre><code>def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace: Namespace identifier.\n        properties: A string dictionary of properties for the given namespace.\n\n    Raises:\n        ValueError: If the identifier is invalid.\n        AlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace)\n\n    try:\n        dataset_ref = DatasetReference(project=self.project_id, dataset_id=database_name)\n        dataset = Dataset(dataset_ref=dataset_ref)\n        dataset.external_catalog_dataset_options = self._create_external_catalog_dataset_options(\n            self._get_default_warehouse_location_for_dataset(database_name), properties, dataset_ref\n        )\n        self.client.create_dataset(dataset)\n    except Conflict as e:\n        raise NamespaceAlreadyExistsError(\"Namespace {database_name} already exists\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/bigquery_metastore/#pyiceberg.catalog.bigquery_metastore.BigQueryMetastoreCatalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create an Iceberg table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Union[Schema, Schema]</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>AlreadyExistsError</code> <p>If a table with the name already exists.</p> <code>ValueError</code> <p>If the identifier is invalid, or no path is given to store metadata.</p> Source code in <code>pyiceberg/catalog/bigquery_metastore.py</code> <pre><code>def create_table(\n    self,\n    identifier: str | Identifier,\n    schema: Union[Schema, \"pa.Schema\"],\n    location: str | None = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"\n    Create an Iceberg table.\n\n    Args:\n        identifier: Table identifier.\n        schema: Table's schema.\n        location: Location for the table. Optional Argument.\n        partition_spec: PartitionSpec for the table.\n        sort_order: SortOrder for the table.\n        properties: Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        AlreadyExistsError: If a table with the name already exists.\n        ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n    \"\"\"\n    schema: Schema = self._convert_schema_if_needed(schema)  # type: ignore\n\n    dataset_name, table_name = self.identifier_to_database_and_table(identifier)\n\n    dataset_ref = DatasetReference(project=self.project_id, dataset_id=dataset_name)\n    location = self._resolve_table_location(location, dataset_name, table_name)\n    provider = load_location_provider(table_location=location, table_properties=properties)\n    metadata_location = provider.new_table_metadata_file_location()\n\n    metadata = new_table_metadata(\n        location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n    )\n\n    io = load_file_io(properties=self.properties, location=metadata_location)\n    self._write_metadata(metadata, io, metadata_location)\n\n    dataset_ref = DatasetReference(project=self.project_id, dataset_id=dataset_name)\n\n    try:\n        table = self._make_new_table(\n            metadata, metadata_location, TableReference(dataset_ref=dataset_ref, table_id=table_name)\n        )\n        self.client.create_table(table)\n    except Conflict as e:\n        raise TableAlreadyExistsError(f\"Table {table_name} already exists\") from e\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/bigquery_metastore/#pyiceberg.catalog.bigquery_metastore.BigQueryMetastoreCatalog.drop_table","title":"<code>drop_table(identifier)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/bigquery_metastore.py</code> <pre><code>def drop_table(self, identifier: str | Identifier) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier: Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    dataset_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n    try:\n        table_ref = TableReference(\n            dataset_ref=DatasetReference(project=self.project_id, dataset_id=dataset_name),\n            table_id=table_name,\n        )\n        self.client.delete_table(table_ref)\n    except NoSuchTableError as e:\n        raise NoSuchTableError(f\"Table does not exist: {dataset_name}.{table_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/bigquery_metastore/#pyiceberg.catalog.bigquery_metastore.BigQueryMetastoreCatalog.load_table","title":"<code>load_table(identifier)</code>","text":"<p>Load the table's metadata and returns the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/bigquery_metastore.py</code> <pre><code>def load_table(self, identifier: str | Identifier) -&gt; Table:\n    \"\"\"\n    Load the table's metadata and returns the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier: Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n    dataset_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n    try:\n        table_ref = TableReference(\n            dataset_ref=DatasetReference(project=self.project_id, dataset_id=dataset_name),\n            table_id=table_name,\n        )\n        table = self.client.get_table(table_ref)\n        return self._convert_bigquery_table_to_iceberg_table(identifier, table)\n    except NotFound as e:\n        raise NoSuchTableError(f\"Table does not exist: {dataset_name}.{table_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/bigquery_metastore/#pyiceberg.catalog.bigquery_metastore.BigQueryMetastoreCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/bigquery_metastore.py</code> <pre><code>def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier (Union[str, Identifier]): Table identifier for the table\n        metadata_location (str): The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    dataset_name, table_name = self.identifier_to_database_and_table(identifier)\n\n    dataset_ref = DatasetReference(project=self.project_id, dataset_id=dataset_name)\n\n    io = self._load_file_io(location=metadata_location)\n    file = io.new_input(metadata_location)\n    metadata = FromInputFile.table_metadata(file)\n\n    try:\n        table = self._make_new_table(\n            metadata, metadata_location, TableReference(dataset_ref=dataset_ref, table_id=table_name)\n        )\n        self.client.create_table(table)\n    except Conflict as e:\n        raise TableAlreadyExistsError(f\"Table {table_name} already exists\") from e\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/","title":"dynamodb","text":""},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog","title":"<code>DynamoDbCatalog</code>","text":"<p>               Bases: <code>MetastoreCatalog</code></p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>class DynamoDbCatalog(MetastoreCatalog):\n    def __init__(self, name: str, client: Optional[\"DynamoDBClient\"] = None, **properties: str):\n        \"\"\"Dynamodb catalog.\n\n        Args:\n            name: Name to identify the catalog.\n            client: An optional boto3 dynamodb client.\n            properties: Properties for dynamodb client construction and configuration.\n        \"\"\"\n        super().__init__(name, **properties)\n        if client is not None:\n            self.dynamodb = client\n        else:\n            session = boto3.Session(\n                profile_name=properties.get(DYNAMODB_PROFILE_NAME),\n                region_name=get_first_property_value(properties, DYNAMODB_REGION, AWS_REGION),\n                botocore_session=properties.get(BOTOCORE_SESSION),\n                aws_access_key_id=get_first_property_value(properties, DYNAMODB_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n                aws_secret_access_key=get_first_property_value(properties, DYNAMODB_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n                aws_session_token=get_first_property_value(properties, DYNAMODB_SESSION_TOKEN, AWS_SESSION_TOKEN),\n            )\n            self.dynamodb = session.client(DYNAMODB_CLIENT)\n\n        self.dynamodb_table_name = self.properties.get(DYNAMODB_TABLE_NAME, DYNAMODB_TABLE_NAME_DEFAULT)\n        self._ensure_catalog_table_exists_or_create()\n\n    def _ensure_catalog_table_exists_or_create(self) -&gt; None:\n        if self._dynamodb_table_exists():\n            return None\n\n        try:\n            self.dynamodb.create_table(\n                TableName=self.dynamodb_table_name,\n                AttributeDefinitions=CREATE_CATALOG_ATTRIBUTE_DEFINITIONS,\n                KeySchema=CREATE_CATALOG_KEY_SCHEMA,\n                GlobalSecondaryIndexes=CREATE_CATALOG_GLOBAL_SECONDARY_INDEXES,\n                BillingMode=DYNAMODB_PAY_PER_REQUEST,\n            )\n        except (\n            self.dynamodb.exceptions.ResourceInUseException,\n            self.dynamodb.exceptions.LimitExceededException,\n            self.dynamodb.exceptions.InternalServerError,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n    def _dynamodb_table_exists(self) -&gt; bool:\n        try:\n            response = self.dynamodb.describe_table(TableName=self.dynamodb_table_name)\n        except self.dynamodb.exceptions.ResourceNotFoundException:\n            return False\n        except self.dynamodb.exceptions.InternalServerError as e:\n            raise GenericDynamoDbError(e.message) from e\n\n        if response[\"Table\"][\"TableStatus\"] != ACTIVE:\n            raise GenericDynamoDbError(f\"DynamoDB table for catalog {self.dynamodb_table_name} is not {ACTIVE}\")\n        else:\n            return True\n\n    def create_table(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"\n        Create an Iceberg table.\n\n        Args:\n            identifier: Table identifier.\n            schema: Table's schema.\n            location: Location for the table. Optional Argument.\n            partition_spec: PartitionSpec for the table.\n            sort_order: SortOrder for the table.\n            properties: Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            AlreadyExistsError: If a table with the name already exists.\n            ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n        \"\"\"\n        schema: Schema = self._convert_schema_if_needed(  # type: ignore\n            schema,\n            int(properties.get(TableProperties.FORMAT_VERSION, TableProperties.DEFAULT_FORMAT_VERSION)),  # type: ignore\n        )\n\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        location = self._resolve_table_location(location, database_name, table_name)\n        provider = load_location_provider(table_location=location, table_properties=properties)\n        metadata_location = provider.new_table_metadata_file_location()\n\n        metadata = new_table_metadata(\n            location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n        )\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        self._write_metadata(metadata, io, metadata_location)\n\n        self._ensure_namespace_exists(database_name=database_name)\n\n        try:\n            self._put_dynamo_item(\n                item=_get_create_table_item(\n                    database_name=database_name, table_name=table_name, properties=properties, metadata_location=metadata_location\n                ),\n                condition_expression=f\"attribute_not_exists({DYNAMODB_COL_IDENTIFIER})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise TableAlreadyExistsError(f\"Table {database_name}.{table_name} already exists\") from e\n\n        return self.load_table(identifier=identifier)\n\n    def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier (Union[str, Identifier]): Table identifier for the table\n            metadata_location (str): The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        raise NotImplementedError\n\n    def commit_table(\n        self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        \"\"\"\n        raise NotImplementedError\n\n    def load_table(self, identifier: str | Identifier) -&gt; Table:\n        \"\"\"\n        Load the table's metadata and returns the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier: Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n        dynamo_table_item = self._get_iceberg_table_item(database_name=database_name, table_name=table_name)\n        return self._convert_dynamo_table_item_to_iceberg_table(dynamo_table_item=dynamo_table_item)\n\n    def drop_table(self, identifier: str | Identifier) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier: Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        try:\n            self._delete_dynamo_item(\n                namespace=database_name,\n                identifier=f\"{database_name}.{table_name}\",\n                condition_expression=f\"attribute_exists({DYNAMODB_COL_IDENTIFIER})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n\n    def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        This method can only rename Iceberg tables in AWS Glue.\n\n        Args:\n            from_identifier: Existing table identifier.\n            to_identifier: New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            ValueError: When from table identifier is invalid.\n            NoSuchTableError: When a table with the name does not exist.\n            NoSuchIcebergTableError: When from table is not a valid iceberg table.\n            NoSuchPropertyException: When from table miss some required properties.\n            NoSuchNamespaceError: When the destination namespace doesn't exist.\n        \"\"\"\n        from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n        to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n\n        from_table_item = self._get_iceberg_table_item(database_name=from_database_name, table_name=from_table_name)\n\n        try:\n            # Verify that from_identifier is a valid iceberg table\n            self._convert_dynamo_table_item_to_iceberg_table(dynamo_table_item=from_table_item)\n        except NoSuchPropertyException as e:\n            raise NoSuchPropertyException(\n                f\"Failed to rename table {from_database_name}.{from_table_name} since it is missing required properties\"\n            ) from e\n        except NoSuchIcebergTableError as e:\n            raise NoSuchIcebergTableError(\n                f\"Failed to rename table {from_database_name}.{from_table_name} since it is not a valid iceberg table\"\n            ) from e\n\n        self._ensure_namespace_exists(database_name=from_database_name)\n        self._ensure_namespace_exists(database_name=to_database_name)\n\n        try:\n            self._put_dynamo_item(\n                item=_get_rename_table_item(\n                    from_dynamo_table_item=from_table_item, to_database_name=to_database_name, to_table_name=to_table_name\n                ),\n                condition_expression=f\"attribute_not_exists({DYNAMODB_COL_IDENTIFIER})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise TableAlreadyExistsError(f\"Table {to_database_name}.{to_table_name} already exists\") from e\n\n        try:\n            self.drop_table(from_identifier)\n        except (NoSuchTableError, GenericDynamoDbError) as e:\n            log_message = f\"Failed to drop old table {from_database_name}.{from_table_name}. \"\n\n            try:\n                self.drop_table(to_identifier)\n                log_message += f\"Rolled back table creation for {to_database_name}.{to_table_name}.\"\n            except (NoSuchTableError, GenericDynamoDbError):\n                log_message += (\n                    f\"Failed to roll back table creation for {to_database_name}.{to_table_name}. Please clean up manually\"\n                )\n\n            raise ValueError(log_message) from e\n\n        return self.load_table(to_identifier)\n\n    def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace: Namespace identifier.\n            properties: A string dictionary of properties for the given namespace.\n\n        Raises:\n            ValueError: If the identifier is invalid.\n            AlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace)\n\n        try:\n            self._put_dynamo_item(\n                item=_get_create_database_item(database_name=database_name, properties=properties),\n                condition_expression=f\"attribute_not_exists({DYNAMODB_COL_NAMESPACE})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n\n    def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        A Glue namespace can only be dropped if it is empty.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        table_identifiers = self.list_tables(namespace=database_name)\n\n        if len(table_identifiers) &gt; 0:\n            raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\")\n\n        try:\n            self._delete_dynamo_item(\n                namespace=database_name,\n                identifier=DYNAMODB_NAMESPACE,\n                condition_expression=f\"attribute_exists({DYNAMODB_COL_IDENTIFIER})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n\n    def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n\n        paginator = self.dynamodb.get_paginator(\"query\")\n\n        try:\n            page_iterator = paginator.paginate(\n                TableName=self.dynamodb_table_name,\n                IndexName=DYNAMODB_NAMESPACE_GSI,\n                KeyConditionExpression=f\"{DYNAMODB_COL_NAMESPACE} = :namespace \",\n                ExpressionAttributeValues={\n                    \":namespace\": {\n                        \"S\": database_name,\n                    }\n                },\n            )\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n            self.dynamodb.exceptions.ResourceNotFoundException,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n        table_identifiers = []\n        for page in page_iterator:\n            for item in page[\"Items\"]:\n                _dict = _convert_dynamo_item_to_regular_dict(item)\n                identifier_col = _dict[DYNAMODB_COL_IDENTIFIER]\n                if identifier_col == DYNAMODB_NAMESPACE:\n                    continue\n\n                table_identifiers.append(self.identifier_to_tuple(identifier_col))\n\n        return table_identifiers\n\n    def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n        \"\"\"List top-level namespaces from the catalog.\n\n        We do not support hierarchical namespace.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n        \"\"\"\n        # Hierarchical namespace is not supported. Return an empty list\n        if namespace:\n            return []\n\n        paginator = self.dynamodb.get_paginator(\"query\")\n\n        try:\n            page_iterator = paginator.paginate(\n                TableName=self.dynamodb_table_name,\n                ConsistentRead=True,\n                KeyConditionExpression=f\"{DYNAMODB_COL_IDENTIFIER} = :identifier\",\n                ExpressionAttributeValues={\n                    \":identifier\": {\n                        \"S\": DYNAMODB_NAMESPACE,\n                    }\n                },\n            )\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n            self.dynamodb.exceptions.ResourceNotFoundException,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n        database_identifiers = []\n        for page in page_iterator:\n            for item in page[\"Items\"]:\n                _dict = _convert_dynamo_item_to_regular_dict(item)\n                namespace_col = _dict[DYNAMODB_COL_NAMESPACE]\n                database_identifiers.append(self.identifier_to_tuple(namespace_col))\n\n        return database_identifiers\n\n    def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n        \"\"\"\n        Get properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        namespace_item = self._get_iceberg_namespace_item(database_name=database_name)\n        namespace_dict = _convert_dynamo_item_to_regular_dict(namespace_item)\n        return _get_namespace_properties(namespace_dict=namespace_dict)\n\n    def update_namespace_properties(\n        self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"\n        Remove or update provided property keys for a namespace.\n\n        Args:\n            namespace: Namespace identifier\n            removals: Set of property keys that need to be removed. Optional Argument.\n            updates: Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist\uff0c or identifier is invalid.\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        namespace_item = self._get_iceberg_namespace_item(database_name=database_name)\n        namespace_dict = _convert_dynamo_item_to_regular_dict(namespace_item)\n        current_properties = _get_namespace_properties(namespace_dict=namespace_dict)\n\n        properties_update_summary, updated_properties = self._get_updated_props_and_update_summary(\n            current_properties=current_properties, removals=removals, updates=updates\n        )\n\n        try:\n            self._put_dynamo_item(\n                item=_get_update_database_item(\n                    namespace_item=namespace_item,\n                    updated_properties=updated_properties,\n                ),\n                condition_expression=f\"attribute_exists({DYNAMODB_COL_NAMESPACE})\",\n            )\n        except ConditionalCheckFailedException as e:\n            raise NoSuchNamespaceError(f\"Database {database_name} does not exist\") from e\n\n        return properties_update_summary\n\n    def list_views(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        raise NotImplementedError\n\n    def drop_view(self, identifier: str | Identifier) -&gt; None:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: str | Identifier) -&gt; bool:\n        raise NotImplementedError\n\n    def _get_iceberg_table_item(self, database_name: str, table_name: str) -&gt; dict[str, Any]:\n        try:\n            return self._get_dynamo_item(identifier=f\"{database_name}.{table_name}\", namespace=database_name)\n        except ValueError as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n\n    def _get_iceberg_namespace_item(self, database_name: str) -&gt; dict[str, Any]:\n        try:\n            return self._get_dynamo_item(identifier=DYNAMODB_NAMESPACE, namespace=database_name)\n        except ValueError as e:\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {database_name}\") from e\n\n    def _ensure_namespace_exists(self, database_name: str) -&gt; dict[str, Any]:\n        return self._get_iceberg_namespace_item(database_name)\n\n    def _get_dynamo_item(self, identifier: str, namespace: str) -&gt; dict[str, Any]:\n        try:\n            response = self.dynamodb.get_item(\n                TableName=self.dynamodb_table_name,\n                ConsistentRead=True,\n                Key={\n                    DYNAMODB_COL_IDENTIFIER: {\n                        \"S\": identifier,\n                    },\n                    DYNAMODB_COL_NAMESPACE: {\n                        \"S\": namespace,\n                    },\n                },\n            )\n            if ITEM in response:\n                return response[ITEM]\n            else:\n                raise ValueError(f\"Item not found. identifier: {identifier} - namespace: {namespace}\")\n        except self.dynamodb.exceptions.ResourceNotFoundException as e:\n            raise ValueError(f\"Item not found. identifier: {identifier} - namespace: {namespace}\") from e\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n    def _put_dynamo_item(self, item: dict[str, Any], condition_expression: str) -&gt; None:\n        try:\n            self.dynamodb.put_item(TableName=self.dynamodb_table_name, Item=item, ConditionExpression=condition_expression)\n        except self.dynamodb.exceptions.ConditionalCheckFailedException as e:\n            raise ConditionalCheckFailedException(f\"Condition expression check failed: {condition_expression} - {item}\") from e\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n            self.dynamodb.exceptions.ResourceNotFoundException,\n            self.dynamodb.exceptions.ItemCollectionSizeLimitExceededException,\n            self.dynamodb.exceptions.TransactionConflictException,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n    def _delete_dynamo_item(self, namespace: str, identifier: str, condition_expression: str) -&gt; None:\n        try:\n            self.dynamodb.delete_item(\n                TableName=self.dynamodb_table_name,\n                Key={\n                    DYNAMODB_COL_IDENTIFIER: {\n                        \"S\": identifier,\n                    },\n                    DYNAMODB_COL_NAMESPACE: {\n                        \"S\": namespace,\n                    },\n                },\n                ConditionExpression=condition_expression,\n            )\n        except self.dynamodb.exceptions.ConditionalCheckFailedException as e:\n            raise ConditionalCheckFailedException(\n                f\"Condition expression check failed: {condition_expression} - {identifier}\"\n            ) from e\n        except (\n            self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n            self.dynamodb.exceptions.RequestLimitExceeded,\n            self.dynamodb.exceptions.InternalServerError,\n            self.dynamodb.exceptions.ResourceNotFoundException,\n            self.dynamodb.exceptions.ItemCollectionSizeLimitExceededException,\n            self.dynamodb.exceptions.TransactionConflictException,\n        ) as e:\n            raise GenericDynamoDbError(e.message) from e\n\n    def _convert_dynamo_table_item_to_iceberg_table(self, dynamo_table_item: dict[str, Any]) -&gt; Table:\n        table_dict = _convert_dynamo_item_to_regular_dict(dynamo_table_item)\n\n        for prop in [_add_property_prefix(prop) for prop in (TABLE_TYPE, METADATA_LOCATION)] + [\n            DYNAMODB_COL_IDENTIFIER,\n            DYNAMODB_COL_NAMESPACE,\n            DYNAMODB_COL_CREATED_AT,\n        ]:\n            if prop not in table_dict.keys():\n                raise NoSuchPropertyException(f\"Iceberg required property {prop} is missing: {dynamo_table_item}\")\n\n        table_type = table_dict[_add_property_prefix(TABLE_TYPE)]\n        identifier = table_dict[DYNAMODB_COL_IDENTIFIER]\n        metadata_location = table_dict[_add_property_prefix(METADATA_LOCATION)]\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        if table_type.lower() != ICEBERG:\n            raise NoSuchIcebergTableError(\n                f\"Property table_type is {table_type}, expected {ICEBERG}: {database_name}.{table_name}\"\n            )\n\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        return Table(\n            identifier=(database_name, table_name),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=self._load_file_io(metadata.properties, metadata_location),\n            catalog=self,\n        )\n\n    def _get_default_warehouse_location(self, database_name: str, table_name: str) -&gt; str:\n        \"\"\"Override the default warehouse location to follow Hive-style conventions.\"\"\"\n        return self._get_hive_style_warehouse_location(database_name, table_name)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.__init__","title":"<code>__init__(name, client=None, **properties)</code>","text":"<p>Dynamodb catalog.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to identify the catalog.</p> required <code>client</code> <code>Optional[DynamoDBClient]</code> <p>An optional boto3 dynamodb client.</p> <code>None</code> <code>properties</code> <code>str</code> <p>Properties for dynamodb client construction and configuration.</p> <code>{}</code> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def __init__(self, name: str, client: Optional[\"DynamoDBClient\"] = None, **properties: str):\n    \"\"\"Dynamodb catalog.\n\n    Args:\n        name: Name to identify the catalog.\n        client: An optional boto3 dynamodb client.\n        properties: Properties for dynamodb client construction and configuration.\n    \"\"\"\n    super().__init__(name, **properties)\n    if client is not None:\n        self.dynamodb = client\n    else:\n        session = boto3.Session(\n            profile_name=properties.get(DYNAMODB_PROFILE_NAME),\n            region_name=get_first_property_value(properties, DYNAMODB_REGION, AWS_REGION),\n            botocore_session=properties.get(BOTOCORE_SESSION),\n            aws_access_key_id=get_first_property_value(properties, DYNAMODB_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n            aws_secret_access_key=get_first_property_value(properties, DYNAMODB_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n            aws_session_token=get_first_property_value(properties, DYNAMODB_SESSION_TOKEN, AWS_SESSION_TOKEN),\n        )\n        self.dynamodb = session.client(DYNAMODB_CLIENT)\n\n    self.dynamodb_table_name = self.properties.get(DYNAMODB_TABLE_NAME, DYNAMODB_TABLE_NAME_DEFAULT)\n    self._ensure_catalog_table_exists_or_create()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def commit_table(\n    self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the identifier is invalid.</p> <code>AlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace: Namespace identifier.\n        properties: A string dictionary of properties for the given namespace.\n\n    Raises:\n        ValueError: If the identifier is invalid.\n        AlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace)\n\n    try:\n        self._put_dynamo_item(\n            item=_get_create_database_item(database_name=database_name, properties=properties),\n            condition_expression=f\"attribute_not_exists({DYNAMODB_COL_NAMESPACE})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create an Iceberg table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Union[Schema, Schema]</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>AlreadyExistsError</code> <p>If a table with the name already exists.</p> <code>ValueError</code> <p>If the identifier is invalid, or no path is given to store metadata.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def create_table(\n    self,\n    identifier: str | Identifier,\n    schema: Union[Schema, \"pa.Schema\"],\n    location: str | None = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"\n    Create an Iceberg table.\n\n    Args:\n        identifier: Table identifier.\n        schema: Table's schema.\n        location: Location for the table. Optional Argument.\n        partition_spec: PartitionSpec for the table.\n        sort_order: SortOrder for the table.\n        properties: Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        AlreadyExistsError: If a table with the name already exists.\n        ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n    \"\"\"\n    schema: Schema = self._convert_schema_if_needed(  # type: ignore\n        schema,\n        int(properties.get(TableProperties.FORMAT_VERSION, TableProperties.DEFAULT_FORMAT_VERSION)),  # type: ignore\n    )\n\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n    location = self._resolve_table_location(location, database_name, table_name)\n    provider = load_location_provider(table_location=location, table_properties=properties)\n    metadata_location = provider.new_table_metadata_file_location()\n\n    metadata = new_table_metadata(\n        location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n    )\n    io = load_file_io(properties=self.properties, location=metadata_location)\n    self._write_metadata(metadata, io, metadata_location)\n\n    self._ensure_namespace_exists(database_name=database_name)\n\n    try:\n        self._put_dynamo_item(\n            item=_get_create_table_item(\n                database_name=database_name, table_name=table_name, properties=properties, metadata_location=metadata_location\n            ),\n            condition_expression=f\"attribute_not_exists({DYNAMODB_COL_IDENTIFIER})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise TableAlreadyExistsError(f\"Table {database_name}.{table_name} already exists\") from e\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>","text":"<p>Drop a namespace.</p> <p>A Glue namespace can only be dropped if it is empty.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    A Glue namespace can only be dropped if it is empty.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    table_identifiers = self.list_tables(namespace=database_name)\n\n    if len(table_identifiers) &gt; 0:\n        raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\")\n\n    try:\n        self._delete_dynamo_item(\n            namespace=database_name,\n            identifier=DYNAMODB_NAMESPACE,\n            condition_expression=f\"attribute_exists({DYNAMODB_COL_IDENTIFIER})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.drop_table","title":"<code>drop_table(identifier)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def drop_table(self, identifier: str | Identifier) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier: Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n    try:\n        self._delete_dynamo_item(\n            namespace=database_name,\n            identifier=f\"{database_name}.{table_name}\",\n            condition_expression=f\"attribute_exists({DYNAMODB_COL_IDENTIFIER})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>","text":"<p>List top-level namespaces from the catalog.</p> <p>We do not support hierarchical namespace.</p> <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n    \"\"\"List top-level namespaces from the catalog.\n\n    We do not support hierarchical namespace.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n    \"\"\"\n    # Hierarchical namespace is not supported. Return an empty list\n    if namespace:\n        return []\n\n    paginator = self.dynamodb.get_paginator(\"query\")\n\n    try:\n        page_iterator = paginator.paginate(\n            TableName=self.dynamodb_table_name,\n            ConsistentRead=True,\n            KeyConditionExpression=f\"{DYNAMODB_COL_IDENTIFIER} = :identifier\",\n            ExpressionAttributeValues={\n                \":identifier\": {\n                    \"S\": DYNAMODB_NAMESPACE,\n                }\n            },\n        )\n    except (\n        self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n        self.dynamodb.exceptions.RequestLimitExceeded,\n        self.dynamodb.exceptions.InternalServerError,\n        self.dynamodb.exceptions.ResourceNotFoundException,\n    ) as e:\n        raise GenericDynamoDbError(e.message) from e\n\n    database_identifiers = []\n    for page in page_iterator:\n        for item in page[\"Items\"]:\n            _dict = _convert_dynamo_item_to_regular_dict(item)\n            namespace_col = _dict[DYNAMODB_COL_NAMESPACE]\n            database_identifiers.append(self.identifier_to_tuple(namespace_col))\n\n    return database_identifiers\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.list_tables","title":"<code>list_tables(namespace)</code>","text":"<p>List Iceberg tables under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n    \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n\n    paginator = self.dynamodb.get_paginator(\"query\")\n\n    try:\n        page_iterator = paginator.paginate(\n            TableName=self.dynamodb_table_name,\n            IndexName=DYNAMODB_NAMESPACE_GSI,\n            KeyConditionExpression=f\"{DYNAMODB_COL_NAMESPACE} = :namespace \",\n            ExpressionAttributeValues={\n                \":namespace\": {\n                    \"S\": database_name,\n                }\n            },\n        )\n    except (\n        self.dynamodb.exceptions.ProvisionedThroughputExceededException,\n        self.dynamodb.exceptions.RequestLimitExceeded,\n        self.dynamodb.exceptions.InternalServerError,\n        self.dynamodb.exceptions.ResourceNotFoundException,\n    ) as e:\n        raise GenericDynamoDbError(e.message) from e\n\n    table_identifiers = []\n    for page in page_iterator:\n        for item in page[\"Items\"]:\n            _dict = _convert_dynamo_item_to_regular_dict(item)\n            identifier_col = _dict[DYNAMODB_COL_IDENTIFIER]\n            if identifier_col == DYNAMODB_NAMESPACE:\n                continue\n\n            table_identifiers.append(self.identifier_to_tuple(identifier_col))\n\n    return table_identifiers\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or identifier is invalid.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n    \"\"\"\n    Get properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    namespace_item = self._get_iceberg_namespace_item(database_name=database_name)\n    namespace_dict = _convert_dynamo_item_to_regular_dict(namespace_item)\n    return _get_namespace_properties(namespace_dict=namespace_dict)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.load_table","title":"<code>load_table(identifier)</code>","text":"<p>Load the table's metadata and returns the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def load_table(self, identifier: str | Identifier) -&gt; Table:\n    \"\"\"\n    Load the table's metadata and returns the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier: Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n    dynamo_table_item = self._get_iceberg_table_item(database_name=database_name, table_name=table_name)\n    return self._convert_dynamo_table_item_to_iceberg_table(dynamo_table_item=dynamo_table_item)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier (Union[str, Identifier]): Table identifier for the table\n        metadata_location (str): The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>","text":"<p>Rename a fully classified table name.</p> <p>This method can only rename Iceberg tables in AWS Glue.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str | Identifier</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>str | Identifier</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When from table identifier is invalid.</p> <code>NoSuchTableError</code> <p>When a table with the name does not exist.</p> <code>NoSuchIcebergTableError</code> <p>When from table is not a valid iceberg table.</p> <code>NoSuchPropertyException</code> <p>When from table miss some required properties.</p> <code>NoSuchNamespaceError</code> <p>When the destination namespace doesn't exist.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    This method can only rename Iceberg tables in AWS Glue.\n\n    Args:\n        from_identifier: Existing table identifier.\n        to_identifier: New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        ValueError: When from table identifier is invalid.\n        NoSuchTableError: When a table with the name does not exist.\n        NoSuchIcebergTableError: When from table is not a valid iceberg table.\n        NoSuchPropertyException: When from table miss some required properties.\n        NoSuchNamespaceError: When the destination namespace doesn't exist.\n    \"\"\"\n    from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n    to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n\n    from_table_item = self._get_iceberg_table_item(database_name=from_database_name, table_name=from_table_name)\n\n    try:\n        # Verify that from_identifier is a valid iceberg table\n        self._convert_dynamo_table_item_to_iceberg_table(dynamo_table_item=from_table_item)\n    except NoSuchPropertyException as e:\n        raise NoSuchPropertyException(\n            f\"Failed to rename table {from_database_name}.{from_table_name} since it is missing required properties\"\n        ) from e\n    except NoSuchIcebergTableError as e:\n        raise NoSuchIcebergTableError(\n            f\"Failed to rename table {from_database_name}.{from_table_name} since it is not a valid iceberg table\"\n        ) from e\n\n    self._ensure_namespace_exists(database_name=from_database_name)\n    self._ensure_namespace_exists(database_name=to_database_name)\n\n    try:\n        self._put_dynamo_item(\n            item=_get_rename_table_item(\n                from_dynamo_table_item=from_table_item, to_database_name=to_database_name, to_table_name=to_table_name\n            ),\n            condition_expression=f\"attribute_not_exists({DYNAMODB_COL_IDENTIFIER})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise TableAlreadyExistsError(f\"Table {to_database_name}.{to_table_name} already exists\") from e\n\n    try:\n        self.drop_table(from_identifier)\n    except (NoSuchTableError, GenericDynamoDbError) as e:\n        log_message = f\"Failed to drop old table {from_database_name}.{from_table_name}. \"\n\n        try:\n            self.drop_table(to_identifier)\n            log_message += f\"Rolled back table creation for {to_database_name}.{to_table_name}.\"\n        except (NoSuchTableError, GenericDynamoDbError):\n            log_message += (\n                f\"Failed to roll back table creation for {to_database_name}.{to_table_name}. Please clean up manually\"\n            )\n\n        raise ValueError(log_message) from e\n\n    return self.load_table(to_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/dynamodb/#pyiceberg.catalog.dynamodb.DynamoDbCatalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>","text":"<p>Remove or update provided property keys for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier</p> required <code>removals</code> <code>set[str] | None</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist\uff0c or identifier is invalid.</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/dynamodb.py</code> <pre><code>def update_namespace_properties(\n    self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"\n    Remove or update provided property keys for a namespace.\n\n    Args:\n        namespace: Namespace identifier\n        removals: Set of property keys that need to be removed. Optional Argument.\n        updates: Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist\uff0c or identifier is invalid.\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    namespace_item = self._get_iceberg_namespace_item(database_name=database_name)\n    namespace_dict = _convert_dynamo_item_to_regular_dict(namespace_item)\n    current_properties = _get_namespace_properties(namespace_dict=namespace_dict)\n\n    properties_update_summary, updated_properties = self._get_updated_props_and_update_summary(\n        current_properties=current_properties, removals=removals, updates=updates\n    )\n\n    try:\n        self._put_dynamo_item(\n            item=_get_update_database_item(\n                namespace_item=namespace_item,\n                updated_properties=updated_properties,\n            ),\n            condition_expression=f\"attribute_exists({DYNAMODB_COL_NAMESPACE})\",\n        )\n    except ConditionalCheckFailedException as e:\n        raise NoSuchNamespaceError(f\"Database {database_name} does not exist\") from e\n\n    return properties_update_summary\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/","title":"glue","text":""},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog","title":"<code>GlueCatalog</code>","text":"<p>               Bases: <code>MetastoreCatalog</code></p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>class GlueCatalog(MetastoreCatalog):\n    glue: \"GlueClient\"\n\n    def __init__(self, name: str, client: Optional[\"GlueClient\"] = None, **properties: Any):\n        \"\"\"Glue Catalog.\n\n        You either need to provide a boto3 glue client, or one will be constructed from the properties.\n\n        Args:\n            name: Name to identify the catalog.\n            client: An optional boto3 glue client.\n            properties: Properties for glue client construction and configuration.\n        \"\"\"\n        super().__init__(name, **properties)\n\n        if client is not None:\n            self.glue = client\n        else:\n            retry_mode_prop_value = get_first_property_value(properties, GLUE_RETRY_MODE)\n\n            session = boto3.Session(\n                profile_name=get_first_property_value(properties, GLUE_PROFILE_NAME, AWS_PROFILE_NAME),\n                region_name=get_first_property_value(properties, GLUE_REGION, AWS_REGION),\n                botocore_session=properties.get(BOTOCORE_SESSION),\n                aws_access_key_id=get_first_property_value(properties, GLUE_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n                aws_secret_access_key=get_first_property_value(properties, GLUE_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n                aws_session_token=get_first_property_value(properties, GLUE_SESSION_TOKEN, AWS_SESSION_TOKEN),\n            )\n            self.glue: GlueClient = session.client(\n                \"glue\",\n                endpoint_url=properties.get(GLUE_CATALOG_ENDPOINT),\n                config=Config(\n                    retries={\n                        \"max_attempts\": properties.get(GLUE_MAX_RETRIES, MAX_RETRIES),\n                        \"mode\": retry_mode_prop_value if retry_mode_prop_value in EXISTING_RETRY_MODES else STANDARD_RETRY_MODE,\n                    }\n                ),\n            )\n\n            if glue_catalog_id := properties.get(GLUE_ID):\n                _register_glue_catalog_id_with_glue_client(self.glue, glue_catalog_id)\n\n    def _convert_glue_to_iceberg(self, glue_table: \"TableTypeDef\") -&gt; Table:\n        if (database_name := glue_table.get(\"DatabaseName\")) is None:\n            raise ValueError(\"Glue table is missing DatabaseName property\")\n\n        if (table_name := glue_table.get(\"Name\")) is None:\n            raise ValueError(\"Glue table is missing Name property\")\n\n        if (parameters := glue_table.get(\"Parameters\")) is None:\n            raise ValueError(\"Glue table is missing Parameters property\")\n\n        if (glue_table_type := parameters.get(TABLE_TYPE)) is None:\n            raise NoSuchPropertyException(\n                f\"Property {TABLE_TYPE} missing, could not determine type: {database_name}.{table_name}\"\n            )\n\n        if glue_table_type.lower() != ICEBERG:\n            raise NoSuchIcebergTableError(\n                f\"Property table_type is {glue_table_type}, expected {ICEBERG}: {database_name}.{table_name}\"\n            )\n\n        if (metadata_location := parameters.get(METADATA_LOCATION)) is None:\n            raise NoSuchPropertyException(\n                f\"Table property {METADATA_LOCATION} is missing, cannot find metadata for: {database_name}.{table_name}\"\n            )\n\n        io = self._load_file_io(location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        return Table(\n            identifier=(database_name, table_name),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=self._load_file_io(metadata.properties, metadata_location),\n            catalog=self,\n        )\n\n    def _create_glue_table(self, database_name: str, table_name: str, table_input: \"TableInputTypeDef\") -&gt; None:\n        try:\n            self.glue.create_table(DatabaseName=database_name, TableInput=table_input)\n        except self.glue.exceptions.AlreadyExistsException as e:\n            raise TableAlreadyExistsError(f\"Table {database_name}.{table_name} already exists\") from e\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchNamespaceError(f\"Database {database_name} does not exist\") from e\n\n    def _update_glue_table(self, database_name: str, table_name: str, table_input: \"TableInputTypeDef\", version_id: str) -&gt; None:\n        try:\n            self.glue.update_table(\n                DatabaseName=database_name,\n                TableInput=table_input,\n                SkipArchive=property_as_bool(self.properties, GLUE_SKIP_ARCHIVE, GLUE_SKIP_ARCHIVE_DEFAULT),\n                VersionId=version_id,\n            )\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name} (Glue table version {version_id})\") from e\n        except self.glue.exceptions.ConcurrentModificationException as e:\n            raise CommitFailedException(\n                f\"Cannot commit {database_name}.{table_name} because Glue detected concurrent update \"\n                f\"to table version {version_id}\"\n            ) from e\n\n    def _get_glue_table(self, database_name: str, table_name: str) -&gt; \"TableTypeDef\":\n        try:\n            load_table_response = self.glue.get_table(DatabaseName=database_name, Name=table_name)\n            return load_table_response[\"Table\"]\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n\n    def create_table(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"\n        Create an Iceberg table.\n\n        Args:\n            identifier: Table identifier.\n            schema: Table's schema.\n            location: Location for the table. Optional Argument.\n            partition_spec: PartitionSpec for the table.\n            sort_order: SortOrder for the table.\n            properties: Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            AlreadyExistsError: If a table with the name already exists.\n            ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n        \"\"\"\n        staged_table = self._create_staged_table(\n            identifier=identifier,\n            schema=schema,\n            location=location,\n            partition_spec=partition_spec,\n            sort_order=sort_order,\n            properties=properties,\n        )\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        self._write_metadata(staged_table.metadata, staged_table.io, staged_table.metadata_location)\n        table_input = _construct_table_input(table_name, staged_table.metadata_location, properties, staged_table.metadata)\n        self._create_glue_table(database_name=database_name, table_name=table_name, table_input=table_input)\n\n        return self.load_table(identifier=identifier)\n\n    def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier (Union[str, Identifier]): Table identifier for the table\n            metadata_location (str): The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n        properties = EMPTY_DICT\n        io = self._load_file_io(location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        table_input = _construct_table_input(table_name, metadata_location, properties, metadata)\n        self._create_glue_table(database_name=database_name, table_name=table_name, table_input=table_input)\n        return self.load_table(identifier=identifier)\n\n    def commit_table(\n        self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        \"\"\"\n        table_identifier = table.name()\n        database_name, table_name = self.identifier_to_database_and_table(table_identifier, NoSuchTableError)\n\n        current_glue_table: TableTypeDef | None\n        glue_table_version_id: str | None\n        current_table: Table | None\n        try:\n            current_glue_table = self._get_glue_table(database_name=database_name, table_name=table_name)\n            glue_table_version_id = current_glue_table.get(\"VersionId\")\n            current_table = self._convert_glue_to_iceberg(glue_table=current_glue_table)\n        except NoSuchTableError:\n            current_glue_table = None\n            glue_table_version_id = None\n            current_table = None\n\n        updated_staged_table = self._update_and_stage_table(current_table, table_identifier, requirements, updates)\n        if current_table and updated_staged_table.metadata == current_table.metadata:\n            # no changes, do nothing\n            return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n        self._write_metadata(\n            metadata=updated_staged_table.metadata,\n            io=updated_staged_table.io,\n            metadata_path=updated_staged_table.metadata_location,\n        )\n\n        if current_table:\n            # table exists, update the table\n            if not glue_table_version_id:\n                raise CommitFailedException(\n                    f\"Cannot commit {database_name}.{table_name} because Glue table version id is missing\"\n                )\n\n            # Pass `version_id` to implement optimistic locking: it ensures updates are rejected if concurrent\n            # modifications occur. See more details at https://iceberg.apache.org/docs/latest/aws/#optimistic-locking\n            update_table_input = _construct_table_input(\n                table_name=table_name,\n                metadata_location=updated_staged_table.metadata_location,\n                properties=updated_staged_table.properties,\n                metadata=updated_staged_table.metadata,\n                glue_table=current_glue_table,\n                prev_metadata_location=current_table.metadata_location,\n            )\n            self._update_glue_table(\n                database_name=database_name,\n                table_name=table_name,\n                table_input=update_table_input,\n                version_id=glue_table_version_id,\n            )\n        else:\n            # table does not exist, create the table\n            create_table_input = _construct_table_input(\n                table_name=table_name,\n                metadata_location=updated_staged_table.metadata_location,\n                properties=updated_staged_table.properties,\n                metadata=updated_staged_table.metadata,\n            )\n            self._create_glue_table(database_name=database_name, table_name=table_name, table_input=create_table_input)\n\n        return CommitTableResponse(\n            metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n        )\n\n    def load_table(self, identifier: str | Identifier) -&gt; Table:\n        \"\"\"Load the table's metadata and returns the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier: Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        return self._convert_glue_to_iceberg(self._get_glue_table(database_name=database_name, table_name=table_name))\n\n    def drop_table(self, identifier: str | Identifier) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier: Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n        try:\n            self.glue.delete_table(DatabaseName=database_name, Name=table_name)\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n\n    def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        This method can only rename Iceberg tables in AWS Glue.\n\n        Args:\n            from_identifier: Existing table identifier.\n            to_identifier: New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            ValueError: When from table identifier is invalid.\n            NoSuchTableError: When a table with the name does not exist.\n            NoSuchIcebergTableError: When from table is not a valid iceberg table.\n            NoSuchPropertyException: When from table miss some required properties.\n            NoSuchNamespaceError: When the destination namespace doesn't exist.\n        \"\"\"\n        from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n        to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n        try:\n            get_table_response = self.glue.get_table(DatabaseName=from_database_name, Name=from_table_name)\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchTableError(f\"Table does not exist: {from_database_name}.{from_table_name}\") from e\n\n        glue_table = get_table_response[\"Table\"]\n\n        try:\n            # verify that from_identifier is a valid iceberg table\n            self._convert_glue_to_iceberg(glue_table=glue_table)\n        except NoSuchPropertyException as e:\n            raise NoSuchPropertyException(\n                f\"Failed to rename table {from_database_name}.{from_table_name} since it is missing required properties\"\n            ) from e\n        except NoSuchIcebergTableError as e:\n            raise NoSuchIcebergTableError(\n                f\"Failed to rename table {from_database_name}.{from_table_name} since it is not a valid iceberg table\"\n            ) from e\n\n        rename_table_input = _construct_rename_table_input(to_table_name=to_table_name, glue_table=glue_table)\n        self._create_glue_table(database_name=to_database_name, table_name=to_table_name, table_input=rename_table_input)\n\n        try:\n            self.drop_table(from_identifier)\n        except Exception as e:\n            log_message = f\"Failed to drop old table {from_database_name}.{from_table_name}. \"\n\n            try:\n                self.drop_table(to_identifier)\n                log_message += f\"Rolled back table creation for {to_database_name}.{to_table_name}.\"\n            except NoSuchTableError:\n                log_message += (\n                    f\"Failed to roll back table creation for {to_database_name}.{to_table_name}. Please clean up manually\"\n                )\n\n            raise ValueError(log_message) from e\n\n        return self.load_table(to_identifier)\n\n    def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace: Namespace identifier.\n            properties: A string dictionary of properties for the given namespace.\n\n        Raises:\n            ValueError: If the identifier is invalid.\n            AlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace)\n        try:\n            self.glue.create_database(DatabaseInput=_construct_database_input(database_name, properties))\n        except self.glue.exceptions.AlreadyExistsException as e:\n            raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n\n    def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        A Glue namespace can only be dropped if it is empty.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        try:\n            table_list_response = self.glue.get_tables(DatabaseName=database_name)\n            table_list = table_list_response[\"TableList\"]\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n\n        if len(table_list) &gt; 0:\n            first_table = table_list[0]\n            if self.__is_iceberg_table(first_table):\n                raise NamespaceNotEmptyError(f\"Cannot drop namespace {database_name} because it still contains Iceberg tables\")\n            else:\n                raise NamespaceNotEmptyError(\n                    f\"Cannot drop namespace {database_name} because it still contains non-Iceberg tables\"\n                )\n        self.glue.delete_database(Name=database_name)\n\n    def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        table_list: list[TableTypeDef] = []\n        next_token: str | None = None\n        try:\n            while True:\n                table_list_response = (\n                    self.glue.get_tables(DatabaseName=database_name)\n                    if not next_token\n                    else self.glue.get_tables(DatabaseName=database_name, NextToken=next_token)\n                )\n                table_list.extend(table_list_response[\"TableList\"])\n                next_token = table_list_response.get(\"NextToken\")\n                if not next_token:\n                    break\n\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n        return [(database_name, table[\"Name\"]) for table in table_list if self.__is_iceberg_table(table)]\n\n    def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n        \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n        \"\"\"\n        # Hierarchical namespace is not supported. Return an empty list\n        if namespace:\n            return []\n\n        database_list: list[DatabaseTypeDef] = []\n        next_token: str | None = None\n\n        while True:\n            databases_response = self.glue.get_databases() if not next_token else self.glue.get_databases(NextToken=next_token)\n            database_list.extend(databases_response[\"DatabaseList\"])\n            next_token = databases_response.get(\"NextToken\")\n            if not next_token:\n                break\n\n        return [self.identifier_to_tuple(database[\"Name\"]) for database in database_list]\n\n    def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n        \"\"\"Get properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        try:\n            database_response = self.glue.get_database(Name=database_name)\n        except self.glue.exceptions.EntityNotFoundException as e:\n            raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n        except self.glue.exceptions.InvalidInputException as e:\n            raise NoSuchNamespaceError(f\"Invalid input for namespace {database_name}\") from e\n\n        database = database_response[\"Database\"]\n\n        properties = dict(database.get(\"Parameters\", {}))\n        if \"LocationUri\" in database:\n            properties[\"location\"] = database[\"LocationUri\"]\n        if \"Description\" in database:\n            properties[\"Description\"] = database[\"Description\"]\n\n        return properties\n\n    def update_namespace_properties(\n        self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"Remove provided property keys and updates properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n            removals: Set of property keys that need to be removed. Optional Argument.\n            updates: Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist\uff0c or identifier is invalid.\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n        current_properties = self.load_namespace_properties(namespace=namespace)\n        properties_update_summary, updated_properties = self._get_updated_props_and_update_summary(\n            current_properties=current_properties, removals=removals, updates=updates\n        )\n\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        self.glue.update_database(Name=database_name, DatabaseInput=_construct_database_input(database_name, updated_properties))\n\n        return properties_update_summary\n\n    def list_views(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        raise NotImplementedError\n\n    def drop_view(self, identifier: str | Identifier) -&gt; None:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: str | Identifier) -&gt; bool:\n        raise NotImplementedError\n\n    @staticmethod\n    def __is_iceberg_table(table: \"TableTypeDef\") -&gt; bool:\n        return table.get(\"Parameters\", {}).get(TABLE_TYPE, \"\").lower() == ICEBERG\n\n    def _get_default_warehouse_location(self, database_name: str, table_name: str) -&gt; str:\n        \"\"\"Override the default warehouse location to follow Hive-style conventions.\"\"\"\n        return self._get_hive_style_warehouse_location(database_name, table_name)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.__init__","title":"<code>__init__(name, client=None, **properties)</code>","text":"<p>Glue Catalog.</p> <p>You either need to provide a boto3 glue client, or one will be constructed from the properties.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to identify the catalog.</p> required <code>client</code> <code>Optional[GlueClient]</code> <p>An optional boto3 glue client.</p> <code>None</code> <code>properties</code> <code>Any</code> <p>Properties for glue client construction and configuration.</p> <code>{}</code> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def __init__(self, name: str, client: Optional[\"GlueClient\"] = None, **properties: Any):\n    \"\"\"Glue Catalog.\n\n    You either need to provide a boto3 glue client, or one will be constructed from the properties.\n\n    Args:\n        name: Name to identify the catalog.\n        client: An optional boto3 glue client.\n        properties: Properties for glue client construction and configuration.\n    \"\"\"\n    super().__init__(name, **properties)\n\n    if client is not None:\n        self.glue = client\n    else:\n        retry_mode_prop_value = get_first_property_value(properties, GLUE_RETRY_MODE)\n\n        session = boto3.Session(\n            profile_name=get_first_property_value(properties, GLUE_PROFILE_NAME, AWS_PROFILE_NAME),\n            region_name=get_first_property_value(properties, GLUE_REGION, AWS_REGION),\n            botocore_session=properties.get(BOTOCORE_SESSION),\n            aws_access_key_id=get_first_property_value(properties, GLUE_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n            aws_secret_access_key=get_first_property_value(properties, GLUE_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n            aws_session_token=get_first_property_value(properties, GLUE_SESSION_TOKEN, AWS_SESSION_TOKEN),\n        )\n        self.glue: GlueClient = session.client(\n            \"glue\",\n            endpoint_url=properties.get(GLUE_CATALOG_ENDPOINT),\n            config=Config(\n                retries={\n                    \"max_attempts\": properties.get(GLUE_MAX_RETRIES, MAX_RETRIES),\n                    \"mode\": retry_mode_prop_value if retry_mode_prop_value in EXISTING_RETRY_MODES else STANDARD_RETRY_MODE,\n                }\n            ),\n        )\n\n        if glue_catalog_id := properties.get(GLUE_ID):\n            _register_glue_catalog_id_with_glue_client(self.glue, glue_catalog_id)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def commit_table(\n    self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n    \"\"\"\n    table_identifier = table.name()\n    database_name, table_name = self.identifier_to_database_and_table(table_identifier, NoSuchTableError)\n\n    current_glue_table: TableTypeDef | None\n    glue_table_version_id: str | None\n    current_table: Table | None\n    try:\n        current_glue_table = self._get_glue_table(database_name=database_name, table_name=table_name)\n        glue_table_version_id = current_glue_table.get(\"VersionId\")\n        current_table = self._convert_glue_to_iceberg(glue_table=current_glue_table)\n    except NoSuchTableError:\n        current_glue_table = None\n        glue_table_version_id = None\n        current_table = None\n\n    updated_staged_table = self._update_and_stage_table(current_table, table_identifier, requirements, updates)\n    if current_table and updated_staged_table.metadata == current_table.metadata:\n        # no changes, do nothing\n        return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n    self._write_metadata(\n        metadata=updated_staged_table.metadata,\n        io=updated_staged_table.io,\n        metadata_path=updated_staged_table.metadata_location,\n    )\n\n    if current_table:\n        # table exists, update the table\n        if not glue_table_version_id:\n            raise CommitFailedException(\n                f\"Cannot commit {database_name}.{table_name} because Glue table version id is missing\"\n            )\n\n        # Pass `version_id` to implement optimistic locking: it ensures updates are rejected if concurrent\n        # modifications occur. See more details at https://iceberg.apache.org/docs/latest/aws/#optimistic-locking\n        update_table_input = _construct_table_input(\n            table_name=table_name,\n            metadata_location=updated_staged_table.metadata_location,\n            properties=updated_staged_table.properties,\n            metadata=updated_staged_table.metadata,\n            glue_table=current_glue_table,\n            prev_metadata_location=current_table.metadata_location,\n        )\n        self._update_glue_table(\n            database_name=database_name,\n            table_name=table_name,\n            table_input=update_table_input,\n            version_id=glue_table_version_id,\n        )\n    else:\n        # table does not exist, create the table\n        create_table_input = _construct_table_input(\n            table_name=table_name,\n            metadata_location=updated_staged_table.metadata_location,\n            properties=updated_staged_table.properties,\n            metadata=updated_staged_table.metadata,\n        )\n        self._create_glue_table(database_name=database_name, table_name=table_name, table_input=create_table_input)\n\n    return CommitTableResponse(\n        metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the identifier is invalid.</p> <code>AlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace: Namespace identifier.\n        properties: A string dictionary of properties for the given namespace.\n\n    Raises:\n        ValueError: If the identifier is invalid.\n        AlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace)\n    try:\n        self.glue.create_database(DatabaseInput=_construct_database_input(database_name, properties))\n    except self.glue.exceptions.AlreadyExistsException as e:\n        raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create an Iceberg table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Union[Schema, Schema]</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>AlreadyExistsError</code> <p>If a table with the name already exists.</p> <code>ValueError</code> <p>If the identifier is invalid, or no path is given to store metadata.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def create_table(\n    self,\n    identifier: str | Identifier,\n    schema: Union[Schema, \"pa.Schema\"],\n    location: str | None = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"\n    Create an Iceberg table.\n\n    Args:\n        identifier: Table identifier.\n        schema: Table's schema.\n        location: Location for the table. Optional Argument.\n        partition_spec: PartitionSpec for the table.\n        sort_order: SortOrder for the table.\n        properties: Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        AlreadyExistsError: If a table with the name already exists.\n        ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n    \"\"\"\n    staged_table = self._create_staged_table(\n        identifier=identifier,\n        schema=schema,\n        location=location,\n        partition_spec=partition_spec,\n        sort_order=sort_order,\n        properties=properties,\n    )\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n    self._write_metadata(staged_table.metadata, staged_table.io, staged_table.metadata_location)\n    table_input = _construct_table_input(table_name, staged_table.metadata_location, properties, staged_table.metadata)\n    self._create_glue_table(database_name=database_name, table_name=table_name, table_input=table_input)\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>","text":"<p>Drop a namespace.</p> <p>A Glue namespace can only be dropped if it is empty.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    A Glue namespace can only be dropped if it is empty.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    try:\n        table_list_response = self.glue.get_tables(DatabaseName=database_name)\n        table_list = table_list_response[\"TableList\"]\n    except self.glue.exceptions.EntityNotFoundException as e:\n        raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n\n    if len(table_list) &gt; 0:\n        first_table = table_list[0]\n        if self.__is_iceberg_table(first_table):\n            raise NamespaceNotEmptyError(f\"Cannot drop namespace {database_name} because it still contains Iceberg tables\")\n        else:\n            raise NamespaceNotEmptyError(\n                f\"Cannot drop namespace {database_name} because it still contains non-Iceberg tables\"\n            )\n    self.glue.delete_database(Name=database_name)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.drop_table","title":"<code>drop_table(identifier)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def drop_table(self, identifier: str | Identifier) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier: Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n    try:\n        self.glue.delete_table(DatabaseName=database_name, Name=table_name)\n    except self.glue.exceptions.EntityNotFoundException as e:\n        raise NoSuchTableError(f\"Table does not exist: {database_name}.{table_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>","text":"<p>List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.</p> <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n    \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n    \"\"\"\n    # Hierarchical namespace is not supported. Return an empty list\n    if namespace:\n        return []\n\n    database_list: list[DatabaseTypeDef] = []\n    next_token: str | None = None\n\n    while True:\n        databases_response = self.glue.get_databases() if not next_token else self.glue.get_databases(NextToken=next_token)\n        database_list.extend(databases_response[\"DatabaseList\"])\n        next_token = databases_response.get(\"NextToken\")\n        if not next_token:\n            break\n\n    return [self.identifier_to_tuple(database[\"Name\"]) for database in database_list]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.list_tables","title":"<code>list_tables(namespace)</code>","text":"<p>List Iceberg tables under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n    \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    table_list: list[TableTypeDef] = []\n    next_token: str | None = None\n    try:\n        while True:\n            table_list_response = (\n                self.glue.get_tables(DatabaseName=database_name)\n                if not next_token\n                else self.glue.get_tables(DatabaseName=database_name, NextToken=next_token)\n            )\n            table_list.extend(table_list_response[\"TableList\"])\n            next_token = table_list_response.get(\"NextToken\")\n            if not next_token:\n                break\n\n    except self.glue.exceptions.EntityNotFoundException as e:\n        raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n    return [(database_name, table[\"Name\"]) for table in table_list if self.__is_iceberg_table(table)]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or identifier is invalid.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n    \"\"\"Get properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    try:\n        database_response = self.glue.get_database(Name=database_name)\n    except self.glue.exceptions.EntityNotFoundException as e:\n        raise NoSuchNamespaceError(f\"Database does not exist: {database_name}\") from e\n    except self.glue.exceptions.InvalidInputException as e:\n        raise NoSuchNamespaceError(f\"Invalid input for namespace {database_name}\") from e\n\n    database = database_response[\"Database\"]\n\n    properties = dict(database.get(\"Parameters\", {}))\n    if \"LocationUri\" in database:\n        properties[\"location\"] = database[\"LocationUri\"]\n    if \"Description\" in database:\n        properties[\"Description\"] = database[\"Description\"]\n\n    return properties\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.load_table","title":"<code>load_table(identifier)</code>","text":"<p>Load the table's metadata and returns the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def load_table(self, identifier: str | Identifier) -&gt; Table:\n    \"\"\"Load the table's metadata and returns the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier: Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n    return self._convert_glue_to_iceberg(self._get_glue_table(database_name=database_name, table_name=table_name))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier (Union[str, Identifier]): Table identifier for the table\n        metadata_location (str): The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n    properties = EMPTY_DICT\n    io = self._load_file_io(location=metadata_location)\n    file = io.new_input(metadata_location)\n    metadata = FromInputFile.table_metadata(file)\n    table_input = _construct_table_input(table_name, metadata_location, properties, metadata)\n    self._create_glue_table(database_name=database_name, table_name=table_name, table_input=table_input)\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>","text":"<p>Rename a fully classified table name.</p> <p>This method can only rename Iceberg tables in AWS Glue.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str | Identifier</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>str | Identifier</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When from table identifier is invalid.</p> <code>NoSuchTableError</code> <p>When a table with the name does not exist.</p> <code>NoSuchIcebergTableError</code> <p>When from table is not a valid iceberg table.</p> <code>NoSuchPropertyException</code> <p>When from table miss some required properties.</p> <code>NoSuchNamespaceError</code> <p>When the destination namespace doesn't exist.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    This method can only rename Iceberg tables in AWS Glue.\n\n    Args:\n        from_identifier: Existing table identifier.\n        to_identifier: New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        ValueError: When from table identifier is invalid.\n        NoSuchTableError: When a table with the name does not exist.\n        NoSuchIcebergTableError: When from table is not a valid iceberg table.\n        NoSuchPropertyException: When from table miss some required properties.\n        NoSuchNamespaceError: When the destination namespace doesn't exist.\n    \"\"\"\n    from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n    to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n    try:\n        get_table_response = self.glue.get_table(DatabaseName=from_database_name, Name=from_table_name)\n    except self.glue.exceptions.EntityNotFoundException as e:\n        raise NoSuchTableError(f\"Table does not exist: {from_database_name}.{from_table_name}\") from e\n\n    glue_table = get_table_response[\"Table\"]\n\n    try:\n        # verify that from_identifier is a valid iceberg table\n        self._convert_glue_to_iceberg(glue_table=glue_table)\n    except NoSuchPropertyException as e:\n        raise NoSuchPropertyException(\n            f\"Failed to rename table {from_database_name}.{from_table_name} since it is missing required properties\"\n        ) from e\n    except NoSuchIcebergTableError as e:\n        raise NoSuchIcebergTableError(\n            f\"Failed to rename table {from_database_name}.{from_table_name} since it is not a valid iceberg table\"\n        ) from e\n\n    rename_table_input = _construct_rename_table_input(to_table_name=to_table_name, glue_table=glue_table)\n    self._create_glue_table(database_name=to_database_name, table_name=to_table_name, table_input=rename_table_input)\n\n    try:\n        self.drop_table(from_identifier)\n    except Exception as e:\n        log_message = f\"Failed to drop old table {from_database_name}.{from_table_name}. \"\n\n        try:\n            self.drop_table(to_identifier)\n            log_message += f\"Rolled back table creation for {to_database_name}.{to_table_name}.\"\n        except NoSuchTableError:\n            log_message += (\n                f\"Failed to roll back table creation for {to_database_name}.{to_table_name}. Please clean up manually\"\n            )\n\n        raise ValueError(log_message) from e\n\n    return self.load_table(to_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/glue/#pyiceberg.catalog.glue.GlueCatalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>","text":"<p>Remove provided property keys and updates properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>removals</code> <code>set[str] | None</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist\uff0c or identifier is invalid.</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/glue.py</code> <pre><code>def update_namespace_properties(\n    self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"Remove provided property keys and updates properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n        removals: Set of property keys that need to be removed. Optional Argument.\n        updates: Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist\uff0c or identifier is invalid.\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n    current_properties = self.load_namespace_properties(namespace=namespace)\n    properties_update_summary, updated_properties = self._get_updated_props_and_update_summary(\n        current_properties=current_properties, removals=removals, updates=updates\n    )\n\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    self.glue.update_database(Name=database_name, DatabaseInput=_construct_database_input(database_name, updated_properties))\n\n    return properties_update_summary\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/","title":"hive","text":""},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog","title":"<code>HiveCatalog</code>","text":"<p>               Bases: <code>MetastoreCatalog</code></p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>class HiveCatalog(MetastoreCatalog):\n    _client: _HiveClient\n\n    def __init__(self, name: str, **properties: str):\n        super().__init__(name, **properties)\n        self._client = self._create_hive_client(properties)\n\n        self._lock_check_min_wait_time = property_as_float(properties, LOCK_CHECK_MIN_WAIT_TIME, DEFAULT_LOCK_CHECK_MIN_WAIT_TIME)\n        self._lock_check_max_wait_time = property_as_float(properties, LOCK_CHECK_MAX_WAIT_TIME, DEFAULT_LOCK_CHECK_MAX_WAIT_TIME)\n        self._lock_check_retries = property_as_float(\n            properties,\n            LOCK_CHECK_RETRIES,\n            DEFAULT_LOCK_CHECK_RETRIES,\n        )\n\n    @staticmethod\n    def _create_hive_client(properties: dict[str, str]) -&gt; _HiveClient:\n        last_exception = None\n        for uri in properties[URI].split(\",\"):\n            try:\n                return _HiveClient(\n                    uri,\n                    properties.get(\"ugi\"),\n                    property_as_bool(properties, HIVE_KERBEROS_AUTH, HIVE_KERBEROS_AUTH_DEFAULT),\n                    properties.get(HIVE_KERBEROS_SERVICE_NAME, HIVE_KERBEROS_SERVICE_NAME_DEFAULT),\n                )\n            except BaseException as e:\n                last_exception = e\n        if last_exception is not None:\n            raise last_exception\n        else:\n            raise ValueError(f\"Unable to connect to hive using uri: {properties[URI]}\")\n\n    def _convert_hive_into_iceberg(self, table: HiveTable) -&gt; Table:\n        properties: dict[str, str] = table.parameters\n        if TABLE_TYPE not in properties:\n            raise NoSuchPropertyException(\n                f\"Property table_type missing, could not determine type: {table.dbName}.{table.tableName}\"\n            )\n\n        table_type = properties[TABLE_TYPE]\n        if table_type.lower() != ICEBERG:\n            raise NoSuchIcebergTableError(\n                f\"Property table_type is {table_type}, expected {ICEBERG}: {table.dbName}.{table.tableName}\"\n            )\n\n        if prop_metadata_location := properties.get(METADATA_LOCATION):\n            metadata_location = prop_metadata_location\n        else:\n            raise NoSuchPropertyException(f\"Table property {METADATA_LOCATION} is missing\")\n\n        io = self._load_file_io(location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        return Table(\n            identifier=(table.dbName, table.tableName),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=self._load_file_io(metadata.properties, metadata_location),\n            catalog=self,\n        )\n\n    def _convert_iceberg_into_hive(self, table: Table) -&gt; HiveTable:\n        identifier_tuple = table.name()\n        database_name, table_name = self.identifier_to_database_and_table(identifier_tuple, NoSuchTableError)\n        current_time_millis = int(time.time() * 1000)\n\n        return HiveTable(\n            dbName=database_name,\n            tableName=table_name,\n            owner=table.properties[OWNER] if table.properties and OWNER in table.properties else getpass.getuser(),\n            createTime=current_time_millis // 1000,\n            lastAccessTime=current_time_millis // 1000,\n            sd=_construct_hive_storage_descriptor(\n                table.schema(),\n                table.location(),\n                property_as_bool(self.properties, HIVE2_COMPATIBLE, HIVE2_COMPATIBLE_DEFAULT),\n            ),\n            tableType=EXTERNAL_TABLE,\n            parameters=_construct_parameters(metadata_location=table.metadata_location, metadata_properties=table.properties),\n        )\n\n    def _create_hive_table(self, open_client: Client, hive_table: HiveTable) -&gt; None:\n        try:\n            open_client.create_table(hive_table)\n        except AlreadyExistsException as e:\n            raise TableAlreadyExistsError(f\"Table {hive_table.dbName}.{hive_table.tableName} already exists\") from e\n\n    def _get_hive_table(self, open_client: Client, database_name: str, table_name: str) -&gt; HiveTable:\n        try:\n            return open_client.get_table(dbname=database_name, tbl_name=table_name)\n        except NoSuchObjectException as e:\n            raise NoSuchTableError(f\"Table does not exists: {table_name}\") from e\n\n    def create_table(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"Create a table.\n\n        Args:\n            identifier: Table identifier.\n            schema: Table's schema.\n            location: Location for the table. Optional Argument.\n            partition_spec: PartitionSpec for the table.\n            sort_order: SortOrder for the table.\n            properties: Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            AlreadyExistsError: If a table with the name already exists.\n            ValueError: If the identifier is invalid.\n        \"\"\"\n        properties = {**DEFAULT_PROPERTIES, **properties}\n        staged_table = self._create_staged_table(\n            identifier=identifier,\n            schema=schema,\n            location=location,\n            partition_spec=partition_spec,\n            sort_order=sort_order,\n            properties=properties,\n        )\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n        self._write_metadata(staged_table.metadata, staged_table.io, staged_table.metadata_location)\n        tbl = self._convert_iceberg_into_hive(staged_table)\n\n        with self._client as open_client:\n            self._create_hive_table(open_client, tbl)\n            hive_table = open_client.get_table(dbname=database_name, tbl_name=table_name)\n\n        return self._convert_hive_into_iceberg(hive_table)\n\n    def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier (Union[str, Identifier]): Table identifier for the table\n            metadata_location (str): The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier)\n        io = self._load_file_io(location=metadata_location)\n        metadata_file = io.new_input(metadata_location)\n        staged_table = StagedTable(\n            identifier=(database_name, table_name),\n            metadata=FromInputFile.table_metadata(metadata_file),\n            metadata_location=metadata_location,\n            io=io,\n            catalog=self,\n        )\n        tbl = self._convert_iceberg_into_hive(staged_table)\n        with self._client as open_client:\n            self._create_hive_table(open_client, tbl)\n            hive_table = open_client.get_table(dbname=database_name, tbl_name=table_name)\n\n        return self._convert_hive_into_iceberg(hive_table)\n\n    def list_views(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: str | Identifier) -&gt; bool:\n        raise NotImplementedError\n\n    def _create_lock_request(self, database_name: str, table_name: str) -&gt; LockRequest:\n        lock_component: LockComponent = LockComponent(\n            level=LockLevel.TABLE, type=LockType.EXCLUSIVE, dbname=database_name, tablename=table_name, isTransactional=True\n        )\n\n        lock_request: LockRequest = LockRequest(component=[lock_component], user=getpass.getuser(), hostname=socket.gethostname())\n\n        return lock_request\n\n    def _wait_for_lock(self, database_name: str, table_name: str, lockid: int, open_client: Client) -&gt; LockResponse:\n        @retry(\n            retry=retry_if_exception_type(WaitingForLockException),\n            wait=wait_exponential(multiplier=2, min=self._lock_check_min_wait_time, max=self._lock_check_max_wait_time),\n            stop=stop_after_attempt(self._lock_check_retries),\n            reraise=True,\n        )\n        def _do_wait_for_lock() -&gt; LockResponse:\n            response: LockResponse = open_client.check_lock(CheckLockRequest(lockid=lockid))\n            if response.state == LockState.ACQUIRED:\n                return response\n            elif response.state == LockState.WAITING:\n                msg = f\"Wait on lock for {database_name}.{table_name}\"\n                logger.warning(msg)\n                raise WaitingForLockException(msg)\n            else:\n                raise CommitFailedException(f\"Failed to check lock for {database_name}.{table_name}, state: {response.state}\")\n\n        return _do_wait_for_lock()\n\n    def commit_table(\n        self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        \"\"\"\n        table_identifier = table.name()\n        database_name, table_name = self.identifier_to_database_and_table(table_identifier, NoSuchTableError)\n        # commit to hive\n        # https://github.com/apache/hive/blob/master/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L1232\n        with self._client as open_client:\n            lock: LockResponse = open_client.lock(self._create_lock_request(database_name, table_name))\n\n            try:\n                if lock.state != LockState.ACQUIRED:\n                    if lock.state == LockState.WAITING:\n                        self._wait_for_lock(database_name, table_name, lock.lockid, open_client)\n                    else:\n                        raise CommitFailedException(f\"Failed to acquire lock for {table_identifier}, state: {lock.state}\")\n\n                hive_table: HiveTable | None\n                current_table: Table | None\n                try:\n                    hive_table = self._get_hive_table(open_client, database_name, table_name)\n                    current_table = self._convert_hive_into_iceberg(hive_table)\n                except NoSuchTableError:\n                    hive_table = None\n                    current_table = None\n\n                updated_staged_table = self._update_and_stage_table(current_table, table_identifier, requirements, updates)\n                if current_table and updated_staged_table.metadata == current_table.metadata:\n                    # no changes, do nothing\n                    return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n                self._write_metadata(\n                    metadata=updated_staged_table.metadata,\n                    io=updated_staged_table.io,\n                    metadata_path=updated_staged_table.metadata_location,\n                )\n\n                if hive_table and current_table:\n                    # Table exists, update it.\n\n                    # Note on table properties:\n                    # - Iceberg table properties are stored in both HMS and Iceberg metadata JSON.\n                    # - Updates are reflected in both locations\n                    # - Existing HMS table properties (set by external systems like Hive/Spark) are preserved.\n                    #\n                    # While it is possible to modify HMS table properties through this API, it is not recommended:\n                    # - Mixing HMS-specific properties in Iceberg metadata can cause confusion\n                    # - New/updated HMS table properties will also be stored in Iceberg metadata (even though it is HMS-specific)\n                    # - HMS-native properties (set outside Iceberg) cannot be deleted since they are not visible to Iceberg\n                    #   (However, if you first SET an HMS property via Iceberg, it becomes tracked in Iceberg metadata,\n                    #   and can then be deleted via Iceberg - which removes it from both Iceberg metadata and HMS)\n                    new_iceberg_properties = _construct_parameters(\n                        metadata_location=updated_staged_table.metadata_location,\n                        previous_metadata_location=current_table.metadata_location,\n                        metadata_properties=updated_staged_table.properties,\n                    )\n                    # Detect properties that were removed from Iceberg metadata\n                    deleted_iceberg_properties = current_table.properties.keys() - updated_staged_table.properties.keys()\n\n                    # Merge: preserve HMS-native properties, remove deleted Iceberg properties, apply new Iceberg properties\n                    existing_hms_parameters = dict(hive_table.parameters or {})\n                    for key in deleted_iceberg_properties:\n                        existing_hms_parameters.pop(key, None)\n                    existing_hms_parameters.update(new_iceberg_properties)\n                    hive_table.parameters = existing_hms_parameters\n\n                    # Update hive's schema and properties\n                    hive_table.sd = _construct_hive_storage_descriptor(\n                        updated_staged_table.schema(),\n                        updated_staged_table.location(),\n                        property_as_bool(self.properties, HIVE2_COMPATIBLE, HIVE2_COMPATIBLE_DEFAULT),\n                    )\n                    open_client.alter_table_with_environment_context(\n                        dbname=database_name,\n                        tbl_name=table_name,\n                        new_tbl=hive_table,\n                        environment_context=EnvironmentContext(properties={DO_NOT_UPDATE_STATS: DO_NOT_UPDATE_STATS_DEFAULT}),\n                    )\n                else:\n                    # Table does not exist, create it.\n                    hive_table = self._convert_iceberg_into_hive(\n                        StagedTable(\n                            identifier=(database_name, table_name),\n                            metadata=updated_staged_table.metadata,\n                            metadata_location=updated_staged_table.metadata_location,\n                            io=updated_staged_table.io,\n                            catalog=self,\n                        )\n                    )\n                    self._create_hive_table(open_client, hive_table)\n            except WaitingForLockException as e:\n                raise CommitFailedException(f\"Failed to acquire lock for {table_identifier}, state: {lock.state}\") from e\n            finally:\n                open_client.unlock(UnlockRequest(lockid=lock.lockid))\n\n        return CommitTableResponse(\n            metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n        )\n\n    def load_table(self, identifier: str | Identifier) -&gt; Table:\n        \"\"\"Load the table's metadata and return the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier: Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n        with self._client as open_client:\n            hive_table = self._get_hive_table(open_client, database_name, table_name)\n\n        return self._convert_hive_into_iceberg(hive_table)\n\n    def drop_table(self, identifier: str | Identifier) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier: Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n        try:\n            with self._client as open_client:\n                open_client.drop_table(dbname=database_name, name=table_name, deleteData=False)\n        except NoSuchObjectException as e:\n            # When the namespace doesn't exist, it throws the same error\n            raise NoSuchTableError(f\"Table does not exists: {table_name}\") from e\n\n    def purge_table(self, identifier: str | Identifier) -&gt; None:\n        # This requires to traverse the reachability set, and drop all the data files.\n        raise NotImplementedError(\"Not yet implemented\")\n\n    def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        Args:\n            from_identifier: Existing table identifier.\n            to_identifier: New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            ValueError: When from table identifier is invalid.\n            NoSuchTableError: When a table with the name does not exist.\n            NoSuchNamespaceError: When the destination namespace doesn't exist.\n            TableAlreadyExistsError: When the destination table already exists.\n        \"\"\"\n        from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n        to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n\n        if self.table_exists(to_identifier):\n            raise TableAlreadyExistsError(f\"Table already exists: {to_table_name}\")\n\n        try:\n            with self._client as open_client:\n                tbl = open_client.get_table(dbname=from_database_name, tbl_name=from_table_name)\n                tbl.dbName = to_database_name\n                tbl.tableName = to_table_name\n                open_client.alter_table_with_environment_context(\n                    dbname=from_database_name,\n                    tbl_name=from_table_name,\n                    new_tbl=tbl,\n                    environment_context=EnvironmentContext(properties={DO_NOT_UPDATE_STATS: DO_NOT_UPDATE_STATS_DEFAULT}),\n                )\n        except NoSuchObjectException as e:\n            raise NoSuchTableError(f\"Table does not exist: {from_table_name}\") from e\n        except InvalidOperationException as e:\n            raise NoSuchNamespaceError(f\"Database does not exists: {to_database_name}\") from e\n        return self.load_table(to_identifier)\n\n    def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace: Namespace identifier.\n            properties: A string dictionary of properties for the given namespace.\n\n        Raises:\n            ValueError: If the identifier is invalid.\n            AlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace)\n        hive_database = HiveDatabase(name=database_name, parameters=properties)\n\n        try:\n            with self._client as open_client:\n                open_client.create_database(_annotate_namespace(hive_database, properties))\n        except AlreadyExistsException as e:\n            raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n\n    def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        try:\n            with self._client as open_client:\n                open_client.drop_database(database_name, deleteData=False, cascade=False)\n        except InvalidOperationException as e:\n            raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\") from e\n        except MetaException as e:\n            raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n\n    def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n        When the database doesn't exist, it will just return an empty list.\n\n        Args:\n            namespace: Database to list.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        with self._client as open_client:\n            return [\n                (database_name, table.tableName)\n                for table in open_client.get_table_objects_by_name(\n                    dbname=database_name, tbl_names=open_client.get_all_tables(db_name=database_name)\n                )\n                if table.parameters.get(TABLE_TYPE, \"\").lower() == ICEBERG\n            ]\n\n    def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n        \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n        \"\"\"\n        # Hierarchical namespace is not supported. Return an empty list\n        if namespace:\n            return []\n\n        with self._client as open_client:\n            return list(map(self.identifier_to_tuple, open_client.get_all_databases()))\n\n    def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n        \"\"\"Get properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n        \"\"\"\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        try:\n            with self._client as open_client:\n                database = open_client.get_database(name=database_name)\n                properties = database.parameters\n                properties[LOCATION] = database.locationUri\n                if comment := database.description:\n                    properties[COMMENT] = comment\n                return properties\n        except NoSuchObjectException as e:\n            raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n\n    def update_namespace_properties(\n        self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"Remove provided property keys and update properties for a namespace.\n\n        Args:\n            namespace: Namespace identifier.\n            removals: Set of property keys that need to be removed. Optional Argument.\n            updates: Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n        self._check_for_overlap(updates=updates, removals=removals)\n        database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n        with self._client as open_client:\n            try:\n                database = open_client.get_database(database_name)\n                parameters = database.parameters\n            except NoSuchObjectException as e:\n                raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n\n            removed: set[str] = set()\n            updated: set[str] = set()\n\n            if removals:\n                for key in removals:\n                    if key in parameters:\n                        parameters.pop(key)\n                        removed.add(key)\n            if updates:\n                for key, value in updates.items():\n                    parameters[key] = value\n                    updated.add(key)\n\n            open_client.alter_database(database_name, _annotate_namespace(database, parameters))\n\n        expected_to_change = (removals or set()).difference(removed)\n\n        return PropertiesUpdateSummary(removed=list(removed or []), updated=list(updated or []), missing=list(expected_to_change))\n\n    def drop_view(self, identifier: str | Identifier) -&gt; None:\n        raise NotImplementedError\n\n    def _get_default_warehouse_location(self, database_name: str, table_name: str) -&gt; str:\n        \"\"\"Override the default warehouse location to follow Hive-style conventions.\"\"\"\n        return self._get_hive_style_warehouse_location(database_name, table_name)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def commit_table(\n    self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n    \"\"\"\n    table_identifier = table.name()\n    database_name, table_name = self.identifier_to_database_and_table(table_identifier, NoSuchTableError)\n    # commit to hive\n    # https://github.com/apache/hive/blob/master/standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift#L1232\n    with self._client as open_client:\n        lock: LockResponse = open_client.lock(self._create_lock_request(database_name, table_name))\n\n        try:\n            if lock.state != LockState.ACQUIRED:\n                if lock.state == LockState.WAITING:\n                    self._wait_for_lock(database_name, table_name, lock.lockid, open_client)\n                else:\n                    raise CommitFailedException(f\"Failed to acquire lock for {table_identifier}, state: {lock.state}\")\n\n            hive_table: HiveTable | None\n            current_table: Table | None\n            try:\n                hive_table = self._get_hive_table(open_client, database_name, table_name)\n                current_table = self._convert_hive_into_iceberg(hive_table)\n            except NoSuchTableError:\n                hive_table = None\n                current_table = None\n\n            updated_staged_table = self._update_and_stage_table(current_table, table_identifier, requirements, updates)\n            if current_table and updated_staged_table.metadata == current_table.metadata:\n                # no changes, do nothing\n                return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n            self._write_metadata(\n                metadata=updated_staged_table.metadata,\n                io=updated_staged_table.io,\n                metadata_path=updated_staged_table.metadata_location,\n            )\n\n            if hive_table and current_table:\n                # Table exists, update it.\n\n                # Note on table properties:\n                # - Iceberg table properties are stored in both HMS and Iceberg metadata JSON.\n                # - Updates are reflected in both locations\n                # - Existing HMS table properties (set by external systems like Hive/Spark) are preserved.\n                #\n                # While it is possible to modify HMS table properties through this API, it is not recommended:\n                # - Mixing HMS-specific properties in Iceberg metadata can cause confusion\n                # - New/updated HMS table properties will also be stored in Iceberg metadata (even though it is HMS-specific)\n                # - HMS-native properties (set outside Iceberg) cannot be deleted since they are not visible to Iceberg\n                #   (However, if you first SET an HMS property via Iceberg, it becomes tracked in Iceberg metadata,\n                #   and can then be deleted via Iceberg - which removes it from both Iceberg metadata and HMS)\n                new_iceberg_properties = _construct_parameters(\n                    metadata_location=updated_staged_table.metadata_location,\n                    previous_metadata_location=current_table.metadata_location,\n                    metadata_properties=updated_staged_table.properties,\n                )\n                # Detect properties that were removed from Iceberg metadata\n                deleted_iceberg_properties = current_table.properties.keys() - updated_staged_table.properties.keys()\n\n                # Merge: preserve HMS-native properties, remove deleted Iceberg properties, apply new Iceberg properties\n                existing_hms_parameters = dict(hive_table.parameters or {})\n                for key in deleted_iceberg_properties:\n                    existing_hms_parameters.pop(key, None)\n                existing_hms_parameters.update(new_iceberg_properties)\n                hive_table.parameters = existing_hms_parameters\n\n                # Update hive's schema and properties\n                hive_table.sd = _construct_hive_storage_descriptor(\n                    updated_staged_table.schema(),\n                    updated_staged_table.location(),\n                    property_as_bool(self.properties, HIVE2_COMPATIBLE, HIVE2_COMPATIBLE_DEFAULT),\n                )\n                open_client.alter_table_with_environment_context(\n                    dbname=database_name,\n                    tbl_name=table_name,\n                    new_tbl=hive_table,\n                    environment_context=EnvironmentContext(properties={DO_NOT_UPDATE_STATS: DO_NOT_UPDATE_STATS_DEFAULT}),\n                )\n            else:\n                # Table does not exist, create it.\n                hive_table = self._convert_iceberg_into_hive(\n                    StagedTable(\n                        identifier=(database_name, table_name),\n                        metadata=updated_staged_table.metadata,\n                        metadata_location=updated_staged_table.metadata_location,\n                        io=updated_staged_table.io,\n                        catalog=self,\n                    )\n                )\n                self._create_hive_table(open_client, hive_table)\n        except WaitingForLockException as e:\n            raise CommitFailedException(f\"Failed to acquire lock for {table_identifier}, state: {lock.state}\") from e\n        finally:\n            open_client.unlock(UnlockRequest(lockid=lock.lockid))\n\n    return CommitTableResponse(\n        metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the identifier is invalid.</p> <code>AlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace: Namespace identifier.\n        properties: A string dictionary of properties for the given namespace.\n\n    Raises:\n        ValueError: If the identifier is invalid.\n        AlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace)\n    hive_database = HiveDatabase(name=database_name, parameters=properties)\n\n    try:\n        with self._client as open_client:\n            open_client.create_database(_annotate_namespace(hive_database, properties))\n    except AlreadyExistsException as e:\n        raise NamespaceAlreadyExistsError(f\"Database {database_name} already exists\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Union[Schema, Schema]</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>AlreadyExistsError</code> <p>If a table with the name already exists.</p> <code>ValueError</code> <p>If the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def create_table(\n    self,\n    identifier: str | Identifier,\n    schema: Union[Schema, \"pa.Schema\"],\n    location: str | None = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"Create a table.\n\n    Args:\n        identifier: Table identifier.\n        schema: Table's schema.\n        location: Location for the table. Optional Argument.\n        partition_spec: PartitionSpec for the table.\n        sort_order: SortOrder for the table.\n        properties: Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        AlreadyExistsError: If a table with the name already exists.\n        ValueError: If the identifier is invalid.\n    \"\"\"\n    properties = {**DEFAULT_PROPERTIES, **properties}\n    staged_table = self._create_staged_table(\n        identifier=identifier,\n        schema=schema,\n        location=location,\n        partition_spec=partition_spec,\n        sort_order=sort_order,\n        properties=properties,\n    )\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n\n    self._write_metadata(staged_table.metadata, staged_table.io, staged_table.metadata_location)\n    tbl = self._convert_iceberg_into_hive(staged_table)\n\n    with self._client as open_client:\n        self._create_hive_table(open_client, tbl)\n        hive_table = open_client.get_table(dbname=database_name, tbl_name=table_name)\n\n    return self._convert_hive_into_iceberg(hive_table)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>","text":"<p>Drop a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    try:\n        with self._client as open_client:\n            open_client.drop_database(database_name, deleteData=False, cascade=False)\n    except InvalidOperationException as e:\n        raise NamespaceNotEmptyError(f\"Database {database_name} is not empty\") from e\n    except MetaException as e:\n        raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.drop_table","title":"<code>drop_table(identifier)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def drop_table(self, identifier: str | Identifier) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier: Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n    try:\n        with self._client as open_client:\n            open_client.drop_table(dbname=database_name, name=table_name, deleteData=False)\n    except NoSuchObjectException as e:\n        # When the namespace doesn't exist, it throws the same error\n        raise NoSuchTableError(f\"Table does not exists: {table_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>","text":"<p>List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.</p> <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n    \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n    \"\"\"\n    # Hierarchical namespace is not supported. Return an empty list\n    if namespace:\n        return []\n\n    with self._client as open_client:\n        return list(map(self.identifier_to_tuple, open_client.get_all_databases()))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.list_tables","title":"<code>list_tables(namespace)</code>","text":"<p>List Iceberg tables under the given namespace in the catalog.</p> <p>When the database doesn't exist, it will just return an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Database to list.</p> required <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n    \"\"\"List Iceberg tables under the given namespace in the catalog.\n\n    When the database doesn't exist, it will just return an empty list.\n\n    Args:\n        namespace: Database to list.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    with self._client as open_client:\n        return [\n            (database_name, table.tableName)\n            for table in open_client.get_table_objects_by_name(\n                dbname=database_name, tbl_names=open_client.get_all_tables(db_name=database_name)\n            )\n            if table.parameters.get(TABLE_TYPE, \"\").lower() == ICEBERG\n        ]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist, or identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n    \"\"\"Get properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist, or identifier is invalid.\n    \"\"\"\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    try:\n        with self._client as open_client:\n            database = open_client.get_database(name=database_name)\n            properties = database.parameters\n            properties[LOCATION] = database.locationUri\n            if comment := database.description:\n                properties[COMMENT] = comment\n            return properties\n    except NoSuchObjectException as e:\n        raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.load_table","title":"<code>load_table(identifier)</code>","text":"<p>Load the table's metadata and return the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist, or the identifier is invalid.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def load_table(self, identifier: str | Identifier) -&gt; Table:\n    \"\"\"Load the table's metadata and return the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except TableNotFoundError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier: Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist, or the identifier is invalid.\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier, NoSuchTableError)\n\n    with self._client as open_client:\n        hive_table = self._get_hive_table(open_client, database_name, table_name)\n\n    return self._convert_hive_into_iceberg(hive_table)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier (Union[str, Identifier]): Table identifier for the table\n        metadata_location (str): The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    database_name, table_name = self.identifier_to_database_and_table(identifier)\n    io = self._load_file_io(location=metadata_location)\n    metadata_file = io.new_input(metadata_location)\n    staged_table = StagedTable(\n        identifier=(database_name, table_name),\n        metadata=FromInputFile.table_metadata(metadata_file),\n        metadata_location=metadata_location,\n        io=io,\n        catalog=self,\n    )\n    tbl = self._convert_iceberg_into_hive(staged_table)\n    with self._client as open_client:\n        self._create_hive_table(open_client, tbl)\n        hive_table = open_client.get_table(dbname=database_name, tbl_name=table_name)\n\n    return self._convert_hive_into_iceberg(hive_table)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>","text":"<p>Rename a fully classified table name.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str | Identifier</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>str | Identifier</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When from table identifier is invalid.</p> <code>NoSuchTableError</code> <p>When a table with the name does not exist.</p> <code>NoSuchNamespaceError</code> <p>When the destination namespace doesn't exist.</p> <code>TableAlreadyExistsError</code> <p>When the destination table already exists.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    Args:\n        from_identifier: Existing table identifier.\n        to_identifier: New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        ValueError: When from table identifier is invalid.\n        NoSuchTableError: When a table with the name does not exist.\n        NoSuchNamespaceError: When the destination namespace doesn't exist.\n        TableAlreadyExistsError: When the destination table already exists.\n    \"\"\"\n    from_database_name, from_table_name = self.identifier_to_database_and_table(from_identifier, NoSuchTableError)\n    to_database_name, to_table_name = self.identifier_to_database_and_table(to_identifier)\n\n    if self.table_exists(to_identifier):\n        raise TableAlreadyExistsError(f\"Table already exists: {to_table_name}\")\n\n    try:\n        with self._client as open_client:\n            tbl = open_client.get_table(dbname=from_database_name, tbl_name=from_table_name)\n            tbl.dbName = to_database_name\n            tbl.tableName = to_table_name\n            open_client.alter_table_with_environment_context(\n                dbname=from_database_name,\n                tbl_name=from_table_name,\n                new_tbl=tbl,\n                environment_context=EnvironmentContext(properties={DO_NOT_UPDATE_STATS: DO_NOT_UPDATE_STATS_DEFAULT}),\n            )\n    except NoSuchObjectException as e:\n        raise NoSuchTableError(f\"Table does not exist: {from_table_name}\") from e\n    except InvalidOperationException as e:\n        raise NoSuchNamespaceError(f\"Database does not exists: {to_database_name}\") from e\n    return self.load_table(to_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/hive/#pyiceberg.catalog.hive.HiveCatalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>","text":"<p>Remove provided property keys and update properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>removals</code> <code>set[str] | None</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/hive.py</code> <pre><code>def update_namespace_properties(\n    self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"Remove provided property keys and update properties for a namespace.\n\n    Args:\n        namespace: Namespace identifier.\n        removals: Set of property keys that need to be removed. Optional Argument.\n        updates: Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n    self._check_for_overlap(updates=updates, removals=removals)\n    database_name = self.identifier_to_database(namespace, NoSuchNamespaceError)\n    with self._client as open_client:\n        try:\n            database = open_client.get_database(database_name)\n            parameters = database.parameters\n        except NoSuchObjectException as e:\n            raise NoSuchNamespaceError(f\"Database does not exists: {database_name}\") from e\n\n        removed: set[str] = set()\n        updated: set[str] = set()\n\n        if removals:\n            for key in removals:\n                if key in parameters:\n                    parameters.pop(key)\n                    removed.add(key)\n        if updates:\n            for key, value in updates.items():\n                parameters[key] = value\n                updated.add(key)\n\n        open_client.alter_database(database_name, _annotate_namespace(database, parameters))\n\n    expected_to_change = (removals or set()).difference(removed)\n\n    return PropertiesUpdateSummary(removed=list(removed or []), updated=list(updated or []), missing=list(expected_to_change))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/memory/","title":"memory","text":""},{"location":"reference/pyiceberg/catalog/memory/#pyiceberg.catalog.memory.InMemoryCatalog","title":"<code>InMemoryCatalog</code>","text":"<p>               Bases: <code>SqlCatalog</code></p> <p>An in-memory catalog implementation that uses SqlCatalog with SQLite in-memory database.</p> <p>This is useful for test, demo, and playground but not in production as it does not support concurrent access.</p> Source code in <code>pyiceberg/catalog/memory.py</code> <pre><code>class InMemoryCatalog(SqlCatalog):\n    \"\"\"\n    An in-memory catalog implementation that uses SqlCatalog with SQLite in-memory database.\n\n    This is useful for test, demo, and playground but not in production as it does not support concurrent access.\n    \"\"\"\n\n    def __init__(self, name: str, warehouse: str = \"file:///tmp/iceberg/warehouse\", **kwargs: str) -&gt; None:\n        self._warehouse_location = warehouse\n        if URI not in kwargs:\n            kwargs[URI] = \"sqlite:///:memory:\"\n        super().__init__(name=name, warehouse=warehouse, **kwargs)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/noop/","title":"noop","text":""},{"location":"reference/pyiceberg/catalog/noop/#pyiceberg.catalog.noop.NoopCatalog","title":"<code>NoopCatalog</code>","text":"<p>               Bases: <code>Catalog</code></p> Source code in <code>pyiceberg/catalog/noop.py</code> <pre><code>class NoopCatalog(Catalog):\n    def create_table(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        raise NotImplementedError\n\n    def create_table_transaction(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; CreateTableTransaction:\n        raise NotImplementedError\n\n    def load_table(self, identifier: str | Identifier) -&gt; Table:\n        raise NotImplementedError\n\n    def table_exists(self, identifier: str | Identifier) -&gt; bool:\n        raise NotImplementedError\n\n    def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier (Union[str, Identifier]): Table identifier for the table\n            metadata_location (str): The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        raise NotImplementedError\n\n    def drop_table(self, identifier: str | Identifier) -&gt; None:\n        raise NotImplementedError\n\n    def purge_table(self, identifier: str | Identifier) -&gt; None:\n        raise NotImplementedError\n\n    def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n        raise NotImplementedError\n\n    def commit_table(\n        self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        raise NotImplementedError\n\n    def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n        raise NotImplementedError\n\n    def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n        raise NotImplementedError\n\n    def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        raise NotImplementedError\n\n    def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n        raise NotImplementedError\n\n    def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n        raise NotImplementedError\n\n    def update_namespace_properties(\n        self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        raise NotImplementedError\n\n    def list_views(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: str | Identifier) -&gt; bool:\n        raise NotImplementedError\n\n    def namespace_exists(self, namespace: str | Identifier) -&gt; bool:\n        raise NotImplementedError\n\n    def drop_view(self, identifier: str | Identifier) -&gt; None:\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/noop/#pyiceberg.catalog.noop.NoopCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/noop.py</code> <pre><code>def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier (Union[str, Identifier]): Table identifier for the table\n        metadata_location (str): The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/","title":"sql","text":""},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog","title":"<code>SqlCatalog</code>","text":"<p>               Bases: <code>MetastoreCatalog</code></p> <p>Implementation of a SQL based catalog.</p> <p>In the <code>JDBCCatalog</code> implementation, a <code>Namespace</code> is composed of a list of strings separated by dots: <code>'ns1.ns2.ns3'</code>. And you can have as many levels as you want, but you need at least one.  The <code>SqlCatalog</code> honors the same convention.</p> <p>In the <code>JDBCCatalog</code> implementation, a <code>TableIdentifier</code> is composed of an optional <code>Namespace</code> and a table name. When a <code>Namespace</code> is present, the full name will be <code>'ns1.ns2.ns3.table'</code>. A valid <code>TableIdentifier</code> could be <code>'name'</code> (no namespace). The <code>SqlCatalog</code> has a different convention where a <code>TableIdentifier</code> requires a <code>Namespace</code>.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>class SqlCatalog(MetastoreCatalog):\n    \"\"\"Implementation of a SQL based catalog.\n\n    In the `JDBCCatalog` implementation, a `Namespace` is composed of a list of strings separated by dots: `'ns1.ns2.ns3'`.\n    And you can have as many levels as you want, but you need at least one.  The `SqlCatalog` honors the same convention.\n\n    In the `JDBCCatalog` implementation, a `TableIdentifier` is composed of an optional `Namespace` and a table name.\n    When a `Namespace` is present, the full name will be `'ns1.ns2.ns3.table'`.\n    A valid `TableIdentifier` could be `'name'` (no namespace).\n    The `SqlCatalog` has a different convention where a `TableIdentifier` requires a `Namespace`.\n    \"\"\"\n\n    def __init__(self, name: str, **properties: str):\n        super().__init__(name, **properties)\n\n        if not (uri_prop := self.properties.get(URI)):\n            raise NoSuchPropertyException(\"SQL connection URI is required\")\n\n        echo_str = str(self.properties.get(\"echo\", DEFAULT_ECHO_VALUE)).lower()\n        echo = strtobool(echo_str) if echo_str != \"debug\" else \"debug\"\n        pool_pre_ping = strtobool(self.properties.get(\"pool_pre_ping\", DEFAULT_POOL_PRE_PING_VALUE))\n        init_catalog_tables = strtobool(self.properties.get(\"init_catalog_tables\", DEFAULT_INIT_CATALOG_TABLES))\n\n        self.engine = create_engine(uri_prop, echo=echo, pool_pre_ping=pool_pre_ping)\n\n        if init_catalog_tables:\n            self._ensure_tables_exist()\n\n    def _ensure_tables_exist(self) -&gt; None:\n        with Session(self.engine) as session:\n            for table in [IcebergTables, IcebergNamespaceProperties]:\n                stmt = select(1).select_from(table)\n                try:\n                    session.scalar(stmt)\n                except (\n                    OperationalError,\n                    ProgrammingError,\n                ):  # sqlalchemy returns OperationalError in case of sqlite and ProgrammingError with postgres.\n                    self.create_tables()\n                    return\n\n    def create_tables(self) -&gt; None:\n        SqlCatalogBaseTable.metadata.create_all(self.engine)\n\n    def destroy_tables(self) -&gt; None:\n        SqlCatalogBaseTable.metadata.drop_all(self.engine)\n\n    def _convert_orm_to_iceberg(self, orm_table: IcebergTables) -&gt; Table:\n        # Check for expected properties.\n        if not (metadata_location := orm_table.metadata_location):\n            raise NoSuchTableError(f\"Table property {METADATA_LOCATION} is missing\")\n        if not (table_namespace := orm_table.table_namespace):\n            raise NoSuchTableError(f\"Table property {IcebergTables.table_namespace} is missing\")\n        if not (table_name := orm_table.table_name):\n            raise NoSuchTableError(f\"Table property {IcebergTables.table_name} is missing\")\n\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        file = io.new_input(metadata_location)\n        metadata = FromInputFile.table_metadata(file)\n        return Table(\n            identifier=Catalog.identifier_to_tuple(table_namespace) + (table_name,),\n            metadata=metadata,\n            metadata_location=metadata_location,\n            io=self._load_file_io(metadata.properties, metadata_location),\n            catalog=self,\n        )\n\n    def create_table(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        \"\"\"\n        Create an Iceberg table.\n\n        Args:\n            identifier: Table identifier.\n            schema: Table's schema.\n            location: Location for the table. Optional Argument.\n            partition_spec: PartitionSpec for the table.\n            sort_order: SortOrder for the table.\n            properties: Table properties that can be a string based dictionary.\n\n        Returns:\n            Table: the created table instance.\n\n        Raises:\n            AlreadyExistsError: If a table with the name already exists.\n            ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n        \"\"\"\n        schema: Schema = self._convert_schema_if_needed(  # type: ignore\n            schema,\n            int(properties.get(TableProperties.FORMAT_VERSION, TableProperties.DEFAULT_FORMAT_VERSION)),  # type: ignore\n        )\n\n        namespace_identifier = Catalog.namespace_from(identifier)\n        table_name = Catalog.table_name_from(identifier)\n        if not self.namespace_exists(namespace_identifier):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace_identifier}\")\n\n        namespace = Catalog.namespace_to_string(namespace_identifier)\n        location = self._resolve_table_location(location, namespace, table_name)\n        location_provider = load_location_provider(table_location=location, table_properties=properties)\n        metadata_location = location_provider.new_table_metadata_file_location()\n        metadata = new_table_metadata(\n            location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n        )\n        io = load_file_io(properties=self.properties, location=metadata_location)\n        self._write_metadata(metadata, io, metadata_location)\n\n        with Session(self.engine) as session:\n            try:\n                session.add(\n                    IcebergTables(\n                        catalog_name=self.name,\n                        table_namespace=namespace,\n                        table_name=table_name,\n                        metadata_location=metadata_location,\n                        previous_metadata_location=None,\n                    )\n                )\n                session.commit()\n            except IntegrityError as e:\n                raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n        return self.load_table(identifier=identifier)\n\n    def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier (Union[str, Identifier]): Table identifier for the table\n            metadata_location (str): The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n            NoSuchNamespaceError: If namespace does not exist\n        \"\"\"\n        namespace_tuple = Catalog.namespace_from(identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple)\n        table_name = Catalog.table_name_from(identifier)\n        if not self.namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n        with Session(self.engine) as session:\n            try:\n                session.add(\n                    IcebergTables(\n                        catalog_name=self.name,\n                        table_namespace=namespace,\n                        table_name=table_name,\n                        metadata_location=metadata_location,\n                        previous_metadata_location=None,\n                    )\n                )\n                session.commit()\n            except IntegrityError as e:\n                raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n        return self.load_table(identifier=identifier)\n\n    def load_table(self, identifier: str | Identifier) -&gt; Table:\n        \"\"\"Load the table's metadata and return the table instance.\n\n        You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'.\n        Note: This method doesn't scan data stored in the table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Returns:\n            Table: the table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n        namespace_tuple = Catalog.namespace_from(identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple)\n        table_name = Catalog.table_name_from(identifier)\n        with Session(self.engine) as session:\n            stmt = select(IcebergTables).where(\n                IcebergTables.catalog_name == self.name,\n                IcebergTables.table_namespace == namespace,\n                IcebergTables.table_name == table_name,\n            )\n            result = session.scalar(stmt)\n        if result:\n            return self._convert_orm_to_iceberg(result)\n        raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\")\n\n    def drop_table(self, identifier: str | Identifier) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n        \"\"\"\n        namespace_tuple = Catalog.namespace_from(identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple)\n        table_name = Catalog.table_name_from(identifier)\n        with Session(self.engine) as session:\n            if self.engine.dialect.supports_sane_rowcount:\n                res = session.execute(\n                    delete(IcebergTables).where(\n                        IcebergTables.catalog_name == self.name,\n                        IcebergTables.table_namespace == namespace,\n                        IcebergTables.table_name == table_name,\n                    )\n                )\n                if res.rowcount &lt; 1:\n                    raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\")\n            else:\n                try:\n                    tbl = (\n                        session.query(IcebergTables)\n                        .with_for_update(of=IcebergTables)\n                        .filter(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == namespace,\n                            IcebergTables.table_name == table_name,\n                        )\n                        .one()\n                    )\n                    session.delete(tbl)\n                except NoResultFound as e:\n                    raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\") from e\n            session.commit()\n\n    def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n        \"\"\"Rename a fully classified table name.\n\n        Args:\n            from_identifier (str | Identifier): Existing table identifier.\n            to_identifier (str | Identifier): New table identifier.\n\n        Returns:\n            Table: the updated table instance with its metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the name does not exist.\n            TableAlreadyExistsError: If a table with the new name already exist.\n            NoSuchNamespaceError: If the target namespace does not exist.\n        \"\"\"\n        from_namespace_tuple = Catalog.namespace_from(from_identifier)\n        from_namespace = Catalog.namespace_to_string(from_namespace_tuple)\n        from_table_name = Catalog.table_name_from(from_identifier)\n        to_namespace_tuple = Catalog.namespace_from(to_identifier)\n        to_namespace = Catalog.namespace_to_string(to_namespace_tuple)\n        to_table_name = Catalog.table_name_from(to_identifier)\n        if not self.namespace_exists(to_namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {to_namespace}\")\n        with Session(self.engine) as session:\n            try:\n                if self.engine.dialect.supports_sane_rowcount:\n                    stmt = (\n                        update(IcebergTables)\n                        .where(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == from_namespace,\n                            IcebergTables.table_name == from_table_name,\n                        )\n                        .values(table_namespace=to_namespace, table_name=to_table_name)\n                    )\n                    result = session.execute(stmt)\n                    if result.rowcount &lt; 1:\n                        raise NoSuchTableError(f\"Table does not exist: {from_table_name}\")\n                else:\n                    try:\n                        tbl = (\n                            session.query(IcebergTables)\n                            .with_for_update(of=IcebergTables)\n                            .filter(\n                                IcebergTables.catalog_name == self.name,\n                                IcebergTables.table_namespace == from_namespace,\n                                IcebergTables.table_name == from_table_name,\n                            )\n                            .one()\n                        )\n                        tbl.table_namespace = to_namespace\n                        tbl.table_name = to_table_name\n                    except NoResultFound as e:\n                        raise NoSuchTableError(f\"Table does not exist: {from_table_name}\") from e\n                session.commit()\n            except IntegrityError as e:\n                raise TableAlreadyExistsError(f\"Table {to_namespace}.{to_table_name} already exists\") from e\n        return self.load_table(to_identifier)\n\n    def commit_table(\n        self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        \"\"\"\n        table_identifier = table.name()\n        namespace_tuple = Catalog.namespace_from(table_identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple)\n        table_name = Catalog.table_name_from(table_identifier)\n\n        current_table: Table | None\n        try:\n            current_table = self.load_table(table_identifier)\n        except NoSuchTableError:\n            current_table = None\n\n        updated_staged_table = self._update_and_stage_table(current_table, table.name(), requirements, updates)\n        if current_table and updated_staged_table.metadata == current_table.metadata:\n            # no changes, do nothing\n            return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n        self._write_metadata(\n            metadata=updated_staged_table.metadata,\n            io=updated_staged_table.io,\n            metadata_path=updated_staged_table.metadata_location,\n        )\n\n        with Session(self.engine) as session:\n            if current_table:\n                # table exists, update it\n                if self.engine.dialect.supports_sane_rowcount:\n                    stmt = (\n                        update(IcebergTables)\n                        .where(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == namespace,\n                            IcebergTables.table_name == table_name,\n                            IcebergTables.metadata_location == current_table.metadata_location,\n                        )\n                        .values(\n                            metadata_location=updated_staged_table.metadata_location,\n                            previous_metadata_location=current_table.metadata_location,\n                        )\n                    )\n                    result = session.execute(stmt)\n                    if result.rowcount &lt; 1:\n                        raise CommitFailedException(f\"Table has been updated by another process: {namespace}.{table_name}\")\n                else:\n                    try:\n                        tbl = (\n                            session.query(IcebergTables)\n                            .with_for_update(of=IcebergTables)\n                            .filter(\n                                IcebergTables.catalog_name == self.name,\n                                IcebergTables.table_namespace == namespace,\n                                IcebergTables.table_name == table_name,\n                                IcebergTables.metadata_location == current_table.metadata_location,\n                            )\n                            .one()\n                        )\n                        tbl.metadata_location = updated_staged_table.metadata_location\n                        tbl.previous_metadata_location = current_table.metadata_location\n                    except NoResultFound as e:\n                        raise CommitFailedException(f\"Table has been updated by another process: {namespace}.{table_name}\") from e\n                session.commit()\n            else:\n                # table does not exist, create it\n                try:\n                    session.add(\n                        IcebergTables(\n                            catalog_name=self.name,\n                            table_namespace=namespace,\n                            table_name=table_name,\n                            metadata_location=updated_staged_table.metadata_location,\n                            previous_metadata_location=None,\n                        )\n                    )\n                    session.commit()\n                except IntegrityError as e:\n                    raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n        return CommitTableResponse(\n            metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n        )\n\n    def namespace_exists(self, identifier: str | Identifier) -&gt; bool:\n        namespace_tuple = Catalog.identifier_to_tuple(identifier)\n        namespace = Catalog.namespace_to_string(namespace_tuple, NoSuchNamespaceError)\n        namespace_starts_with = namespace.replace(\"!\", \"!!\").replace(\"_\", \"!_\").replace(\"%\", \"!%\") + \".%\"\n\n        with Session(self.engine) as session:\n            stmt = (\n                select(IcebergTables)\n                .where(\n                    IcebergTables.catalog_name == self.name,\n                    (IcebergTables.table_namespace == namespace)\n                    | (IcebergTables.table_namespace.like(namespace_starts_with, escape=\"!\")),\n                )\n                .limit(1)\n            )\n            result = session.execute(stmt).all()\n            if result:\n                return True\n            stmt = (\n                select(IcebergNamespaceProperties)\n                .where(\n                    IcebergNamespaceProperties.catalog_name == self.name,\n                    (IcebergNamespaceProperties.namespace == namespace)\n                    | (IcebergNamespaceProperties.namespace.like(namespace_starts_with, escape=\"!\")),\n                )\n                .limit(1)\n            )\n            result = session.execute(stmt).all()\n            if result:\n                return True\n        return False\n\n    def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n        \"\"\"Create a namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            properties (Properties): A string dictionary of properties for the given namespace.\n\n        Raises:\n            NamespaceAlreadyExistsError: If a namespace with the given name already exists.\n        \"\"\"\n        if self.namespace_exists(namespace):\n            raise NamespaceAlreadyExistsError(f\"Namespace {namespace} already exists\")\n\n        if not properties:\n            properties = IcebergNamespaceProperties.NAMESPACE_MINIMAL_PROPERTIES\n        create_properties = properties if properties else IcebergNamespaceProperties.NAMESPACE_MINIMAL_PROPERTIES\n        with Session(self.engine) as session:\n            for key, value in create_properties.items():\n                session.add(\n                    IcebergNamespaceProperties(\n                        catalog_name=self.name,\n                        namespace=Catalog.namespace_to_string(namespace, NoSuchNamespaceError),\n                        property_key=key,\n                        property_value=value,\n                    )\n                )\n            session.commit()\n\n    def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n        \"\"\"Drop a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n            NamespaceNotEmptyError: If the namespace is not empty.\n        \"\"\"\n        if not self.namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n        namespace_str = Catalog.namespace_to_string(namespace)\n        if tables := self.list_tables(namespace):\n            raise NamespaceNotEmptyError(f\"Namespace {namespace_str} is not empty. {len(tables)} tables exist.\")\n\n        with Session(self.engine) as session:\n            session.execute(\n                delete(IcebergNamespaceProperties).where(\n                    IcebergNamespaceProperties.catalog_name == self.name,\n                    IcebergNamespaceProperties.namespace == namespace_str,\n                )\n            )\n            session.commit()\n\n    def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        \"\"\"List tables under the given namespace in the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: list of table identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n        if namespace and not self.namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n        namespace = Catalog.namespace_to_string(namespace)\n        stmt = select(IcebergTables).where(IcebergTables.catalog_name == self.name, IcebergTables.table_namespace == namespace)\n        with Session(self.engine) as session:\n            result = session.scalars(stmt)\n            return [(Catalog.identifier_to_tuple(table.table_namespace) + (table.table_name,)) for table in result]\n\n    def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n        \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier to search.\n\n        Returns:\n            List[Identifier]: a List of namespace identifiers.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n        if namespace and not self.namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n        table_stmt = select(IcebergTables.table_namespace).where(IcebergTables.catalog_name == self.name)\n        namespace_stmt = select(IcebergNamespaceProperties.namespace).where(IcebergNamespaceProperties.catalog_name == self.name)\n        if namespace:\n            namespace_like = Catalog.namespace_to_string(namespace, NoSuchNamespaceError) + \"%\"\n            table_stmt = table_stmt.where(IcebergTables.table_namespace.like(namespace_like))\n            namespace_stmt = namespace_stmt.where(IcebergNamespaceProperties.namespace.like(namespace_like))\n        stmt = union(\n            table_stmt,\n            namespace_stmt,\n        )\n        with Session(self.engine) as session:\n            namespace_tuple = Catalog.identifier_to_tuple(namespace)\n            sub_namespaces_level_length = len(namespace_tuple) + 1\n\n            namespaces = list(\n                {  # only get distinct namespaces\n                    ns[:sub_namespaces_level_length]  # truncate to the required level\n                    for ns in {Catalog.identifier_to_tuple(ns) for ns in session.execute(stmt).scalars()}\n                    if len(ns) &gt;= sub_namespaces_level_length  # only get sub namespaces/children\n                    and ns[: sub_namespaces_level_length - 1] == namespace_tuple\n                    # exclude fuzzy matches when `namespace` contains `%` or `_`\n                }\n            )\n\n            return namespaces\n\n    def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n        \"\"\"Get properties for a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n\n        Returns:\n            Properties: Properties for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n        \"\"\"\n        namespace_str = Catalog.namespace_to_string(namespace)\n        if not self.namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace {namespace_str} does not exists\")\n\n        stmt = select(IcebergNamespaceProperties).where(\n            IcebergNamespaceProperties.catalog_name == self.name, IcebergNamespaceProperties.namespace == namespace_str\n        )\n        with Session(self.engine) as session:\n            result = session.scalars(stmt)\n            return {props.property_key: props.property_value for props in result}\n\n    def update_namespace_properties(\n        self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        \"\"\"Remove provided property keys and update properties for a namespace.\n\n        Args:\n            namespace (str | Identifier): Namespace identifier.\n            removals (Set[str]): Set of property keys that need to be removed. Optional Argument.\n            updates (Properties): Properties to be updated for the given namespace.\n\n        Raises:\n            NoSuchNamespaceError: If a namespace with the given name does not exist.\n            ValueError: If removals and updates have overlapping keys.\n        \"\"\"\n        namespace_str = Catalog.namespace_to_string(namespace)\n        if not self.namespace_exists(namespace):\n            raise NoSuchNamespaceError(f\"Namespace {namespace_str} does not exists\")\n\n        current_properties = self.load_namespace_properties(namespace=namespace)\n        properties_update_summary = self._get_updated_props_and_update_summary(\n            current_properties=current_properties, removals=removals, updates=updates\n        )[0]\n\n        with Session(self.engine) as session:\n            if removals:\n                delete_stmt = delete(IcebergNamespaceProperties).where(\n                    IcebergNamespaceProperties.catalog_name == self.name,\n                    IcebergNamespaceProperties.namespace == namespace_str,\n                    IcebergNamespaceProperties.property_key.in_(removals),\n                )\n                session.execute(delete_stmt)\n\n            if updates:\n                # SQLAlchemy does not (yet) support engine agnostic UPSERT\n                # https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-upsert-statements\n                # This is not a problem since it runs in a single transaction\n                delete_stmt = delete(IcebergNamespaceProperties).where(\n                    IcebergNamespaceProperties.catalog_name == self.name,\n                    IcebergNamespaceProperties.namespace == namespace_str,\n                    IcebergNamespaceProperties.property_key.in_(set(updates.keys())),\n                )\n                session.execute(delete_stmt)\n                insert_stmt_values = [\n                    {\n                        IcebergNamespaceProperties.catalog_name: self.name,\n                        IcebergNamespaceProperties.namespace: namespace_str,\n                        IcebergNamespaceProperties.property_key: property_key,\n                        IcebergNamespaceProperties.property_value: property_value,\n                    }\n                    for property_key, property_value in updates.items()\n                ]\n                insert_stmt = insert(IcebergNamespaceProperties).values(insert_stmt_values)\n                session.execute(insert_stmt)\n            session.commit()\n        return properties_update_summary\n\n    def list_views(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        raise NotImplementedError\n\n    def view_exists(self, identifier: str | Identifier) -&gt; bool:\n        raise NotImplementedError\n\n    def drop_view(self, identifier: str | Identifier) -&gt; None:\n        raise NotImplementedError\n\n    def close(self) -&gt; None:\n        \"\"\"Close the catalog and release database connections.\n\n        This method closes the SQLAlchemy engine and disposes of all connection pools.\n        This ensures that any cached connections are properly closed, which is especially\n        important for blobfuse scenarios where file handles need to be closed for\n        data to be flushed to persistent storage.\n        \"\"\"\n        if hasattr(self, \"engine\"):\n            self.engine.dispose()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.close","title":"<code>close()</code>","text":"<p>Close the catalog and release database connections.</p> <p>This method closes the SQLAlchemy engine and disposes of all connection pools. This ensures that any cached connections are properly closed, which is especially important for blobfuse scenarios where file handles need to be closed for data to be flushed to persistent storage.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the catalog and release database connections.\n\n    This method closes the SQLAlchemy engine and disposes of all connection pools.\n    This ensures that any cached connections are properly closed, which is especially\n    important for blobfuse scenarios where file handles need to be closed for\n    data to be flushed to persistent storage.\n    \"\"\"\n    if hasattr(self, \"engine\"):\n        self.engine.dispose()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def commit_table(\n    self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n    \"\"\"\n    table_identifier = table.name()\n    namespace_tuple = Catalog.namespace_from(table_identifier)\n    namespace = Catalog.namespace_to_string(namespace_tuple)\n    table_name = Catalog.table_name_from(table_identifier)\n\n    current_table: Table | None\n    try:\n        current_table = self.load_table(table_identifier)\n    except NoSuchTableError:\n        current_table = None\n\n    updated_staged_table = self._update_and_stage_table(current_table, table.name(), requirements, updates)\n    if current_table and updated_staged_table.metadata == current_table.metadata:\n        # no changes, do nothing\n        return CommitTableResponse(metadata=current_table.metadata, metadata_location=current_table.metadata_location)\n    self._write_metadata(\n        metadata=updated_staged_table.metadata,\n        io=updated_staged_table.io,\n        metadata_path=updated_staged_table.metadata_location,\n    )\n\n    with Session(self.engine) as session:\n        if current_table:\n            # table exists, update it\n            if self.engine.dialect.supports_sane_rowcount:\n                stmt = (\n                    update(IcebergTables)\n                    .where(\n                        IcebergTables.catalog_name == self.name,\n                        IcebergTables.table_namespace == namespace,\n                        IcebergTables.table_name == table_name,\n                        IcebergTables.metadata_location == current_table.metadata_location,\n                    )\n                    .values(\n                        metadata_location=updated_staged_table.metadata_location,\n                        previous_metadata_location=current_table.metadata_location,\n                    )\n                )\n                result = session.execute(stmt)\n                if result.rowcount &lt; 1:\n                    raise CommitFailedException(f\"Table has been updated by another process: {namespace}.{table_name}\")\n            else:\n                try:\n                    tbl = (\n                        session.query(IcebergTables)\n                        .with_for_update(of=IcebergTables)\n                        .filter(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == namespace,\n                            IcebergTables.table_name == table_name,\n                            IcebergTables.metadata_location == current_table.metadata_location,\n                        )\n                        .one()\n                    )\n                    tbl.metadata_location = updated_staged_table.metadata_location\n                    tbl.previous_metadata_location = current_table.metadata_location\n                except NoResultFound as e:\n                    raise CommitFailedException(f\"Table has been updated by another process: {namespace}.{table_name}\") from e\n            session.commit()\n        else:\n            # table does not exist, create it\n            try:\n                session.add(\n                    IcebergTables(\n                        catalog_name=self.name,\n                        table_namespace=namespace,\n                        table_name=table_name,\n                        metadata_location=updated_staged_table.metadata_location,\n                        previous_metadata_location=None,\n                    )\n                )\n                session.commit()\n            except IntegrityError as e:\n                raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n    return CommitTableResponse(\n        metadata=updated_staged_table.metadata, metadata_location=updated_staged_table.metadata_location\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.create_namespace","title":"<code>create_namespace(namespace, properties=EMPTY_DICT)</code>","text":"<p>Create a namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>properties</code> <code>Properties</code> <p>A string dictionary of properties for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NamespaceAlreadyExistsError</code> <p>If a namespace with the given name already exists.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n    \"\"\"Create a namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        properties (Properties): A string dictionary of properties for the given namespace.\n\n    Raises:\n        NamespaceAlreadyExistsError: If a namespace with the given name already exists.\n    \"\"\"\n    if self.namespace_exists(namespace):\n        raise NamespaceAlreadyExistsError(f\"Namespace {namespace} already exists\")\n\n    if not properties:\n        properties = IcebergNamespaceProperties.NAMESPACE_MINIMAL_PROPERTIES\n    create_properties = properties if properties else IcebergNamespaceProperties.NAMESPACE_MINIMAL_PROPERTIES\n    with Session(self.engine) as session:\n        for key, value in create_properties.items():\n            session.add(\n                IcebergNamespaceProperties(\n                    catalog_name=self.name,\n                    namespace=Catalog.namespace_to_string(namespace, NoSuchNamespaceError),\n                    property_key=key,\n                    property_value=value,\n                )\n            )\n        session.commit()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.create_table","title":"<code>create_table(identifier, schema, location=None, partition_spec=UNPARTITIONED_PARTITION_SPEC, sort_order=UNSORTED_SORT_ORDER, properties=EMPTY_DICT)</code>","text":"<p>Create an Iceberg table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>schema</code> <code>Union[Schema, Schema]</code> <p>Table's schema.</p> required <code>location</code> <code>str | None</code> <p>Location for the table. Optional Argument.</p> <code>None</code> <code>partition_spec</code> <code>PartitionSpec</code> <p>PartitionSpec for the table.</p> <code>UNPARTITIONED_PARTITION_SPEC</code> <code>sort_order</code> <code>SortOrder</code> <p>SortOrder for the table.</p> <code>UNSORTED_SORT_ORDER</code> <code>properties</code> <code>Properties</code> <p>Table properties that can be a string based dictionary.</p> <code>EMPTY_DICT</code> <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the created table instance.</p> <p>Raises:</p> Type Description <code>AlreadyExistsError</code> <p>If a table with the name already exists.</p> <code>ValueError</code> <p>If the identifier is invalid, or no path is given to store metadata.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def create_table(\n    self,\n    identifier: str | Identifier,\n    schema: Union[Schema, \"pa.Schema\"],\n    location: str | None = None,\n    partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n    sort_order: SortOrder = UNSORTED_SORT_ORDER,\n    properties: Properties = EMPTY_DICT,\n) -&gt; Table:\n    \"\"\"\n    Create an Iceberg table.\n\n    Args:\n        identifier: Table identifier.\n        schema: Table's schema.\n        location: Location for the table. Optional Argument.\n        partition_spec: PartitionSpec for the table.\n        sort_order: SortOrder for the table.\n        properties: Table properties that can be a string based dictionary.\n\n    Returns:\n        Table: the created table instance.\n\n    Raises:\n        AlreadyExistsError: If a table with the name already exists.\n        ValueError: If the identifier is invalid, or no path is given to store metadata.\n\n    \"\"\"\n    schema: Schema = self._convert_schema_if_needed(  # type: ignore\n        schema,\n        int(properties.get(TableProperties.FORMAT_VERSION, TableProperties.DEFAULT_FORMAT_VERSION)),  # type: ignore\n    )\n\n    namespace_identifier = Catalog.namespace_from(identifier)\n    table_name = Catalog.table_name_from(identifier)\n    if not self.namespace_exists(namespace_identifier):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace_identifier}\")\n\n    namespace = Catalog.namespace_to_string(namespace_identifier)\n    location = self._resolve_table_location(location, namespace, table_name)\n    location_provider = load_location_provider(table_location=location, table_properties=properties)\n    metadata_location = location_provider.new_table_metadata_file_location()\n    metadata = new_table_metadata(\n        location=location, schema=schema, partition_spec=partition_spec, sort_order=sort_order, properties=properties\n    )\n    io = load_file_io(properties=self.properties, location=metadata_location)\n    self._write_metadata(metadata, io, metadata_location)\n\n    with Session(self.engine) as session:\n        try:\n            session.add(\n                IcebergTables(\n                    catalog_name=self.name,\n                    table_namespace=namespace,\n                    table_name=table_name,\n                    metadata_location=metadata_location,\n                    previous_metadata_location=None,\n                )\n            )\n            session.commit()\n        except IntegrityError as e:\n            raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.drop_namespace","title":"<code>drop_namespace(namespace)</code>","text":"<p>Drop a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> <code>NamespaceNotEmptyError</code> <p>If the namespace is not empty.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n    \"\"\"Drop a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n        NamespaceNotEmptyError: If the namespace is not empty.\n    \"\"\"\n    if not self.namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n    namespace_str = Catalog.namespace_to_string(namespace)\n    if tables := self.list_tables(namespace):\n        raise NamespaceNotEmptyError(f\"Namespace {namespace_str} is not empty. {len(tables)} tables exist.\")\n\n    with Session(self.engine) as session:\n        session.execute(\n            delete(IcebergNamespaceProperties).where(\n                IcebergNamespaceProperties.catalog_name == self.name,\n                IcebergNamespaceProperties.namespace == namespace_str,\n            )\n        )\n        session.commit()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.drop_table","title":"<code>drop_table(identifier)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def drop_table(self, identifier: str | Identifier) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n    namespace_tuple = Catalog.namespace_from(identifier)\n    namespace = Catalog.namespace_to_string(namespace_tuple)\n    table_name = Catalog.table_name_from(identifier)\n    with Session(self.engine) as session:\n        if self.engine.dialect.supports_sane_rowcount:\n            res = session.execute(\n                delete(IcebergTables).where(\n                    IcebergTables.catalog_name == self.name,\n                    IcebergTables.table_namespace == namespace,\n                    IcebergTables.table_name == table_name,\n                )\n            )\n            if res.rowcount &lt; 1:\n                raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\")\n        else:\n            try:\n                tbl = (\n                    session.query(IcebergTables)\n                    .with_for_update(of=IcebergTables)\n                    .filter(\n                        IcebergTables.catalog_name == self.name,\n                        IcebergTables.table_namespace == namespace,\n                        IcebergTables.table_name == table_name,\n                    )\n                    .one()\n                )\n                session.delete(tbl)\n            except NoResultFound as e:\n                raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\") from e\n        session.commit()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.list_namespaces","title":"<code>list_namespaces(namespace=())</code>","text":"<p>List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> <code>()</code> <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: a List of namespace identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n    \"\"\"List namespaces from the given namespace. If not given, list top-level namespaces from the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: a List of namespace identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n    if namespace and not self.namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n    table_stmt = select(IcebergTables.table_namespace).where(IcebergTables.catalog_name == self.name)\n    namespace_stmt = select(IcebergNamespaceProperties.namespace).where(IcebergNamespaceProperties.catalog_name == self.name)\n    if namespace:\n        namespace_like = Catalog.namespace_to_string(namespace, NoSuchNamespaceError) + \"%\"\n        table_stmt = table_stmt.where(IcebergTables.table_namespace.like(namespace_like))\n        namespace_stmt = namespace_stmt.where(IcebergNamespaceProperties.namespace.like(namespace_like))\n    stmt = union(\n        table_stmt,\n        namespace_stmt,\n    )\n    with Session(self.engine) as session:\n        namespace_tuple = Catalog.identifier_to_tuple(namespace)\n        sub_namespaces_level_length = len(namespace_tuple) + 1\n\n        namespaces = list(\n            {  # only get distinct namespaces\n                ns[:sub_namespaces_level_length]  # truncate to the required level\n                for ns in {Catalog.identifier_to_tuple(ns) for ns in session.execute(stmt).scalars()}\n                if len(ns) &gt;= sub_namespaces_level_length  # only get sub namespaces/children\n                and ns[: sub_namespaces_level_length - 1] == namespace_tuple\n                # exclude fuzzy matches when `namespace` contains `%` or `_`\n            }\n        )\n\n        return namespaces\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.list_tables","title":"<code>list_tables(namespace)</code>","text":"<p>List tables under the given namespace in the catalog.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier to search.</p> required <p>Returns:</p> Type Description <code>list[Identifier]</code> <p>List[Identifier]: list of table identifiers.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n    \"\"\"List tables under the given namespace in the catalog.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier to search.\n\n    Returns:\n        List[Identifier]: list of table identifiers.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n    if namespace and not self.namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n    namespace = Catalog.namespace_to_string(namespace)\n    stmt = select(IcebergTables).where(IcebergTables.catalog_name == self.name, IcebergTables.table_namespace == namespace)\n    with Session(self.engine) as session:\n        result = session.scalars(stmt)\n        return [(Catalog.identifier_to_tuple(table.table_namespace) + (table.table_name,)) for table in result]\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.load_namespace_properties","title":"<code>load_namespace_properties(namespace)</code>","text":"<p>Get properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <p>Returns:</p> Name Type Description <code>Properties</code> <code>Properties</code> <p>Properties for the given namespace.</p> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n    \"\"\"Get properties for a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n\n    Returns:\n        Properties: Properties for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n    \"\"\"\n    namespace_str = Catalog.namespace_to_string(namespace)\n    if not self.namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace {namespace_str} does not exists\")\n\n    stmt = select(IcebergNamespaceProperties).where(\n        IcebergNamespaceProperties.catalog_name == self.name, IcebergNamespaceProperties.namespace == namespace_str\n    )\n    with Session(self.engine) as session:\n        result = session.scalars(stmt)\n        return {props.property_key: props.property_value for props in result}\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.load_table","title":"<code>load_table(identifier)</code>","text":"<p>Load the table's metadata and return the table instance.</p> <p>You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'. Note: This method doesn't scan data stored in the table.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def load_table(self, identifier: str | Identifier) -&gt; Table:\n    \"\"\"Load the table's metadata and return the table instance.\n\n    You can also use this method to check for table existence using 'try catalog.table() except NoSuchTableError'.\n    Note: This method doesn't scan data stored in the table.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Returns:\n        Table: the table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n    \"\"\"\n    namespace_tuple = Catalog.namespace_from(identifier)\n    namespace = Catalog.namespace_to_string(namespace_tuple)\n    table_name = Catalog.table_name_from(identifier)\n    with Session(self.engine) as session:\n        stmt = select(IcebergTables).where(\n            IcebergTables.catalog_name == self.name,\n            IcebergTables.table_namespace == namespace,\n            IcebergTables.table_name == table_name,\n        )\n        result = session.scalar(stmt)\n    if result:\n        return self._convert_orm_to_iceberg(result)\n    raise NoSuchTableError(f\"Table does not exist: {namespace}.{table_name}\")\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> <code>NoSuchNamespaceError</code> <p>If namespace does not exist</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier (Union[str, Identifier]): Table identifier for the table\n        metadata_location (str): The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n        NoSuchNamespaceError: If namespace does not exist\n    \"\"\"\n    namespace_tuple = Catalog.namespace_from(identifier)\n    namespace = Catalog.namespace_to_string(namespace_tuple)\n    table_name = Catalog.table_name_from(identifier)\n    if not self.namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {namespace}\")\n\n    with Session(self.engine) as session:\n        try:\n            session.add(\n                IcebergTables(\n                    catalog_name=self.name,\n                    table_namespace=namespace,\n                    table_name=table_name,\n                    metadata_location=metadata_location,\n                    previous_metadata_location=None,\n                )\n            )\n            session.commit()\n        except IntegrityError as e:\n            raise TableAlreadyExistsError(f\"Table {namespace}.{table_name} already exists\") from e\n\n    return self.load_table(identifier=identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.rename_table","title":"<code>rename_table(from_identifier, to_identifier)</code>","text":"<p>Rename a fully classified table name.</p> <p>Parameters:</p> Name Type Description Default <code>from_identifier</code> <code>str | Identifier</code> <p>Existing table identifier.</p> required <code>to_identifier</code> <code>str | Identifier</code> <p>New table identifier.</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>the updated table instance with its metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the name does not exist.</p> <code>TableAlreadyExistsError</code> <p>If a table with the new name already exist.</p> <code>NoSuchNamespaceError</code> <p>If the target namespace does not exist.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n    \"\"\"Rename a fully classified table name.\n\n    Args:\n        from_identifier (str | Identifier): Existing table identifier.\n        to_identifier (str | Identifier): New table identifier.\n\n    Returns:\n        Table: the updated table instance with its metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the name does not exist.\n        TableAlreadyExistsError: If a table with the new name already exist.\n        NoSuchNamespaceError: If the target namespace does not exist.\n    \"\"\"\n    from_namespace_tuple = Catalog.namespace_from(from_identifier)\n    from_namespace = Catalog.namespace_to_string(from_namespace_tuple)\n    from_table_name = Catalog.table_name_from(from_identifier)\n    to_namespace_tuple = Catalog.namespace_from(to_identifier)\n    to_namespace = Catalog.namespace_to_string(to_namespace_tuple)\n    to_table_name = Catalog.table_name_from(to_identifier)\n    if not self.namespace_exists(to_namespace):\n        raise NoSuchNamespaceError(f\"Namespace does not exist: {to_namespace}\")\n    with Session(self.engine) as session:\n        try:\n            if self.engine.dialect.supports_sane_rowcount:\n                stmt = (\n                    update(IcebergTables)\n                    .where(\n                        IcebergTables.catalog_name == self.name,\n                        IcebergTables.table_namespace == from_namespace,\n                        IcebergTables.table_name == from_table_name,\n                    )\n                    .values(table_namespace=to_namespace, table_name=to_table_name)\n                )\n                result = session.execute(stmt)\n                if result.rowcount &lt; 1:\n                    raise NoSuchTableError(f\"Table does not exist: {from_table_name}\")\n            else:\n                try:\n                    tbl = (\n                        session.query(IcebergTables)\n                        .with_for_update(of=IcebergTables)\n                        .filter(\n                            IcebergTables.catalog_name == self.name,\n                            IcebergTables.table_namespace == from_namespace,\n                            IcebergTables.table_name == from_table_name,\n                        )\n                        .one()\n                    )\n                    tbl.table_namespace = to_namespace\n                    tbl.table_name = to_table_name\n                except NoResultFound as e:\n                    raise NoSuchTableError(f\"Table does not exist: {from_table_name}\") from e\n            session.commit()\n        except IntegrityError as e:\n            raise TableAlreadyExistsError(f\"Table {to_namespace}.{to_table_name} already exists\") from e\n    return self.load_table(to_identifier)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/sql/#pyiceberg.catalog.sql.SqlCatalog.update_namespace_properties","title":"<code>update_namespace_properties(namespace, removals=None, updates=EMPTY_DICT)</code>","text":"<p>Remove provided property keys and update properties for a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>str | Identifier</code> <p>Namespace identifier.</p> required <code>removals</code> <code>Set[str]</code> <p>Set of property keys that need to be removed. Optional Argument.</p> <code>None</code> <code>updates</code> <code>Properties</code> <p>Properties to be updated for the given namespace.</p> <code>EMPTY_DICT</code> <p>Raises:</p> Type Description <code>NoSuchNamespaceError</code> <p>If a namespace with the given name does not exist.</p> <code>ValueError</code> <p>If removals and updates have overlapping keys.</p> Source code in <code>pyiceberg/catalog/sql.py</code> <pre><code>def update_namespace_properties(\n    self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n) -&gt; PropertiesUpdateSummary:\n    \"\"\"Remove provided property keys and update properties for a namespace.\n\n    Args:\n        namespace (str | Identifier): Namespace identifier.\n        removals (Set[str]): Set of property keys that need to be removed. Optional Argument.\n        updates (Properties): Properties to be updated for the given namespace.\n\n    Raises:\n        NoSuchNamespaceError: If a namespace with the given name does not exist.\n        ValueError: If removals and updates have overlapping keys.\n    \"\"\"\n    namespace_str = Catalog.namespace_to_string(namespace)\n    if not self.namespace_exists(namespace):\n        raise NoSuchNamespaceError(f\"Namespace {namespace_str} does not exists\")\n\n    current_properties = self.load_namespace_properties(namespace=namespace)\n    properties_update_summary = self._get_updated_props_and_update_summary(\n        current_properties=current_properties, removals=removals, updates=updates\n    )[0]\n\n    with Session(self.engine) as session:\n        if removals:\n            delete_stmt = delete(IcebergNamespaceProperties).where(\n                IcebergNamespaceProperties.catalog_name == self.name,\n                IcebergNamespaceProperties.namespace == namespace_str,\n                IcebergNamespaceProperties.property_key.in_(removals),\n            )\n            session.execute(delete_stmt)\n\n        if updates:\n            # SQLAlchemy does not (yet) support engine agnostic UPSERT\n            # https://docs.sqlalchemy.org/en/20/orm/queryguide/dml.html#orm-upsert-statements\n            # This is not a problem since it runs in a single transaction\n            delete_stmt = delete(IcebergNamespaceProperties).where(\n                IcebergNamespaceProperties.catalog_name == self.name,\n                IcebergNamespaceProperties.namespace == namespace_str,\n                IcebergNamespaceProperties.property_key.in_(set(updates.keys())),\n            )\n            session.execute(delete_stmt)\n            insert_stmt_values = [\n                {\n                    IcebergNamespaceProperties.catalog_name: self.name,\n                    IcebergNamespaceProperties.namespace: namespace_str,\n                    IcebergNamespaceProperties.property_key: property_key,\n                    IcebergNamespaceProperties.property_value: property_value,\n                }\n                for property_key, property_value in updates.items()\n            ]\n            insert_stmt = insert(IcebergNamespaceProperties).values(insert_stmt_values)\n            session.execute(insert_stmt)\n        session.commit()\n    return properties_update_summary\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/","title":"rest","text":""},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.Endpoint","title":"<code>Endpoint</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>class Endpoint(IcebergBaseModel):\n    model_config = ConfigDict(frozen=True)\n\n    http_method: HttpMethod = Field()\n    path: str = Field()\n\n    @field_validator(\"path\", mode=\"before\")\n    @classmethod\n    def _validate_path(cls, raw_path: str) -&gt; str:\n        raw_path = raw_path.strip()\n        if not raw_path:\n            raise ValueError(\"Invalid path: empty\")\n        return raw_path\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Endpoint class.\"\"\"\n        return f\"{self.http_method.value} {self.path}\"\n\n    @classmethod\n    def from_string(cls, endpoint: str) -&gt; \"Endpoint\":\n        elements = endpoint.strip().split(None, 1)\n        if len(elements) != 2:\n            raise ValueError(f\"Invalid endpoint (must consist of two elements separated by a single space): {endpoint}\")\n        return cls(http_method=HttpMethod(elements[0].upper()), path=elements[1])\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.Endpoint.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Endpoint class.</p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Endpoint class.\"\"\"\n    return f\"{self.http_method.value} {self.path}\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog","title":"<code>RestCatalog</code>","text":"<p>               Bases: <code>Catalog</code></p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>class RestCatalog(Catalog):\n    uri: str\n    _session: Session\n    _auth_manager: AuthManager | None\n    _supported_endpoints: set[Endpoint]\n    _namespace_separator: str\n\n    def __init__(self, name: str, **properties: str):\n        \"\"\"Rest Catalog.\n\n        You either need to provide a client_id and client_secret, or an already valid token.\n\n        Args:\n            name: Name to identify the catalog.\n            properties: Properties that are passed along to the configuration.\n        \"\"\"\n        super().__init__(name, **properties)\n        self._auth_manager: AuthManager | None = None\n        self.uri = properties[URI]\n        self._fetch_config()\n        self._session = self._create_session()\n\n    def _create_session(self) -&gt; Session:\n        \"\"\"Create a request session with provided catalog configuration.\"\"\"\n        session = Session()\n\n        # Set HTTP headers\n        self._config_headers(session)\n\n        # Sets the client side and server side SSL cert verification, if provided as properties.\n        if ssl_config := self.properties.get(SSL):\n            if ssl_ca_bundle := ssl_config.get(CA_BUNDLE):\n                session.verify = ssl_ca_bundle\n            if ssl_client := ssl_config.get(CLIENT):\n                if all(k in ssl_client for k in (CERT, KEY)):\n                    session.cert = (ssl_client[CERT], ssl_client[KEY])\n                elif ssl_client_cert := ssl_client.get(CERT):\n                    session.cert = ssl_client_cert\n\n        if auth_config := self.properties.get(AUTH):\n            auth_type = auth_config.get(\"type\")\n            if auth_type is None:\n                raise ValueError(\"auth.type must be defined\")\n            auth_type_config = auth_config.get(auth_type, {})\n            auth_impl = auth_config.get(\"impl\")\n\n            if auth_type == CUSTOM and not auth_impl:\n                raise ValueError(\"auth.impl must be specified when using custom auth.type\")\n\n            if auth_type != CUSTOM and auth_impl:\n                raise ValueError(\"auth.impl can only be specified when using custom auth.type\")\n\n            self._auth_manager = AuthManagerFactory.create(auth_impl or auth_type, auth_type_config)\n            session.auth = AuthManagerAdapter(self._auth_manager)\n        else:\n            self._auth_manager = self._create_legacy_oauth2_auth_manager(session)\n            session.auth = AuthManagerAdapter(self._auth_manager)\n\n        # Configure SigV4 Request Signing\n        if property_as_bool(self.properties, SIGV4, False):\n            self._init_sigv4(session)\n\n        return session\n\n    def _load_file_io(self, properties: Properties = EMPTY_DICT, location: str | None = None) -&gt; FileIO:\n        merged_properties = {**self.properties, **properties}\n        if self._auth_manager:\n            merged_properties[AUTH_MANAGER] = self._auth_manager\n        return load_file_io(merged_properties, location)\n\n    def supports_server_side_planning(self) -&gt; bool:\n        \"\"\"Check if the catalog supports server-side scan planning.\"\"\"\n        return Capability.V1_SUBMIT_TABLE_SCAN_PLAN in self._supported_endpoints and property_as_bool(\n            self.properties, REST_SCAN_PLANNING_ENABLED, REST_SCAN_PLANNING_ENABLED_DEFAULT\n        )\n\n    @retry(**_RETRY_ARGS)\n    def _plan_table_scan(self, identifier: str | Identifier, request: PlanTableScanRequest) -&gt; PlanningResponse:\n        \"\"\"Submit a scan plan request to the REST server.\n\n        Args:\n            identifier: Table identifier.\n            request: The scan plan request parameters.\n\n        Returns:\n            PlanningResponse the result of the scan plan request representing the status\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n        \"\"\"\n        self._check_endpoint(Capability.V1_SUBMIT_TABLE_SCAN_PLAN)\n        response = self._session.post(\n            self.url(Endpoints.plan_table_scan, prefixed=True, **self._split_identifier_for_path(identifier)),\n            data=request.model_dump_json(by_alias=True, exclude_none=True).encode(UTF8),\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchTableError})\n\n        return _PLANNING_RESPONSE_ADAPTER.validate_json(response.text)\n\n    @retry(**_RETRY_ARGS)\n    def _fetch_scan_tasks(self, identifier: str | Identifier, plan_task: str) -&gt; ScanTasks:\n        \"\"\"Fetch additional scan tasks using a plan task token.\n\n        Args:\n            identifier: Table identifier.\n            plan_task: The plan task token from a previous response.\n\n        Returns:\n            ScanTasks containing file scan tasks and possibly more plan-task tokens.\n\n        Raises:\n            NoSuchPlanTaskError: If a plan task with the given identifier or task does not exist.\n        \"\"\"\n        self._check_endpoint(Capability.V1_TABLE_SCAN_PLAN_TASKS)\n        request = FetchScanTasksRequest(plan_task=plan_task)\n        response = self._session.post(\n            self.url(Endpoints.fetch_scan_tasks, prefixed=True, **self._split_identifier_for_path(identifier)),\n            data=request.model_dump_json(by_alias=True).encode(UTF8),\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchPlanTaskError})\n\n        return ScanTasks.model_validate_json(response.text)\n\n    def plan_scan(self, identifier: str | Identifier, request: PlanTableScanRequest) -&gt; list[FileScanTask]:\n        \"\"\"Plan a table scan and return FileScanTasks.\n\n        Handles the full scan planning lifecycle including pagination.\n\n        Args:\n            identifier: Table identifier.\n            request: The scan plan request parameters.\n\n        Returns:\n            List of FileScanTask objects ready for execution.\n\n        Raises:\n            RuntimeError: If planning fails, is cancelled, or returns unexpected response.\n            NotImplementedError: If async planning is required but not yet supported.\n        \"\"\"\n        response = self._plan_table_scan(identifier, request)\n\n        if isinstance(response, PlanFailed):\n            error_msg = response.error.message if response.error else \"unknown error\"\n            raise RuntimeError(f\"Received status: failed: {error_msg}\")\n\n        if isinstance(response, PlanCancelled):\n            raise RuntimeError(\"Received status: cancelled\")\n\n        if isinstance(response, PlanSubmitted):\n            # TODO: implement polling for async planning\n            raise NotImplementedError(f\"Async scan planning not yet supported for planId: {response.plan_id}\")\n\n        if not isinstance(response, PlanCompleted):\n            raise RuntimeError(f\"Invalid planStatus for response: {type(response).__name__}\")\n\n        tasks: list[FileScanTask] = []\n\n        # Collect tasks from initial response\n        for task in response.file_scan_tasks:\n            tasks.append(FileScanTask.from_rest_response(task, response.delete_files))\n\n        # Fetch and collect from additional batches\n        pending_tasks = deque(response.plan_tasks)\n        while pending_tasks:\n            plan_task = pending_tasks.popleft()\n            batch = self._fetch_scan_tasks(identifier, plan_task)\n            for task in batch.file_scan_tasks:\n                tasks.append(FileScanTask.from_rest_response(task, batch.delete_files))\n            pending_tasks.extend(batch.plan_tasks)\n\n        return tasks\n\n    def _create_legacy_oauth2_auth_manager(self, session: Session) -&gt; AuthManager:\n        \"\"\"Create the LegacyOAuth2AuthManager by fetching required properties.\n\n        This will be removed in PyIceberg 1.0\n        \"\"\"\n        client_credentials = self.properties.get(CREDENTIAL)\n        # We want to call `self.auth_url` only when we are using CREDENTIAL\n        # with the legacy OAUTH2 flow as it will raise a DeprecationWarning\n        auth_url = self.auth_url if client_credentials is not None else None\n\n        auth_config = {\n            \"session\": session,\n            \"auth_url\": auth_url,\n            \"credential\": client_credentials,\n            \"initial_token\": self.properties.get(TOKEN),\n            \"optional_oauth_params\": self._extract_optional_oauth_params(),\n        }\n\n        return AuthManagerFactory.create(\"legacyoauth2\", auth_config)\n\n    def _check_valid_namespace_identifier(self, identifier: str | Identifier) -&gt; Identifier:\n        \"\"\"Check if the identifier has at least one element.\"\"\"\n        identifier_tuple = Catalog.identifier_to_tuple(identifier)\n        if len(identifier_tuple) &lt; 1:\n            raise NoSuchNamespaceError(f\"Empty namespace identifier: {identifier}\")\n        return identifier_tuple\n\n    def url(self, endpoint: str, prefixed: bool = True, **kwargs: Any) -&gt; str:\n        \"\"\"Construct the endpoint.\n\n        Args:\n            endpoint: Resource identifier that points to the REST catalog.\n            prefixed: If the prefix return by the config needs to be appended.\n\n        Returns:\n            The base url of the rest catalog.\n        \"\"\"\n        url = self.uri\n        url = url + \"v1/\" if url.endswith(\"/\") else url + \"/v1/\"\n\n        if prefixed:\n            url += self.properties.get(PREFIX, \"\")\n            url = url if url.endswith(\"/\") else url + \"/\"\n\n        return url + endpoint.format(**kwargs)\n\n    def _check_endpoint(self, endpoint: Endpoint) -&gt; None:\n        \"\"\"Check if an endpoint is supported by the server.\n\n        Args:\n            endpoint: The endpoint to check against the set of supported endpoints\n\n        Raises:\n            NotImplementedError: If the endpoint is not supported.\n        \"\"\"\n        if endpoint not in self._supported_endpoints:\n            raise NotImplementedError(f\"Server does not support endpoint: {endpoint}\")\n\n    @property\n    def auth_url(self) -&gt; str:\n        self._warn_oauth_tokens_deprecation()\n\n        if url := self.properties.get(OAUTH2_SERVER_URI):\n            return url\n        else:\n            return self.url(Endpoints.get_token, prefixed=False)\n\n    def _warn_oauth_tokens_deprecation(self) -&gt; None:\n        has_oauth_server_uri = OAUTH2_SERVER_URI in self.properties\n        has_credential = CREDENTIAL in self.properties\n        has_init_token = TOKEN in self.properties\n        has_sigv4_enabled = property_as_bool(self.properties, SIGV4, False)\n\n        if not has_oauth_server_uri and (has_init_token or has_credential) and not has_sigv4_enabled:\n            deprecation_message(\n                deprecated_in=\"0.8.0\",\n                removed_in=\"1.0.0\",\n                help_message=\"Iceberg REST client is missing the OAuth2 server URI \"\n                f\"configuration and defaults to {self.uri}{Endpoints.get_token}. \"\n                \"This automatic fallback will be removed in a future Iceberg release.\"\n                f\"It is recommended to configure the OAuth2 endpoint using the '{OAUTH2_SERVER_URI}'\"\n                \"property to be prepared. This warning will disappear if the OAuth2\"\n                \"endpoint is explicitly configured. See https://github.com/apache/iceberg/issues/10537\",\n            )\n\n    def _extract_optional_oauth_params(self) -&gt; dict[str, str]:\n        optional_oauth_param = {SCOPE: self.properties.get(SCOPE) or CATALOG_SCOPE}\n        set_of_optional_params = {AUDIENCE, RESOURCE}\n        for param in set_of_optional_params:\n            if param_value := self.properties.get(param):\n                optional_oauth_param[param] = param_value\n\n        return optional_oauth_param\n\n    def _encode_namespace_path(self, namespace: Identifier) -&gt; str:\n        \"\"\"\n        Encode a namespace for use as a path parameter in a URL.\n\n        Each part of the namespace is URL-encoded using `urllib.parse.quote`\n        (ensuring characters like '/' are encoded) and then joined by the\n        configured namespace separator.\n        \"\"\"\n        return self._namespace_separator.join(quote(part, safe=\"\") for part in namespace)\n\n    def _fetch_config(self) -&gt; None:\n        params = {}\n        if warehouse_location := self.properties.get(WAREHOUSE_LOCATION):\n            params[WAREHOUSE_LOCATION] = warehouse_location\n\n        with self._create_session() as session:\n            response = session.get(self.url(Endpoints.get_config, prefixed=False), params=params)\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {})\n        config_response = ConfigResponse.model_validate_json(response.text)\n\n        config = config_response.defaults\n        config.update(self.properties)\n        config.update(config_response.overrides)\n        self.properties = config\n\n        # Update URI based on overrides\n        self.uri = config[URI]\n\n        # Determine supported endpoints\n        endpoints = config_response.endpoints\n        if endpoints:\n            self._supported_endpoints = set(endpoints)\n        else:\n            # Use default endpoints for legacy servers that don't return endpoints\n            self._supported_endpoints = set(DEFAULT_ENDPOINTS)\n            # Conditionally add view endpoints based on config\n            if property_as_bool(self.properties, VIEW_ENDPOINTS_SUPPORTED, VIEW_ENDPOINTS_SUPPORTED_DEFAULT):\n                self._supported_endpoints.update(VIEW_ENDPOINTS)\n\n        separator_from_properties = self.properties.get(NAMESPACE_SEPARATOR_PROPERTY, DEFAULT_NAMESPACE_SEPARATOR)\n        if not separator_from_properties:\n            raise ValueError(\"Namespace separator cannot be an empty string\")\n        self._namespace_separator = unquote(separator_from_properties)\n\n    def _identifier_to_validated_tuple(self, identifier: str | Identifier) -&gt; Identifier:\n        identifier_tuple = self.identifier_to_tuple(identifier)\n        if len(identifier_tuple) &lt;= 1:\n            raise NoSuchIdentifierError(f\"Missing namespace or invalid identifier: {'.'.join(identifier_tuple)}\")\n        return identifier_tuple\n\n    def _split_identifier_for_path(\n        self, identifier: str | Identifier | TableIdentifier, kind: IdentifierKind = IdentifierKind.TABLE\n    ) -&gt; Properties:\n        if isinstance(identifier, TableIdentifier):\n            return {\n                \"namespace\": self._encode_namespace_path(tuple(identifier.namespace.root)),\n                kind.value: quote(identifier.name, safe=\"\"),\n            }\n        identifier_tuple = self._identifier_to_validated_tuple(identifier)\n\n        # Use quote to ensure that '/' aren't treated as path separators.\n        return {\n            \"namespace\": self._encode_namespace_path(identifier_tuple[:-1]),\n            kind.value: quote(identifier_tuple[-1], safe=\"\"),\n        }\n\n    def _split_identifier_for_json(self, identifier: str | Identifier) -&gt; dict[str, Identifier | str]:\n        identifier_tuple = self._identifier_to_validated_tuple(identifier)\n        return {\"namespace\": identifier_tuple[:-1], \"name\": identifier_tuple[-1]}\n\n    def _init_sigv4(self, session: Session) -&gt; None:\n        from urllib import parse\n\n        import boto3\n        from botocore.auth import SigV4Auth\n        from botocore.awsrequest import AWSRequest\n        from requests import PreparedRequest\n        from requests.adapters import HTTPAdapter\n\n        class SigV4Adapter(HTTPAdapter):\n            def __init__(self, **properties: str):\n                super().__init__()\n                self._properties = properties\n                self._boto_session = boto3.Session(\n                    region_name=get_first_property_value(self._properties, AWS_REGION),\n                    botocore_session=self._properties.get(BOTOCORE_SESSION),\n                    aws_access_key_id=get_first_property_value(self._properties, AWS_ACCESS_KEY_ID),\n                    aws_secret_access_key=get_first_property_value(self._properties, AWS_SECRET_ACCESS_KEY),\n                    aws_session_token=get_first_property_value(self._properties, AWS_SESSION_TOKEN),\n                )\n\n            def add_headers(self, request: PreparedRequest, **kwargs: Any) -&gt; None:  # pylint: disable=W0613\n                credentials = self._boto_session.get_credentials().get_frozen_credentials()\n                region = self._properties.get(SIGV4_REGION, self._boto_session.region_name)\n                service = self._properties.get(SIGV4_SERVICE, \"execute-api\")\n\n                url = str(request.url).split(\"?\")[0]\n                query = str(parse.urlsplit(request.url).query)\n                params = dict(parse.parse_qsl(query))\n\n                # remove the connection header as it will be updated after signing\n                if \"connection\" in request.headers:\n                    del request.headers[\"connection\"]\n                # For empty bodies, explicitly set the content hash header to the SHA256 of an empty string\n                if not request.body:\n                    request.headers[\"x-amz-content-sha256\"] = EMPTY_BODY_SHA256\n\n                aws_request = AWSRequest(\n                    method=request.method, url=url, params=params, data=request.body, headers=dict(request.headers)\n                )\n\n                SigV4Auth(credentials, service, region).add_auth(aws_request)\n                original_header = request.headers\n                signed_headers = aws_request.headers\n                relocated_headers = {}\n\n                # relocate headers if there is a conflict with signed headers\n                for header, value in original_header.items():\n                    if header in signed_headers and signed_headers[header] != value:\n                        relocated_headers[f\"Original-{header}\"] = value\n\n                request.headers.update(relocated_headers)\n                request.headers.update(signed_headers)\n\n        session.mount(self.uri, SigV4Adapter(**self.properties))\n\n    def _response_to_table(self, identifier_tuple: tuple[str, ...], table_response: TableResponse) -&gt; Table:\n        return Table(\n            identifier=identifier_tuple,\n            metadata_location=table_response.metadata_location,  # type: ignore\n            metadata=table_response.metadata,\n            io=self._load_file_io(\n                {**table_response.metadata.properties, **table_response.config}, table_response.metadata_location\n            ),\n            catalog=self,\n            config=table_response.config,\n        )\n\n    def _response_to_staged_table(self, identifier_tuple: tuple[str, ...], table_response: TableResponse) -&gt; StagedTable:\n        return StagedTable(\n            identifier=identifier_tuple,\n            metadata_location=table_response.metadata_location,  # type: ignore\n            metadata=table_response.metadata,\n            io=self._load_file_io(\n                {**table_response.metadata.properties, **table_response.config}, table_response.metadata_location\n            ),\n            catalog=self,\n        )\n\n    def _refresh_token(self) -&gt; None:\n        # Reactive token refresh is atypical - we should proactively refresh tokens in a separate thread\n        # instead of retrying on Auth Exceptions. Keeping refresh behavior for the LegacyOAuth2AuthManager\n        # for backward compatibility\n        auth_manager = self._session.auth.auth_manager  # type: ignore[union-attr]\n        if isinstance(auth_manager, LegacyOAuth2AuthManager):\n            auth_manager._refresh_token()\n\n    def _config_headers(self, session: Session) -&gt; None:\n        header_properties = get_header_properties(self.properties)\n        session.headers.update(header_properties)\n        session.headers[\"Content-type\"] = \"application/json\"\n        session.headers[\"User-Agent\"] = f\"PyIceberg/{__version__}\"\n        session.headers[\"X-Client-Version\"] = f\"PyIceberg {__version__}\"\n        session.headers.setdefault(\"X-Iceberg-Access-Delegation\", ACCESS_DELEGATION_DEFAULT)\n\n    def _create_table(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n        stage_create: bool = False,\n    ) -&gt; TableResponse:\n        self._check_endpoint(Capability.V1_CREATE_TABLE)\n        iceberg_schema = self._convert_schema_if_needed(\n            schema,\n            int(properties.get(TableProperties.FORMAT_VERSION, TableProperties.DEFAULT_FORMAT_VERSION)),  # type: ignore\n        )\n        fresh_schema = assign_fresh_schema_ids(iceberg_schema)\n        fresh_partition_spec = assign_fresh_partition_spec_ids(partition_spec, iceberg_schema, fresh_schema)\n        fresh_sort_order = assign_fresh_sort_order_ids(sort_order, iceberg_schema, fresh_schema)\n\n        namespace_and_table = self._split_identifier_for_path(identifier)\n        if location:\n            location = location.rstrip(\"/\")\n        request = CreateTableRequest(\n            name=self._identifier_to_validated_tuple(identifier)[-1],\n            location=location,\n            table_schema=fresh_schema,\n            partition_spec=fresh_partition_spec,\n            write_order=fresh_sort_order,\n            stage_create=stage_create,\n            properties=properties,\n        )\n        serialized_json = request.model_dump_json().encode(UTF8)\n        response = self._session.post(\n            self.url(Endpoints.create_table, namespace=namespace_and_table[\"namespace\"]),\n            data=serialized_json,\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {409: TableAlreadyExistsError, 404: NoSuchNamespaceError})\n        return TableResponse.model_validate_json(response.text)\n\n    @retry(**_RETRY_ARGS)\n    def create_table(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; Table:\n        table_response = self._create_table(\n            identifier=identifier,\n            schema=schema,\n            location=location,\n            partition_spec=partition_spec,\n            sort_order=sort_order,\n            properties=properties,\n            stage_create=False,\n        )\n        return self._response_to_table(self.identifier_to_tuple(identifier), table_response)\n\n    @retry(**_RETRY_ARGS)\n    def create_table_transaction(\n        self,\n        identifier: str | Identifier,\n        schema: Union[Schema, \"pa.Schema\"],\n        location: str | None = None,\n        partition_spec: PartitionSpec = UNPARTITIONED_PARTITION_SPEC,\n        sort_order: SortOrder = UNSORTED_SORT_ORDER,\n        properties: Properties = EMPTY_DICT,\n    ) -&gt; CreateTableTransaction:\n        table_response = self._create_table(\n            identifier=identifier,\n            schema=schema,\n            location=location,\n            partition_spec=partition_spec,\n            sort_order=sort_order,\n            properties=properties,\n            stage_create=True,\n        )\n        staged_table = self._response_to_staged_table(self.identifier_to_tuple(identifier), table_response)\n        return CreateTableTransaction(staged_table)\n\n    @retry(**_RETRY_ARGS)\n    def register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n        \"\"\"Register a new table using existing metadata.\n\n        Args:\n            identifier (Union[str, Identifier]): Table identifier for the table\n            metadata_location (str): The location to the metadata\n\n        Returns:\n            Table: The newly registered table\n\n        Raises:\n            TableAlreadyExistsError: If the table already exists\n        \"\"\"\n        self._check_endpoint(Capability.V1_REGISTER_TABLE)\n        namespace_and_table = self._split_identifier_for_path(identifier)\n        request = RegisterTableRequest(\n            name=self._identifier_to_validated_tuple(identifier)[-1],\n            metadata_location=metadata_location,\n        )\n        serialized_json = request.model_dump_json().encode(UTF8)\n        response = self._session.post(\n            self.url(Endpoints.register_table, namespace=namespace_and_table[\"namespace\"]),\n            data=serialized_json,\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {409: TableAlreadyExistsError})\n\n        table_response = TableResponse.model_validate_json(response.text)\n        return self._response_to_table(self.identifier_to_tuple(identifier), table_response)\n\n    @retry(**_RETRY_ARGS)\n    def list_tables(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        self._check_endpoint(Capability.V1_LIST_TABLES)\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace_concat = self._encode_namespace_path(namespace_tuple)\n        response = self._session.get(self.url(Endpoints.list_tables, namespace=namespace_concat))\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchNamespaceError})\n        return [(*table.namespace, table.name) for table in ListTablesResponse.model_validate_json(response.text).identifiers]\n\n    @retry(**_RETRY_ARGS)\n    def load_table(self, identifier: str | Identifier) -&gt; Table:\n        self._check_endpoint(Capability.V1_LOAD_TABLE)\n        params = {}\n        if mode := self.properties.get(SNAPSHOT_LOADING_MODE):\n            if mode in {\"all\", \"refs\"}:\n                params[\"snapshots\"] = mode\n            else:\n                raise ValueError(\"Invalid snapshot-loading-mode: {}\")\n\n        response = self._session.get(\n            self.url(Endpoints.load_table, prefixed=True, **self._split_identifier_for_path(identifier)), params=params\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchTableError})\n\n        table_response = TableResponse.model_validate_json(response.text)\n        return self._response_to_table(self.identifier_to_tuple(identifier), table_response)\n\n    @retry(**_RETRY_ARGS)\n    def drop_table(self, identifier: str | Identifier, purge_requested: bool = False) -&gt; None:\n        self._check_endpoint(Capability.V1_DELETE_TABLE)\n        response = self._session.delete(\n            self.url(Endpoints.drop_table, prefixed=True, **self._split_identifier_for_path(identifier)),\n            params={\"purgeRequested\": purge_requested},\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchTableError})\n\n    @retry(**_RETRY_ARGS)\n    def purge_table(self, identifier: str | Identifier) -&gt; None:\n        self.drop_table(identifier=identifier, purge_requested=True)\n\n    @retry(**_RETRY_ARGS)\n    def rename_table(self, from_identifier: str | Identifier, to_identifier: str | Identifier) -&gt; Table:\n        self._check_endpoint(Capability.V1_RENAME_TABLE)\n        payload = {\n            \"source\": self._split_identifier_for_json(from_identifier),\n            \"destination\": self._split_identifier_for_json(to_identifier),\n        }\n\n        # Ensure that namespaces exist on source and destination.\n        source_namespace = self._split_identifier_for_json(from_identifier)[\"namespace\"]\n        if not self.namespace_exists(source_namespace):\n            raise NoSuchNamespaceError(f\"Source namespace does not exist: {source_namespace}\")\n\n        destination_namespace = self._split_identifier_for_json(to_identifier)[\"namespace\"]\n        if not self.namespace_exists(destination_namespace):\n            raise NoSuchNamespaceError(f\"Destination namespace does not exist: {destination_namespace}\")\n\n        response = self._session.post(self.url(Endpoints.rename_table), json=payload)\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchTableError, 409: TableAlreadyExistsError})\n\n        return self.load_table(to_identifier)\n\n    def _remove_catalog_name_from_table_request_identifier(self, table_request: CommitTableRequest) -&gt; CommitTableRequest:\n        if table_request.identifier.namespace.root[0] == self.name:\n            return table_request.model_copy(\n                update={\n                    \"identifier\": TableIdentifier(\n                        namespace=table_request.identifier.namespace.root[1:], name=table_request.identifier.name\n                    )\n                }\n            )\n        return table_request\n\n    @retry(**_RETRY_ARGS)\n    def list_views(self, namespace: str | Identifier) -&gt; list[Identifier]:\n        if Capability.V1_LIST_VIEWS not in self._supported_endpoints:\n            return []\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace_concat = self._encode_namespace_path(namespace_tuple)\n        response = self._session.get(self.url(Endpoints.list_views, namespace=namespace_concat))\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchNamespaceError})\n        return [(*view.namespace, view.name) for view in ListViewsResponse.model_validate_json(response.text).identifiers]\n\n    @retry(**_RETRY_ARGS)\n    def commit_table(\n        self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n    ) -&gt; CommitTableResponse:\n        \"\"\"Commit updates to a table.\n\n        Args:\n            table (Table): The table to be updated.\n            requirements: (Tuple[TableRequirement, ...]): Table requirements.\n            updates: (Tuple[TableUpdate, ...]): Table updates.\n\n        Returns:\n            CommitTableResponse: The updated metadata.\n\n        Raises:\n            NoSuchTableError: If a table with the given identifier does not exist.\n            CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n            CommitStateUnknownException: Failed due to an internal exception on the side of the catalog.\n        \"\"\"\n        self._check_endpoint(Capability.V1_UPDATE_TABLE)\n        identifier = table.name()\n        table_identifier = TableIdentifier(namespace=identifier[:-1], name=identifier[-1])\n        table_request = CommitTableRequest(identifier=table_identifier, requirements=requirements, updates=updates)\n\n        headers = self._session.headers\n        if table_token := table.config.get(TOKEN):\n            headers[AUTHORIZATION_HEADER] = f\"{BEARER_PREFIX} {table_token}\"\n\n        response = self._session.post(\n            self.url(Endpoints.update_table, prefixed=True, **self._split_identifier_for_path(table_request.identifier)),\n            data=table_request.model_dump_json().encode(UTF8),\n            headers=headers,\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(\n                exc,\n                {\n                    409: CommitFailedException,\n                    500: CommitStateUnknownException,\n                    502: CommitStateUnknownException,\n                    504: CommitStateUnknownException,\n                },\n            )\n        return CommitTableResponse.model_validate_json(response.text)\n\n    @retry(**_RETRY_ARGS)\n    def create_namespace(self, namespace: str | Identifier, properties: Properties = EMPTY_DICT) -&gt; None:\n        self._check_endpoint(Capability.V1_CREATE_NAMESPACE)\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        payload = {\"namespace\": namespace_tuple, \"properties\": properties}\n        response = self._session.post(self.url(Endpoints.create_namespace), json=payload)\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {409: NamespaceAlreadyExistsError})\n\n    @retry(**_RETRY_ARGS)\n    def drop_namespace(self, namespace: str | Identifier) -&gt; None:\n        self._check_endpoint(Capability.V1_DELETE_NAMESPACE)\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace = self._encode_namespace_path(namespace_tuple)\n        response = self._session.delete(self.url(Endpoints.drop_namespace, namespace=namespace))\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchNamespaceError, 409: NamespaceNotEmptyError})\n\n    @retry(**_RETRY_ARGS)\n    def list_namespaces(self, namespace: str | Identifier = ()) -&gt; list[Identifier]:\n        self._check_endpoint(Capability.V1_LIST_NAMESPACES)\n        namespace_tuple = self.identifier_to_tuple(namespace)\n        response = self._session.get(\n            self.url(\n                f\"{Endpoints.list_namespaces}?parent={self._encode_namespace_path(namespace_tuple)}\"\n                if namespace_tuple\n                else Endpoints.list_namespaces\n            ),\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchNamespaceError})\n\n        return ListNamespaceResponse.model_validate_json(response.text).namespaces\n\n    @retry(**_RETRY_ARGS)\n    def load_namespace_properties(self, namespace: str | Identifier) -&gt; Properties:\n        self._check_endpoint(Capability.V1_LOAD_NAMESPACE)\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace = self._encode_namespace_path(namespace_tuple)\n        response = self._session.get(self.url(Endpoints.load_namespace_metadata, namespace=namespace))\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchNamespaceError})\n\n        return NamespaceResponse.model_validate_json(response.text).properties\n\n    @retry(**_RETRY_ARGS)\n    def update_namespace_properties(\n        self, namespace: str | Identifier, removals: set[str] | None = None, updates: Properties = EMPTY_DICT\n    ) -&gt; PropertiesUpdateSummary:\n        self._check_endpoint(Capability.V1_UPDATE_NAMESPACE)\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace = self._encode_namespace_path(namespace_tuple)\n        payload = {\"removals\": list(removals or []), \"updates\": updates}\n        response = self._session.post(self.url(Endpoints.update_namespace_properties, namespace=namespace), json=payload)\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchNamespaceError})\n        parsed_response = UpdateNamespacePropertiesResponse.model_validate_json(response.text)\n        return PropertiesUpdateSummary(\n            removed=parsed_response.removed,\n            updated=parsed_response.updated,\n            missing=parsed_response.missing,\n        )\n\n    @retry(**_RETRY_ARGS)\n    def namespace_exists(self, namespace: str | Identifier) -&gt; bool:\n        namespace_tuple = self._check_valid_namespace_identifier(namespace)\n        namespace = self._encode_namespace_path(namespace_tuple)\n\n        # fallback in order to work with older rest catalog implementations\n        if Capability.V1_NAMESPACE_EXISTS not in self._supported_endpoints:\n            try:\n                self.load_namespace_properties(namespace_tuple)\n                return True\n            except NoSuchNamespaceError:\n                return False\n\n        response = self._session.head(self.url(Endpoints.namespace_exists, namespace=namespace))\n\n        if response.status_code == 404:\n            return False\n        elif response.status_code in (200, 204):\n            return True\n\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {})\n\n        return False\n\n    @retry(**_RETRY_ARGS)\n    def table_exists(self, identifier: str | Identifier) -&gt; bool:\n        \"\"\"Check if a table exists.\n\n        Args:\n            identifier (str | Identifier): Table identifier.\n\n        Returns:\n            bool: True if the table exists, False otherwise.\n        \"\"\"\n        # fallback in order to work with older rest catalog implementations\n        if Capability.V1_TABLE_EXISTS not in self._supported_endpoints:\n            try:\n                self.load_table(identifier)\n                return True\n            except NoSuchTableError:\n                return False\n\n        response = self._session.head(\n            self.url(Endpoints.load_table, prefixed=True, **self._split_identifier_for_path(identifier))\n        )\n\n        if response.status_code == 404:\n            return False\n        elif response.status_code in (200, 204):\n            return True\n\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {})\n\n        return False\n\n    @retry(**_RETRY_ARGS)\n    def view_exists(self, identifier: str | Identifier) -&gt; bool:\n        \"\"\"Check if a view exists.\n\n        Args:\n            identifier (str | Identifier): View identifier.\n\n        Returns:\n            bool: True if the view exists, False otherwise.\n        \"\"\"\n        response = self._session.head(\n            self.url(Endpoints.view_exists, prefixed=True, **self._split_identifier_for_path(identifier, IdentifierKind.VIEW)),\n        )\n        if response.status_code == 404:\n            return False\n        elif response.status_code in [200, 204]:\n            return True\n\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {})\n\n        return False\n\n    @retry(**_RETRY_ARGS)\n    def drop_view(self, identifier: str) -&gt; None:\n        self._check_endpoint(Capability.V1_DELETE_VIEW)\n        response = self._session.delete(\n            self.url(Endpoints.drop_view, prefixed=True, **self._split_identifier_for_path(identifier, IdentifierKind.VIEW)),\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {404: NoSuchViewError})\n\n    def close(self) -&gt; None:\n        \"\"\"Close the catalog and release Session connection adapters.\n\n        This method closes mounted HttpAdapters' pooled connections and any active Proxy pooled connections.\n        \"\"\"\n        self._session.close()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.__init__","title":"<code>__init__(name, **properties)</code>","text":"<p>Rest Catalog.</p> <p>You either need to provide a client_id and client_secret, or an already valid token.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name to identify the catalog.</p> required <code>properties</code> <code>str</code> <p>Properties that are passed along to the configuration.</p> <code>{}</code> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>def __init__(self, name: str, **properties: str):\n    \"\"\"Rest Catalog.\n\n    You either need to provide a client_id and client_secret, or an already valid token.\n\n    Args:\n        name: Name to identify the catalog.\n        properties: Properties that are passed along to the configuration.\n    \"\"\"\n    super().__init__(name, **properties)\n    self._auth_manager: AuthManager | None = None\n    self.uri = properties[URI]\n    self._fetch_config()\n    self._session = self._create_session()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.close","title":"<code>close()</code>","text":"<p>Close the catalog and release Session connection adapters.</p> <p>This method closes mounted HttpAdapters' pooled connections and any active Proxy pooled connections.</p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the catalog and release Session connection adapters.\n\n    This method closes mounted HttpAdapters' pooled connections and any active Proxy pooled connections.\n    \"\"\"\n    self._session.close()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.commit_table","title":"<code>commit_table(table, requirements, updates)</code>","text":"<p>Commit updates to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to be updated.</p> required <code>requirements</code> <code>tuple[TableRequirement, ...]</code> <p>(Tuple[TableRequirement, ...]): Table requirements.</p> required <code>updates</code> <code>tuple[TableUpdate, ...]</code> <p>(Tuple[TableUpdate, ...]): Table updates.</p> required <p>Returns:</p> Name Type Description <code>CommitTableResponse</code> <code>CommitTableResponse</code> <p>The updated metadata.</p> <p>Raises:</p> Type Description <code>NoSuchTableError</code> <p>If a table with the given identifier does not exist.</p> <code>CommitFailedException</code> <p>Requirement not met, or a conflict with a concurrent commit.</p> <code>CommitStateUnknownException</code> <p>Failed due to an internal exception on the side of the catalog.</p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>@retry(**_RETRY_ARGS)\ndef commit_table(\n    self, table: Table, requirements: tuple[TableRequirement, ...], updates: tuple[TableUpdate, ...]\n) -&gt; CommitTableResponse:\n    \"\"\"Commit updates to a table.\n\n    Args:\n        table (Table): The table to be updated.\n        requirements: (Tuple[TableRequirement, ...]): Table requirements.\n        updates: (Tuple[TableUpdate, ...]): Table updates.\n\n    Returns:\n        CommitTableResponse: The updated metadata.\n\n    Raises:\n        NoSuchTableError: If a table with the given identifier does not exist.\n        CommitFailedException: Requirement not met, or a conflict with a concurrent commit.\n        CommitStateUnknownException: Failed due to an internal exception on the side of the catalog.\n    \"\"\"\n    self._check_endpoint(Capability.V1_UPDATE_TABLE)\n    identifier = table.name()\n    table_identifier = TableIdentifier(namespace=identifier[:-1], name=identifier[-1])\n    table_request = CommitTableRequest(identifier=table_identifier, requirements=requirements, updates=updates)\n\n    headers = self._session.headers\n    if table_token := table.config.get(TOKEN):\n        headers[AUTHORIZATION_HEADER] = f\"{BEARER_PREFIX} {table_token}\"\n\n    response = self._session.post(\n        self.url(Endpoints.update_table, prefixed=True, **self._split_identifier_for_path(table_request.identifier)),\n        data=table_request.model_dump_json().encode(UTF8),\n        headers=headers,\n    )\n    try:\n        response.raise_for_status()\n    except HTTPError as exc:\n        _handle_non_200_response(\n            exc,\n            {\n                409: CommitFailedException,\n                500: CommitStateUnknownException,\n                502: CommitStateUnknownException,\n                504: CommitStateUnknownException,\n            },\n        )\n    return CommitTableResponse.model_validate_json(response.text)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.plan_scan","title":"<code>plan_scan(identifier, request)</code>","text":"<p>Plan a table scan and return FileScanTasks.</p> <p>Handles the full scan planning lifecycle including pagination.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <code>request</code> <code>PlanTableScanRequest</code> <p>The scan plan request parameters.</p> required <p>Returns:</p> Type Description <code>list[FileScanTask]</code> <p>List of FileScanTask objects ready for execution.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If planning fails, is cancelled, or returns unexpected response.</p> <code>NotImplementedError</code> <p>If async planning is required but not yet supported.</p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>def plan_scan(self, identifier: str | Identifier, request: PlanTableScanRequest) -&gt; list[FileScanTask]:\n    \"\"\"Plan a table scan and return FileScanTasks.\n\n    Handles the full scan planning lifecycle including pagination.\n\n    Args:\n        identifier: Table identifier.\n        request: The scan plan request parameters.\n\n    Returns:\n        List of FileScanTask objects ready for execution.\n\n    Raises:\n        RuntimeError: If planning fails, is cancelled, or returns unexpected response.\n        NotImplementedError: If async planning is required but not yet supported.\n    \"\"\"\n    response = self._plan_table_scan(identifier, request)\n\n    if isinstance(response, PlanFailed):\n        error_msg = response.error.message if response.error else \"unknown error\"\n        raise RuntimeError(f\"Received status: failed: {error_msg}\")\n\n    if isinstance(response, PlanCancelled):\n        raise RuntimeError(\"Received status: cancelled\")\n\n    if isinstance(response, PlanSubmitted):\n        # TODO: implement polling for async planning\n        raise NotImplementedError(f\"Async scan planning not yet supported for planId: {response.plan_id}\")\n\n    if not isinstance(response, PlanCompleted):\n        raise RuntimeError(f\"Invalid planStatus for response: {type(response).__name__}\")\n\n    tasks: list[FileScanTask] = []\n\n    # Collect tasks from initial response\n    for task in response.file_scan_tasks:\n        tasks.append(FileScanTask.from_rest_response(task, response.delete_files))\n\n    # Fetch and collect from additional batches\n    pending_tasks = deque(response.plan_tasks)\n    while pending_tasks:\n        plan_task = pending_tasks.popleft()\n        batch = self._fetch_scan_tasks(identifier, plan_task)\n        for task in batch.file_scan_tasks:\n            tasks.append(FileScanTask.from_rest_response(task, batch.delete_files))\n        pending_tasks.extend(batch.plan_tasks)\n\n    return tasks\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.register_table","title":"<code>register_table(identifier, metadata_location)</code>","text":"<p>Register a new table using existing metadata.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>Union[str, Identifier]</code> <p>Table identifier for the table</p> required <code>metadata_location</code> <code>str</code> <p>The location to the metadata</p> required <p>Returns:</p> Name Type Description <code>Table</code> <code>Table</code> <p>The newly registered table</p> <p>Raises:</p> Type Description <code>TableAlreadyExistsError</code> <p>If the table already exists</p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>@retry(**_RETRY_ARGS)\ndef register_table(self, identifier: str | Identifier, metadata_location: str) -&gt; Table:\n    \"\"\"Register a new table using existing metadata.\n\n    Args:\n        identifier (Union[str, Identifier]): Table identifier for the table\n        metadata_location (str): The location to the metadata\n\n    Returns:\n        Table: The newly registered table\n\n    Raises:\n        TableAlreadyExistsError: If the table already exists\n    \"\"\"\n    self._check_endpoint(Capability.V1_REGISTER_TABLE)\n    namespace_and_table = self._split_identifier_for_path(identifier)\n    request = RegisterTableRequest(\n        name=self._identifier_to_validated_tuple(identifier)[-1],\n        metadata_location=metadata_location,\n    )\n    serialized_json = request.model_dump_json().encode(UTF8)\n    response = self._session.post(\n        self.url(Endpoints.register_table, namespace=namespace_and_table[\"namespace\"]),\n        data=serialized_json,\n    )\n    try:\n        response.raise_for_status()\n    except HTTPError as exc:\n        _handle_non_200_response(exc, {409: TableAlreadyExistsError})\n\n    table_response = TableResponse.model_validate_json(response.text)\n    return self._response_to_table(self.identifier_to_tuple(identifier), table_response)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.supports_server_side_planning","title":"<code>supports_server_side_planning()</code>","text":"<p>Check if the catalog supports server-side scan planning.</p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>def supports_server_side_planning(self) -&gt; bool:\n    \"\"\"Check if the catalog supports server-side scan planning.\"\"\"\n    return Capability.V1_SUBMIT_TABLE_SCAN_PLAN in self._supported_endpoints and property_as_bool(\n        self.properties, REST_SCAN_PLANNING_ENABLED, REST_SCAN_PLANNING_ENABLED_DEFAULT\n    )\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.table_exists","title":"<code>table_exists(identifier)</code>","text":"<p>Check if a table exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>Table identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the table exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>@retry(**_RETRY_ARGS)\ndef table_exists(self, identifier: str | Identifier) -&gt; bool:\n    \"\"\"Check if a table exists.\n\n    Args:\n        identifier (str | Identifier): Table identifier.\n\n    Returns:\n        bool: True if the table exists, False otherwise.\n    \"\"\"\n    # fallback in order to work with older rest catalog implementations\n    if Capability.V1_TABLE_EXISTS not in self._supported_endpoints:\n        try:\n            self.load_table(identifier)\n            return True\n        except NoSuchTableError:\n            return False\n\n    response = self._session.head(\n        self.url(Endpoints.load_table, prefixed=True, **self._split_identifier_for_path(identifier))\n    )\n\n    if response.status_code == 404:\n        return False\n    elif response.status_code in (200, 204):\n        return True\n\n    try:\n        response.raise_for_status()\n    except HTTPError as exc:\n        _handle_non_200_response(exc, {})\n\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.url","title":"<code>url(endpoint, prefixed=True, **kwargs)</code>","text":"<p>Construct the endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>Resource identifier that points to the REST catalog.</p> required <code>prefixed</code> <code>bool</code> <p>If the prefix return by the config needs to be appended.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The base url of the rest catalog.</p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>def url(self, endpoint: str, prefixed: bool = True, **kwargs: Any) -&gt; str:\n    \"\"\"Construct the endpoint.\n\n    Args:\n        endpoint: Resource identifier that points to the REST catalog.\n        prefixed: If the prefix return by the config needs to be appended.\n\n    Returns:\n        The base url of the rest catalog.\n    \"\"\"\n    url = self.uri\n    url = url + \"v1/\" if url.endswith(\"/\") else url + \"/v1/\"\n\n    if prefixed:\n        url += self.properties.get(PREFIX, \"\")\n        url = url if url.endswith(\"/\") else url + \"/\"\n\n    return url + endpoint.format(**kwargs)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/#pyiceberg.catalog.rest.RestCatalog.view_exists","title":"<code>view_exists(identifier)</code>","text":"<p>Check if a view exists.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str | Identifier</code> <p>View identifier.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the view exists, False otherwise.</p> Source code in <code>pyiceberg/catalog/rest/__init__.py</code> <pre><code>@retry(**_RETRY_ARGS)\ndef view_exists(self, identifier: str | Identifier) -&gt; bool:\n    \"\"\"Check if a view exists.\n\n    Args:\n        identifier (str | Identifier): View identifier.\n\n    Returns:\n        bool: True if the view exists, False otherwise.\n    \"\"\"\n    response = self._session.head(\n        self.url(Endpoints.view_exists, prefixed=True, **self._split_identifier_for_path(identifier, IdentifierKind.VIEW)),\n    )\n    if response.status_code == 404:\n        return False\n    elif response.status_code in [200, 204]:\n        return True\n\n    try:\n        response.raise_for_status()\n    except HTTPError as exc:\n        _handle_non_200_response(exc, {})\n\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/","title":"auth","text":""},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.AuthManager","title":"<code>AuthManager</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for Authentication Managers used to supply authorization headers to HTTP clients (e.g. requests.Session).</p> <p>Subclasses must implement the <code>auth_header</code> method to return an Authorization header value.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class AuthManager(ABC):\n    \"\"\"\n    Abstract base class for Authentication Managers used to supply authorization headers to HTTP clients (e.g. requests.Session).\n\n    Subclasses must implement the `auth_header` method to return an Authorization header value.\n    \"\"\"\n\n    @abstractmethod\n    def auth_header(self) -&gt; str | None:\n        \"\"\"Return the Authorization header value, or None if not applicable.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.AuthManager.auth_header","title":"<code>auth_header()</code>  <code>abstractmethod</code>","text":"<p>Return the Authorization header value, or None if not applicable.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>@abstractmethod\ndef auth_header(self) -&gt; str | None:\n    \"\"\"Return the Authorization header value, or None if not applicable.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.AuthManagerAdapter","title":"<code>AuthManagerAdapter</code>","text":"<p>               Bases: <code>AuthBase</code></p> <p>A <code>requests.auth.AuthBase</code> adapter for integrating an <code>AuthManager</code> into a <code>requests.Session</code>.</p> <p>This adapter automatically attaches the appropriate Authorization header to every request. This adapter is useful when working with <code>requests.Session.auth</code> and allows reuse of authentication strategies defined by <code>AuthManager</code>. This AuthManagerAdapter is only intended to be used against the REST Catalog Server that expects the Authorization Header.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class AuthManagerAdapter(AuthBase):\n    \"\"\"A `requests.auth.AuthBase` adapter for integrating an `AuthManager` into a `requests.Session`.\n\n    This adapter automatically attaches the appropriate Authorization header to every request.\n    This adapter is useful when working with `requests.Session.auth`\n    and allows reuse of authentication strategies defined by `AuthManager`.\n    This AuthManagerAdapter is only intended to be used against the REST Catalog\n    Server that expects the Authorization Header.\n    \"\"\"\n\n    def __init__(self, auth_manager: AuthManager):\n        \"\"\"\n        Initialize AuthManagerAdapter.\n\n        Args:\n            auth_manager (AuthManager): An instance of an AuthManager subclass.\n        \"\"\"\n        self.auth_manager = auth_manager\n\n    def __call__(self, request: PreparedRequest) -&gt; PreparedRequest:\n        \"\"\"\n        Modify the outgoing request to include the Authorization header.\n\n        Args:\n            request (requests.PreparedRequest): The HTTP request being prepared.\n\n        Returns:\n            requests.PreparedRequest: The modified request with Authorization header.\n        \"\"\"\n        if auth_header := self.auth_manager.auth_header():\n            request.headers[\"Authorization\"] = auth_header\n        return request\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.AuthManagerAdapter.__call__","title":"<code>__call__(request)</code>","text":"<p>Modify the outgoing request to include the Authorization header.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>PreparedRequest</code> <p>The HTTP request being prepared.</p> required <p>Returns:</p> Type Description <code>PreparedRequest</code> <p>requests.PreparedRequest: The modified request with Authorization header.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>def __call__(self, request: PreparedRequest) -&gt; PreparedRequest:\n    \"\"\"\n    Modify the outgoing request to include the Authorization header.\n\n    Args:\n        request (requests.PreparedRequest): The HTTP request being prepared.\n\n    Returns:\n        requests.PreparedRequest: The modified request with Authorization header.\n    \"\"\"\n    if auth_header := self.auth_manager.auth_header():\n        request.headers[\"Authorization\"] = auth_header\n    return request\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.AuthManagerAdapter.__init__","title":"<code>__init__(auth_manager)</code>","text":"<p>Initialize AuthManagerAdapter.</p> <p>Parameters:</p> Name Type Description Default <code>auth_manager</code> <code>AuthManager</code> <p>An instance of an AuthManager subclass.</p> required Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>def __init__(self, auth_manager: AuthManager):\n    \"\"\"\n    Initialize AuthManagerAdapter.\n\n    Args:\n        auth_manager (AuthManager): An instance of an AuthManager subclass.\n    \"\"\"\n    self.auth_manager = auth_manager\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.AuthManagerFactory","title":"<code>AuthManagerFactory</code>","text":"Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class AuthManagerFactory:\n    _registry: dict[str, type[\"AuthManager\"]] = {}\n\n    @classmethod\n    def register(cls, name: str, auth_manager_class: type[\"AuthManager\"]) -&gt; None:\n        \"\"\"\n        Register a string name to a known AuthManager class.\n\n        Args:\n            name (str): unique name like 'oauth2' to register the AuthManager with\n            auth_manager_class (Type[\"AuthManager\"]): Implementation of AuthManager\n\n        Returns:\n            None\n        \"\"\"\n        cls._registry[name] = auth_manager_class\n\n    @classmethod\n    def create(cls, class_or_name: str, config: dict[str, Any]) -&gt; AuthManager:\n        \"\"\"\n        Create an AuthManager by name or fully-qualified class path.\n\n        Args:\n            class_or_name (str): Either a name like 'oauth2' or a full class path like 'my.module.CustomAuthManager'\n            config (Dict[str, Any]): Configuration passed to the AuthManager constructor\n\n        Returns:\n            AuthManager: An instantiated AuthManager subclass\n        \"\"\"\n        if class_or_name in cls._registry:\n            manager_cls = cls._registry[class_or_name]\n        else:\n            try:\n                module_path, class_name = class_or_name.rsplit(\".\", 1)\n                module = importlib.import_module(module_path)\n                manager_cls = getattr(module, class_name)\n            except Exception as err:\n                raise ValueError(f\"Could not load AuthManager class for '{class_or_name}'\") from err\n\n        return manager_cls(**config)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.AuthManagerFactory.create","title":"<code>create(class_or_name, config)</code>  <code>classmethod</code>","text":"<p>Create an AuthManager by name or fully-qualified class path.</p> <p>Parameters:</p> Name Type Description Default <code>class_or_name</code> <code>str</code> <p>Either a name like 'oauth2' or a full class path like 'my.module.CustomAuthManager'</p> required <code>config</code> <code>Dict[str, Any]</code> <p>Configuration passed to the AuthManager constructor</p> required <p>Returns:</p> Name Type Description <code>AuthManager</code> <code>AuthManager</code> <p>An instantiated AuthManager subclass</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>@classmethod\ndef create(cls, class_or_name: str, config: dict[str, Any]) -&gt; AuthManager:\n    \"\"\"\n    Create an AuthManager by name or fully-qualified class path.\n\n    Args:\n        class_or_name (str): Either a name like 'oauth2' or a full class path like 'my.module.CustomAuthManager'\n        config (Dict[str, Any]): Configuration passed to the AuthManager constructor\n\n    Returns:\n        AuthManager: An instantiated AuthManager subclass\n    \"\"\"\n    if class_or_name in cls._registry:\n        manager_cls = cls._registry[class_or_name]\n    else:\n        try:\n            module_path, class_name = class_or_name.rsplit(\".\", 1)\n            module = importlib.import_module(module_path)\n            manager_cls = getattr(module, class_name)\n        except Exception as err:\n            raise ValueError(f\"Could not load AuthManager class for '{class_or_name}'\") from err\n\n    return manager_cls(**config)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.AuthManagerFactory.register","title":"<code>register(name, auth_manager_class)</code>  <code>classmethod</code>","text":"<p>Register a string name to a known AuthManager class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>unique name like 'oauth2' to register the AuthManager with</p> required <code>auth_manager_class</code> <code>Type[AuthManager]</code> <p>Implementation of AuthManager</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>@classmethod\ndef register(cls, name: str, auth_manager_class: type[\"AuthManager\"]) -&gt; None:\n    \"\"\"\n    Register a string name to a known AuthManager class.\n\n    Args:\n        name (str): unique name like 'oauth2' to register the AuthManager with\n        auth_manager_class (Type[\"AuthManager\"]): Implementation of AuthManager\n\n    Returns:\n        None\n    \"\"\"\n    cls._registry[name] = auth_manager_class\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.BasicAuthManager","title":"<code>BasicAuthManager</code>","text":"<p>               Bases: <code>AuthManager</code></p> <p>AuthManager implementation that supports basic password auth.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class BasicAuthManager(AuthManager):\n    \"\"\"AuthManager implementation that supports basic password auth.\"\"\"\n\n    def __init__(self, username: str, password: str):\n        credentials = f\"{username}:{password}\"\n        self._token = base64.b64encode(credentials.encode()).decode()\n\n    def auth_header(self) -&gt; str:\n        return f\"Basic {self._token}\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.EntraAuthManager","title":"<code>EntraAuthManager</code>","text":"<p>               Bases: <code>AuthManager</code></p> <p>Auth Manager implementation that supports Microsoft Entra ID (Azure AD) authentication.</p> <p>This manager uses the Azure Identity library's DefaultAzureCredential which automatically tries multiple authentication methods including environment variables, managed identity, and Azure CLI.</p> <p>See https://learn.microsoft.com/en-us/azure/developer/python/sdk/authentication/credential-chains for more details on DefaultAzureCredential.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class EntraAuthManager(AuthManager):\n    \"\"\"Auth Manager implementation that supports Microsoft Entra ID (Azure AD) authentication.\n\n    This manager uses the Azure Identity library's DefaultAzureCredential which automatically\n    tries multiple authentication methods including environment variables, managed identity,\n    and Azure CLI.\n\n    See https://learn.microsoft.com/en-us/azure/developer/python/sdk/authentication/credential-chains\n    for more details on DefaultAzureCredential.\n    \"\"\"\n\n    DEFAULT_SCOPE = \"https://storage.azure.com/.default\"\n\n    def __init__(\n        self,\n        scopes: list[str] | None = None,\n        **credential_kwargs: Any,\n    ):\n        \"\"\"\n        Initialize EntraAuthManager.\n\n        Args:\n            scopes: List of OAuth2 scopes. Defaults to [\"https://storage.azure.com/.default\"].\n            **credential_kwargs: Arguments passed to DefaultAzureCredential.\n                Supported authentication methods:\n                - Environment Variables: Set AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET\n                - Managed Identity: Works automatically on Azure; for user-assigned, pass managed_identity_client_id\n                - Azure CLI: Works automatically if logged in via `az login`\n                - Workload Identity: Works automatically in AKS with workload identity configured  # codespell:ignore aks\n        \"\"\"\n        try:\n            from azure.identity import DefaultAzureCredential\n        except ImportError as e:\n            raise ImportError(\"Azure Identity library not found. Please install with: pip install pyiceberg[entra-auth]\") from e\n\n        self._scopes = scopes or [self.DEFAULT_SCOPE]\n        self._lock = threading.Lock()\n        self._token: str | None = None\n        self._expires_at: float = 0\n        self._credential = DefaultAzureCredential(**credential_kwargs)\n\n    def _refresh_token(self) -&gt; None:\n        \"\"\"Refresh the access token from Azure.\"\"\"\n        token = self._credential.get_token(*self._scopes)\n        self._token = token.token\n        # expires_on is a Unix timestamp; add a 60-second margin for safety\n        self._expires_at = token.expires_on - 60\n\n    def _get_token(self) -&gt; str:\n        \"\"\"Get a valid access token, refreshing if necessary.\"\"\"\n        with self._lock:\n            if not self._token or time.time() &gt;= self._expires_at:\n                self._refresh_token()\n            if self._token is None:\n                raise ValueError(\"Failed to obtain Entra access token\")\n            return self._token\n\n    def auth_header(self) -&gt; str:\n        \"\"\"Return the Authorization header value with a valid Bearer token.\"\"\"\n        return f\"Bearer {self._get_token()}\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.EntraAuthManager.__init__","title":"<code>__init__(scopes=None, **credential_kwargs)</code>","text":"<p>Initialize EntraAuthManager.</p> <p>Parameters:</p> Name Type Description Default <code>scopes</code> <code>list[str] | None</code> <p>List of OAuth2 scopes. Defaults to [\"https://storage.azure.com/.default\"].</p> <code>None</code> <code>**credential_kwargs</code> <code>Any</code> <p>Arguments passed to DefaultAzureCredential. Supported authentication methods: - Environment Variables: Set AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET - Managed Identity: Works automatically on Azure; for user-assigned, pass managed_identity_client_id - Azure CLI: Works automatically if logged in via <code>az login</code> - Workload Identity: Works automatically in AKS with workload identity configured  # codespell:ignore aks</p> <code>{}</code> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>def __init__(\n    self,\n    scopes: list[str] | None = None,\n    **credential_kwargs: Any,\n):\n    \"\"\"\n    Initialize EntraAuthManager.\n\n    Args:\n        scopes: List of OAuth2 scopes. Defaults to [\"https://storage.azure.com/.default\"].\n        **credential_kwargs: Arguments passed to DefaultAzureCredential.\n            Supported authentication methods:\n            - Environment Variables: Set AZURE_TENANT_ID, AZURE_CLIENT_ID, AZURE_CLIENT_SECRET\n            - Managed Identity: Works automatically on Azure; for user-assigned, pass managed_identity_client_id\n            - Azure CLI: Works automatically if logged in via `az login`\n            - Workload Identity: Works automatically in AKS with workload identity configured  # codespell:ignore aks\n    \"\"\"\n    try:\n        from azure.identity import DefaultAzureCredential\n    except ImportError as e:\n        raise ImportError(\"Azure Identity library not found. Please install with: pip install pyiceberg[entra-auth]\") from e\n\n    self._scopes = scopes or [self.DEFAULT_SCOPE]\n    self._lock = threading.Lock()\n    self._token: str | None = None\n    self._expires_at: float = 0\n    self._credential = DefaultAzureCredential(**credential_kwargs)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.EntraAuthManager.auth_header","title":"<code>auth_header()</code>","text":"<p>Return the Authorization header value with a valid Bearer token.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>def auth_header(self) -&gt; str:\n    \"\"\"Return the Authorization header value with a valid Bearer token.\"\"\"\n    return f\"Bearer {self._get_token()}\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.GoogleAuthManager","title":"<code>GoogleAuthManager</code>","text":"<p>               Bases: <code>AuthManager</code></p> <p>An auth manager that is responsible for handling Google credentials.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class GoogleAuthManager(AuthManager):\n    \"\"\"An auth manager that is responsible for handling Google credentials.\"\"\"\n\n    def __init__(self, credentials_path: str | None = None, scopes: list[str] | None = None):\n        \"\"\"\n        Initialize GoogleAuthManager.\n\n        Args:\n            credentials_path: Optional path to Google credentials JSON file.\n            scopes: Optional list of OAuth2 scopes.\n        \"\"\"\n        try:\n            import google.auth\n            import google.auth.transport.requests\n        except ImportError as e:\n            raise ImportError(\"Google Auth libraries not found. Please install 'google-auth'.\") from e\n\n        if credentials_path:\n            self.credentials, _ = google.auth.load_credentials_from_file(credentials_path, scopes=scopes)\n        else:\n            logger.info(\"Using Google Default Application Credentials\")\n            self.credentials, _ = google.auth.default(scopes=scopes)\n        self._auth_request = google.auth.transport.requests.Request()\n\n    def auth_header(self) -&gt; str:\n        self.credentials.refresh(self._auth_request)\n        return f\"Bearer {self.credentials.token}\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.GoogleAuthManager.__init__","title":"<code>__init__(credentials_path=None, scopes=None)</code>","text":"<p>Initialize GoogleAuthManager.</p> <p>Parameters:</p> Name Type Description Default <code>credentials_path</code> <code>str | None</code> <p>Optional path to Google credentials JSON file.</p> <code>None</code> <code>scopes</code> <code>list[str] | None</code> <p>Optional list of OAuth2 scopes.</p> <code>None</code> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>def __init__(self, credentials_path: str | None = None, scopes: list[str] | None = None):\n    \"\"\"\n    Initialize GoogleAuthManager.\n\n    Args:\n        credentials_path: Optional path to Google credentials JSON file.\n        scopes: Optional list of OAuth2 scopes.\n    \"\"\"\n    try:\n        import google.auth\n        import google.auth.transport.requests\n    except ImportError as e:\n        raise ImportError(\"Google Auth libraries not found. Please install 'google-auth'.\") from e\n\n    if credentials_path:\n        self.credentials, _ = google.auth.load_credentials_from_file(credentials_path, scopes=scopes)\n    else:\n        logger.info(\"Using Google Default Application Credentials\")\n        self.credentials, _ = google.auth.default(scopes=scopes)\n    self._auth_request = google.auth.transport.requests.Request()\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.LegacyOAuth2AuthManager","title":"<code>LegacyOAuth2AuthManager</code>","text":"<p>               Bases: <code>AuthManager</code></p> <p>Legacy OAuth2 AuthManager implementation.</p> <p>This class exists for backward compatibility, and will be removed in PyIceberg 1.0.0 in favor of OAuth2AuthManager.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class LegacyOAuth2AuthManager(AuthManager):\n    \"\"\"Legacy OAuth2 AuthManager implementation.\n\n    This class exists for backward compatibility, and will be removed in\n    PyIceberg 1.0.0 in favor of OAuth2AuthManager.\n    \"\"\"\n\n    _session: Session\n    _auth_url: str | None\n    _token: str | None\n    _credential: str | None\n    _optional_oauth_params: dict[str, str] | None\n\n    def __init__(\n        self,\n        session: Session,\n        auth_url: str | None = None,\n        credential: str | None = None,\n        initial_token: str | None = None,\n        optional_oauth_params: dict[str, str] | None = None,\n    ):\n        self._session = session\n        self._auth_url = auth_url\n        self._token = initial_token\n        self._credential = credential\n        self._optional_oauth_params = optional_oauth_params\n        self._refresh_token()\n\n    def _fetch_access_token(self, credential: str) -&gt; str:\n        if COLON in credential:\n            client_id, client_secret = credential.split(COLON, maxsplit=1)\n        else:\n            client_id, client_secret = None, credential\n\n        data = {\"grant_type\": \"client_credentials\", \"client_id\": client_id, \"client_secret\": client_secret}\n\n        if self._optional_oauth_params:\n            data.update(self._optional_oauth_params)\n\n        if self._auth_url is None:\n            raise ValueError(\"Cannot fetch access token from undefined auth_url\")\n\n        response = self._session.post(\n            url=self._auth_url, data=data, headers={**self._session.headers, \"Content-type\": \"application/x-www-form-urlencoded\"}\n        )\n        try:\n            response.raise_for_status()\n        except HTTPError as exc:\n            _handle_non_200_response(exc, {400: OAuthError, 401: OAuthError})\n\n        return TokenResponse.model_validate_json(response.text).access_token\n\n    def _refresh_token(self) -&gt; None:\n        if self._credential is not None:\n            self._token = self._fetch_access_token(self._credential)\n\n    def auth_header(self) -&gt; str:\n        return f\"Bearer {self._token}\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.NoopAuthManager","title":"<code>NoopAuthManager</code>","text":"<p>               Bases: <code>AuthManager</code></p> <p>Auth Manager implementation with no auth.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class NoopAuthManager(AuthManager):\n    \"\"\"Auth Manager implementation with no auth.\"\"\"\n\n    def auth_header(self) -&gt; str | None:\n        return None\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.OAuth2AuthManager","title":"<code>OAuth2AuthManager</code>","text":"<p>               Bases: <code>AuthManager</code></p> <p>Auth Manager implementation that supports OAuth2 as defined in IETF RFC6749.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class OAuth2AuthManager(AuthManager):\n    \"\"\"Auth Manager implementation that supports OAuth2 as defined in IETF RFC6749.\"\"\"\n\n    def __init__(\n        self,\n        client_id: str,\n        client_secret: str,\n        token_url: str,\n        scope: str | None = None,\n        refresh_margin: int = 60,\n        expires_in: int | None = None,\n    ):\n        self.token_provider = OAuth2TokenProvider(\n            client_id,\n            client_secret,\n            token_url,\n            scope,\n            refresh_margin,\n            expires_in,\n        )\n\n    def auth_header(self) -&gt; str:\n        return f\"Bearer {self.token_provider.get_token()}\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/auth/#pyiceberg.catalog.rest.auth.OAuth2TokenProvider","title":"<code>OAuth2TokenProvider</code>","text":"<p>Thread-safe OAuth2 token provider with token refresh support.</p> Source code in <code>pyiceberg/catalog/rest/auth.py</code> <pre><code>class OAuth2TokenProvider:\n    \"\"\"Thread-safe OAuth2 token provider with token refresh support.\"\"\"\n\n    client_id: str\n    client_secret: str\n    token_url: str\n    scope: str | None\n    refresh_margin: int\n    expires_in: int | None\n\n    _token: str | None\n    _expires_at: int\n    _lock: threading.Lock\n\n    def __init__(\n        self,\n        client_id: str,\n        client_secret: str,\n        token_url: str,\n        scope: str | None = None,\n        refresh_margin: int = 60,\n        expires_in: int | None = None,\n    ):\n        self.client_id = client_id\n        self.client_secret = client_secret\n        self.token_url = token_url\n        self.scope = scope\n        self.refresh_margin = refresh_margin\n        self.expires_in = expires_in\n\n        self._token = None\n        self._expires_at = 0\n        self._lock = threading.Lock()\n\n    @cached_property\n    def _client_secret_header(self) -&gt; str:\n        creds = f\"{self.client_id}:{self.client_secret}\"\n        creds_bytes = creds.encode(\"utf-8\")\n        b64_creds = base64.b64encode(creds_bytes).decode(\"utf-8\")\n        return f\"Basic {b64_creds}\"\n\n    def _refresh_token(self) -&gt; None:\n        data = {\"grant_type\": \"client_credentials\"}\n        if self.scope:\n            data[\"scope\"] = self.scope\n\n        response = requests.post(self.token_url, data=data, headers={\"Authorization\": self._client_secret_header})\n        response.raise_for_status()\n        result = response.json()\n\n        self._token = result[\"access_token\"]\n        expires_in = result.get(\"expires_in\", self.expires_in)\n        if expires_in is None:\n            raise ValueError(\n                \"The expiration time of the Token must be provided by the Server in the Access Token Response \"\n                \"in `expires_in` field, or by the PyIceberg Client.\"\n            )\n        self._expires_at = time.monotonic() + expires_in - self.refresh_margin\n\n    def get_token(self) -&gt; str:\n        with self._lock:\n            if not self._token or time.monotonic() &gt;= self._expires_at:\n                self._refresh_token()\n            if self._token is None:\n                raise ValueError(\"Authorization token is None after refresh\")\n            return self._token\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/response/","title":"response","text":""},{"location":"reference/pyiceberg/catalog/rest/scan_planning/","title":"scan_planning","text":""},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.CountMap","title":"<code>CountMap</code>","text":"<p>               Bases: <code>KeyValueMap[int]</code></p> <p>Map of field IDs to counts.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class CountMap(KeyValueMap[int]):\n    \"\"\"Map of field IDs to counts.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.FetchScanTasksRequest","title":"<code>FetchScanTasksRequest</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Request body for fetching scan tasks endpoint.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class FetchScanTasksRequest(IcebergBaseModel):\n    \"\"\"Request body for fetching scan tasks endpoint.\"\"\"\n\n    plan_task: str = Field(alias=\"plan-task\")\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.KeyValueMap","title":"<code>KeyValueMap</code>","text":"<p>               Bases: <code>IcebergBaseModel</code>, <code>Generic[V]</code></p> <p>Map serialized as parallel key/value arrays for column statistics.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class KeyValueMap(IcebergBaseModel, Generic[V]):\n    \"\"\"Map serialized as parallel key/value arrays for column statistics.\"\"\"\n\n    keys: list[int] = Field(default_factory=list)\n    values: list[V] = Field(default_factory=list)\n\n    @model_validator(mode=\"after\")\n    def _validate_lengths_match(self) -&gt; KeyValueMap[V]:\n        if len(self.keys) != len(self.values):\n            raise ValueError(f\"keys and values must have same length: {len(self.keys)} != {len(self.values)}\")\n        return self\n\n    def to_dict(self) -&gt; dict[int, V]:\n        \"\"\"Convert to dictionary mapping field ID to value.\"\"\"\n        return dict(zip(self.keys, self.values, strict=True))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.KeyValueMap.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary mapping field ID to value.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>def to_dict(self) -&gt; dict[int, V]:\n    \"\"\"Convert to dictionary mapping field ID to value.\"\"\"\n    return dict(zip(self.keys, self.values, strict=True))\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.PlanCancelled","title":"<code>PlanCancelled</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Planning was cancelled.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class PlanCancelled(IcebergBaseModel):\n    \"\"\"Planning was cancelled.\"\"\"\n\n    status: Literal[\"cancelled\"] = \"cancelled\"\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.PlanCompleted","title":"<code>PlanCompleted</code>","text":"<p>               Bases: <code>ScanTasks</code></p> <p>Completed scan plan result.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class PlanCompleted(ScanTasks):\n    \"\"\"Completed scan plan result.\"\"\"\n\n    status: Literal[\"completed\"] = \"completed\"\n    plan_id: str | None = Field(alias=\"plan-id\", default=None)\n    storage_credentials: list[StorageCredential] | None = Field(alias=\"storage-credentials\", default=None)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.PlanFailed","title":"<code>PlanFailed</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Planning failed with error.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class PlanFailed(IcebergBaseModel):\n    \"\"\"Planning failed with error.\"\"\"\n\n    status: Literal[\"failed\"] = \"failed\"\n    error: ErrorResponseMessage\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.PlanSubmitted","title":"<code>PlanSubmitted</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Scan plan submitted, poll for completion.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class PlanSubmitted(IcebergBaseModel):\n    \"\"\"Scan plan submitted, poll for completion.\"\"\"\n\n    status: Literal[\"submitted\"] = \"submitted\"\n    plan_id: str | None = Field(alias=\"plan-id\", default=None)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.PlanTableScanRequest","title":"<code>PlanTableScanRequest</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Request body for planning a REST scan.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class PlanTableScanRequest(IcebergBaseModel):\n    \"\"\"Request body for planning a REST scan.\"\"\"\n\n    snapshot_id: int | None = Field(alias=\"snapshot-id\", default=None)\n    select: list[str] | None = Field(default=None)\n    filter: SerializableBooleanExpression | None = Field(default=None)\n    case_sensitive: bool = Field(alias=\"case-sensitive\", default=True)\n    use_snapshot_schema: bool = Field(alias=\"use-snapshot-schema\", default=False)\n    start_snapshot_id: int | None = Field(alias=\"start-snapshot-id\", default=None)\n    end_snapshot_id: int | None = Field(alias=\"end-snapshot-id\", default=None)\n    stats_fields: list[str] | None = Field(alias=\"stats-fields\", default=None)\n    min_rows_requested: int | None = Field(alias=\"min-rows-requested\", default=None)\n\n    @model_validator(mode=\"after\")\n    def _validate_snapshot_fields(self) -&gt; PlanTableScanRequest:\n        if self.start_snapshot_id is not None and self.end_snapshot_id is None:\n            raise ValueError(\"end-snapshot-id is required when start-snapshot-id is specified\")\n        if self.snapshot_id is not None and self.start_snapshot_id is not None:\n            raise ValueError(\"Cannot specify both snapshot-id and start-snapshot-id\")\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.RESTContentFile","title":"<code>RESTContentFile</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Base model for data and delete files from REST API.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class RESTContentFile(IcebergBaseModel):\n    \"\"\"Base model for data and delete files from REST API.\"\"\"\n\n    spec_id: int = Field(alias=\"spec-id\")\n    partition: list[PrimitiveTypeValue] = Field(default_factory=list)\n    content: Literal[\"data\", \"position-deletes\", \"equality-deletes\"]\n    file_path: str = Field(alias=\"file-path\")\n    file_format: FileFormat = Field(alias=\"file-format\")\n    file_size_in_bytes: int = Field(alias=\"file-size-in-bytes\")\n    record_count: int = Field(alias=\"record-count\")\n    key_metadata: str | None = Field(alias=\"key-metadata\", default=None)\n    split_offsets: list[int] | None = Field(alias=\"split-offsets\", default=None)\n    sort_order_id: int | None = Field(alias=\"sort-order-id\", default=None)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.RESTDataFile","title":"<code>RESTDataFile</code>","text":"<p>               Bases: <code>RESTContentFile</code></p> <p>Data file from REST API.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class RESTDataFile(RESTContentFile):\n    \"\"\"Data file from REST API.\"\"\"\n\n    content: Literal[\"data\"] = Field(default=\"data\")\n    first_row_id: int | None = Field(alias=\"first-row-id\", default=None)\n    column_sizes: CountMap | None = Field(alias=\"column-sizes\", default=None)\n    value_counts: CountMap | None = Field(alias=\"value-counts\", default=None)\n    null_value_counts: CountMap | None = Field(alias=\"null-value-counts\", default=None)\n    nan_value_counts: CountMap | None = Field(alias=\"nan-value-counts\", default=None)\n    lower_bounds: ValueMap | None = Field(alias=\"lower-bounds\", default=None)\n    upper_bounds: ValueMap | None = Field(alias=\"upper-bounds\", default=None)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.RESTEqualityDeleteFile","title":"<code>RESTEqualityDeleteFile</code>","text":"<p>               Bases: <code>RESTContentFile</code></p> <p>Equality delete file from REST API.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class RESTEqualityDeleteFile(RESTContentFile):\n    \"\"\"Equality delete file from REST API.\"\"\"\n\n    content: Literal[\"equality-deletes\"] = Field(default=\"equality-deletes\")\n    equality_ids: list[int] | None = Field(alias=\"equality-ids\", default=None)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.RESTFileScanTask","title":"<code>RESTFileScanTask</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>A file scan task from the REST server.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class RESTFileScanTask(IcebergBaseModel):\n    \"\"\"A file scan task from the REST server.\"\"\"\n\n    data_file: RESTDataFile = Field(alias=\"data-file\")\n    delete_file_references: list[int] | None = Field(alias=\"delete-file-references\", default=None)\n    residual_filter: BooleanExpression | None = Field(alias=\"residual-filter\", default=None)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.RESTPositionDeleteFile","title":"<code>RESTPositionDeleteFile</code>","text":"<p>               Bases: <code>RESTContentFile</code></p> <p>Position delete file from REST API.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class RESTPositionDeleteFile(RESTContentFile):\n    \"\"\"Position delete file from REST API.\"\"\"\n\n    content: Literal[\"position-deletes\"] = Field(default=\"position-deletes\")\n    referenced_data_file: str | None = Field(alias=\"referenced-data-file\", default=None)\n    content_offset: int | None = Field(alias=\"content-offset\", default=None)\n    content_size_in_bytes: int | None = Field(alias=\"content-size-in-bytes\", default=None)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.ScanTasks","title":"<code>ScanTasks</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Container for scan tasks returned by the server.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class ScanTasks(IcebergBaseModel):\n    \"\"\"Container for scan tasks returned by the server.\"\"\"\n\n    delete_files: list[RESTDeleteFile] = Field(alias=\"delete-files\", default_factory=list)\n    file_scan_tasks: list[RESTFileScanTask] = Field(alias=\"file-scan-tasks\", default_factory=list)\n    plan_tasks: list[str] = Field(alias=\"plan-tasks\", default_factory=list)\n\n    @model_validator(mode=\"after\")\n    def _validate_delete_file_references(self) -&gt; ScanTasks:\n        # validate delete file references are in bounds\n        max_idx = len(self.delete_files) - 1\n        for task in self.file_scan_tasks:\n            for idx in task.delete_file_references or []:\n                if idx &lt; 0 or idx &gt; max_idx:\n                    raise ValueError(f\"Invalid delete file reference: {idx} (valid range: 0-{max_idx})\")\n\n        if self.delete_files and not self.file_scan_tasks:\n            raise ValueError(\"Invalid response: deleteFiles should only be returned with fileScanTasks that reference them\")\n\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.StorageCredential","title":"<code>StorageCredential</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Storage credential for accessing content files.</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class StorageCredential(IcebergBaseModel):\n    \"\"\"Storage credential for accessing content files.\"\"\"\n\n    prefix: str = Field(description=\"Storage location prefix this credential applies to\")\n    config: dict[str, str] = Field(default_factory=dict)\n</code></pre>"},{"location":"reference/pyiceberg/catalog/rest/scan_planning/#pyiceberg.catalog.rest.scan_planning.ValueMap","title":"<code>ValueMap</code>","text":"<p>               Bases: <code>KeyValueMap[PrimitiveTypeValue]</code></p> <p>Map of field IDs to primitive values (for lower/upper bounds).</p> Source code in <code>pyiceberg/catalog/rest/scan_planning.py</code> <pre><code>class ValueMap(KeyValueMap[PrimitiveTypeValue]):\n    \"\"\"Map of field IDs to primitive values (for lower/upper bounds).\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/","title":"cli","text":""},{"location":"reference/pyiceberg/cli/console/","title":"console","text":""},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.create","title":"<code>create()</code>","text":"<p>Operation to create a namespace.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.group()\ndef create() -&gt; None:\n    \"\"\"Operation to create a namespace.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.describe","title":"<code>describe(ctx, entity, identifier)</code>","text":"<p>Describe a namespace or a table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.option(\"--entity\", type=click.Choice([\"any\", \"namespace\", \"table\"]), default=\"any\")\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef describe(ctx: Context, entity: Literal[\"name\", \"namespace\", \"table\"], identifier: str) -&gt; None:\n    \"\"\"Describe a namespace or a table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    identifier_tuple = Catalog.identifier_to_tuple(identifier)\n\n    is_namespace = False\n    if entity in {\"namespace\", \"any\"} and len(identifier_tuple) &gt; 0:\n        try:\n            namespace_properties = catalog.load_namespace_properties(identifier_tuple)\n            output.describe_properties(namespace_properties)\n            is_namespace = True\n        except NoSuchNamespaceError as exc:\n            if entity != \"any\" or len(identifier_tuple) == 1:  # type: ignore\n                raise exc\n\n    is_table = False\n    if entity in {\"table\", \"any\"} and len(identifier_tuple) &gt; 1:\n        try:\n            catalog_table = catalog.load_table(identifier)\n            output.describe_table(catalog_table)\n            is_table = True\n        except NoSuchTableError as exc:\n            if entity != \"any\":\n                raise exc\n\n    if is_namespace is False and is_table is False:\n        raise NoSuchTableError(f\"Table or namespace does not exist: {identifier}\")\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.drop","title":"<code>drop()</code>","text":"<p>Operations to drop a namespace or table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.group()\ndef drop() -&gt; None:\n    \"\"\"Operations to drop a namespace or table.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.files","title":"<code>files(ctx, identifier, history)</code>","text":"<p>List all the files of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.option(\"--history\", is_flag=True)\n@click.pass_context\n@catch_exception()\ndef files(ctx: Context, identifier: str, history: bool) -&gt; None:\n    \"\"\"List all the files of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n\n    catalog_table = catalog.load_table(identifier)\n    output.files(catalog_table, history)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.get","title":"<code>get()</code>","text":"<p>Fetch properties on tables/namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@properties.group()\ndef get() -&gt; None:\n    \"\"\"Fetch properties on tables/namespaces.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.get_namespace","title":"<code>get_namespace(ctx, identifier, property_name)</code>","text":"<p>Fetch properties on a namespace.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@get.command(\"namespace\")\n@click.argument(\"identifier\")\n@click.argument(\"property_name\", required=False)\n@click.pass_context\n@catch_exception()\ndef get_namespace(ctx: Context, identifier: str, property_name: str) -&gt; None:\n    \"\"\"Fetch properties on a namespace.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    identifier_tuple = Catalog.identifier_to_tuple(identifier)\n\n    namespace_properties = catalog.load_namespace_properties(identifier_tuple)\n\n    if property_name:\n        if property_value := namespace_properties.get(property_name):\n            output.text(property_value)\n        else:\n            raise NoSuchPropertyException(f\"Could not find property {property_name} on namespace {identifier}\")\n    else:\n        output.describe_properties(namespace_properties)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.get_table","title":"<code>get_table(ctx, identifier, property_name)</code>","text":"<p>Fetch properties on a table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@get.command(\"table\")\n@click.argument(\"identifier\")\n@click.argument(\"property_name\", required=False)\n@click.pass_context\n@catch_exception()\ndef get_table(ctx: Context, identifier: str, property_name: str) -&gt; None:\n    \"\"\"Fetch properties on a table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    identifier_tuple = Catalog.identifier_to_tuple(identifier)\n\n    metadata = catalog.load_table(identifier_tuple).metadata\n\n    if property_name:\n        if property_value := metadata.properties.get(property_name):\n            output.text(property_value)\n        else:\n            raise NoSuchPropertyException(f\"Could not find property {property_name} on table {identifier}\")\n    else:\n        output.describe_properties(metadata.properties)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.list","title":"<code>list(ctx, parent)</code>","text":"<p>List tables or namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.pass_context\n@click.argument(\"parent\", required=False)\n@catch_exception()\ndef list(ctx: Context, parent: str | None) -&gt; None:  # pylint: disable=redefined-builtin\n    \"\"\"List tables or namespaces.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n\n    identifiers = []\n    if parent:\n        # Do we have tables under parent namespace?\n        identifiers = catalog.list_tables(parent)\n    if not identifiers:\n        # List hierarchical namespaces if parent, root namespaces otherwise.\n        identifiers = catalog.list_namespaces(parent or ())\n    output.identifiers(identifiers)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.list_refs","title":"<code>list_refs(ctx, identifier, type, verbose)</code>","text":"<p>List all the refs in the provided table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.option(\"--type\", required=False)\n@click.option(\"--verbose\", type=click.BOOL)\n@click.pass_context\n@catch_exception()\ndef list_refs(ctx: Context, identifier: str, type: str, verbose: bool) -&gt; None:\n    \"\"\"List all the refs in the provided table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    refs = table.refs()\n    if type:\n        type = type.lower()\n        if type not in {SnapshotRefType.BRANCH, SnapshotRefType.TAG}:\n            raise ValueError(f\"Type must be either branch or tag, got: {type}\")\n\n    relevant_refs = [\n        (ref_name, ref.snapshot_ref_type, _retention_properties(ref, table.properties))\n        for (ref_name, ref) in refs.items()\n        if not type or ref.snapshot_ref_type == type\n    ]\n\n    output.describe_refs(relevant_refs)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.location","title":"<code>location(ctx, identifier)</code>","text":"<p>Return the location of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef location(ctx: Context, identifier: str) -&gt; None:\n    \"\"\"Return the location of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    output.text(table.location())\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.namespace","title":"<code>namespace(ctx, identifier, property_name)</code>","text":"<p>Remove a property from a namespace.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@remove.command()  # type: ignore\n@click.argument(\"identifier\")\n@click.argument(\"property_name\")\n@click.pass_context\n@catch_exception()\ndef namespace(ctx: Context, identifier: str, property_name: str) -&gt; None:  # noqa: F811\n    \"\"\"Remove a property from a namespace.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n\n    result = catalog.update_namespace_properties(identifier, removals={property_name})\n\n    if result.removed == [property_name]:\n        output.text(f\"Property {property_name} removed from {identifier}\")\n    else:\n        raise NoSuchPropertyException(f\"Property {property_name} does not exist on {identifier}\")\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.properties","title":"<code>properties()</code>","text":"<p>Properties on tables/namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.group()\ndef properties() -&gt; None:\n    \"\"\"Properties on tables/namespaces.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.remove","title":"<code>remove()</code>","text":"<p>Remove a property from tables/namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@properties.group()\ndef remove() -&gt; None:\n    \"\"\"Remove a property from tables/namespaces.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.rename","title":"<code>rename(ctx, from_identifier, to_identifier)</code>","text":"<p>Rename a table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"from_identifier\")\n@click.argument(\"to_identifier\")\n@click.pass_context\n@catch_exception()\ndef rename(ctx: Context, from_identifier: str, to_identifier: str) -&gt; None:\n    \"\"\"Rename a table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n\n    catalog.rename_table(from_identifier, to_identifier)\n    output.text(f\"Renamed table from {from_identifier} to {to_identifier}\")\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.schema","title":"<code>schema(ctx, identifier)</code>","text":"<p>Get the schema of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef schema(ctx: Context, identifier: str) -&gt; None:\n    \"\"\"Get the schema of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    output.schema(table.schema())\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.set","title":"<code>set()</code>","text":"<p>Set a property on tables/namespaces.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@properties.group()\ndef set() -&gt; None:\n    \"\"\"Set a property on tables/namespaces.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.spec","title":"<code>spec(ctx, identifier)</code>","text":"<p>Return the partition spec of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef spec(ctx: Context, identifier: str) -&gt; None:\n    \"\"\"Return the partition spec of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    output.spec(table.spec())\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.table","title":"<code>table(ctx, identifier, property_name)</code>","text":"<p>Remove a property from a table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@remove.command()  # type: ignore\n@click.argument(\"identifier\")\n@click.argument(\"property_name\")\n@click.pass_context\n@catch_exception()\ndef table(ctx: Context, identifier: str, property_name: str) -&gt; None:  # noqa: F811\n    \"\"\"Remove a property from a table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    table = catalog.load_table(identifier)\n    if property_name in table.metadata.properties:\n        with table.transaction() as tx:\n            tx.remove_properties(property_name)\n        output.text(f\"Property {property_name} removed from {identifier}\")\n    else:\n        raise NoSuchPropertyException(f\"Property {property_name} does not exist on {identifier}\")\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.uuid","title":"<code>uuid(ctx, identifier)</code>","text":"<p>Return the UUID of the table.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.argument(\"identifier\")\n@click.pass_context\n@catch_exception()\ndef uuid(ctx: Context, identifier: str) -&gt; None:\n    \"\"\"Return the UUID of the table.\"\"\"\n    catalog, output = _catalog_and_output(ctx)\n    metadata = catalog.load_table(identifier).metadata\n    output.uuid(metadata.table_uuid)\n</code></pre>"},{"location":"reference/pyiceberg/cli/console/#pyiceberg.cli.console.version","title":"<code>version(ctx)</code>","text":"<p>Print pyiceberg version.</p> Source code in <code>pyiceberg/cli/console.py</code> <pre><code>@run.command()\n@click.pass_context\n@catch_exception()\ndef version(ctx: Context) -&gt; None:\n    \"\"\"Print pyiceberg version.\"\"\"\n    ctx.obj[\"output\"].version(__version__)\n</code></pre>"},{"location":"reference/pyiceberg/cli/output/","title":"output","text":""},{"location":"reference/pyiceberg/cli/output/#pyiceberg.cli.output.ConsoleOutput","title":"<code>ConsoleOutput</code>","text":"<p>               Bases: <code>Output</code></p> <p>Writes to the console.</p> Source code in <code>pyiceberg/cli/output.py</code> <pre><code>class ConsoleOutput(Output):\n    \"\"\"Writes to the console.\"\"\"\n\n    verbose: bool\n\n    def __init__(self, **properties: Any) -&gt; None:\n        self.verbose = properties.get(\"verbose\", False)\n\n    @property\n    def _table(self) -&gt; RichTable:\n        return RichTable.grid(padding=(0, 2))\n\n    def exception(self, ex: Exception) -&gt; None:\n        if self.verbose:\n            Console(stderr=True).print_exception()\n        else:\n            Console(stderr=True).print(ex)\n\n    def identifiers(self, identifiers: list[Identifier]) -&gt; None:\n        table = self._table\n        for identifier in identifiers:\n            table.add_row(\".\".join(identifier))\n\n        Console().print(table)\n\n    def describe_table(self, table: Table) -&gt; None:\n        metadata = table.metadata\n        table_properties = self._table\n\n        for key, value in metadata.properties.items():\n            table_properties.add_row(key, value)\n\n        schema_tree = Tree(f\"Schema, id={table.metadata.current_schema_id}\")\n        for field in table.schema().fields:\n            schema_tree.add(str(field))\n\n        snapshot_tree = Tree(\"Snapshots\")\n        for snapshot in metadata.snapshots:\n            snapshot_tree.add(f\"Snapshot {snapshot.snapshot_id}, schema {snapshot.schema_id}: {snapshot.manifest_list}\")\n\n        output_table = self._table\n        output_table.add_row(\"Table format version\", str(metadata.format_version))\n        output_table.add_row(\"Metadata location\", table.metadata_location)\n        output_table.add_row(\"Table UUID\", str(table.metadata.table_uuid))\n        output_table.add_row(\"Last Updated\", str(metadata.last_updated_ms))\n        output_table.add_row(\"Partition spec\", str(table.spec()))\n        output_table.add_row(\"Sort order\", str(table.sort_order()))\n        output_table.add_row(\"Current schema\", schema_tree)\n        output_table.add_row(\"Current snapshot\", str(table.current_snapshot()))\n        output_table.add_row(\"Snapshots\", snapshot_tree)\n        output_table.add_row(\"Properties\", table_properties)\n        Console().print(output_table)\n\n    def files(self, table: Table, history: bool) -&gt; None:\n        if history:\n            snapshots = table.metadata.snapshots\n        else:\n            if snapshot := table.current_snapshot():\n                snapshots = [snapshot]\n            else:\n                snapshots = []\n\n        snapshot_tree = Tree(f\"Snapshots: {'.'.join(table.name())}\")\n        io = table.io\n\n        for snapshot in snapshots:\n            list_tree = snapshot_tree.add(\n                f\"Snapshot {snapshot.snapshot_id}, schema {snapshot.schema_id}: {snapshot.manifest_list}\"\n            )\n\n            manifest_list = snapshot.manifests(io)\n            for manifest in manifest_list:\n                manifest_tree = list_tree.add(f\"Manifest: {manifest.manifest_path}\")\n                for manifest_entry in manifest.fetch_manifest_entry(io, discard_deleted=False):\n                    manifest_tree.add(f\"Datafile: {manifest_entry.data_file.file_path}\")\n        Console().print(snapshot_tree)\n\n    def describe_properties(self, properties: Properties) -&gt; None:\n        output_table = self._table\n        for k, v in properties.items():\n            output_table.add_row(k, v)\n        Console().print(output_table)\n\n    def text(self, response: str) -&gt; None:\n        Console(soft_wrap=True).print(response)\n\n    def schema(self, schema: Schema) -&gt; None:\n        output_table = self._table\n        for field in schema.fields:\n            output_table.add_row(field.name, str(field.field_type), field.doc or \"\")\n        Console().print(output_table)\n\n    def spec(self, spec: PartitionSpec) -&gt; None:\n        Console().print(str(spec))\n\n    def uuid(self, uuid: UUID | None) -&gt; None:\n        Console().print(str(uuid) if uuid else \"missing\")\n\n    def version(self, version: str) -&gt; None:\n        Console().print(version)\n\n    def describe_refs(self, ref_details: list[tuple[str, SnapshotRefType, dict[str, str]]]) -&gt; None:\n        refs_table = RichTable(title=\"Snapshot Refs\")\n        refs_table.add_column(\"Ref\")\n        refs_table.add_column(\"Type\")\n        refs_table.add_column(\"Max ref age ms\")\n        refs_table.add_column(\"Min snapshots to keep\")\n        refs_table.add_column(\"Max snapshot age ms\")\n        for name, type, ref_detail in ref_details:\n            refs_table.add_row(\n                name, type, ref_detail[\"max_ref_age_ms\"], ref_detail[\"min_snapshots_to_keep\"], ref_detail[\"max_snapshot_age_ms\"]\n            )\n        Console().print(refs_table)\n</code></pre>"},{"location":"reference/pyiceberg/cli/output/#pyiceberg.cli.output.JsonOutput","title":"<code>JsonOutput</code>","text":"<p>               Bases: <code>Output</code></p> <p>Writes json to stdout.</p> Source code in <code>pyiceberg/cli/output.py</code> <pre><code>class JsonOutput(Output):\n    \"\"\"Writes json to stdout.\"\"\"\n\n    verbose: bool\n\n    def __init__(self, **properties: Any) -&gt; None:\n        self.verbose = properties.get(\"verbose\", False)\n\n    def _out(self, d: Any) -&gt; None:\n        print(json.dumps(d))\n\n    def exception(self, ex: Exception) -&gt; None:\n        self._out({\"type\": ex.__class__.__name__, \"message\": str(ex)})\n\n    def identifiers(self, identifiers: list[Identifier]) -&gt; None:\n        self._out([\".\".join(identifier) for identifier in identifiers])\n\n    def describe_table(self, table: Table) -&gt; None:\n        class FauxTable(IcebergBaseModel):\n            \"\"\"Just to encode it using Pydantic.\"\"\"\n\n            identifier: Identifier\n            metadata_location: str\n            metadata: TableMetadata\n\n        print(\n            FauxTable(\n                identifier=table.name(), metadata=table.metadata, metadata_location=table.metadata_location\n            ).model_dump_json()\n        )\n\n    def describe_properties(self, properties: Properties) -&gt; None:\n        self._out(properties)\n\n    def text(self, response: str) -&gt; None:\n        print(json.dumps(response))\n\n    def schema(self, schema: Schema) -&gt; None:\n        print(schema.model_dump_json())\n\n    def files(self, table: Table, history: bool) -&gt; None:\n        pass\n\n    def spec(self, spec: PartitionSpec) -&gt; None:\n        print(spec.model_dump_json())\n\n    def uuid(self, uuid: UUID | None) -&gt; None:\n        self._out({\"uuid\": str(uuid) if uuid else \"missing\"})\n\n    def version(self, version: str) -&gt; None:\n        self._out({\"version\": version})\n\n    def describe_refs(self, refs: list[tuple[str, SnapshotRefType, dict[str, str]]]) -&gt; None:\n        self._out(\n            [\n                {\"name\": name, \"type\": type, detail_key: detail_val}\n                for name, type, detail in refs\n                for detail_key, detail_val in detail.items()\n            ]\n        )\n</code></pre>"},{"location":"reference/pyiceberg/cli/output/#pyiceberg.cli.output.Output","title":"<code>Output</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Output interface for exporting.</p> Source code in <code>pyiceberg/cli/output.py</code> <pre><code>class Output(ABC):\n    \"\"\"Output interface for exporting.\"\"\"\n\n    @abstractmethod\n    def exception(self, ex: Exception) -&gt; None: ...\n\n    @abstractmethod\n    def identifiers(self, identifiers: list[Identifier]) -&gt; None: ...\n\n    @abstractmethod\n    def describe_table(self, table: Table) -&gt; None: ...\n\n    @abstractmethod\n    def files(self, table: Table, history: bool) -&gt; None: ...\n\n    @abstractmethod\n    def describe_properties(self, properties: Properties) -&gt; None: ...\n\n    @abstractmethod\n    def text(self, response: str) -&gt; None: ...\n\n    @abstractmethod\n    def schema(self, schema: Schema) -&gt; None: ...\n\n    @abstractmethod\n    def spec(self, spec: PartitionSpec) -&gt; None: ...\n\n    @abstractmethod\n    def uuid(self, uuid: UUID | None) -&gt; None: ...\n\n    @abstractmethod\n    def version(self, version: str) -&gt; None: ...\n\n    @abstractmethod\n    def describe_refs(self, refs: list[tuple[str, SnapshotRefType, dict[str, str]]]) -&gt; None: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/","title":"expressions","text":""},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysFalse","title":"<code>AlwaysFalse</code>","text":"<p>               Bases: <code>BooleanExpression</code>, <code>Singleton</code>, <code>IcebergRootModel[bool]</code></p> <p>FALSE expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class AlwaysFalse(BooleanExpression, Singleton, IcebergRootModel[bool]):\n    \"\"\"FALSE expression.\"\"\"\n\n    root: bool = False\n\n    def __invert__(self) -&gt; AlwaysTrue:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return AlwaysTrue()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the AlwaysFalse class.\"\"\"\n        return \"AlwaysFalse()\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the AlwaysFalse class.\"\"\"\n        return \"AlwaysFalse()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysFalse.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; AlwaysTrue:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return AlwaysTrue()\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysFalse.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the AlwaysFalse class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the AlwaysFalse class.\"\"\"\n    return \"AlwaysFalse()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysFalse.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the AlwaysFalse class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the AlwaysFalse class.\"\"\"\n    return \"AlwaysFalse()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysTrue","title":"<code>AlwaysTrue</code>","text":"<p>               Bases: <code>BooleanExpression</code>, <code>Singleton</code>, <code>IcebergRootModel[bool]</code></p> <p>TRUE expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class AlwaysTrue(BooleanExpression, Singleton, IcebergRootModel[bool]):\n    \"\"\"TRUE expression.\"\"\"\n\n    root: bool = True\n\n    def __invert__(self) -&gt; AlwaysFalse:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return AlwaysFalse()\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the AlwaysTrue class.\"\"\"\n        return \"AlwaysTrue()\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the AlwaysTrue class.\"\"\"\n        return \"AlwaysTrue()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysTrue.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; AlwaysFalse:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return AlwaysFalse()\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysTrue.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the AlwaysTrue class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the AlwaysTrue class.\"\"\"\n    return \"AlwaysTrue()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.AlwaysTrue.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the AlwaysTrue class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the AlwaysTrue class.\"\"\"\n    return \"AlwaysTrue()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And","title":"<code>And</code>","text":"<p>               Bases: <code>BooleanExpression</code></p> <p>AND operation expression - logical conjunction.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class And(BooleanExpression):\n    \"\"\"AND operation expression - logical conjunction.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    type: TypingLiteral[\"and\"] = Field(default=\"and\", alias=\"type\")\n    left: SerializableBooleanExpression = Field()\n    right: SerializableBooleanExpression = Field()\n\n    def __init__(self, left: BooleanExpression, right: BooleanExpression, *rest: BooleanExpression, **_: Any) -&gt; None:\n        if isinstance(self, And) and not hasattr(self, \"left\") and not hasattr(self, \"right\"):\n            super().__init__(left=left, right=right)\n\n    def __new__(cls, left: BooleanExpression, right: BooleanExpression, *rest: BooleanExpression, **_: Any) -&gt; BooleanExpression:\n        if rest:\n            return _build_balanced_tree(And, (left, right, *rest))\n        if left is AlwaysFalse() or right is AlwaysFalse():\n            return AlwaysFalse()\n        elif left is AlwaysTrue():\n            return right\n        elif right is AlwaysTrue():\n            return left\n        else:\n            return super().__new__(cls)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the And class.\"\"\"\n        return self.left == other.left and self.right == other.right if isinstance(other, And) else False\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the And class.\"\"\"\n        return f\"And(left={str(self.left)}, right={str(self.right)})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the And class.\"\"\"\n        return f\"And(left={repr(self.left)}, right={repr(self.right)})\"\n\n    def __invert__(self) -&gt; BooleanExpression:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        # De Morgan's law: not (A and B) = (not A) or (not B)\n        return Or(~self.left, ~self.right)\n\n    def __getnewargs__(self) -&gt; tuple[BooleanExpression, BooleanExpression]:\n        \"\"\"Pickle the And class.\"\"\"\n        return (self.left, self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the And class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the And class.\"\"\"\n    return self.left == other.left and self.right == other.right if isinstance(other, And) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the And class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[BooleanExpression, BooleanExpression]:\n    \"\"\"Pickle the And class.\"\"\"\n    return (self.left, self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BooleanExpression:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    # De Morgan's law: not (A and B) = (not A) or (not B)\n    return Or(~self.left, ~self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the And class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the And class.\"\"\"\n    return f\"And(left={repr(self.left)}, right={repr(self.right)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.And.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the And class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the And class.\"\"\"\n    return f\"And(left={str(self.left)}, right={str(self.right)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BooleanExpression","title":"<code>BooleanExpression</code>","text":"<p>               Bases: <code>IcebergBaseModel</code>, <code>ABC</code></p> <p>An expression that evaluates to a boolean.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BooleanExpression(IcebergBaseModel, ABC):\n    \"\"\"An expression that evaluates to a boolean.\"\"\"\n\n    @abstractmethod\n    def __invert__(self) -&gt; BooleanExpression:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n\n    def __and__(self, other: BooleanExpression) -&gt; BooleanExpression:\n        \"\"\"Perform and operation on another expression.\"\"\"\n        if not isinstance(other, BooleanExpression):\n            raise ValueError(f\"Expected BooleanExpression, got: {other}\")\n\n        return And(self, other)\n\n    def __or__(self, other: BooleanExpression) -&gt; BooleanExpression:\n        \"\"\"Perform or operation on another expression.\"\"\"\n        if not isinstance(other, BooleanExpression):\n            raise ValueError(f\"Expected BooleanExpression, got: {other}\")\n\n        return Or(self, other)\n\n    @model_validator(mode=\"wrap\")\n    @classmethod\n    def handle_primitive_type(cls, v: Any, handler: ValidatorFunctionWrapHandler) -&gt; BooleanExpression:\n        \"\"\"Apply custom deserialization logic before validation.\"\"\"\n        # Already a BooleanExpression? return as-is so we keep the concrete subclass.\n        if isinstance(v, BooleanExpression):\n            return v\n\n        # Handle different input formats\n        if isinstance(v, bool):\n            return AlwaysTrue() if v is True else AlwaysFalse()\n\n        if isinstance(v, dict) and (field_type := v.get(\"type\")):\n            # Unary\n            if field_type == \"is-null\":\n                return IsNull(**v)\n            elif field_type == \"not-null\":\n                return NotNull(**v)\n            elif field_type == \"is-nan\":\n                return IsNaN(**v)\n            elif field_type == \"not-nan\":\n                return NotNaN(**v)\n\n            # Literal\n            elif field_type == \"lt\":\n                return LessThan(**v)\n            elif field_type == \"lt-eq\":\n                return LessThanOrEqual(**v)\n            elif field_type == \"gt\":\n                return GreaterThan(**v)\n            elif field_type == \"gt-eq\":\n                return GreaterThanOrEqual(**v)\n            elif field_type == \"eq\":\n                return EqualTo(**v)\n            elif field_type == \"not-eq\":\n                return NotEqualTo(**v)\n            elif field_type == \"starts-with\":\n                return StartsWith(**v)\n            elif field_type == \"not-starts-with\":\n                return NotStartsWith(**v)\n\n            # Set\n            elif field_type == \"in\":\n                return In(**v)\n            elif field_type == \"not-in\":\n                return NotIn(**v)\n\n            # Other\n            elif field_type == \"and\":\n                return And(**v)\n            elif field_type == \"or\":\n                return Or(**v)\n            elif field_type == \"not\":\n                return Not(**v)\n\n        return handler(v)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BooleanExpression.__and__","title":"<code>__and__(other)</code>","text":"<p>Perform and operation on another expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __and__(self, other: BooleanExpression) -&gt; BooleanExpression:\n    \"\"\"Perform and operation on another expression.\"\"\"\n    if not isinstance(other, BooleanExpression):\n        raise ValueError(f\"Expected BooleanExpression, got: {other}\")\n\n    return And(self, other)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BooleanExpression.__invert__","title":"<code>__invert__()</code>  <code>abstractmethod</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>@abstractmethod\ndef __invert__(self) -&gt; BooleanExpression:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BooleanExpression.__or__","title":"<code>__or__(other)</code>","text":"<p>Perform or operation on another expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __or__(self, other: BooleanExpression) -&gt; BooleanExpression:\n    \"\"\"Perform or operation on another expression.\"\"\"\n    if not isinstance(other, BooleanExpression):\n        raise ValueError(f\"Expected BooleanExpression, got: {other}\")\n\n    return Or(self, other)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BooleanExpression.handle_primitive_type","title":"<code>handle_primitive_type(v, handler)</code>  <code>classmethod</code>","text":"<p>Apply custom deserialization logic before validation.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>@model_validator(mode=\"wrap\")\n@classmethod\ndef handle_primitive_type(cls, v: Any, handler: ValidatorFunctionWrapHandler) -&gt; BooleanExpression:\n    \"\"\"Apply custom deserialization logic before validation.\"\"\"\n    # Already a BooleanExpression? return as-is so we keep the concrete subclass.\n    if isinstance(v, BooleanExpression):\n        return v\n\n    # Handle different input formats\n    if isinstance(v, bool):\n        return AlwaysTrue() if v is True else AlwaysFalse()\n\n    if isinstance(v, dict) and (field_type := v.get(\"type\")):\n        # Unary\n        if field_type == \"is-null\":\n            return IsNull(**v)\n        elif field_type == \"not-null\":\n            return NotNull(**v)\n        elif field_type == \"is-nan\":\n            return IsNaN(**v)\n        elif field_type == \"not-nan\":\n            return NotNaN(**v)\n\n        # Literal\n        elif field_type == \"lt\":\n            return LessThan(**v)\n        elif field_type == \"lt-eq\":\n            return LessThanOrEqual(**v)\n        elif field_type == \"gt\":\n            return GreaterThan(**v)\n        elif field_type == \"gt-eq\":\n            return GreaterThanOrEqual(**v)\n        elif field_type == \"eq\":\n            return EqualTo(**v)\n        elif field_type == \"not-eq\":\n            return NotEqualTo(**v)\n        elif field_type == \"starts-with\":\n            return StartsWith(**v)\n        elif field_type == \"not-starts-with\":\n            return NotStartsWith(**v)\n\n        # Set\n        elif field_type == \"in\":\n            return In(**v)\n        elif field_type == \"not-in\":\n            return NotIn(**v)\n\n        # Other\n        elif field_type == \"and\":\n            return And(**v)\n        elif field_type == \"or\":\n            return Or(**v)\n        elif field_type == \"not\":\n            return Not(**v)\n\n    return handler(v)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Bound","title":"<code>Bound</code>","text":"<p>Represents a bound value expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Bound:\n    \"\"\"Represents a bound value expression.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundEqualTo","title":"<code>BoundEqualTo</code>","text":"<p>               Bases: <code>BoundLiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundEqualTo(BoundLiteralPredicate):\n    def __invert__(self) -&gt; BoundNotEqualTo:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotEqualTo(self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; type[EqualTo]:\n        return EqualTo\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundEqualTo.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotEqualTo:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotEqualTo(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundGreaterThan","title":"<code>BoundGreaterThan</code>","text":"<p>               Bases: <code>BoundLiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundGreaterThan(BoundLiteralPredicate):\n    def __invert__(self) -&gt; BoundLessThanOrEqual:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundLessThanOrEqual(self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; type[GreaterThan]:\n        return GreaterThan\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundGreaterThan.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundLessThanOrEqual:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundLessThanOrEqual(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundGreaterThanOrEqual","title":"<code>BoundGreaterThanOrEqual</code>","text":"<p>               Bases: <code>BoundLiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundGreaterThanOrEqual(BoundLiteralPredicate):\n    def __invert__(self) -&gt; BoundLessThan:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundLessThan(self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; type[GreaterThanOrEqual]:\n        return GreaterThanOrEqual\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundGreaterThanOrEqual.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundLessThan:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundLessThan(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIn","title":"<code>BoundIn</code>","text":"<p>               Bases: <code>BoundSetPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundIn(BoundSetPredicate):\n    def __new__(cls, term: BoundTerm, literals: set[LiteralValue]) -&gt; BooleanExpression:  # pylint: disable=W0221\n        count = len(literals)\n        if count == 0:\n            return AlwaysFalse()\n        elif count == 1:\n            return BoundEqualTo(term, next(iter(literals)))\n        else:\n            return super().__new__(cls)\n\n    def __invert__(self) -&gt; BoundNotIn:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotIn(self.term, self.literals)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundIn class.\"\"\"\n        return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n\n    @property\n    def as_unbound(self) -&gt; type[In]:\n        return In\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIn.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundIn class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundIn class.\"\"\"\n    return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIn.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotIn:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotIn(self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIsNaN","title":"<code>BoundIsNaN</code>","text":"<p>               Bases: <code>BoundUnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundIsNaN(BoundUnaryPredicate):\n    def __new__(cls, term: BoundTerm) -&gt; BooleanExpression:  # pylint: disable=W0221\n        bound_type = term.ref().field.field_type\n        if isinstance(bound_type, (FloatType, DoubleType)):\n            return super().__new__(cls)\n        return AlwaysFalse()\n\n    def __invert__(self) -&gt; BoundNotNaN:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotNaN(self.term)\n\n    @property\n    def as_unbound(self) -&gt; type[IsNaN]:\n        return IsNaN\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIsNaN.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotNaN:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotNaN(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIsNull","title":"<code>BoundIsNull</code>","text":"<p>               Bases: <code>BoundUnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundIsNull(BoundUnaryPredicate):\n    def __new__(cls, term: BoundTerm) -&gt; BooleanExpression:  # pylint: disable=W0221\n        if term.ref().field.required:\n            return AlwaysFalse()\n        return super().__new__(cls)\n\n    def __invert__(self) -&gt; BoundNotNull:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotNull(self.term)\n\n    @property\n    def as_unbound(self) -&gt; type[IsNull]:\n        return IsNull\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundIsNull.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotNull:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotNull(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLessThan","title":"<code>BoundLessThan</code>","text":"<p>               Bases: <code>BoundLiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundLessThan(BoundLiteralPredicate):\n    def __invert__(self) -&gt; BoundGreaterThanOrEqual:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundGreaterThanOrEqual(self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; type[LessThan]:\n        return LessThan\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLessThan.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundGreaterThanOrEqual:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundGreaterThanOrEqual(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLessThanOrEqual","title":"<code>BoundLessThanOrEqual</code>","text":"<p>               Bases: <code>BoundLiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundLessThanOrEqual(BoundLiteralPredicate):\n    def __invert__(self) -&gt; BoundGreaterThan:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundGreaterThan(self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; type[LessThanOrEqual]:\n        return LessThanOrEqual\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLessThanOrEqual.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundGreaterThan:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundGreaterThan(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLiteralPredicate","title":"<code>BoundLiteralPredicate</code>","text":"<p>               Bases: <code>BoundPredicate</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundLiteralPredicate(BoundPredicate, ABC):\n    literal: LiteralValue\n\n    def __init__(self, term: BoundTerm, literal: LiteralValue):  # pylint: disable=W0621\n        super().__init__(term=term, literal=literal)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundLiteralPredicate class.\"\"\"\n        if isinstance(other, self.__class__):\n            return self.term == other.term and self.literal == other.literal\n        return False\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundLiteralPredicate class.\"\"\"\n        return f\"{self.__class__.__name__}(term={str(self.term)}, literal={repr(self.literal)})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundLiteralPredicate class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n\n    @property\n    @abstractmethod\n    def as_unbound(self) -&gt; type[LiteralPredicate]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLiteralPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundLiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundLiteralPredicate class.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.term == other.term and self.literal == other.literal\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLiteralPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BoundLiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundLiteralPredicate class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundLiteralPredicate.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the BoundLiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundLiteralPredicate class.\"\"\"\n    return f\"{self.__class__.__name__}(term={str(self.term)}, literal={repr(self.literal)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotEqualTo","title":"<code>BoundNotEqualTo</code>","text":"<p>               Bases: <code>BoundLiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotEqualTo(BoundLiteralPredicate):\n    def __invert__(self) -&gt; BoundEqualTo:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundEqualTo(self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; type[NotEqualTo]:\n        return NotEqualTo\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotEqualTo.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundEqualTo:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundEqualTo(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotIn","title":"<code>BoundNotIn</code>","text":"<p>               Bases: <code>BoundSetPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotIn(BoundSetPredicate):\n    def __new__(  # pylint: disable=W0221\n        cls,\n        term: BoundTerm,\n        literals: set[LiteralValue],\n    ) -&gt; BooleanExpression:\n        count = len(literals)\n        if count == 0:\n            return AlwaysTrue()\n        elif count == 1:\n            return BoundNotEqualTo(term, next(iter(literals)))\n        else:\n            return super().__new__(cls)\n\n    def __invert__(self) -&gt; BoundIn:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundIn(self.term, self.literals)\n\n    @property\n    def as_unbound(self) -&gt; type[NotIn]:\n        return NotIn\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotIn.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundIn:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundIn(self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotNaN","title":"<code>BoundNotNaN</code>","text":"<p>               Bases: <code>BoundUnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotNaN(BoundUnaryPredicate):\n    def __new__(cls, term: BoundTerm) -&gt; BooleanExpression:  # pylint: disable=W0221\n        bound_type = term.ref().field.field_type\n        if isinstance(bound_type, (FloatType, DoubleType)):\n            return super().__new__(cls)\n        return AlwaysTrue()\n\n    def __invert__(self) -&gt; BoundIsNaN:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundIsNaN(self.term)\n\n    @property\n    def as_unbound(self) -&gt; type[NotNaN]:\n        return NotNaN\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotNaN.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundIsNaN:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundIsNaN(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotNull","title":"<code>BoundNotNull</code>","text":"<p>               Bases: <code>BoundUnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotNull(BoundUnaryPredicate):\n    def __new__(cls, term: BoundTerm) -&gt; BooleanExpression:  # pylint: disable=W0221\n        if term.ref().field.required:\n            return AlwaysTrue()\n        return super().__new__(cls)\n\n    def __invert__(self) -&gt; BoundIsNull:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundIsNull(self.term)\n\n    @property\n    def as_unbound(self) -&gt; type[NotNull]:\n        return NotNull\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotNull.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundIsNull:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundIsNull(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotStartsWith","title":"<code>BoundNotStartsWith</code>","text":"<p>               Bases: <code>BoundLiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundNotStartsWith(BoundLiteralPredicate):\n    def __invert__(self) -&gt; BoundStartsWith:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundStartsWith(self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; type[NotStartsWith]:\n        return NotStartsWith\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundNotStartsWith.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundStartsWith:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundStartsWith(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundPredicate","title":"<code>BoundPredicate</code>","text":"<p>               Bases: <code>Bound</code>, <code>BooleanExpression</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundPredicate(Bound, BooleanExpression, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    term: BoundTerm\n\n    def __init__(self, term: BoundTerm, **kwargs: Any) -&gt; None:\n        super().__init__(term=term, **kwargs)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundPredicate class.\"\"\"\n        if isinstance(other, self.__class__):\n            return self.term == other.term\n        return False\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundPredicate class.\"\"\"\n        return f\"{self.__class__.__name__}(term={str(self.term)})\"\n\n    @property\n    @abstractmethod\n    def as_unbound(self) -&gt; type[UnboundPredicate]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundPredicate class.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.term == other.term\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundPredicate.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the BoundPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundPredicate class.\"\"\"\n    return f\"{self.__class__.__name__}(term={str(self.term)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference","title":"<code>BoundReference</code>","text":"<p>               Bases: <code>BoundTerm</code></p> <p>A reference bound to a field in a schema.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>NestedField</code> <p>A referenced field in an Iceberg schema.</p> required <code>accessor</code> <code>Accessor</code> <p>An Accessor object to access the value at the field's position.</p> required Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundReference(BoundTerm):\n    \"\"\"A reference bound to a field in a schema.\n\n    Args:\n        field (NestedField): A referenced field in an Iceberg schema.\n        accessor (Accessor): An Accessor object to access the value at the field's position.\n    \"\"\"\n\n    field: NestedField\n    accessor: Accessor\n\n    def __init__(self, field: NestedField, accessor: Accessor):\n        self.field = field\n        self.accessor = accessor\n\n    def eval(self, struct: StructProtocol) -&gt; Any:\n        \"\"\"Return the value at the referenced field's position in an object that abides by the StructProtocol.\n\n        Args:\n            struct (StructProtocol): A row object that abides by the StructProtocol and returns values given a position.\n        Returns:\n            Any: The value at the referenced field's position in `struct`.\n        \"\"\"\n        return self.accessor.get(struct)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundReference class.\"\"\"\n        return self.field == other.field if isinstance(other, BoundReference) else False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundReference class.\"\"\"\n        return f\"BoundReference(field={repr(self.field)}, accessor={repr(self.accessor)})\"\n\n    def ref(self) -&gt; BoundReference:\n        return self\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return hash value of the BoundReference class.\"\"\"\n        return hash(str(self))\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundReference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundReference class.\"\"\"\n    return self.field == other.field if isinstance(other, BoundReference) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference.__hash__","title":"<code>__hash__()</code>","text":"<p>Return hash value of the BoundReference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return hash value of the BoundReference class.\"\"\"\n    return hash(str(self))\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BoundReference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundReference class.\"\"\"\n    return f\"BoundReference(field={repr(self.field)}, accessor={repr(self.accessor)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundReference.eval","title":"<code>eval(struct)</code>","text":"<p>Return the value at the referenced field's position in an object that abides by the StructProtocol.</p> <p>Parameters:</p> Name Type Description Default <code>struct</code> <code>StructProtocol</code> <p>A row object that abides by the StructProtocol and returns values given a position.</p> required <p>Returns:     Any: The value at the referenced field's position in <code>struct</code>.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def eval(self, struct: StructProtocol) -&gt; Any:\n    \"\"\"Return the value at the referenced field's position in an object that abides by the StructProtocol.\n\n    Args:\n        struct (StructProtocol): A row object that abides by the StructProtocol and returns values given a position.\n    Returns:\n        Any: The value at the referenced field's position in `struct`.\n    \"\"\"\n    return self.accessor.get(struct)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate","title":"<code>BoundSetPredicate</code>","text":"<p>               Bases: <code>BoundPredicate</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundSetPredicate(BoundPredicate, ABC):\n    literals: set[LiteralValue]\n\n    def __init__(self, term: BoundTerm, literals: set[LiteralValue]) -&gt; None:\n        literal_set = _to_literal_set(literals)\n        super().__init__(term=term, literals=literal_set)\n\n    @cached_property\n    def value_set(self) -&gt; set[Any]:\n        return {lit.value for lit in self.literals}\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundSetPredicate class.\"\"\"\n        # Sort to make it deterministic\n        literals_str = \", \".join(sorted([str(literal) for literal in self.literals]))\n        return f\"{str(self.__class__.__name__)}({str(self.term)}, {{{literals_str}}})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundSetPredicate class.\"\"\"\n        # Sort to make it deterministic\n        literals_repr = \", \".join(sorted([repr(literal) for literal in self.literals]))\n        return f\"{str(self.__class__.__name__)}({repr(self.term)}, {{{literals_repr}}})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the BoundSetPredicate class.\"\"\"\n        return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n\n    def __getnewargs__(self) -&gt; tuple[BoundTerm, set[LiteralValue]]:\n        \"\"\"Pickle the BoundSetPredicate class.\"\"\"\n        return (self.term, self.literals)\n\n    @property\n    @abstractmethod\n    def as_unbound(self) -&gt; type[SetPredicate]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the BoundSetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the BoundSetPredicate class.\"\"\"\n    return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the BoundSetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[BoundTerm, set[LiteralValue]]:\n    \"\"\"Pickle the BoundSetPredicate class.\"\"\"\n    return (self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BoundSetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundSetPredicate class.\"\"\"\n    # Sort to make it deterministic\n    literals_repr = \", \".join(sorted([repr(literal) for literal in self.literals]))\n    return f\"{str(self.__class__.__name__)}({repr(self.term)}, {{{literals_repr}}})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundSetPredicate.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the BoundSetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundSetPredicate class.\"\"\"\n    # Sort to make it deterministic\n    literals_str = \", \".join(sorted([str(literal) for literal in self.literals]))\n    return f\"{str(self.__class__.__name__)}({str(self.term)}, {{{literals_str}}})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundStartsWith","title":"<code>BoundStartsWith</code>","text":"<p>               Bases: <code>BoundLiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundStartsWith(BoundLiteralPredicate):\n    def __invert__(self) -&gt; BoundNotStartsWith:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return BoundNotStartsWith(self.term, self.literal)\n\n    @property\n    def as_unbound(self) -&gt; type[StartsWith]:\n        return StartsWith\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundStartsWith.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BoundNotStartsWith:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return BoundNotStartsWith(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundTerm","title":"<code>BoundTerm</code>","text":"<p>               Bases: <code>Term</code>, <code>Bound</code>, <code>ABC</code></p> <p>Represents a bound term.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundTerm(Term, Bound, ABC):\n    \"\"\"Represents a bound term.\"\"\"\n\n    @abstractmethod\n    def ref(self) -&gt; BoundReference:\n        \"\"\"Return the bound reference.\"\"\"\n\n    @abstractmethod\n    def eval(self, struct: StructProtocol) -&gt; Any:  # pylint: disable=W0613\n        \"\"\"Return the value at the referenced field's position in an object that abides by the StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundTerm.eval","title":"<code>eval(struct)</code>  <code>abstractmethod</code>","text":"<p>Return the value at the referenced field's position in an object that abides by the StructProtocol.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>@abstractmethod\ndef eval(self, struct: StructProtocol) -&gt; Any:  # pylint: disable=W0613\n    \"\"\"Return the value at the referenced field's position in an object that abides by the StructProtocol.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundTerm.ref","title":"<code>ref()</code>  <code>abstractmethod</code>","text":"<p>Return the bound reference.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>@abstractmethod\ndef ref(self) -&gt; BoundReference:\n    \"\"\"Return the bound reference.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundUnaryPredicate","title":"<code>BoundUnaryPredicate</code>","text":"<p>               Bases: <code>BoundPredicate</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class BoundUnaryPredicate(BoundPredicate, ABC):\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BoundUnaryPredicate class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(term={repr(self.term)})\"\n\n    @property\n    @abstractmethod\n    def as_unbound(self) -&gt; type[UnaryPredicate]: ...\n\n    def __getnewargs__(self) -&gt; tuple[BoundTerm]:\n        \"\"\"Pickle the BoundUnaryPredicate class.\"\"\"\n        return (self.term,)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundUnaryPredicate.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the BoundUnaryPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[BoundTerm]:\n    \"\"\"Pickle the BoundUnaryPredicate class.\"\"\"\n    return (self.term,)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.BoundUnaryPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BoundUnaryPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BoundUnaryPredicate class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(term={repr(self.term)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.EqualTo","title":"<code>EqualTo</code>","text":"<p>               Bases: <code>LiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class EqualTo(LiteralPredicate):\n    type: TypingLiteral[\"eq\"] = Field(default=\"eq\", alias=\"type\")\n\n    def __invert__(self) -&gt; NotEqualTo:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotEqualTo(self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; type[BoundEqualTo]:  # type: ignore\n        return BoundEqualTo\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.EqualTo.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotEqualTo:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotEqualTo(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.GreaterThan","title":"<code>GreaterThan</code>","text":"<p>               Bases: <code>LiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class GreaterThan(LiteralPredicate):\n    type: TypingLiteral[\"gt\"] = Field(default=\"gt\", alias=\"type\")\n\n    def __invert__(self) -&gt; LessThanOrEqual:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return LessThanOrEqual(self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; type[BoundGreaterThan]:  # type: ignore\n        return BoundGreaterThan\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.GreaterThan.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; LessThanOrEqual:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return LessThanOrEqual(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.GreaterThanOrEqual","title":"<code>GreaterThanOrEqual</code>","text":"<p>               Bases: <code>LiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class GreaterThanOrEqual(LiteralPredicate):\n    type: TypingLiteral[\"gt-eq\"] = Field(default=\"gt-eq\", alias=\"type\")\n\n    def __invert__(self) -&gt; LessThan:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return LessThan(self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; type[BoundGreaterThanOrEqual]:  # type: ignore\n        return BoundGreaterThanOrEqual\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.GreaterThanOrEqual.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; LessThan:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return LessThan(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.In","title":"<code>In</code>","text":"<p>               Bases: <code>SetPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class In(SetPredicate):\n    type: TypingLiteral[\"in\"] = Field(default=\"in\", alias=\"type\")\n\n    def __new__(  # pylint: disable=W0221\n        cls, term: str | UnboundTerm, literals: Iterable[Any] | Iterable[LiteralValue] | None = None, **kwargs: Any\n    ) -&gt; In:\n        if literals is None and \"values\" in kwargs:\n            literals = kwargs[\"values\"]\n\n        if literals is None:\n            literals_set: set[LiteralValue] = set()\n        else:\n            literals_set = _to_literal_set(literals)\n        count = len(literals_set)\n        if count == 0:\n            return AlwaysFalse()\n        elif count == 1:\n            return EqualTo(term, next(iter(literals_set)))\n        else:\n            return super().__new__(cls)\n\n    def __invert__(self) -&gt; NotIn:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotIn(self.term, self.literals)\n\n    @property\n    def as_bound(self) -&gt; type[BoundIn]:  # type: ignore\n        return BoundIn\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.In.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotIn:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotIn(self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.IsNaN","title":"<code>IsNaN</code>","text":"<p>               Bases: <code>UnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class IsNaN(UnaryPredicate):\n    type: TypingLiteral[\"is-nan\"] = Field(default=\"is-nan\")\n\n    def __invert__(self) -&gt; NotNaN:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotNaN(self.term)\n\n    @property\n    def as_bound(self) -&gt; type[BoundIsNaN]:  # type: ignore\n        return BoundIsNaN\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.IsNaN.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotNaN:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotNaN(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.IsNull","title":"<code>IsNull</code>","text":"<p>               Bases: <code>UnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class IsNull(UnaryPredicate):\n    type: TypingLiteral[\"is-null\"] = Field(default=\"is-null\")\n\n    def __invert__(self) -&gt; NotNull:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotNull(self.term)\n\n    @property\n    def as_bound(self) -&gt; type[BoundIsNull]:  # type: ignore\n        return BoundIsNull\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.IsNull.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotNull:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotNull(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LessThan","title":"<code>LessThan</code>","text":"<p>               Bases: <code>LiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class LessThan(LiteralPredicate):\n    type: TypingLiteral[\"lt\"] = Field(default=\"lt\", alias=\"type\")\n\n    def __invert__(self) -&gt; GreaterThanOrEqual:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return GreaterThanOrEqual(self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; type[BoundLessThan]:  # type: ignore\n        return BoundLessThan\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LessThan.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; GreaterThanOrEqual:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return GreaterThanOrEqual(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LessThanOrEqual","title":"<code>LessThanOrEqual</code>","text":"<p>               Bases: <code>LiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class LessThanOrEqual(LiteralPredicate):\n    type: TypingLiteral[\"lt-eq\"] = Field(default=\"lt-eq\", alias=\"type\")\n\n    def __invert__(self) -&gt; GreaterThan:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return GreaterThan(self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; type[BoundLessThanOrEqual]:  # type: ignore\n        return BoundLessThanOrEqual\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LessThanOrEqual.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; GreaterThan:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return GreaterThan(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LiteralPredicate","title":"<code>LiteralPredicate</code>","text":"<p>               Bases: <code>UnboundPredicate</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class LiteralPredicate(UnboundPredicate, ABC):\n    type: TypingLiteral[\"lt\", \"lt-eq\", \"gt\", \"gt-eq\", \"eq\", \"not-eq\", \"starts-with\", \"not-starts-with\"] = Field(alias=\"type\")\n    term: UnboundTerm\n    value: LiteralValue = Field()\n    model_config = ConfigDict(populate_by_name=True, frozen=True, arbitrary_types_allowed=True)\n\n    def __init__(self, term: str | UnboundTerm, literal: Any | None = None, **kwargs: Any) -&gt; None:\n        if literal is None and \"value\" in kwargs:\n            literal = kwargs[\"value\"]\n\n        super().__init__(term=_to_unbound_term(term), value=_to_literal(literal))\n\n    @property\n    def literal(self) -&gt; LiteralValue:\n        return self.value\n\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundLiteralPredicate:\n        bound_term = self.term.bind(schema, case_sensitive)\n        lit = self.literal.to(bound_term.ref().field.field_type)\n\n        if isinstance(lit, AboveMax):\n            if isinstance(self, (LessThan, LessThanOrEqual, NotEqualTo)):\n                return AlwaysTrue()\n            elif isinstance(self, (GreaterThan, GreaterThanOrEqual, EqualTo)):\n                return AlwaysFalse()\n        elif isinstance(lit, BelowMin):\n            if isinstance(self, (GreaterThan, GreaterThanOrEqual, NotEqualTo)):\n                return AlwaysTrue()\n            elif isinstance(self, (LessThan, LessThanOrEqual, EqualTo)):\n                return AlwaysFalse()\n\n        return self.as_bound(bound_term, lit)  # type: ignore\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the LiteralPredicate class.\"\"\"\n        if isinstance(other, self.__class__):\n            return self.term == other.term and self.literal == other.literal\n        return False\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the LiteralPredicate class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the LiteralPredicate class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; type[BoundLiteralPredicate]: ...  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LiteralPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the LiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the LiteralPredicate class.\"\"\"\n    if isinstance(other, self.__class__):\n        return self.term == other.term and self.literal == other.literal\n    return False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LiteralPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the LiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the LiteralPredicate class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.LiteralPredicate.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the LiteralPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the LiteralPredicate class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(term={repr(self.term)}, literal={repr(self.literal)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not","title":"<code>Not</code>","text":"<p>               Bases: <code>BooleanExpression</code></p> <p>NOT operation expression - logical negation.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Not(BooleanExpression):\n    \"\"\"NOT operation expression - logical negation.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    type: TypingLiteral[\"not\"] = Field(default=\"not\")\n    child: SerializableBooleanExpression = Field()\n\n    def __init__(self, child: BooleanExpression, **_: Any) -&gt; None:\n        super().__init__(child=child)\n\n    def __new__(cls, child: BooleanExpression, **_: Any) -&gt; BooleanExpression:\n        if child is AlwaysTrue():\n            return AlwaysFalse()\n        elif child is AlwaysFalse():\n            return AlwaysTrue()\n        elif isinstance(child, Not):\n            return child.child\n        else:\n            return super().__new__(cls)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Not class.\"\"\"\n        return f\"Not(child={self.child})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Not class.\"\"\"\n        return f\"Not(child={repr(self.child)})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Not class.\"\"\"\n        return self.child == other.child if isinstance(other, Not) else False\n\n    def __invert__(self) -&gt; BooleanExpression:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return self.child\n\n    def __getnewargs__(self) -&gt; tuple[BooleanExpression]:\n        \"\"\"Pickle the Not class.\"\"\"\n        return (self.child,)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Not class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Not class.\"\"\"\n    return self.child == other.child if isinstance(other, Not) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the Not class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[BooleanExpression]:\n    \"\"\"Pickle the Not class.\"\"\"\n    return (self.child,)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BooleanExpression:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return self.child\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Not class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Not class.\"\"\"\n    return f\"Not(child={repr(self.child)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Not.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Not class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Not class.\"\"\"\n    return f\"Not(child={self.child})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotEqualTo","title":"<code>NotEqualTo</code>","text":"<p>               Bases: <code>LiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotEqualTo(LiteralPredicate):\n    type: TypingLiteral[\"not-eq\"] = Field(default=\"not-eq\", alias=\"type\")\n\n    def __invert__(self) -&gt; EqualTo:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return EqualTo(self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; type[BoundNotEqualTo]:  # type: ignore\n        return BoundNotEqualTo\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotEqualTo.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; EqualTo:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return EqualTo(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotIn","title":"<code>NotIn</code>","text":"<p>               Bases: <code>SetPredicate</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotIn(SetPredicate, ABC):\n    type: TypingLiteral[\"not-in\"] = Field(default=\"not-in\", alias=\"type\")\n\n    def __new__(  # pylint: disable=W0221\n        cls, term: str | UnboundTerm, literals: Iterable[Any] | Iterable[LiteralValue] | None = None, **kwargs: Any\n    ) -&gt; NotIn:\n        if literals is None and \"values\" in kwargs:\n            literals = kwargs[\"values\"]\n\n        if literals is None:\n            literals_set: set[LiteralValue] = set()\n        else:\n            literals_set = _to_literal_set(literals)\n        count = len(literals_set)\n        if count == 0:\n            return AlwaysTrue()\n        elif count == 1:\n            return NotEqualTo(term, next(iter(literals_set)))\n        else:\n            return super().__new__(cls)\n\n    def __invert__(self) -&gt; In:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return In(self.term, self.literals)\n\n    @property\n    def as_bound(self) -&gt; type[BoundNotIn]:  # type: ignore\n        return BoundNotIn\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotIn.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; In:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return In(self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotNaN","title":"<code>NotNaN</code>","text":"<p>               Bases: <code>UnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotNaN(UnaryPredicate):\n    type: TypingLiteral[\"not-nan\"] = Field(default=\"not-nan\")\n\n    def __invert__(self) -&gt; IsNaN:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return IsNaN(self.term)\n\n    @property\n    def as_bound(self) -&gt; type[BoundNotNaN]:  # type: ignore\n        return BoundNotNaN\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotNaN.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; IsNaN:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return IsNaN(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotNull","title":"<code>NotNull</code>","text":"<p>               Bases: <code>UnaryPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotNull(UnaryPredicate):\n    type: TypingLiteral[\"not-null\"] = Field(default=\"not-null\")\n\n    def __invert__(self) -&gt; IsNull:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return IsNull(self.term)\n\n    @property\n    def as_bound(self) -&gt; type[BoundNotNull]:  # type: ignore\n        return BoundNotNull\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotNull.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; IsNull:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return IsNull(self.term)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotStartsWith","title":"<code>NotStartsWith</code>","text":"<p>               Bases: <code>LiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class NotStartsWith(LiteralPredicate):\n    type: TypingLiteral[\"not-starts-with\"] = Field(default=\"not-starts-with\", alias=\"type\")\n\n    def __invert__(self) -&gt; StartsWith:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return StartsWith(self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; type[BoundNotStartsWith]:  # type: ignore\n        return BoundNotStartsWith\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.NotStartsWith.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; StartsWith:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return StartsWith(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or","title":"<code>Or</code>","text":"<p>               Bases: <code>BooleanExpression</code></p> <p>OR operation expression - logical disjunction.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Or(BooleanExpression):\n    \"\"\"OR operation expression - logical disjunction.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    type: TypingLiteral[\"or\"] = Field(default=\"or\", alias=\"type\")\n    left: SerializableBooleanExpression = Field()\n    right: SerializableBooleanExpression = Field()\n\n    def __init__(self, left: BooleanExpression, right: BooleanExpression, *rest: BooleanExpression, **_: Any) -&gt; None:\n        if isinstance(self, Or) and not hasattr(self, \"left\") and not hasattr(self, \"right\"):\n            super().__init__(left=left, right=right)\n\n    def __new__(cls, left: BooleanExpression, right: BooleanExpression, *rest: BooleanExpression, **_: Any) -&gt; BooleanExpression:\n        if rest:\n            return _build_balanced_tree(Or, (left, right, *rest))\n        if left is AlwaysTrue() or right is AlwaysTrue():\n            return AlwaysTrue()\n        elif left is AlwaysFalse():\n            return right\n        elif right is AlwaysFalse():\n            return left\n        else:\n            return super().__new__(cls)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Or class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(left={repr(self.left)}, right={repr(self.right)})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Or class.\"\"\"\n        return self.left == other.left and self.right == other.right if isinstance(other, Or) else False\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Or class.\"\"\"\n        return f\"Or(left={repr(self.left)}, right={repr(self.right)})\"\n\n    def __invert__(self) -&gt; BooleanExpression:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        # De Morgan's law: not (A or B) = (not A) and (not B)\n        return And(~self.left, ~self.right)\n\n    def __getnewargs__(self) -&gt; tuple[BooleanExpression, BooleanExpression]:\n        \"\"\"Pickle the Or class.\"\"\"\n        return (self.left, self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Or class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Or class.\"\"\"\n    return self.left == other.left and self.right == other.right if isinstance(other, Or) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the Or class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[BooleanExpression, BooleanExpression]:\n    \"\"\"Pickle the Or class.\"\"\"\n    return (self.left, self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; BooleanExpression:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    # De Morgan's law: not (A or B) = (not A) and (not B)\n    return And(~self.left, ~self.right)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Or class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Or class.\"\"\"\n    return f\"Or(left={repr(self.left)}, right={repr(self.right)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Or.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Or class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Or class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(left={repr(self.left)}, right={repr(self.right)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Reference","title":"<code>Reference</code>","text":"<p>               Bases: <code>UnboundTerm</code>, <code>IcebergRootModel[str]</code></p> <p>A reference not yet bound to a field in a schema.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field.</p> required Note <p>An unbound reference is sometimes referred to as a \"named\" reference.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Reference(UnboundTerm, IcebergRootModel[str]):\n    \"\"\"A reference not yet bound to a field in a schema.\n\n    Args:\n        name (str): The name of the field.\n\n    Note:\n        An unbound reference is sometimes referred to as a \"named\" reference.\n    \"\"\"\n\n    root: str = Field()\n\n    def __init__(self, name: str) -&gt; None:\n        super().__init__(name)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Reference class.\"\"\"\n        return f\"Reference(name={repr(self.root)})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Reference class.\"\"\"\n        return f\"Reference(name={repr(self.root)})\"\n\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundReference:\n        \"\"\"Bind the reference to an Iceberg schema.\n\n        Args:\n            schema (Schema): An Iceberg schema.\n            case_sensitive (bool): Whether to consider case when binding the reference to the field.\n\n        Raises:\n            ValueError: If an empty name is provided.\n\n        Returns:\n            BoundReference: A reference bound to the specific field in the Iceberg schema.\n        \"\"\"\n        field = schema.find_field(name_or_id=self.name, case_sensitive=case_sensitive)\n        accessor = schema.accessor_for_field(field.field_id)\n        return self.as_bound(field=field, accessor=accessor)\n\n    @property\n    def name(self) -&gt; str:\n        return self.root\n\n    @property\n    def as_bound(self) -&gt; type[BoundReference]:\n        return BoundReference\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Reference.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Reference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Reference class.\"\"\"\n    return f\"Reference(name={repr(self.root)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Reference.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Reference class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Reference class.\"\"\"\n    return f\"Reference(name={repr(self.root)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Reference.bind","title":"<code>bind(schema, case_sensitive=True)</code>","text":"<p>Bind the reference to an Iceberg schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>An Iceberg schema.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to consider case when binding the reference to the field.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an empty name is provided.</p> <p>Returns:</p> Name Type Description <code>BoundReference</code> <code>BoundReference</code> <p>A reference bound to the specific field in the Iceberg schema.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundReference:\n    \"\"\"Bind the reference to an Iceberg schema.\n\n    Args:\n        schema (Schema): An Iceberg schema.\n        case_sensitive (bool): Whether to consider case when binding the reference to the field.\n\n    Raises:\n        ValueError: If an empty name is provided.\n\n    Returns:\n        BoundReference: A reference bound to the specific field in the Iceberg schema.\n    \"\"\"\n    field = schema.find_field(name_or_id=self.name, case_sensitive=case_sensitive)\n    accessor = schema.accessor_for_field(field.field_id)\n    return self.as_bound(field=field, accessor=accessor)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate","title":"<code>SetPredicate</code>","text":"<p>               Bases: <code>UnboundPredicate</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class SetPredicate(UnboundPredicate, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    type: TypingLiteral[\"in\", \"not-in\"] = Field(default=\"in\")\n    literals: set[LiteralValue] = Field(alias=\"values\")\n\n    def __init__(\n        self, term: str | UnboundTerm, literals: Iterable[Any] | Iterable[LiteralValue] | None = None, **kwargs: Any\n    ) -&gt; None:\n        if literals is None and \"values\" in kwargs:\n            literals = kwargs[\"values\"]\n\n        if literals is None:\n            literal_set: set[LiteralValue] = set()\n        else:\n            literal_set = _to_literal_set(literals)\n        super().__init__(term=_to_unbound_term(term), values=literal_set)\n\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundSetPredicate:\n        bound_term = self.term.bind(schema, case_sensitive)\n        literal_set = self.literals\n        return self.as_bound(bound_term, {lit.to(bound_term.ref().field.field_type) for lit in literal_set})  # type: ignore\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SetPredicate class.\"\"\"\n        # Sort to make it deterministic\n        literals_str = \", \".join(sorted([str(literal) for literal in self.literals]))\n        return f\"{str(self.__class__.__name__)}({str(self.term)}, {{{literals_str}}})\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the SetPredicate class.\"\"\"\n        # Sort to make it deterministic\n        literals_repr = \", \".join(sorted([repr(literal) for literal in self.literals]))\n        return f\"{str(self.__class__.__name__)}({repr(self.term)}, {{{literals_repr}}})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the SetPredicate class.\"\"\"\n        return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n\n    def __getnewargs__(self) -&gt; tuple[UnboundTerm, set[Any]]:\n        \"\"\"Pickle the SetPredicate class.\"\"\"\n        return (self.term, self.literals)\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; type[BoundSetPredicate]:  # type: ignore\n        return BoundSetPredicate\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the SetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the SetPredicate class.\"\"\"\n    return self.term == other.term and self.literals == other.literals if isinstance(other, self.__class__) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate.__getnewargs__","title":"<code>__getnewargs__()</code>","text":"<p>Pickle the SetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __getnewargs__(self) -&gt; tuple[UnboundTerm, set[Any]]:\n    \"\"\"Pickle the SetPredicate class.\"\"\"\n    return (self.term, self.literals)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the SetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the SetPredicate class.\"\"\"\n    # Sort to make it deterministic\n    literals_repr = \", \".join(sorted([repr(literal) for literal in self.literals]))\n    return f\"{str(self.__class__.__name__)}({repr(self.term)}, {{{literals_repr}}})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.SetPredicate.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SetPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SetPredicate class.\"\"\"\n    # Sort to make it deterministic\n    literals_str = \", \".join(sorted([str(literal) for literal in self.literals]))\n    return f\"{str(self.__class__.__name__)}({str(self.term)}, {{{literals_str}}})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.StartsWith","title":"<code>StartsWith</code>","text":"<p>               Bases: <code>LiteralPredicate</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class StartsWith(LiteralPredicate):\n    type: TypingLiteral[\"starts-with\"] = Field(default=\"starts-with\", alias=\"type\")\n\n    def __invert__(self) -&gt; NotStartsWith:\n        \"\"\"Transform the Expression into its negated version.\"\"\"\n        return NotStartsWith(self.term, self.literal)\n\n    @property\n    def as_bound(self) -&gt; type[BoundStartsWith]:  # type: ignore\n        return BoundStartsWith\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.StartsWith.__invert__","title":"<code>__invert__()</code>","text":"<p>Transform the Expression into its negated version.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __invert__(self) -&gt; NotStartsWith:\n    \"\"\"Transform the Expression into its negated version.\"\"\"\n    return NotStartsWith(self.term, self.literal)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Term","title":"<code>Term</code>","text":"<p>A simple expression that evaluates to a value.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Term:\n    \"\"\"A simple expression that evaluates to a value.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnaryPredicate","title":"<code>UnaryPredicate</code>","text":"<p>               Bases: <code>UnboundPredicate</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class UnaryPredicate(UnboundPredicate, ABC):\n    type: TypingLiteral[\"is-null\", \"not-null\", \"is-nan\", \"not-nan\"] = Field()\n\n    model_config = {\"arbitrary_types_allowed\": True}\n\n    def __init__(self, term: str | UnboundTerm, **_: Any) -&gt; None:\n        unbound = _to_unbound_term(term)\n        super().__init__(term=unbound)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the UnaryPredicate class.\"\"\"\n        # Sort to make it deterministic\n        return f\"{str(self.__class__.__name__)}(term={str(self.term)})\"\n\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundUnaryPredicate:\n        bound_term = self.term.bind(schema, case_sensitive)\n        bound_type = self.as_bound\n        return bound_type(bound_term)  # type: ignore[misc]\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the UnaryPredicate class.\"\"\"\n        return f\"{str(self.__class__.__name__)}(term={repr(self.term)})\"\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; type[BoundUnaryPredicate]: ...  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnaryPredicate.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the UnaryPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the UnaryPredicate class.\"\"\"\n    return f\"{str(self.__class__.__name__)}(term={repr(self.term)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnaryPredicate.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the UnaryPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the UnaryPredicate class.\"\"\"\n    # Sort to make it deterministic\n    return f\"{str(self.__class__.__name__)}(term={str(self.term)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.Unbound","title":"<code>Unbound</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Represents an unbound value expression.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class Unbound(ABC):\n    \"\"\"Represents an unbound value expression.\"\"\"\n\n    @abstractmethod\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; Bound | BooleanExpression: ...\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; type[Bound]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnboundPredicate","title":"<code>UnboundPredicate</code>","text":"<p>               Bases: <code>Unbound</code>, <code>BooleanExpression</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class UnboundPredicate(Unbound, BooleanExpression, ABC):\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    term: UnboundTerm\n\n    def __init__(self, term: str | UnboundTerm, **kwargs: Any) -&gt; None:\n        super().__init__(term=_to_unbound_term(term), **kwargs)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the UnboundPredicate class.\"\"\"\n        return self.term == other.term if isinstance(other, self.__class__) else False\n\n    @abstractmethod\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BooleanExpression: ...\n\n    @property\n    @abstractmethod\n    def as_bound(self) -&gt; type[BoundPredicate]: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnboundPredicate.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the UnboundPredicate class.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the UnboundPredicate class.\"\"\"\n    return self.term == other.term if isinstance(other, self.__class__) else False\n</code></pre>"},{"location":"reference/pyiceberg/expressions/#pyiceberg.expressions.UnboundTerm","title":"<code>UnboundTerm</code>","text":"<p>               Bases: <code>Term</code>, <code>Unbound</code>, <code>ABC</code></p> <p>Represents an unbound term.</p> Source code in <code>pyiceberg/expressions/__init__.py</code> <pre><code>class UnboundTerm(Term, Unbound, ABC):\n    \"\"\"Represents an unbound term.\"\"\"\n\n    @abstractmethod\n    def bind(self, schema: Schema, case_sensitive: bool = True) -&gt; BoundTerm: ...\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/","title":"literals","text":""},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.AboveMax","title":"<code>AboveMax</code>","text":"<p>               Bases: <code>Literal[L]</code></p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class AboveMax(Literal[L]):\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the AboveMax class.\"\"\"\n        return f\"{self.__class__.__name__}()\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the AboveMax class.\"\"\"\n        return self.__class__.__name__\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.AboveMax.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the AboveMax class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the AboveMax class.\"\"\"\n    return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.AboveMax.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the AboveMax class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the AboveMax class.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.BelowMin","title":"<code>BelowMin</code>","text":"<p>               Bases: <code>Literal[L]</code></p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class BelowMin(Literal[L]):\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the BelowMin class.\"\"\"\n        return f\"{self.__class__.__name__}()\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the BelowMin class.\"\"\"\n        return self.__class__.__name__\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.BelowMin.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the BelowMin class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the BelowMin class.\"\"\"\n    return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.BelowMin.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the BelowMin class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the BelowMin class.\"\"\"\n    return self.__class__.__name__\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral","title":"<code>FloatLiteral</code>","text":"<p>               Bases: <code>Literal[float]</code></p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class FloatLiteral(Literal[float]):\n    def __init__(self, value: float) -&gt; None:\n        super().__init__(value, float)\n        self._value32 = struct.unpack(\"&lt;f\", struct.pack(\"&lt;f\", value))[0]\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the FloatLiteral class.\"\"\"\n        return self._value32 == other\n\n    def __lt__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the FloatLiteral class is less than another instance.\"\"\"\n        return self._value32 &lt; other\n\n    def __gt__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the FloatLiteral class is greater than another instance.\"\"\"\n        return self._value32 &gt; other\n\n    def __le__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the FloatLiteral class is less than or equal to another instance.\"\"\"\n        return self._value32 &lt;= other\n\n    def __ge__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the FloatLiteral class is greater than or equal to another instance.\"\"\"\n        return self._value32 &gt;= other\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the FloatLiteral class.\"\"\"\n        return hash(self._value32)\n\n    @singledispatchmethod\n    def to(self, type_var: IcebergType) -&gt; Literal:  # type: ignore\n        raise TypeError(f\"Cannot convert FloatLiteral into {type_var}\")\n\n    @to.register(FloatType)\n    def _(self, _: FloatType) -&gt; Literal[float]:\n        return self\n\n    @to.register(DoubleType)\n    def _(self, _: DoubleType) -&gt; Literal[float]:\n        return DoubleLiteral(self.value)\n\n    @to.register(DecimalType)\n    def _(self, type_var: DecimalType) -&gt; Literal[Decimal]:\n        return DecimalLiteral(Decimal(self.value).quantize(Decimal((0, (1,), -type_var.scale)), rounding=ROUND_HALF_UP))\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the FloatLiteral class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the FloatLiteral class.\"\"\"\n    return self._value32 == other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__ge__","title":"<code>__ge__(other)</code>","text":"<p>Return if one instance of the FloatLiteral class is greater than or equal to another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the FloatLiteral class is greater than or equal to another instance.\"\"\"\n    return self._value32 &gt;= other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__gt__","title":"<code>__gt__(other)</code>","text":"<p>Return if one instance of the FloatLiteral class is greater than another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the FloatLiteral class is greater than another instance.\"\"\"\n    return self._value32 &gt; other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the FloatLiteral class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the FloatLiteral class.\"\"\"\n    return hash(self._value32)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__le__","title":"<code>__le__(other)</code>","text":"<p>Return if one instance of the FloatLiteral class is less than or equal to another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the FloatLiteral class is less than or equal to another instance.\"\"\"\n    return self._value32 &lt;= other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.FloatLiteral.__lt__","title":"<code>__lt__(other)</code>","text":"<p>Return if one instance of the FloatLiteral class is less than another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the FloatLiteral class is less than another instance.\"\"\"\n    return self._value32 &lt; other\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal","title":"<code>Literal</code>","text":"<p>               Bases: <code>IcebergRootModel[L]</code>, <code>Generic[L]</code>, <code>ABC</code></p> <p>Literal which has a value and can be converted between types.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class Literal(IcebergRootModel[L], Generic[L], ABC):  # type: ignore\n    \"\"\"Literal which has a value and can be converted between types.\"\"\"\n\n    root: L = Field()\n\n    def __init__(self, value: L, value_type: type[L], /, **data):  # type: ignore\n        if value is None:\n            raise TypeError(\"Invalid literal value: None\")\n\n        super().__init__(value)\n        if value is None or not isinstance(value, value_type):\n            raise TypeError(f\"Invalid literal value: {value!r} (not a {value_type})\")\n        if isinstance(value, float) and isnan(value):\n            raise ValueError(\"Cannot create expression literal from NaN.\")\n\n    @property\n    def value(self) -&gt; L:\n        return self.root\n\n    @singledispatchmethod\n    @abstractmethod\n    def to(self, type_var: IcebergType) -&gt; Literal[L]: ...  # pragma: no cover\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Literal class.\"\"\"\n        return f\"{type(self).__name__}({self.value!r})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Literal class.\"\"\"\n        return str(self.value)\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Return a hashed representation of the Literal class.\"\"\"\n        return hash(self.value)\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Literal class.\"\"\"\n        if not isinstance(other, Literal):\n            return False\n        return self.value == other.value\n\n    def __ne__(self, other: Any) -&gt; bool:\n        \"\"\"Return the inequality of two instances of the Literal class.\"\"\"\n        return not self.__eq__(other)\n\n    def __lt__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the Literal class is less than another instance.\"\"\"\n        return self.value &lt; other.value\n\n    def __gt__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the Literal class is greater than another instance.\"\"\"\n        return self.value &gt; other.value\n\n    def __le__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the Literal class is less than or equal to another instance.\"\"\"\n        return self.value &lt;= other.value\n\n    def __ge__(self, other: Any) -&gt; bool:\n        \"\"\"Return if one instance of the Literal class is greater than or equal to another instance.\"\"\"\n        return self.value &gt;= other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Literal class.\"\"\"\n    if not isinstance(other, Literal):\n        return False\n    return self.value == other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__ge__","title":"<code>__ge__(other)</code>","text":"<p>Return if one instance of the Literal class is greater than or equal to another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __ge__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the Literal class is greater than or equal to another instance.\"\"\"\n    return self.value &gt;= other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__gt__","title":"<code>__gt__(other)</code>","text":"<p>Return if one instance of the Literal class is greater than another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __gt__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the Literal class is greater than another instance.\"\"\"\n    return self.value &gt; other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__hash__","title":"<code>__hash__()</code>","text":"<p>Return a hashed representation of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Return a hashed representation of the Literal class.\"\"\"\n    return hash(self.value)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__le__","title":"<code>__le__(other)</code>","text":"<p>Return if one instance of the Literal class is less than or equal to another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __le__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the Literal class is less than or equal to another instance.\"\"\"\n    return self.value &lt;= other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__lt__","title":"<code>__lt__(other)</code>","text":"<p>Return if one instance of the Literal class is less than another instance.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __lt__(self, other: Any) -&gt; bool:\n    \"\"\"Return if one instance of the Literal class is less than another instance.\"\"\"\n    return self.value &lt; other.value\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__ne__","title":"<code>__ne__(other)</code>","text":"<p>Return the inequality of two instances of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __ne__(self, other: Any) -&gt; bool:\n    \"\"\"Return the inequality of two instances of the Literal class.\"\"\"\n    return not self.__eq__(other)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Literal class.\"\"\"\n    return f\"{type(self).__name__}({self.value!r})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.Literal.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Literal class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Literal class.\"\"\"\n    return str(self.value)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.StringLiteral","title":"<code>StringLiteral</code>","text":"<p>               Bases: <code>Literal[str]</code></p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>class StringLiteral(Literal[str]):\n    def __init__(self, value: str) -&gt; None:\n        super().__init__(value, str)\n\n    @singledispatchmethod\n    def to(self, type_var: IcebergType) -&gt; Literal:  # type: ignore\n        raise TypeError(f\"Cannot convert StringLiteral into {type_var}\")\n\n    @to.register(StringType)\n    def _(self, _: StringType) -&gt; Literal[str]:\n        return self\n\n    @to.register(IntegerType)\n    def _(self, type_var: IntegerType) -&gt; Literal[int]:\n        try:\n            number = int(float(self.value))\n\n            if IntegerType.max &lt; number:\n                return IntAboveMax()\n            elif IntegerType.min &gt; number:\n                return IntBelowMin()\n            return LongLiteral(number)\n        except ValueError as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    @to.register(LongType)\n    def _(self, type_var: LongType) -&gt; Literal[int]:\n        try:\n            long_value = int(float(self.value))\n            if LongType.max &lt; long_value:\n                return LongAboveMax()\n            elif LongType.min &gt; long_value:\n                return LongBelowMin()\n            else:\n                return LongLiteral(long_value)\n        except (TypeError, ValueError) as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    @to.register(DateType)\n    def _(self, type_var: DateType) -&gt; Literal[int]:\n        try:\n            return DateLiteral(date_str_to_days(self.value))\n        except (TypeError, ValueError) as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    @to.register(TimeType)\n    def _(self, type_var: TimeType) -&gt; Literal[int]:\n        try:\n            return TimeLiteral(time_str_to_micros(self.value))\n        except (TypeError, ValueError) as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    @to.register(TimestampType)\n    def _(self, _: TimestampType) -&gt; Literal[int]:\n        return TimestampLiteral(timestamp_to_micros(self.value))\n\n    @to.register(TimestamptzType)\n    def _(self, _: TimestamptzType) -&gt; Literal[int]:\n        return TimestampLiteral(timestamptz_to_micros(self.value))\n\n    @to.register(UUIDType)\n    def _(self, _: UUIDType) -&gt; Literal[bytes]:\n        return UUIDLiteral(UUID(self.value).bytes)\n\n    @to.register(DecimalType)\n    def _(self, type_var: DecimalType) -&gt; Literal[Decimal]:\n        dec = Decimal(self.value)\n        scale = abs(int(dec.as_tuple().exponent))\n        if type_var.scale == scale:\n            return DecimalLiteral(dec)\n        else:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}, scales differ {type_var.scale} &lt;&gt; {scale}\")\n\n    @to.register(BooleanType)\n    def _(self, type_var: BooleanType) -&gt; Literal[bool]:\n        value_upper = self.value.upper()\n        if value_upper in [\"TRUE\", \"FALSE\"]:\n            return BooleanLiteral(value_upper == \"TRUE\")\n        else:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\")\n\n    @to.register(FloatType)\n    def _(self, type_var: FloatType) -&gt; Literal[float]:\n        try:\n            number = float(self.value)\n            if FloatType.max &lt; number:\n                return FloatAboveMax()\n            elif FloatType.min &gt; number:\n                return FloatBelowMin()\n            return FloatLiteral(number)\n        except ValueError as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    @to.register(DoubleType)\n    def _(self, type_var: DoubleType) -&gt; Literal[float]:\n        try:\n            number = float(self.value)\n            return DoubleLiteral(number)\n        except ValueError as e:\n            raise ValueError(f\"Could not convert {self.value} into a {type_var}\") from e\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the StringLiteral class.\"\"\"\n        return f\"literal({repr(self.value)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.StringLiteral.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the StringLiteral class.</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the StringLiteral class.\"\"\"\n    return f\"literal({repr(self.value)})\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/literals/#pyiceberg.expressions.literals.literal","title":"<code>literal(value)</code>","text":"<p>Construct an Iceberg Literal based on Python primitive data type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Python primitive type</code> <p>the value to be associated with literal.</p> required Example <p>from pyiceberg.expressions.literals import literal.</p> <p>literal(123) LongLiteral(123)</p> Source code in <code>pyiceberg/expressions/literals.py</code> <pre><code>def literal(value: L) -&gt; Literal[L]:\n    \"\"\"\n    Construct an Iceberg Literal based on Python primitive data type.\n\n    Args:\n        value (Python primitive type): the value to be associated with literal.\n\n    Example:\n        from pyiceberg.expressions.literals import literal.\n        &gt;&gt;&gt; literal(123)\n        LongLiteral(123)\n    \"\"\"\n    if isinstance(value, float):\n        return DoubleLiteral(value)\n    elif isinstance(value, bool):\n        return BooleanLiteral(value)\n    elif isinstance(value, int):\n        return LongLiteral(value)\n    elif isinstance(value, str):\n        return StringLiteral(value)\n    elif isinstance(value, UUID):\n        return UUIDLiteral(value.bytes)\n    elif isinstance(value, bytes):\n        return BinaryLiteral(value)\n    elif isinstance(value, Decimal):\n        return DecimalLiteral(value)\n    elif isinstance(value, datetime):\n        return TimestampLiteral(datetime_to_micros(value))\n    elif isinstance(value, date):\n        return DateLiteral(date_to_days(value))\n    elif isinstance(value, time):\n        return TimeLiteral(time_to_micros(value))\n    else:\n        raise TypeError(f\"Invalid literal value: {repr(value)}\")\n</code></pre>"},{"location":"reference/pyiceberg/expressions/parser/","title":"parser","text":""},{"location":"reference/pyiceberg/expressions/parser/#pyiceberg.expressions.parser.parse","title":"<code>parse(expr)</code>","text":"<p>Parse a boolean expression.</p> Source code in <code>pyiceberg/expressions/parser.py</code> <pre><code>def parse(expr: str) -&gt; BooleanExpression:\n    \"\"\"Parse a boolean expression.\"\"\"\n    return boolean_expression.parse_string(expr, parse_all=True)[0]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/","title":"visitors","text":""},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BindVisitor","title":"<code>BindVisitor</code>","text":"<p>               Bases: <code>BooleanExpressionVisitor[BooleanExpression]</code></p> <p>Rewrites a boolean expression by replacing unbound references with references to fields in a struct schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>A schema to use when binding the expression.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to consider case when binding a reference to a field in a schema, defaults to True.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>In the case a predicate is already bound.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class BindVisitor(BooleanExpressionVisitor[BooleanExpression]):\n    \"\"\"Rewrites a boolean expression by replacing unbound references with references to fields in a struct schema.\n\n    Args:\n      schema (Schema): A schema to use when binding the expression.\n      case_sensitive (bool): Whether to consider case when binding a reference to a field in a schema, defaults to True.\n\n    Raises:\n        TypeError: In the case a predicate is already bound.\n    \"\"\"\n\n    schema: Schema\n    case_sensitive: bool\n\n    def __init__(self, schema: Schema, case_sensitive: bool) -&gt; None:\n        self.schema = schema\n        self.case_sensitive = case_sensitive\n\n    def visit_true(self) -&gt; BooleanExpression:\n        return AlwaysTrue()\n\n    def visit_false(self) -&gt; BooleanExpression:\n        return AlwaysFalse()\n\n    def visit_not(self, child_result: BooleanExpression) -&gt; BooleanExpression:\n        return Not(child=child_result)\n\n    def visit_and(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return And(left=left_result, right=right_result)\n\n    def visit_or(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return Or(left=left_result, right=right_result)\n\n    def visit_unbound_predicate(self, predicate: UnboundPredicate) -&gt; BooleanExpression:\n        return predicate.bind(self.schema, case_sensitive=self.case_sensitive)\n\n    def visit_bound_predicate(self, predicate: BoundPredicate) -&gt; BooleanExpression:\n        raise TypeError(f\"Found already bound predicate: {predicate}\")\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor","title":"<code>BooleanExpressionVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class BooleanExpressionVisitor(Generic[T], ABC):\n    @abstractmethod\n    def visit_true(self) -&gt; T:\n        \"\"\"Visit method for an AlwaysTrue boolean expression.\n\n        Note: This visit method has no arguments since AlwaysTrue instances have no context.\n        \"\"\"\n\n    @abstractmethod\n    def visit_false(self) -&gt; T:\n        \"\"\"Visit method for an AlwaysFalse boolean expression.\n\n        Note: This visit method has no arguments since AlwaysFalse instances have no context.\n        \"\"\"\n\n    @abstractmethod\n    def visit_not(self, child_result: T) -&gt; T:\n        \"\"\"Visit method for a Not boolean expression.\n\n        Args:\n            child_result (T): The result of visiting the child of the Not boolean expression.\n        \"\"\"\n\n    @abstractmethod\n    def visit_and(self, left_result: T, right_result: T) -&gt; T:\n        \"\"\"Visit method for an And boolean expression.\n\n        Args:\n            left_result (T): The result of visiting the left side of the expression.\n            right_result (T): The result of visiting the right side of the expression.\n        \"\"\"\n\n    @abstractmethod\n    def visit_or(self, left_result: T, right_result: T) -&gt; T:\n        \"\"\"Visit method for an Or boolean expression.\n\n        Args:\n            left_result (T): The result of visiting the left side of the expression.\n            right_result (T): The result of visiting the right side of the expression.\n        \"\"\"\n\n    @abstractmethod\n    def visit_unbound_predicate(self, predicate: UnboundPredicate) -&gt; T:\n        \"\"\"Visit method for an unbound predicate in an expression tree.\n\n        Args:\n            predicate (UnboundPredicate): An instance of an UnboundPredicate.\n        \"\"\"\n\n    @abstractmethod\n    def visit_bound_predicate(self, predicate: BoundPredicate) -&gt; T:\n        \"\"\"Visit method for a bound predicate in an expression tree.\n\n        Args:\n            predicate (BoundPredicate): An instance of a BoundPredicate.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_and","title":"<code>visit_and(left_result, right_result)</code>  <code>abstractmethod</code>","text":"<p>Visit method for an And boolean expression.</p> <p>Parameters:</p> Name Type Description Default <code>left_result</code> <code>T</code> <p>The result of visiting the left side of the expression.</p> required <code>right_result</code> <code>T</code> <p>The result of visiting the right side of the expression.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_and(self, left_result: T, right_result: T) -&gt; T:\n    \"\"\"Visit method for an And boolean expression.\n\n    Args:\n        left_result (T): The result of visiting the left side of the expression.\n        right_result (T): The result of visiting the right side of the expression.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_bound_predicate","title":"<code>visit_bound_predicate(predicate)</code>  <code>abstractmethod</code>","text":"<p>Visit method for a bound predicate in an expression tree.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>BoundPredicate</code> <p>An instance of a BoundPredicate.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_bound_predicate(self, predicate: BoundPredicate) -&gt; T:\n    \"\"\"Visit method for a bound predicate in an expression tree.\n\n    Args:\n        predicate (BoundPredicate): An instance of a BoundPredicate.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_false","title":"<code>visit_false()</code>  <code>abstractmethod</code>","text":"<p>Visit method for an AlwaysFalse boolean expression.</p> <p>Note: This visit method has no arguments since AlwaysFalse instances have no context.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_false(self) -&gt; T:\n    \"\"\"Visit method for an AlwaysFalse boolean expression.\n\n    Note: This visit method has no arguments since AlwaysFalse instances have no context.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_not","title":"<code>visit_not(child_result)</code>  <code>abstractmethod</code>","text":"<p>Visit method for a Not boolean expression.</p> <p>Parameters:</p> Name Type Description Default <code>child_result</code> <code>T</code> <p>The result of visiting the child of the Not boolean expression.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not(self, child_result: T) -&gt; T:\n    \"\"\"Visit method for a Not boolean expression.\n\n    Args:\n        child_result (T): The result of visiting the child of the Not boolean expression.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_or","title":"<code>visit_or(left_result, right_result)</code>  <code>abstractmethod</code>","text":"<p>Visit method for an Or boolean expression.</p> <p>Parameters:</p> Name Type Description Default <code>left_result</code> <code>T</code> <p>The result of visiting the left side of the expression.</p> required <code>right_result</code> <code>T</code> <p>The result of visiting the right side of the expression.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_or(self, left_result: T, right_result: T) -&gt; T:\n    \"\"\"Visit method for an Or boolean expression.\n\n    Args:\n        left_result (T): The result of visiting the left side of the expression.\n        right_result (T): The result of visiting the right side of the expression.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_true","title":"<code>visit_true()</code>  <code>abstractmethod</code>","text":"<p>Visit method for an AlwaysTrue boolean expression.</p> <p>Note: This visit method has no arguments since AlwaysTrue instances have no context.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_true(self) -&gt; T:\n    \"\"\"Visit method for an AlwaysTrue boolean expression.\n\n    Note: This visit method has no arguments since AlwaysTrue instances have no context.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BooleanExpressionVisitor.visit_unbound_predicate","title":"<code>visit_unbound_predicate(predicate)</code>  <code>abstractmethod</code>","text":"<p>Visit method for an unbound predicate in an expression tree.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>UnboundPredicate</code> <p>An instance of an UnboundPredicate.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_unbound_predicate(self, predicate: UnboundPredicate) -&gt; T:\n    \"\"\"Visit method for an unbound predicate in an expression tree.\n\n    Args:\n        predicate (UnboundPredicate): An instance of an UnboundPredicate.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor","title":"<code>BoundBooleanExpressionVisitor</code>","text":"<p>               Bases: <code>BooleanExpressionVisitor[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class BoundBooleanExpressionVisitor(BooleanExpressionVisitor[T], ABC):\n    @abstractmethod\n    def visit_in(self, term: BoundTerm, literals: set[L]) -&gt; T:\n        \"\"\"Visit a bound In predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_in(self, term: BoundTerm, literals: set[L]) -&gt; T:\n        \"\"\"Visit a bound NotIn predicate.\"\"\"\n\n    @abstractmethod\n    def visit_is_nan(self, term: BoundTerm) -&gt; T:\n        \"\"\"Visit a bound IsNan predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_nan(self, term: BoundTerm) -&gt; T:\n        \"\"\"Visit a bound NotNan predicate.\"\"\"\n\n    @abstractmethod\n    def visit_is_null(self, term: BoundTerm) -&gt; T:\n        \"\"\"Visit a bound IsNull predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_null(self, term: BoundTerm) -&gt; T:\n        \"\"\"Visit a bound NotNull predicate.\"\"\"\n\n    @abstractmethod\n    def visit_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n        \"\"\"Visit a bound Equal predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n        \"\"\"Visit a bound NotEqual predicate.\"\"\"\n\n    @abstractmethod\n    def visit_greater_than_or_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n        \"\"\"Visit a bound GreaterThanOrEqual predicate.\"\"\"\n\n    @abstractmethod\n    def visit_greater_than(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n        \"\"\"Visit a bound GreaterThan predicate.\"\"\"\n\n    @abstractmethod\n    def visit_less_than(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n        \"\"\"Visit a bound LessThan predicate.\"\"\"\n\n    @abstractmethod\n    def visit_less_than_or_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n        \"\"\"Visit a bound LessThanOrEqual predicate.\"\"\"\n\n    @abstractmethod\n    def visit_true(self) -&gt; T:\n        \"\"\"Visit a bound True predicate.\"\"\"\n\n    @abstractmethod\n    def visit_false(self) -&gt; T:\n        \"\"\"Visit a bound False predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not(self, child_result: T) -&gt; T:\n        \"\"\"Visit a bound Not predicate.\"\"\"\n\n    @abstractmethod\n    def visit_and(self, left_result: T, right_result: T) -&gt; T:\n        \"\"\"Visit a bound And predicate.\"\"\"\n\n    @abstractmethod\n    def visit_or(self, left_result: T, right_result: T) -&gt; T:\n        \"\"\"Visit a bound Or predicate.\"\"\"\n\n    @abstractmethod\n    def visit_starts_with(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n        \"\"\"Visit bound StartsWith predicate.\"\"\"\n\n    @abstractmethod\n    def visit_not_starts_with(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n        \"\"\"Visit bound NotStartsWith predicate.\"\"\"\n\n    def visit_unbound_predicate(self, predicate: UnboundPredicate) -&gt; T:\n        \"\"\"Visit an unbound predicate.\n\n        Args:\n            predicate (UnboundPredicate): An unbound predicate.\n        Raises:\n            TypeError: This always raises since an unbound predicate is not expected in a bound boolean expression.\n        \"\"\"\n        raise TypeError(f\"Not a bound predicate: {predicate}\")\n\n    def visit_bound_predicate(self, predicate: BoundPredicate) -&gt; T:\n        \"\"\"Visit a bound predicate.\n\n        Args:\n            predicate (BoundPredicate): A bound predicate.\n        \"\"\"\n        return visit_bound_predicate(predicate, self)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_and","title":"<code>visit_and(left_result, right_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound And predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_and(self, left_result: T, right_result: T) -&gt; T:\n    \"\"\"Visit a bound And predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_bound_predicate","title":"<code>visit_bound_predicate(predicate)</code>","text":"<p>Visit a bound predicate.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>BoundPredicate</code> <p>A bound predicate.</p> required Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def visit_bound_predicate(self, predicate: BoundPredicate) -&gt; T:\n    \"\"\"Visit a bound predicate.\n\n    Args:\n        predicate (BoundPredicate): A bound predicate.\n    \"\"\"\n    return visit_bound_predicate(predicate, self)\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_equal","title":"<code>visit_equal(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound Equal predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n    \"\"\"Visit a bound Equal predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_false","title":"<code>visit_false()</code>  <code>abstractmethod</code>","text":"<p>Visit a bound False predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_false(self) -&gt; T:\n    \"\"\"Visit a bound False predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_greater_than","title":"<code>visit_greater_than(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound GreaterThan predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_greater_than(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n    \"\"\"Visit a bound GreaterThan predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_greater_than_or_equal","title":"<code>visit_greater_than_or_equal(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound GreaterThanOrEqual predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_greater_than_or_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n    \"\"\"Visit a bound GreaterThanOrEqual predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_in","title":"<code>visit_in(term, literals)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound In predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_in(self, term: BoundTerm, literals: set[L]) -&gt; T:\n    \"\"\"Visit a bound In predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_is_nan","title":"<code>visit_is_nan(term)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound IsNan predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_is_nan(self, term: BoundTerm) -&gt; T:\n    \"\"\"Visit a bound IsNan predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_is_null","title":"<code>visit_is_null(term)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound IsNull predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_is_null(self, term: BoundTerm) -&gt; T:\n    \"\"\"Visit a bound IsNull predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_less_than","title":"<code>visit_less_than(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound LessThan predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_less_than(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n    \"\"\"Visit a bound LessThan predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_less_than_or_equal","title":"<code>visit_less_than_or_equal(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound LessThanOrEqual predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_less_than_or_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n    \"\"\"Visit a bound LessThanOrEqual predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not","title":"<code>visit_not(child_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound Not predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not(self, child_result: T) -&gt; T:\n    \"\"\"Visit a bound Not predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_equal","title":"<code>visit_not_equal(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound NotEqual predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n    \"\"\"Visit a bound NotEqual predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_in","title":"<code>visit_not_in(term, literals)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound NotIn predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_in(self, term: BoundTerm, literals: set[L]) -&gt; T:\n    \"\"\"Visit a bound NotIn predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_nan","title":"<code>visit_not_nan(term)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound NotNan predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_nan(self, term: BoundTerm) -&gt; T:\n    \"\"\"Visit a bound NotNan predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_null","title":"<code>visit_not_null(term)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound NotNull predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_null(self, term: BoundTerm) -&gt; T:\n    \"\"\"Visit a bound NotNull predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_not_starts_with","title":"<code>visit_not_starts_with(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit bound NotStartsWith predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_not_starts_with(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n    \"\"\"Visit bound NotStartsWith predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_or","title":"<code>visit_or(left_result, right_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a bound Or predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_or(self, left_result: T, right_result: T) -&gt; T:\n    \"\"\"Visit a bound Or predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_starts_with","title":"<code>visit_starts_with(term, literal)</code>  <code>abstractmethod</code>","text":"<p>Visit bound StartsWith predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_starts_with(self, term: BoundTerm, literal: LiteralValue) -&gt; T:\n    \"\"\"Visit bound StartsWith predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_true","title":"<code>visit_true()</code>  <code>abstractmethod</code>","text":"<p>Visit a bound True predicate.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@abstractmethod\ndef visit_true(self) -&gt; T:\n    \"\"\"Visit a bound True predicate.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.BoundBooleanExpressionVisitor.visit_unbound_predicate","title":"<code>visit_unbound_predicate(predicate)</code>","text":"<p>Visit an unbound predicate.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>UnboundPredicate</code> <p>An unbound predicate.</p> required <p>Raises:     TypeError: This always raises since an unbound predicate is not expected in a bound boolean expression.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def visit_unbound_predicate(self, predicate: UnboundPredicate) -&gt; T:\n    \"\"\"Visit an unbound predicate.\n\n    Args:\n        predicate (UnboundPredicate): An unbound predicate.\n    Raises:\n        TypeError: This always raises since an unbound predicate is not expected in a bound boolean expression.\n    \"\"\"\n    raise TypeError(f\"Not a bound predicate: {predicate}\")\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.ResidualVisitor","title":"<code>ResidualVisitor</code>","text":"<p>               Bases: <code>BoundBooleanExpressionVisitor[BooleanExpression]</code>, <code>ABC</code></p> <p>Finds the residuals for an Expression the partitions in the given PartitionSpec.</p> <p>A residual expression is made by partially evaluating an expression using partition values. For example, if a table is partitioned by day(utc_timestamp) and is read with a filter expression utc_timestamp &gt; a and utc_timestamp &lt; b, then there are 4 possible residuals expressions for the partition data, d:</p> <ol> <li>If d &gt; day(a) and d &lt; day(b), the residual is always true</li> <li>If d == day(a) and d != day(b), the residual is utc_timestamp &gt; a</li> <li>if d == day(b) and d != day(a), the residual is utc_timestamp &lt; b</li> <li>If d == day(a) == day(b), the residual is utc_timestamp &gt; a and utc_timestamp &lt; b Partition data is passed using StructLike. Residuals are returned by residualFor(StructLike).</li> </ol> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>class ResidualVisitor(BoundBooleanExpressionVisitor[BooleanExpression], ABC):\n    \"\"\"Finds the residuals for an Expression the partitions in the given PartitionSpec.\n\n    A residual expression is made by partially evaluating an expression using partition values.\n    For example, if a table is partitioned by day(utc_timestamp) and is read with a filter expression\n    utc_timestamp &gt; a and utc_timestamp &lt; b, then there are 4 possible residuals expressions\n    for the partition data, d:\n\n\n    1. If d &gt; day(a) and d &amp;lt; day(b), the residual is always true\n    2. If d == day(a) and d != day(b), the residual is utc_timestamp &gt; a\n    3. if d == day(b) and d != day(a), the residual is utc_timestamp &lt; b\n    4. If d == day(a) == day(b), the residual is utc_timestamp &gt; a and utc_timestamp &lt; b\n    Partition data is passed using StructLike. Residuals are returned by residualFor(StructLike).\n    \"\"\"\n\n    schema: Schema\n    spec: PartitionSpec\n    case_sensitive: bool\n    expr: BooleanExpression\n\n    def __init__(self, schema: Schema, spec: PartitionSpec, case_sensitive: bool, expr: BooleanExpression) -&gt; None:\n        self.schema = schema\n        self.spec = spec\n        self.case_sensitive = case_sensitive\n        self.expr = expr\n\n    def eval(self, partition_data: Record) -&gt; BooleanExpression:\n        self.struct = partition_data\n        return visit(self.expr, visitor=self)\n\n    def visit_true(self) -&gt; BooleanExpression:\n        return AlwaysTrue()\n\n    def visit_false(self) -&gt; BooleanExpression:\n        return AlwaysFalse()\n\n    def visit_not(self, child_result: BooleanExpression) -&gt; BooleanExpression:\n        return Not(child_result)\n\n    def visit_and(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return And(left_result, right_result)\n\n    def visit_or(self, left_result: BooleanExpression, right_result: BooleanExpression) -&gt; BooleanExpression:\n        return Or(left_result, right_result)\n\n    def visit_is_null(self, term: BoundTerm) -&gt; BooleanExpression:\n        if term.eval(self.struct) is None:\n            return AlwaysTrue()\n        else:\n            return AlwaysFalse()\n\n    def visit_not_null(self, term: BoundTerm) -&gt; BooleanExpression:\n        if term.eval(self.struct) is not None:\n            return AlwaysTrue()\n        else:\n            return AlwaysFalse()\n\n    def visit_is_nan(self, term: BoundTerm) -&gt; BooleanExpression:\n        val = term.eval(self.struct)\n        if isinstance(val, SupportsFloat) and math.isnan(val):\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_not_nan(self, term: BoundTerm) -&gt; BooleanExpression:\n        val = term.eval(self.struct)\n        if isinstance(val, SupportsFloat) and not math.isnan(val):\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_less_than(self, term: BoundTerm, literal: LiteralValue) -&gt; BooleanExpression:\n        if term.eval(self.struct) &lt; literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_less_than_or_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; BooleanExpression:\n        if term.eval(self.struct) &lt;= literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_greater_than(self, term: BoundTerm, literal: LiteralValue) -&gt; BooleanExpression:\n        if term.eval(self.struct) &gt; literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_greater_than_or_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; BooleanExpression:\n        if term.eval(self.struct) &gt;= literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; BooleanExpression:\n        if term.eval(self.struct) == literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_not_equal(self, term: BoundTerm, literal: LiteralValue) -&gt; BooleanExpression:\n        if term.eval(self.struct) != literal.value:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_in(self, term: BoundTerm, literals: set[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) in literals:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_not_in(self, term: BoundTerm, literals: set[L]) -&gt; BooleanExpression:\n        if term.eval(self.struct) not in literals:\n            return self.visit_true()\n        else:\n            return self.visit_false()\n\n    def visit_starts_with(self, term: BoundTerm, literal: LiteralValue) -&gt; BooleanExpression:\n        eval_res = term.eval(self.struct)\n        if eval_res is not None and str(eval_res).startswith(str(literal.value)):\n            return AlwaysTrue()\n        else:\n            return AlwaysFalse()\n\n    def visit_not_starts_with(self, term: BoundTerm, literal: LiteralValue) -&gt; BooleanExpression:\n        if not self.visit_starts_with(term, literal):\n            return AlwaysTrue()\n        else:\n            return AlwaysFalse()\n\n    def visit_bound_predicate(self, predicate: BoundPredicate) -&gt; BooleanExpression:\n        \"\"\"\n        If there is no strict projection or if it evaluates to false, then return the predicate.\n\n        Get the strict projection and inclusive projection of this predicate in partition data,\n        then use them to determine whether to return the original predicate. The strict projection\n        returns true iff the original predicate would have returned true, so the predicate can be\n        eliminated if the strict projection evaluates to true. Similarly the inclusive projection\n        returns false iff the original predicate would have returned false, so the predicate can\n        also be eliminated if the inclusive projection evaluates to false.\n\n        \"\"\"\n        parts = self.spec.fields_by_source_id(predicate.term.ref().field.field_id)\n        if parts == []:\n            return predicate\n\n        def struct_to_schema(struct: StructType) -&gt; Schema:\n            return Schema(*struct.fields)\n\n        for part in parts:\n            strict_projection = part.transform.strict_project(part.name, predicate)\n            strict_result = None\n\n            if strict_projection is not None:\n                bound = strict_projection.bind(\n                    struct_to_schema(self.spec.partition_type(self.schema)), case_sensitive=self.case_sensitive\n                )\n                if isinstance(bound, BoundPredicate):\n                    strict_result = super().visit_bound_predicate(bound)\n                else:\n                    # if the result is not a predicate, then it must be a constant like alwaysTrue or alwaysFalse\n                    strict_result = bound\n\n            if isinstance(strict_result, AlwaysTrue):\n                return AlwaysTrue()\n\n            inclusive_projection = part.transform.project(part.name, predicate)\n            inclusive_result = None\n            if inclusive_projection is not None:\n                bound_inclusive = inclusive_projection.bind(\n                    struct_to_schema(self.spec.partition_type(self.schema)), case_sensitive=self.case_sensitive\n                )\n                if isinstance(bound_inclusive, BoundPredicate):\n                    # using predicate method specific to inclusive\n                    inclusive_result = super().visit_bound_predicate(bound_inclusive)\n                else:\n                    # if the result is not a predicate, then it must be a constant like alwaysTrue or\n                    # alwaysFalse\n                    inclusive_result = bound_inclusive\n            if isinstance(inclusive_result, AlwaysFalse):\n                return AlwaysFalse()\n\n        return predicate\n\n    def visit_unbound_predicate(self, predicate: UnboundPredicate) -&gt; BooleanExpression:\n        bound = predicate.bind(self.schema, case_sensitive=self.case_sensitive)\n\n        if isinstance(bound, BoundPredicate):\n            bound_residual = self.visit_bound_predicate(predicate=bound)\n            if not isinstance(bound_residual, (AlwaysFalse, AlwaysTrue)):\n                # replace inclusive original unbound predicate\n                return predicate\n\n            # use the non-predicate residual (e.g. alwaysTrue)\n            return bound_residual\n\n        # if binding didn't result in a Predicate, return the expression\n        return bound\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.ResidualVisitor.visit_bound_predicate","title":"<code>visit_bound_predicate(predicate)</code>","text":"<p>If there is no strict projection or if it evaluates to false, then return the predicate.</p> <p>Get the strict projection and inclusive projection of this predicate in partition data, then use them to determine whether to return the original predicate. The strict projection returns true iff the original predicate would have returned true, so the predicate can be eliminated if the strict projection evaluates to true. Similarly the inclusive projection returns false iff the original predicate would have returned false, so the predicate can also be eliminated if the inclusive projection evaluates to false.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def visit_bound_predicate(self, predicate: BoundPredicate) -&gt; BooleanExpression:\n    \"\"\"\n    If there is no strict projection or if it evaluates to false, then return the predicate.\n\n    Get the strict projection and inclusive projection of this predicate in partition data,\n    then use them to determine whether to return the original predicate. The strict projection\n    returns true iff the original predicate would have returned true, so the predicate can be\n    eliminated if the strict projection evaluates to true. Similarly the inclusive projection\n    returns false iff the original predicate would have returned false, so the predicate can\n    also be eliminated if the inclusive projection evaluates to false.\n\n    \"\"\"\n    parts = self.spec.fields_by_source_id(predicate.term.ref().field.field_id)\n    if parts == []:\n        return predicate\n\n    def struct_to_schema(struct: StructType) -&gt; Schema:\n        return Schema(*struct.fields)\n\n    for part in parts:\n        strict_projection = part.transform.strict_project(part.name, predicate)\n        strict_result = None\n\n        if strict_projection is not None:\n            bound = strict_projection.bind(\n                struct_to_schema(self.spec.partition_type(self.schema)), case_sensitive=self.case_sensitive\n            )\n            if isinstance(bound, BoundPredicate):\n                strict_result = super().visit_bound_predicate(bound)\n            else:\n                # if the result is not a predicate, then it must be a constant like alwaysTrue or alwaysFalse\n                strict_result = bound\n\n        if isinstance(strict_result, AlwaysTrue):\n            return AlwaysTrue()\n\n        inclusive_projection = part.transform.project(part.name, predicate)\n        inclusive_result = None\n        if inclusive_projection is not None:\n            bound_inclusive = inclusive_projection.bind(\n                struct_to_schema(self.spec.partition_type(self.schema)), case_sensitive=self.case_sensitive\n            )\n            if isinstance(bound_inclusive, BoundPredicate):\n                # using predicate method specific to inclusive\n                inclusive_result = super().visit_bound_predicate(bound_inclusive)\n            else:\n                # if the result is not a predicate, then it must be a constant like alwaysTrue or\n                # alwaysFalse\n                inclusive_result = bound_inclusive\n        if isinstance(inclusive_result, AlwaysFalse):\n            return AlwaysFalse()\n\n    return predicate\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.bind","title":"<code>bind(schema, expression, case_sensitive)</code>","text":"<p>Travers over an expression to bind the predicates to the schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>A schema to use when binding the expression.</p> required <code>expression</code> <code>BooleanExpression</code> <p>An expression containing UnboundPredicates that can be bound.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to consider case when binding a reference to a field in a schema, defaults to True.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>In the case a predicate is already bound.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def bind(schema: Schema, expression: BooleanExpression, case_sensitive: bool) -&gt; BooleanExpression:\n    \"\"\"Travers over an expression to bind the predicates to the schema.\n\n    Args:\n      schema (Schema): A schema to use when binding the expression.\n      expression (BooleanExpression): An expression containing UnboundPredicates that can be bound.\n      case_sensitive (bool): Whether to consider case when binding a reference to a field in a schema, defaults to True.\n\n    Raises:\n        TypeError: In the case a predicate is already bound.\n    \"\"\"\n    return visit(expression, BindVisitor(schema, case_sensitive))\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.expression_to_plain_format","title":"<code>expression_to_plain_format(expressions, cast_int_to_datetime=False)</code>","text":"<p>Format a Disjunctive Normal Form expression.</p> <p>These are the formats that the expression can be fed into:</p> <ul> <li>https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html</li> <li>https://docs.dask.org/en/stable/generated/dask.dataframe.read_parquet.html</li> </ul> <p>Contrary to normal DNF that may contain Not expressions, but here they should have been rewritten. This can be done using <code>rewrite_not(...)</code>.</p> <p>Keep in mind that this is only used for page skipping, and still needs to filter on a row level.</p> <p>Parameters:</p> Name Type Description Default <code>expressions</code> <code>tuple[BooleanExpression, ...]</code> <p>Expression in Disjunctive Normal Form.</p> required <p>Returns:</p> Type Description <code>list[list[tuple[str, str, Any]]]</code> <p>Formatter filter compatible with Dask and PyArrow.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>def expression_to_plain_format(\n    expressions: tuple[BooleanExpression, ...], cast_int_to_datetime: bool = False\n) -&gt; list[list[tuple[str, str, Any]]]:\n    \"\"\"Format a Disjunctive Normal Form expression.\n\n    These are the formats that the expression can be fed into:\n\n    - https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html\n    - https://docs.dask.org/en/stable/generated/dask.dataframe.read_parquet.html\n\n    Contrary to normal DNF that may contain Not expressions, but here they should have\n    been rewritten. This can be done using ``rewrite_not(...)``.\n\n    Keep in mind that this is only used for page skipping, and still needs to filter\n    on a row level.\n\n    Args:\n        expressions: Expression in Disjunctive Normal Form.\n\n    Returns:\n        Formatter filter compatible with Dask and PyArrow.\n    \"\"\"\n    # In the form of expr1 \u2228 expr2 \u2228 ... \u2228 exprN\n    visitor = ExpressionToPlainFormat(cast_int_to_datetime)\n    return [visit(expression, visitor) for expression in expressions]\n</code></pre>"},{"location":"reference/pyiceberg/expressions/visitors/#pyiceberg.expressions.visitors.visit","title":"<code>visit(obj, visitor)</code>","text":"<p>Apply a boolean expression visitor to any point within an expression.</p> <p>The function traverses the expression in post-order fashion.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>BooleanExpression</code> <p>An instance of a BooleanExpression.</p> required <code>visitor</code> <code>BooleanExpressionVisitor[T]</code> <p>An instance of an implementation of the generic BooleanExpressionVisitor base class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to visit an unsupported expression.</p> Source code in <code>pyiceberg/expressions/visitors.py</code> <pre><code>@singledispatch\ndef visit(obj: BooleanExpression, visitor: BooleanExpressionVisitor[T]) -&gt; T:\n    \"\"\"Apply a boolean expression visitor to any point within an expression.\n\n    The function traverses the expression in post-order fashion.\n\n    Args:\n        obj (BooleanExpression): An instance of a BooleanExpression.\n        visitor (BooleanExpressionVisitor[T]): An instance of an implementation of the generic\n            BooleanExpressionVisitor base class.\n\n    Raises:\n        NotImplementedError: If attempting to visit an unsupported expression.\n    \"\"\"\n    raise NotImplementedError(f\"Cannot visit unsupported expression: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/io/","title":"io","text":"<p>Base FileIO classes for implementing reading and writing table files.</p> <p>The FileIO abstraction includes a subset of full filesystem implementations. Specifically, Iceberg needs to read or write a file at a given location (as a seekable stream), as well as check if a file exists. An implementation of the FileIO abstract base class is responsible for returning an InputFile instance, an OutputFile instance, and deleting a file given its location.</p>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.FileIO","title":"<code>FileIO</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for FileIO implementations.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>class FileIO(ABC):\n    \"\"\"A base class for FileIO implementations.\"\"\"\n\n    properties: Properties\n\n    def __init__(self, properties: Properties = EMPTY_DICT):\n        self.properties = properties\n\n    @abstractmethod\n    def new_input(self, location: str) -&gt; InputFile:\n        \"\"\"Get an InputFile instance to read bytes from the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n        \"\"\"\n\n    @abstractmethod\n    def new_output(self, location: str) -&gt; OutputFile:\n        \"\"\"Get an OutputFile instance to write bytes to the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n        \"\"\"\n\n    @abstractmethod\n    def delete(self, location: str | InputFile | OutputFile) -&gt; None:\n        \"\"\"Delete the file at the given path.\n\n        Args:\n            location (Union[str, InputFile, OutputFile]): A URI or a path to a local file--if an InputFile instance or\n                an OutputFile instance is provided, the location attribute for that instance is used as the URI to delete.\n\n        Raises:\n            PermissionError: If the file at location cannot be accessed due to a permission error.\n            FileNotFoundError: When the file at the provided location does not exist.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.FileIO.delete","title":"<code>delete(location)</code>  <code>abstractmethod</code>","text":"<p>Delete the file at the given path.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Union[str, InputFile, OutputFile]</code> <p>A URI or a path to a local file--if an InputFile instance or an OutputFile instance is provided, the location attribute for that instance is used as the URI to delete.</p> required <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at location cannot be accessed due to a permission error.</p> <code>FileNotFoundError</code> <p>When the file at the provided location does not exist.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef delete(self, location: str | InputFile | OutputFile) -&gt; None:\n    \"\"\"Delete the file at the given path.\n\n    Args:\n        location (Union[str, InputFile, OutputFile]): A URI or a path to a local file--if an InputFile instance or\n            an OutputFile instance is provided, the location attribute for that instance is used as the URI to delete.\n\n    Raises:\n        PermissionError: If the file at location cannot be accessed due to a permission error.\n        FileNotFoundError: When the file at the provided location does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.FileIO.new_input","title":"<code>new_input(location)</code>  <code>abstractmethod</code>","text":"<p>Get an InputFile instance to read bytes from the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef new_input(self, location: str) -&gt; InputFile:\n    \"\"\"Get an InputFile instance to read bytes from the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.FileIO.new_output","title":"<code>new_output(location)</code>  <code>abstractmethod</code>","text":"<p>Get an OutputFile instance to write bytes to the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef new_output(self, location: str) -&gt; OutputFile:\n    \"\"\"Get an OutputFile instance to write bytes to the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile","title":"<code>InputFile</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for InputFile implementations.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Attributes:</p> Name Type Description <code>location</code> <code>str</code> <p>The URI or path to a local file for an InputFile instance.</p> <code>exists</code> <code>bool</code> <p>Whether the file exists or not.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>class InputFile(ABC):\n    \"\"\"A base class for InputFile implementations.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Attributes:\n        location (str): The URI or path to a local file for an InputFile instance.\n        exists (bool): Whether the file exists or not.\n    \"\"\"\n\n    def __init__(self, location: str):\n        self._location = location\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n\n    @property\n    def location(self) -&gt; str:\n        \"\"\"The fully-qualified location of the input file.\"\"\"\n        return self._location\n\n    @abstractmethod\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error.\n        \"\"\"\n\n    @abstractmethod\n    def open(self, seekable: bool = True) -&gt; InputStream:\n        \"\"\"Return an object that matches the InputStream protocol.\n\n        Args:\n            seekable: If the stream should support seek, or if it is consumed sequential.\n\n        Returns:\n            InputStream: An object that matches the InputStream protocol.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error.\n            FileNotFoundError: If the file at self.location does not exist.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile.location","title":"<code>location</code>  <code>property</code>","text":"<p>The fully-qualified location of the input file.</p>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile.exists","title":"<code>exists()</code>  <code>abstractmethod</code>","text":"<p>Check whether the location exists.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\n\n    Raises:\n        PermissionError: If the file at self.location cannot be accessed due to a permission error.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputFile.open","title":"<code>open(seekable=True)</code>  <code>abstractmethod</code>","text":"<p>Return an object that matches the InputStream protocol.</p> <p>Parameters:</p> Name Type Description Default <code>seekable</code> <code>bool</code> <p>If the stream should support seek, or if it is consumed sequential.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>InputStream</code> <code>InputStream</code> <p>An object that matches the InputStream protocol.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error.</p> <code>FileNotFoundError</code> <p>If the file at self.location does not exist.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef open(self, seekable: bool = True) -&gt; InputStream:\n    \"\"\"Return an object that matches the InputStream protocol.\n\n    Args:\n        seekable: If the stream should support seek, or if it is consumed sequential.\n\n    Returns:\n        InputStream: An object that matches the InputStream protocol.\n\n    Raises:\n        PermissionError: If the file at self.location cannot be accessed due to a permission error.\n        FileNotFoundError: If the file at self.location does not exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputStream","title":"<code>InputStream</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for the file-like object returned by InputFile.open(...).</p> <p>This outlines the minimally required methods for a seekable input stream returned from an InputFile implementation's <code>open(...)</code> method. These methods are a subset of IOBase/RawIOBase.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@runtime_checkable\nclass InputStream(Protocol):\n    \"\"\"A protocol for the file-like object returned by InputFile.open(...).\n\n    This outlines the minimally required methods for a seekable input stream returned from an InputFile\n    implementation's `open(...)` method. These methods are a subset of IOBase/RawIOBase.\n    \"\"\"\n\n    @abstractmethod\n    def read(self, size: int = 0) -&gt; bytes: ...\n\n    @abstractmethod\n    def seek(self, offset: int, whence: int = SEEK_SET) -&gt; int: ...\n\n    @abstractmethod\n    def tell(self) -&gt; int: ...\n\n    @abstractmethod\n    def close(self) -&gt; None: ...\n\n    def __enter__(self) -&gt; InputStream:\n        \"\"\"Provide setup when opening an InputStream using a 'with' statement.\"\"\"\n\n    @abstractmethod\n    def __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n        \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputStream.__enter__","title":"<code>__enter__()</code>","text":"<p>Provide setup when opening an InputStream using a 'with' statement.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>def __enter__(self) -&gt; InputStream:\n    \"\"\"Provide setup when opening an InputStream using a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.InputStream.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>  <code>abstractmethod</code>","text":"<p>Perform cleanup when exiting the scope of a 'with' statement.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n    \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile","title":"<code>OutputFile</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for OutputFile implementations.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Attributes:</p> Name Type Description <code>location</code> <code>str</code> <p>The URI or path to a local file for an OutputFile instance.</p> <code>exists</code> <code>bool</code> <p>Whether the file exists or not.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>class OutputFile(ABC):\n    \"\"\"A base class for OutputFile implementations.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Attributes:\n        location (str): The URI or path to a local file for an OutputFile instance.\n        exists (bool): Whether the file exists or not.\n    \"\"\"\n\n    def __init__(self, location: str):\n        self._location = location\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n\n    @property\n    def location(self) -&gt; str:\n        \"\"\"The fully-qualified location of the output file.\"\"\"\n        return self._location\n\n    @abstractmethod\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error.\n        \"\"\"\n\n    @abstractmethod\n    def to_input_file(self) -&gt; InputFile:\n        \"\"\"Return an InputFile for the location of this output file.\"\"\"\n\n    @abstractmethod\n    def create(self, overwrite: bool = False) -&gt; OutputStream:\n        \"\"\"Return an object that matches the OutputStream protocol.\n\n        Args:\n            overwrite (bool): If the file already exists at `self.location`\n                and `overwrite` is False a FileExistsError should be raised.\n\n        Returns:\n            OutputStream: An object that matches the OutputStream protocol.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error.\n            FileExistsError: If the file at self.location already exists and `overwrite=False`.\n        \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.location","title":"<code>location</code>  <code>property</code>","text":"<p>The fully-qualified location of the output file.</p>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.create","title":"<code>create(overwrite=False)</code>  <code>abstractmethod</code>","text":"<p>Return an object that matches the OutputStream protocol.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>If the file already exists at <code>self.location</code> and <code>overwrite</code> is False a FileExistsError should be raised.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>OutputStream</code> <code>OutputStream</code> <p>An object that matches the OutputStream protocol.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error.</p> <code>FileExistsError</code> <p>If the file at self.location already exists and <code>overwrite=False</code>.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef create(self, overwrite: bool = False) -&gt; OutputStream:\n    \"\"\"Return an object that matches the OutputStream protocol.\n\n    Args:\n        overwrite (bool): If the file already exists at `self.location`\n            and `overwrite` is False a FileExistsError should be raised.\n\n    Returns:\n        OutputStream: An object that matches the OutputStream protocol.\n\n    Raises:\n        PermissionError: If the file at self.location cannot be accessed due to a permission error.\n        FileExistsError: If the file at self.location already exists and `overwrite=False`.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.exists","title":"<code>exists()</code>  <code>abstractmethod</code>","text":"<p>Check whether the location exists.</p> <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\n\n    Raises:\n        PermissionError: If the file at self.location cannot be accessed due to a permission error.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputFile.to_input_file","title":"<code>to_input_file()</code>  <code>abstractmethod</code>","text":"<p>Return an InputFile for the location of this output file.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef to_input_file(self) -&gt; InputFile:\n    \"\"\"Return an InputFile for the location of this output file.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputStream","title":"<code>OutputStream</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A protocol for the file-like object returned by OutputFile.create(...).</p> <p>This outlines the minimally required methods for a writable output stream returned from an OutputFile implementation's <code>create(...)</code> method. These methods are a subset of IOBase/RawIOBase.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@runtime_checkable\nclass OutputStream(Protocol):  # pragma: no cover\n    \"\"\"A protocol for the file-like object returned by OutputFile.create(...).\n\n    This outlines the minimally required methods for a writable output stream returned from an OutputFile\n    implementation's `create(...)` method. These methods are a subset of IOBase/RawIOBase.\n    \"\"\"\n\n    @abstractmethod\n    def write(self, b: bytes) -&gt; int: ...\n\n    @abstractmethod\n    def close(self) -&gt; None: ...\n\n    @abstractmethod\n    def __enter__(self) -&gt; OutputStream:\n        \"\"\"Provide setup when opening an OutputStream using a 'with' statement.\"\"\"\n\n    @abstractmethod\n    def __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n        \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputStream.__enter__","title":"<code>__enter__()</code>  <code>abstractmethod</code>","text":"<p>Provide setup when opening an OutputStream using a 'with' statement.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __enter__(self) -&gt; OutputStream:\n    \"\"\"Provide setup when opening an OutputStream using a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/#pyiceberg.io.OutputStream.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>  <code>abstractmethod</code>","text":"<p>Perform cleanup when exiting the scope of a 'with' statement.</p> Source code in <code>pyiceberg/io/__init__.py</code> <pre><code>@abstractmethod\ndef __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n    \"\"\"Perform cleanup when exiting the scope of a 'with' statement.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/","title":"fsspec","text":"<p>FileIO implementation for reading and writing table files that uses fsspec compatible filesystems.</p>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO","title":"<code>FsspecFileIO</code>","text":"<p>               Bases: <code>FileIO</code></p> <p>A FileIO implementation that uses fsspec.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>class FsspecFileIO(FileIO):\n    \"\"\"A FileIO implementation that uses fsspec.\"\"\"\n\n    def __init__(self, properties: Properties):\n        self._scheme_to_fs = {}\n        self._scheme_to_fs.update(SCHEME_TO_FS)\n        self._thread_locals = threading.local()\n        super().__init__(properties=properties)\n\n    def new_input(self, location: str) -&gt; FsspecInputFile:\n        \"\"\"Get an FsspecInputFile instance to read bytes from the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n\n        Returns:\n            FsspecInputFile: An FsspecInputFile instance for the given location.\n        \"\"\"\n        uri = urlparse(location)\n        fs = self.get_fs(uri.scheme)\n        return FsspecInputFile(location=location, fs=fs)\n\n    def new_output(self, location: str) -&gt; FsspecOutputFile:\n        \"\"\"Get an FsspecOutputFile instance to write bytes to the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n\n        Returns:\n            FsspecOutputFile: An FsspecOutputFile instance for the given location.\n        \"\"\"\n        uri = urlparse(location)\n        fs = self.get_fs(uri.scheme)\n        return FsspecOutputFile(location=location, fs=fs)\n\n    def delete(self, location: str | InputFile | OutputFile) -&gt; None:\n        \"\"\"Delete the file at the given location.\n\n        Args:\n            location (Union[str, InputFile, OutputFile]): The URI to the file--if an InputFile instance or an\n                OutputFile instance is provided, the location attribute for that instance is used as the location\n                to delete.\n        \"\"\"\n        if isinstance(location, (InputFile, OutputFile)):\n            str_location = location.location  # Use InputFile or OutputFile location\n        else:\n            str_location = location\n\n        uri = urlparse(str_location)\n        fs = self.get_fs(uri.scheme)\n        fs.rm(str_location)\n\n    def get_fs(self, scheme: str) -&gt; AbstractFileSystem:\n        \"\"\"Get a filesystem for a specific scheme, cached per thread.\"\"\"\n        if not hasattr(self._thread_locals, \"get_fs_cached\"):\n            self._thread_locals.get_fs_cached = lru_cache(self._get_fs)\n\n        return self._thread_locals.get_fs_cached(scheme)\n\n    def _get_fs(self, scheme: str) -&gt; AbstractFileSystem:\n        \"\"\"Get a filesystem for a specific scheme.\"\"\"\n        if scheme not in self._scheme_to_fs:\n            raise ValueError(f\"No registered filesystem for scheme: {scheme}\")\n        return self._scheme_to_fs[scheme](self.properties)\n\n    def __getstate__(self) -&gt; dict[str, Any]:\n        \"\"\"Create a dictionary of the FsSpecFileIO fields used when pickling.\"\"\"\n        fileio_copy = copy(self.__dict__)\n        del fileio_copy[\"_thread_locals\"]\n        return fileio_copy\n\n    def __setstate__(self, state: dict[str, Any]) -&gt; None:\n        \"\"\"Deserialize the state into a FsSpecFileIO instance.\"\"\"\n        self.__dict__ = state\n        self._thread_locals = threading.local()\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Create a dictionary of the FsSpecFileIO fields used when pickling.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def __getstate__(self) -&gt; dict[str, Any]:\n    \"\"\"Create a dictionary of the FsSpecFileIO fields used when pickling.\"\"\"\n    fileio_copy = copy(self.__dict__)\n    del fileio_copy[\"_thread_locals\"]\n    return fileio_copy\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Deserialize the state into a FsSpecFileIO instance.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def __setstate__(self, state: dict[str, Any]) -&gt; None:\n    \"\"\"Deserialize the state into a FsSpecFileIO instance.\"\"\"\n    self.__dict__ = state\n    self._thread_locals = threading.local()\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.delete","title":"<code>delete(location)</code>","text":"<p>Delete the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Union[str, InputFile, OutputFile]</code> <p>The URI to the file--if an InputFile instance or an OutputFile instance is provided, the location attribute for that instance is used as the location to delete.</p> required Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def delete(self, location: str | InputFile | OutputFile) -&gt; None:\n    \"\"\"Delete the file at the given location.\n\n    Args:\n        location (Union[str, InputFile, OutputFile]): The URI to the file--if an InputFile instance or an\n            OutputFile instance is provided, the location attribute for that instance is used as the location\n            to delete.\n    \"\"\"\n    if isinstance(location, (InputFile, OutputFile)):\n        str_location = location.location  # Use InputFile or OutputFile location\n    else:\n        str_location = location\n\n    uri = urlparse(str_location)\n    fs = self.get_fs(uri.scheme)\n    fs.rm(str_location)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.get_fs","title":"<code>get_fs(scheme)</code>","text":"<p>Get a filesystem for a specific scheme, cached per thread.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def get_fs(self, scheme: str) -&gt; AbstractFileSystem:\n    \"\"\"Get a filesystem for a specific scheme, cached per thread.\"\"\"\n    if not hasattr(self._thread_locals, \"get_fs_cached\"):\n        self._thread_locals.get_fs_cached = lru_cache(self._get_fs)\n\n    return self._thread_locals.get_fs_cached(scheme)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.new_input","title":"<code>new_input(location)</code>","text":"<p>Get an FsspecInputFile instance to read bytes from the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Returns:</p> Name Type Description <code>FsspecInputFile</code> <code>FsspecInputFile</code> <p>An FsspecInputFile instance for the given location.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def new_input(self, location: str) -&gt; FsspecInputFile:\n    \"\"\"Get an FsspecInputFile instance to read bytes from the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Returns:\n        FsspecInputFile: An FsspecInputFile instance for the given location.\n    \"\"\"\n    uri = urlparse(location)\n    fs = self.get_fs(uri.scheme)\n    return FsspecInputFile(location=location, fs=fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecFileIO.new_output","title":"<code>new_output(location)</code>","text":"<p>Get an FsspecOutputFile instance to write bytes to the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Returns:</p> Name Type Description <code>FsspecOutputFile</code> <code>FsspecOutputFile</code> <p>An FsspecOutputFile instance for the given location.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def new_output(self, location: str) -&gt; FsspecOutputFile:\n    \"\"\"Get an FsspecOutputFile instance to write bytes to the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Returns:\n        FsspecOutputFile: An FsspecOutputFile instance for the given location.\n    \"\"\"\n    uri = urlparse(location)\n    fs = self.get_fs(uri.scheme)\n    return FsspecOutputFile(location=location, fs=fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecInputFile","title":"<code>FsspecInputFile</code>","text":"<p>               Bases: <code>InputFile</code></p> <p>An input file implementation for the FsspecFileIO.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI to a file location.</p> required <code>fs</code> <code>AbstractFileSystem</code> <p>An fsspec filesystem instance.</p> required Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>class FsspecInputFile(InputFile):\n    \"\"\"An input file implementation for the FsspecFileIO.\n\n    Args:\n        location (str): A URI to a file location.\n        fs (AbstractFileSystem): An fsspec filesystem instance.\n    \"\"\"\n\n    def __init__(self, location: str, fs: AbstractFileSystem):\n        self._fs = fs\n        super().__init__(location=location)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n        object_info = self._fs.info(self.location)\n        if size := object_info.get(\"Size\"):\n            return size\n        elif size := object_info.get(\"size\"):\n            return size\n        raise RuntimeError(f\"Cannot retrieve object info: {self.location}\")\n\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\"\"\"\n        return self._fs.lexists(self.location)\n\n    def open(self, seekable: bool = True) -&gt; InputStream:\n        \"\"\"Create an input stream for reading the contents of the file.\n\n        Args:\n            seekable: If the stream should support seek, or if it is consumed sequential.\n\n        Returns:\n            OpenFile: An fsspec compliant file-like object.\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n        \"\"\"\n        try:\n            return self._fs.open(self.location, \"rb\")\n        except FileNotFoundError as e:\n            # To have a consistent error handling experience, make sure exception contains missing file location.\n            raise e if e.filename else FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.location) from e\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecInputFile.__len__","title":"<code>__len__()</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n    object_info = self._fs.info(self.location)\n    if size := object_info.get(\"Size\"):\n        return size\n    elif size := object_info.get(\"size\"):\n        return size\n    raise RuntimeError(f\"Cannot retrieve object info: {self.location}\")\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecInputFile.exists","title":"<code>exists()</code>","text":"<p>Check whether the location exists.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\"\"\"\n    return self._fs.lexists(self.location)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecInputFile.open","title":"<code>open(seekable=True)</code>","text":"<p>Create an input stream for reading the contents of the file.</p> <p>Parameters:</p> Name Type Description Default <code>seekable</code> <code>bool</code> <p>If the stream should support seek, or if it is consumed sequential.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>OpenFile</code> <code>InputStream</code> <p>An fsspec compliant file-like object.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def open(self, seekable: bool = True) -&gt; InputStream:\n    \"\"\"Create an input stream for reading the contents of the file.\n\n    Args:\n        seekable: If the stream should support seek, or if it is consumed sequential.\n\n    Returns:\n        OpenFile: An fsspec compliant file-like object.\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    try:\n        return self._fs.open(self.location, \"rb\")\n    except FileNotFoundError as e:\n        # To have a consistent error handling experience, make sure exception contains missing file location.\n        raise e if e.filename else FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), self.location) from e\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile","title":"<code>FsspecOutputFile</code>","text":"<p>               Bases: <code>OutputFile</code></p> <p>An output file implementation for the FsspecFileIO.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI to a file location.</p> required <code>fs</code> <code>AbstractFileSystem</code> <p>An fsspec filesystem instance.</p> required Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>class FsspecOutputFile(OutputFile):\n    \"\"\"An output file implementation for the FsspecFileIO.\n\n    Args:\n        location (str): A URI to a file location.\n        fs (AbstractFileSystem): An fsspec filesystem instance.\n    \"\"\"\n\n    def __init__(self, location: str, fs: AbstractFileSystem):\n        self._fs = fs\n        super().__init__(location=location)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n        object_info = self._fs.info(self.location)\n        if size := object_info.get(\"Size\"):\n            return size\n        elif size := object_info.get(\"size\"):\n            return size\n        raise RuntimeError(f\"Cannot retrieve object info: {self.location}\")\n\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\"\"\"\n        return self._fs.lexists(self.location)\n\n    def create(self, overwrite: bool = False) -&gt; OutputStream:\n        \"\"\"Create an output stream for reading the contents of the file.\n\n        Args:\n            overwrite (bool): Whether to overwrite the file if it already exists.\n\n        Returns:\n            OpenFile: An fsspec compliant file-like object.\n\n        Raises:\n            FileExistsError: If the file already exists at the location and overwrite is set to False.\n\n        Note:\n            If overwrite is set to False, a check is first performed to verify that the file does not exist.\n            This is not thread-safe and a possibility does exist that the file can be created by a concurrent\n            process after the existence check yet before the output stream is created. In such a case, the default\n            behavior will truncate the contents of the existing file when opening the output stream.\n        \"\"\"\n        if not overwrite and self.exists():\n            raise FileExistsError(f\"Cannot create file, file already exists: {self.location}\")\n        return self._fs.open(self.location, \"wb\")\n\n    def to_input_file(self) -&gt; FsspecInputFile:\n        \"\"\"Return a new FsspecInputFile for the location at `self.location`.\"\"\"\n        return FsspecInputFile(location=self.location, fs=self._fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile.__len__","title":"<code>__len__()</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n    object_info = self._fs.info(self.location)\n    if size := object_info.get(\"Size\"):\n        return size\n    elif size := object_info.get(\"size\"):\n        return size\n    raise RuntimeError(f\"Cannot retrieve object info: {self.location}\")\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile.create","title":"<code>create(overwrite=False)</code>","text":"<p>Create an output stream for reading the contents of the file.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>OpenFile</code> <code>OutputStream</code> <p>An fsspec compliant file-like object.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file already exists at the location and overwrite is set to False.</p> Note <p>If overwrite is set to False, a check is first performed to verify that the file does not exist. This is not thread-safe and a possibility does exist that the file can be created by a concurrent process after the existence check yet before the output stream is created. In such a case, the default behavior will truncate the contents of the existing file when opening the output stream.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def create(self, overwrite: bool = False) -&gt; OutputStream:\n    \"\"\"Create an output stream for reading the contents of the file.\n\n    Args:\n        overwrite (bool): Whether to overwrite the file if it already exists.\n\n    Returns:\n        OpenFile: An fsspec compliant file-like object.\n\n    Raises:\n        FileExistsError: If the file already exists at the location and overwrite is set to False.\n\n    Note:\n        If overwrite is set to False, a check is first performed to verify that the file does not exist.\n        This is not thread-safe and a possibility does exist that the file can be created by a concurrent\n        process after the existence check yet before the output stream is created. In such a case, the default\n        behavior will truncate the contents of the existing file when opening the output stream.\n    \"\"\"\n    if not overwrite and self.exists():\n        raise FileExistsError(f\"Cannot create file, file already exists: {self.location}\")\n    return self._fs.open(self.location, \"wb\")\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile.exists","title":"<code>exists()</code>","text":"<p>Check whether the location exists.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\"\"\"\n    return self._fs.lexists(self.location)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.FsspecOutputFile.to_input_file","title":"<code>to_input_file()</code>","text":"<p>Return a new FsspecInputFile for the location at <code>self.location</code>.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>def to_input_file(self) -&gt; FsspecInputFile:\n    \"\"\"Return a new FsspecInputFile for the location at `self.location`.\"\"\"\n    return FsspecInputFile(location=self.location, fs=self._fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.S3RequestSigner","title":"<code>S3RequestSigner</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for S3 request signers.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>class S3RequestSigner(abc.ABC):\n    \"\"\"Abstract base class for S3 request signers.\"\"\"\n\n    properties: Properties\n\n    def __init__(self, properties: Properties) -&gt; None:\n        self.properties = properties\n\n    @abc.abstractmethod\n    def __call__(self, request: \"AWSRequest\", **_: Any) -&gt; None:\n        pass\n</code></pre>"},{"location":"reference/pyiceberg/io/fsspec/#pyiceberg.io.fsspec.S3V4RestSigner","title":"<code>S3V4RestSigner</code>","text":"<p>               Bases: <code>S3RequestSigner</code></p> <p>An S3 request signer that uses an external REST signing service to sign requests.</p> Source code in <code>pyiceberg/io/fsspec.py</code> <pre><code>class S3V4RestSigner(S3RequestSigner):\n    \"\"\"An S3 request signer that uses an external REST signing service to sign requests.\"\"\"\n\n    _session: requests.Session\n\n    def __init__(self, properties: Properties) -&gt; None:\n        super().__init__(properties)\n        self._session = requests.Session()\n\n    def __call__(self, request: \"AWSRequest\", **_: Any) -&gt; None:\n        signer_url = self.properties.get(S3_SIGNER_URI, self.properties[URI]).rstrip(\"/\")  # type: ignore\n        signer_endpoint = self.properties.get(S3_SIGNER_ENDPOINT, S3_SIGNER_ENDPOINT_DEFAULT)\n\n        signer_headers: dict[str, str] = {}\n\n        auth_header: str | None = None\n        if auth_manager := self.properties.get(AUTH_MANAGER):\n            auth_header = auth_manager.auth_header()\n        elif token := self.properties.get(TOKEN):\n            auth_header = f\"Bearer {token}\"\n\n        if auth_header:\n            signer_headers[\"Authorization\"] = auth_header\n\n        signer_headers.update(get_header_properties(self.properties))\n\n        signer_body = {\n            \"method\": request.method,\n            \"region\": request.context[\"client_region\"],\n            \"uri\": request.url,\n            \"headers\": {key: [val] for key, val in request.headers.items()},\n        }\n\n        response = self._session.post(f\"{signer_url}/{signer_endpoint.strip()}\", headers=signer_headers, json=signer_body)\n        try:\n            response.raise_for_status()\n            response_json = response.json()\n        except HTTPError as e:\n            raise SignError(f\"Failed to sign request {response.status_code}: {signer_body}\") from e\n\n        for key, value in response_json[\"headers\"].items():\n            request.headers.add_header(key, \", \".join(value))\n\n        request.url = response_json[\"uri\"]\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/","title":"pyarrow","text":"<p>FileIO implementation for reading and writing table files that uses pyarrow.fs.</p> <p>This file contains a FileIO implementation that relies on the filesystem interface provided by PyArrow. It relies on PyArrow's <code>from_uri</code> method that infers the correct filesystem type to use. Theoretically, this allows the supported storage types to grow naturally with the pyarrow library.</p>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.ArrowScan","title":"<code>ArrowScan</code>","text":"Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class ArrowScan:\n    _table_metadata: TableMetadata\n    _io: FileIO\n    _projected_schema: Schema\n    _bound_row_filter: BooleanExpression\n    _case_sensitive: bool\n    _limit: int | None\n    _downcast_ns_timestamp_to_us: bool | None\n    \"\"\"Scan the Iceberg Table and create an Arrow construct.\n\n    Attributes:\n        _table_metadata: Current table metadata of the Iceberg table\n        _io: PyIceberg FileIO implementation from which to fetch the io properties\n        _projected_schema: Iceberg Schema to project onto the data files\n        _bound_row_filter: Schema bound row expression to filter the data with\n        _case_sensitive: Case sensitivity when looking up column names\n        _limit: Limit the number of records.\n    \"\"\"\n\n    def __init__(\n        self,\n        table_metadata: TableMetadata,\n        io: FileIO,\n        projected_schema: Schema,\n        row_filter: BooleanExpression,\n        case_sensitive: bool = True,\n        limit: int | None = None,\n    ) -&gt; None:\n        self._table_metadata = table_metadata\n        self._io = io\n        self._projected_schema = projected_schema\n        self._bound_row_filter = bind(table_metadata.schema(), row_filter, case_sensitive=case_sensitive)\n        self._case_sensitive = case_sensitive\n        self._limit = limit\n        self._downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE)\n\n    @property\n    def _projected_field_ids(self) -&gt; set[int]:\n        \"\"\"Set of field IDs that should be projected from the data files.\"\"\"\n        return {\n            id\n            for id in self._projected_schema.field_ids\n            if not isinstance(self._projected_schema.find_type(id), (MapType, ListType))\n        }.union(extract_field_ids(self._bound_row_filter))\n\n    def to_table(self, tasks: Iterable[FileScanTask]) -&gt; pa.Table:\n        \"\"\"Scan the Iceberg table and return a pa.Table.\n\n        Returns a pa.Table with data from the Iceberg table by resolving the\n        right columns that match the current table schema. Only data that\n        matches the provided row_filter expression is returned.\n\n        Args:\n            tasks: FileScanTasks representing the data files and delete files to read from.\n\n        Returns:\n            A PyArrow table. Total number of rows will be capped if specified.\n\n        Raises:\n            ResolveError: When a required field cannot be found in the file\n            ValueError: When a field type in the file cannot be projected to the schema type\n        \"\"\"\n        arrow_schema = schema_to_pyarrow(self._projected_schema, include_field_ids=False)\n\n        batches = self.to_record_batches(tasks)\n        try:\n            first_batch = next(batches)\n        except StopIteration:\n            # Empty\n            return arrow_schema.empty_table()\n\n        # Note: cannot use pa.Table.from_batches(itertools.chain([first_batch], batches)))\n        #       as different batches can use different schema's (due to large_ types)\n        result = pa.concat_tables(\n            (pa.Table.from_batches([batch]) for batch in itertools.chain([first_batch], batches)), promote_options=\"permissive\"\n        )\n\n        return result\n\n    def to_record_batches(self, tasks: Iterable[FileScanTask]) -&gt; Iterator[pa.RecordBatch]:\n        \"\"\"Scan the Iceberg table and return an Iterator[pa.RecordBatch].\n\n        Returns an Iterator of pa.RecordBatch with data from the Iceberg table\n        by resolving the right columns that match the current table schema.\n        Only data that matches the provided row_filter expression is returned.\n\n        Args:\n            tasks: FileScanTasks representing the data files and delete files to read from.\n\n        Returns:\n            An Iterator of PyArrow RecordBatches.\n            Total number of rows will be capped if specified.\n\n        Raises:\n            ResolveError: When a required field cannot be found in the file\n            ValueError: When a field type in the file cannot be projected to the schema type\n        \"\"\"\n        deletes_per_file = _read_all_delete_files(self._io, tasks)\n\n        total_row_count = 0\n        executor = ExecutorFactory.get_or_create()\n\n        def batches_for_task(task: FileScanTask) -&gt; list[pa.RecordBatch]:\n            # Materialize the iterator here to ensure execution happens within the executor.\n            # Otherwise, the iterator would be lazily consumed later (in the main thread),\n            # defeating the purpose of using executor.map.\n            return list(self._record_batches_from_scan_tasks_and_deletes([task], deletes_per_file))\n\n        limit_reached = False\n        for batches in executor.map(batches_for_task, tasks):\n            for batch in batches:\n                current_batch_size = len(batch)\n                if self._limit is not None and total_row_count + current_batch_size &gt;= self._limit:\n                    yield batch.slice(0, self._limit - total_row_count)\n\n                    limit_reached = True\n                    break\n                else:\n                    yield batch\n                    total_row_count += current_batch_size\n\n            if limit_reached:\n                # This break will also cancel all running tasks in the executor\n                break\n\n    def _record_batches_from_scan_tasks_and_deletes(\n        self, tasks: Iterable[FileScanTask], deletes_per_file: dict[str, list[ChunkedArray]]\n    ) -&gt; Iterator[pa.RecordBatch]:\n        total_row_count = 0\n        for task in tasks:\n            if self._limit is not None and total_row_count &gt;= self._limit:\n                break\n            batches = _task_to_record_batches(\n                self._io,\n                task,\n                self._bound_row_filter,\n                self._projected_schema,\n                self._table_metadata.schema(),\n                self._projected_field_ids,\n                deletes_per_file.get(task.file.file_path),\n                self._case_sensitive,\n                self._table_metadata.name_mapping(),\n                self._table_metadata.specs().get(task.file.spec_id),\n                self._table_metadata.format_version,\n                self._downcast_ns_timestamp_to_us,\n            )\n            for batch in batches:\n                if self._limit is not None:\n                    if total_row_count &gt;= self._limit:\n                        break\n                    elif total_row_count + len(batch) &gt;= self._limit:\n                        batch = batch.slice(0, self._limit - total_row_count)\n                yield batch\n                total_row_count += len(batch)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.ArrowScan.to_record_batches","title":"<code>to_record_batches(tasks)</code>","text":"<p>Scan the Iceberg table and return an Iterator[pa.RecordBatch].</p> <p>Returns an Iterator of pa.RecordBatch with data from the Iceberg table by resolving the right columns that match the current table schema. Only data that matches the provided row_filter expression is returned.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Iterable[FileScanTask]</code> <p>FileScanTasks representing the data files and delete files to read from.</p> required <p>Returns:</p> Type Description <code>Iterator[RecordBatch]</code> <p>An Iterator of PyArrow RecordBatches.</p> <code>Iterator[RecordBatch]</code> <p>Total number of rows will be capped if specified.</p> <p>Raises:</p> Type Description <code>ResolveError</code> <p>When a required field cannot be found in the file</p> <code>ValueError</code> <p>When a field type in the file cannot be projected to the schema type</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def to_record_batches(self, tasks: Iterable[FileScanTask]) -&gt; Iterator[pa.RecordBatch]:\n    \"\"\"Scan the Iceberg table and return an Iterator[pa.RecordBatch].\n\n    Returns an Iterator of pa.RecordBatch with data from the Iceberg table\n    by resolving the right columns that match the current table schema.\n    Only data that matches the provided row_filter expression is returned.\n\n    Args:\n        tasks: FileScanTasks representing the data files and delete files to read from.\n\n    Returns:\n        An Iterator of PyArrow RecordBatches.\n        Total number of rows will be capped if specified.\n\n    Raises:\n        ResolveError: When a required field cannot be found in the file\n        ValueError: When a field type in the file cannot be projected to the schema type\n    \"\"\"\n    deletes_per_file = _read_all_delete_files(self._io, tasks)\n\n    total_row_count = 0\n    executor = ExecutorFactory.get_or_create()\n\n    def batches_for_task(task: FileScanTask) -&gt; list[pa.RecordBatch]:\n        # Materialize the iterator here to ensure execution happens within the executor.\n        # Otherwise, the iterator would be lazily consumed later (in the main thread),\n        # defeating the purpose of using executor.map.\n        return list(self._record_batches_from_scan_tasks_and_deletes([task], deletes_per_file))\n\n    limit_reached = False\n    for batches in executor.map(batches_for_task, tasks):\n        for batch in batches:\n            current_batch_size = len(batch)\n            if self._limit is not None and total_row_count + current_batch_size &gt;= self._limit:\n                yield batch.slice(0, self._limit - total_row_count)\n\n                limit_reached = True\n                break\n            else:\n                yield batch\n                total_row_count += current_batch_size\n\n        if limit_reached:\n            # This break will also cancel all running tasks in the executor\n            break\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.ArrowScan.to_table","title":"<code>to_table(tasks)</code>","text":"<p>Scan the Iceberg table and return a pa.Table.</p> <p>Returns a pa.Table with data from the Iceberg table by resolving the right columns that match the current table schema. Only data that matches the provided row_filter expression is returned.</p> <p>Parameters:</p> Name Type Description Default <code>tasks</code> <code>Iterable[FileScanTask]</code> <p>FileScanTasks representing the data files and delete files to read from.</p> required <p>Returns:</p> Type Description <code>Table</code> <p>A PyArrow table. Total number of rows will be capped if specified.</p> <p>Raises:</p> Type Description <code>ResolveError</code> <p>When a required field cannot be found in the file</p> <code>ValueError</code> <p>When a field type in the file cannot be projected to the schema type</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def to_table(self, tasks: Iterable[FileScanTask]) -&gt; pa.Table:\n    \"\"\"Scan the Iceberg table and return a pa.Table.\n\n    Returns a pa.Table with data from the Iceberg table by resolving the\n    right columns that match the current table schema. Only data that\n    matches the provided row_filter expression is returned.\n\n    Args:\n        tasks: FileScanTasks representing the data files and delete files to read from.\n\n    Returns:\n        A PyArrow table. Total number of rows will be capped if specified.\n\n    Raises:\n        ResolveError: When a required field cannot be found in the file\n        ValueError: When a field type in the file cannot be projected to the schema type\n    \"\"\"\n    arrow_schema = schema_to_pyarrow(self._projected_schema, include_field_ids=False)\n\n    batches = self.to_record_batches(tasks)\n    try:\n        first_batch = next(batches)\n    except StopIteration:\n        # Empty\n        return arrow_schema.empty_table()\n\n    # Note: cannot use pa.Table.from_batches(itertools.chain([first_batch], batches)))\n    #       as different batches can use different schema's (due to large_ types)\n    result = pa.concat_tables(\n        (pa.Table.from_batches([batch]) for batch in itertools.chain([first_batch], batches)), promote_options=\"permissive\"\n    )\n\n    return result\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile","title":"<code>PyArrowFile</code>","text":"<p>               Bases: <code>InputFile</code>, <code>OutputFile</code></p> <p>A combined InputFile and OutputFile implementation using pyarrow filesystem.</p> <p>This class generates pyarrow.lib.NativeFile instances.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Attributes:</p> Name Type Description <code>location(str)</code> <p>The URI or path to a local file for a PyArrowFile instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyiceberg.io.pyarrow import PyArrowFile\n&gt;&gt;&gt; # input_file = PyArrowFile(\"s3://foo/bar.txt\")\n&gt;&gt;&gt; # Read the contents of the PyArrowFile instance\n&gt;&gt;&gt; # Make sure that you have permissions to read/write\n&gt;&gt;&gt; # file_content = input_file.open().read()\n</code></pre> <pre><code>&gt;&gt;&gt; # output_file = PyArrowFile(\"s3://baz/qux.txt\")\n&gt;&gt;&gt; # Write bytes to a file\n&gt;&gt;&gt; # Make sure that you have permissions to read/write\n&gt;&gt;&gt; # output_file.create().write(b'foobytes')\n</code></pre> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class PyArrowFile(InputFile, OutputFile):\n    \"\"\"A combined InputFile and OutputFile implementation using pyarrow filesystem.\n\n    This class generates pyarrow.lib.NativeFile instances.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Attributes:\n        location(str): The URI or path to a local file for a PyArrowFile instance.\n\n    Examples:\n        &gt;&gt;&gt; from pyiceberg.io.pyarrow import PyArrowFile\n        &gt;&gt;&gt; # input_file = PyArrowFile(\"s3://foo/bar.txt\")\n        &gt;&gt;&gt; # Read the contents of the PyArrowFile instance\n        &gt;&gt;&gt; # Make sure that you have permissions to read/write\n        &gt;&gt;&gt; # file_content = input_file.open().read()\n\n        &gt;&gt;&gt; # output_file = PyArrowFile(\"s3://baz/qux.txt\")\n        &gt;&gt;&gt; # Write bytes to a file\n        &gt;&gt;&gt; # Make sure that you have permissions to read/write\n        &gt;&gt;&gt; # output_file.create().write(b'foobytes')\n    \"\"\"\n\n    _filesystem: FileSystem\n    _path: str\n    _buffer_size: int\n\n    def __init__(self, location: str, path: str, fs: FileSystem, buffer_size: int = ONE_MEGABYTE):\n        self._filesystem = fs\n        self._path = path\n        self._buffer_size = buffer_size\n        super().__init__(location=location)\n\n    def _file_info(self) -&gt; FileInfo:\n        \"\"\"Retrieve a pyarrow.fs.FileInfo object for the location.\n\n        Raises:\n            PermissionError: If the file at self.location cannot be accessed due to a permission error such as\n                an AWS error code 15.\n        \"\"\"\n        try:\n            file_info = self._filesystem.get_file_info(self._path)\n        except OSError as e:\n            if e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n                raise PermissionError(f\"Cannot get file info, access denied: {self.location}\") from e\n            raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n\n        if file_info.type == FileType.NotFound:\n            raise FileNotFoundError(f\"Cannot get file info, file not found: {self.location}\")\n        return file_info\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the total length of the file, in bytes.\"\"\"\n        file_info = self._file_info()\n        return file_info.size\n\n    def exists(self) -&gt; bool:\n        \"\"\"Check whether the location exists.\"\"\"\n        try:\n            self._file_info()  # raises FileNotFoundError if it does not exist\n            return True\n        except FileNotFoundError:\n            return False\n\n    def open(self, seekable: bool = True) -&gt; InputStream:\n        \"\"\"Open the location using a PyArrow FileSystem inferred from the location.\n\n        Args:\n            seekable: If the stream should support seek, or if it is consumed sequential.\n\n        Returns:\n            pyarrow.lib.NativeFile: A NativeFile instance for the file located at `self.location`.\n\n        Raises:\n            FileNotFoundError: If the file at self.location does not exist.\n            PermissionError: If the file at self.location cannot be accessed due to a permission error such as\n                an AWS error code 15.\n        \"\"\"\n        try:\n            if seekable:\n                input_file = self._filesystem.open_input_file(self._path)\n            else:\n                input_file = self._filesystem.open_input_stream(self._path, buffer_size=self._buffer_size)\n        except (FileNotFoundError, PermissionError):\n            raise\n        except OSError as e:\n            if e.errno == 2 or \"Path does not exist\" in str(e):\n                raise FileNotFoundError(f\"Cannot open file, does not exist: {self.location}\") from e\n            elif e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n                raise PermissionError(f\"Cannot open file, access denied: {self.location}\") from e\n            raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n        return input_file\n\n    def create(self, overwrite: bool = False) -&gt; OutputStream:\n        \"\"\"Create a writable pyarrow.lib.NativeFile for this PyArrowFile's location.\n\n        Args:\n            overwrite (bool): Whether to overwrite the file if it already exists.\n\n        Returns:\n            pyarrow.lib.NativeFile: A NativeFile instance for the file located at self.location.\n\n        Raises:\n            FileExistsError: If the file already exists at `self.location` and `overwrite` is False.\n\n        Note:\n            This retrieves a pyarrow NativeFile by opening an output stream. If overwrite is set to False,\n            a check is first performed to verify that the file does not exist. This is not thread-safe and\n            a possibility does exist that the file can be created by a concurrent process after the existence\n            check yet before the output stream is created. In such a case, the default pyarrow behavior will\n            truncate the contents of the existing file when opening the output stream.\n        \"\"\"\n        try:\n            if not overwrite and self.exists() is True:\n                raise FileExistsError(f\"Cannot create file, already exists: {self.location}\")\n            output_file = self._filesystem.open_output_stream(self._path, buffer_size=self._buffer_size)\n        except PermissionError:\n            raise\n        except OSError as e:\n            if e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n                raise PermissionError(f\"Cannot create file, access denied: {self.location}\") from e\n            raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n        return output_file\n\n    def to_input_file(self) -&gt; PyArrowFile:\n        \"\"\"Return a new PyArrowFile for the location of an existing PyArrowFile instance.\n\n        This method is included to abide by the OutputFile abstract base class. Since this implementation uses a single\n        PyArrowFile class (as opposed to separate InputFile and OutputFile implementations), this method effectively returns\n        a copy of the same instance.\n        \"\"\"\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.__len__","title":"<code>__len__()</code>","text":"<p>Return the total length of the file, in bytes.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the total length of the file, in bytes.\"\"\"\n    file_info = self._file_info()\n    return file_info.size\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.create","title":"<code>create(overwrite=False)</code>","text":"<p>Create a writable pyarrow.lib.NativeFile for this PyArrowFile's location.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>False</code> <p>Returns:</p> Type Description <code>OutputStream</code> <p>pyarrow.lib.NativeFile: A NativeFile instance for the file located at self.location.</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file already exists at <code>self.location</code> and <code>overwrite</code> is False.</p> Note <p>This retrieves a pyarrow NativeFile by opening an output stream. If overwrite is set to False, a check is first performed to verify that the file does not exist. This is not thread-safe and a possibility does exist that the file can be created by a concurrent process after the existence check yet before the output stream is created. In such a case, the default pyarrow behavior will truncate the contents of the existing file when opening the output stream.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def create(self, overwrite: bool = False) -&gt; OutputStream:\n    \"\"\"Create a writable pyarrow.lib.NativeFile for this PyArrowFile's location.\n\n    Args:\n        overwrite (bool): Whether to overwrite the file if it already exists.\n\n    Returns:\n        pyarrow.lib.NativeFile: A NativeFile instance for the file located at self.location.\n\n    Raises:\n        FileExistsError: If the file already exists at `self.location` and `overwrite` is False.\n\n    Note:\n        This retrieves a pyarrow NativeFile by opening an output stream. If overwrite is set to False,\n        a check is first performed to verify that the file does not exist. This is not thread-safe and\n        a possibility does exist that the file can be created by a concurrent process after the existence\n        check yet before the output stream is created. In such a case, the default pyarrow behavior will\n        truncate the contents of the existing file when opening the output stream.\n    \"\"\"\n    try:\n        if not overwrite and self.exists() is True:\n            raise FileExistsError(f\"Cannot create file, already exists: {self.location}\")\n        output_file = self._filesystem.open_output_stream(self._path, buffer_size=self._buffer_size)\n    except PermissionError:\n        raise\n    except OSError as e:\n        if e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n            raise PermissionError(f\"Cannot create file, access denied: {self.location}\") from e\n        raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n    return output_file\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.exists","title":"<code>exists()</code>","text":"<p>Check whether the location exists.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"Check whether the location exists.\"\"\"\n    try:\n        self._file_info()  # raises FileNotFoundError if it does not exist\n        return True\n    except FileNotFoundError:\n        return False\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.open","title":"<code>open(seekable=True)</code>","text":"<p>Open the location using a PyArrow FileSystem inferred from the location.</p> <p>Parameters:</p> Name Type Description Default <code>seekable</code> <code>bool</code> <p>If the stream should support seek, or if it is consumed sequential.</p> <code>True</code> <p>Returns:</p> Type Description <code>InputStream</code> <p>pyarrow.lib.NativeFile: A NativeFile instance for the file located at <code>self.location</code>.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file at self.location does not exist.</p> <code>PermissionError</code> <p>If the file at self.location cannot be accessed due to a permission error such as an AWS error code 15.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def open(self, seekable: bool = True) -&gt; InputStream:\n    \"\"\"Open the location using a PyArrow FileSystem inferred from the location.\n\n    Args:\n        seekable: If the stream should support seek, or if it is consumed sequential.\n\n    Returns:\n        pyarrow.lib.NativeFile: A NativeFile instance for the file located at `self.location`.\n\n    Raises:\n        FileNotFoundError: If the file at self.location does not exist.\n        PermissionError: If the file at self.location cannot be accessed due to a permission error such as\n            an AWS error code 15.\n    \"\"\"\n    try:\n        if seekable:\n            input_file = self._filesystem.open_input_file(self._path)\n        else:\n            input_file = self._filesystem.open_input_stream(self._path, buffer_size=self._buffer_size)\n    except (FileNotFoundError, PermissionError):\n        raise\n    except OSError as e:\n        if e.errno == 2 or \"Path does not exist\" in str(e):\n            raise FileNotFoundError(f\"Cannot open file, does not exist: {self.location}\") from e\n        elif e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n            raise PermissionError(f\"Cannot open file, access denied: {self.location}\") from e\n        raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n    return input_file\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFile.to_input_file","title":"<code>to_input_file()</code>","text":"<p>Return a new PyArrowFile for the location of an existing PyArrowFile instance.</p> <p>This method is included to abide by the OutputFile abstract base class. Since this implementation uses a single PyArrowFile class (as opposed to separate InputFile and OutputFile implementations), this method effectively returns a copy of the same instance.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def to_input_file(self) -&gt; PyArrowFile:\n    \"\"\"Return a new PyArrowFile for the location of an existing PyArrowFile instance.\n\n    This method is included to abide by the OutputFile abstract base class. Since this implementation uses a single\n    PyArrowFile class (as opposed to separate InputFile and OutputFile implementations), this method effectively returns\n    a copy of the same instance.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO","title":"<code>PyArrowFileIO</code>","text":"<p>               Bases: <code>FileIO</code></p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class PyArrowFileIO(FileIO):\n    fs_by_scheme: Callable[[str, str | None], FileSystem]\n\n    def __init__(self, properties: Properties = EMPTY_DICT):\n        self.fs_by_scheme: Callable[[str, str | None], FileSystem] = lru_cache(self._initialize_fs)\n        super().__init__(properties=properties)\n\n    @staticmethod\n    def parse_location(location: str, properties: Properties = EMPTY_DICT) -&gt; tuple[str, str, str]:\n        \"\"\"Return (scheme, netloc, path) for the given location.\n\n        Uses DEFAULT_SCHEME and DEFAULT_NETLOC if scheme/netloc are missing.\n        \"\"\"\n        uri = urlparse(location)\n\n        if not uri.scheme:\n            default_scheme = properties.get(\"DEFAULT_SCHEME\", \"file\")\n            default_netloc = properties.get(\"DEFAULT_NETLOC\", \"\")\n            return default_scheme, default_netloc, os.path.abspath(location)\n        elif uri.scheme in (\"hdfs\", \"viewfs\"):\n            return uri.scheme, uri.netloc, uri.path\n        else:\n            return uri.scheme, uri.netloc, f\"{uri.netloc}{uri.path}\"\n\n    def _initialize_fs(self, scheme: str, netloc: str | None = None) -&gt; FileSystem:\n        \"\"\"Initialize FileSystem for different scheme.\"\"\"\n        if scheme in {\"oss\"}:\n            return self._initialize_oss_fs()\n\n        elif scheme in {\"s3\", \"s3a\", \"s3n\"}:\n            return self._initialize_s3_fs(netloc)\n\n        elif scheme in {\"hdfs\", \"viewfs\"}:\n            return self._initialize_hdfs_fs(scheme, netloc)\n\n        elif scheme in {\"gs\", \"gcs\"}:\n            return self._initialize_gcs_fs()\n\n        elif scheme in {\"abfs\", \"abfss\", \"wasb\", \"wasbs\"}:\n            return self._initialize_azure_fs()\n\n        elif scheme in {\"file\"}:\n            return self._initialize_local_fs()\n\n        else:\n            raise ValueError(f\"Unrecognized filesystem type in URI: {scheme}\")\n\n    def _initialize_oss_fs(self) -&gt; FileSystem:\n        from pyarrow.fs import S3FileSystem\n\n        client_kwargs: dict[str, Any] = {\n            \"endpoint_override\": self.properties.get(S3_ENDPOINT),\n            \"access_key\": get_first_property_value(self.properties, S3_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n            \"secret_key\": get_first_property_value(self.properties, S3_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n            \"session_token\": get_first_property_value(self.properties, S3_SESSION_TOKEN, AWS_SESSION_TOKEN),\n            \"region\": get_first_property_value(self.properties, S3_REGION, AWS_REGION),\n            \"force_virtual_addressing\": property_as_bool(self.properties, S3_FORCE_VIRTUAL_ADDRESSING, True),\n        }\n\n        if proxy_uri := self.properties.get(S3_PROXY_URI):\n            client_kwargs[\"proxy_options\"] = proxy_uri\n\n        if connect_timeout := self.properties.get(S3_CONNECT_TIMEOUT):\n            client_kwargs[\"connect_timeout\"] = float(connect_timeout)\n\n        if request_timeout := self.properties.get(S3_REQUEST_TIMEOUT):\n            client_kwargs[\"request_timeout\"] = float(request_timeout)\n\n        if role_arn := get_first_property_value(self.properties, S3_ROLE_ARN, AWS_ROLE_ARN):\n            client_kwargs[\"role_arn\"] = role_arn\n\n        if session_name := get_first_property_value(self.properties, S3_ROLE_SESSION_NAME, AWS_ROLE_SESSION_NAME):\n            client_kwargs[\"session_name\"] = session_name\n\n        if s3_anonymous := self.properties.get(S3_ANONYMOUS):\n            client_kwargs[\"anonymous\"] = strtobool(s3_anonymous)\n\n        return S3FileSystem(**client_kwargs)\n\n    def _initialize_s3_fs(self, netloc: str | None) -&gt; FileSystem:\n        from pyarrow.fs import S3FileSystem\n\n        provided_region = get_first_property_value(self.properties, S3_REGION, AWS_REGION)\n\n        # Do this when we don't provide the region at all, or when we explicitly enable it\n        if provided_region is None or property_as_bool(self.properties, S3_RESOLVE_REGION, False) is True:\n            # Resolve region from netloc(bucket), fallback to user-provided region\n            # Only supported by buckets hosted by S3\n            bucket_region = _cached_resolve_s3_region(bucket=netloc) or provided_region\n            if provided_region is not None and bucket_region != provided_region:\n                logger.warning(\n                    f\"PyArrow FileIO overriding S3 bucket region for bucket {netloc}: \"\n                    f\"provided region {provided_region}, actual region {bucket_region}\"\n                )\n        else:\n            bucket_region = provided_region\n\n        client_kwargs: dict[str, Any] = {\n            \"endpoint_override\": self.properties.get(S3_ENDPOINT),\n            \"access_key\": get_first_property_value(self.properties, S3_ACCESS_KEY_ID, AWS_ACCESS_KEY_ID),\n            \"secret_key\": get_first_property_value(self.properties, S3_SECRET_ACCESS_KEY, AWS_SECRET_ACCESS_KEY),\n            \"session_token\": get_first_property_value(self.properties, S3_SESSION_TOKEN, AWS_SESSION_TOKEN),\n            \"region\": bucket_region,\n        }\n\n        if proxy_uri := self.properties.get(S3_PROXY_URI):\n            client_kwargs[\"proxy_options\"] = proxy_uri\n\n        if connect_timeout := self.properties.get(S3_CONNECT_TIMEOUT):\n            client_kwargs[\"connect_timeout\"] = float(connect_timeout)\n\n        if request_timeout := self.properties.get(S3_REQUEST_TIMEOUT):\n            client_kwargs[\"request_timeout\"] = float(request_timeout)\n\n        if role_arn := get_first_property_value(self.properties, S3_ROLE_ARN, AWS_ROLE_ARN):\n            client_kwargs[\"role_arn\"] = role_arn\n\n        if session_name := get_first_property_value(self.properties, S3_ROLE_SESSION_NAME, AWS_ROLE_SESSION_NAME):\n            client_kwargs[\"session_name\"] = session_name\n\n        if self.properties.get(S3_FORCE_VIRTUAL_ADDRESSING) is not None:\n            client_kwargs[\"force_virtual_addressing\"] = property_as_bool(self.properties, S3_FORCE_VIRTUAL_ADDRESSING, False)\n\n        if (retry_strategy_impl := self.properties.get(S3_RETRY_STRATEGY_IMPL)) and (\n            retry_instance := _import_retry_strategy(retry_strategy_impl)\n        ):\n            client_kwargs[\"retry_strategy\"] = retry_instance\n\n        if s3_anonymous := self.properties.get(S3_ANONYMOUS):\n            client_kwargs[\"anonymous\"] = strtobool(s3_anonymous)\n\n        return S3FileSystem(**client_kwargs)\n\n    def _initialize_azure_fs(self) -&gt; FileSystem:\n        # https://arrow.apache.org/docs/python/generated/pyarrow.fs.AzureFileSystem.html\n        from packaging import version\n\n        MIN_PYARROW_VERSION_SUPPORTING_AZURE_FS = \"20.0.0\"\n        if version.parse(pyarrow.__version__) &lt; version.parse(MIN_PYARROW_VERSION_SUPPORTING_AZURE_FS):\n            raise ImportError(\n                f\"pyarrow version &gt;= {MIN_PYARROW_VERSION_SUPPORTING_AZURE_FS} required for AzureFileSystem support, \"\n                f\"but found version {pyarrow.__version__}.\"\n            )\n\n        from pyarrow.fs import AzureFileSystem\n\n        client_kwargs: dict[str, str] = {}\n\n        if account_name := self.properties.get(ADLS_ACCOUNT_NAME):\n            client_kwargs[\"account_name\"] = account_name\n\n        if account_key := self.properties.get(ADLS_ACCOUNT_KEY):\n            client_kwargs[\"account_key\"] = account_key\n\n        if blob_storage_authority := self.properties.get(ADLS_BLOB_STORAGE_AUTHORITY):\n            client_kwargs[\"blob_storage_authority\"] = blob_storage_authority\n\n        if dfs_storage_authority := self.properties.get(ADLS_DFS_STORAGE_AUTHORITY):\n            client_kwargs[\"dfs_storage_authority\"] = dfs_storage_authority\n\n        if blob_storage_scheme := self.properties.get(ADLS_BLOB_STORAGE_SCHEME):\n            client_kwargs[\"blob_storage_scheme\"] = blob_storage_scheme\n\n        if dfs_storage_scheme := self.properties.get(ADLS_DFS_STORAGE_SCHEME):\n            client_kwargs[\"dfs_storage_scheme\"] = dfs_storage_scheme\n\n        if sas_token := self.properties.get(ADLS_SAS_TOKEN):\n            client_kwargs[\"sas_token\"] = sas_token\n\n        if client_id := self.properties.get(ADLS_CLIENT_ID):\n            client_kwargs[\"client_id\"] = client_id\n        if client_secret := self.properties.get(ADLS_CLIENT_SECRET):\n            client_kwargs[\"client_secret\"] = client_secret\n        if tenant_id := self.properties.get(ADLS_TENANT_ID):\n            client_kwargs[\"tenant_id\"] = tenant_id\n\n        # Validate that all three are provided together for ClientSecretCredential\n        credential_keys = [\"client_id\", \"client_secret\", \"tenant_id\"]\n        provided_keys = [key for key in credential_keys if key in client_kwargs]\n        if provided_keys and len(provided_keys) != len(credential_keys):\n            missing_keys = [key for key in credential_keys if key not in client_kwargs]\n            raise ValueError(\n                f\"client_id, client_secret, and tenant_id must all be provided together \"\n                f\"to use ClientSecretCredential for Azure authentication. \"\n                f\"Provided: {provided_keys}, Missing: {missing_keys}\"\n            )\n\n        return AzureFileSystem(**client_kwargs)\n\n    def _initialize_hdfs_fs(self, scheme: str, netloc: str | None) -&gt; FileSystem:\n        from pyarrow.fs import HadoopFileSystem\n\n        hdfs_kwargs: dict[str, Any] = {}\n        if netloc:\n            return HadoopFileSystem.from_uri(f\"{scheme}://{netloc}\")\n        if host := self.properties.get(HDFS_HOST):\n            hdfs_kwargs[\"host\"] = host\n        if port := self.properties.get(HDFS_PORT):\n            # port should be an integer type\n            hdfs_kwargs[\"port\"] = int(port)\n        if user := self.properties.get(HDFS_USER):\n            hdfs_kwargs[\"user\"] = user\n        if kerb_ticket := self.properties.get(HDFS_KERB_TICKET):\n            hdfs_kwargs[\"kerb_ticket\"] = kerb_ticket\n\n        return HadoopFileSystem(**hdfs_kwargs)\n\n    def _initialize_gcs_fs(self) -&gt; FileSystem:\n        from pyarrow.fs import GcsFileSystem\n\n        gcs_kwargs: dict[str, Any] = {}\n        if access_token := self.properties.get(GCS_TOKEN):\n            gcs_kwargs[\"access_token\"] = access_token\n        if expiration := self.properties.get(GCS_TOKEN_EXPIRES_AT_MS):\n            gcs_kwargs[\"credential_token_expiration\"] = millis_to_datetime(int(expiration))\n        if bucket_location := self.properties.get(GCS_DEFAULT_LOCATION):\n            gcs_kwargs[\"default_bucket_location\"] = bucket_location\n        if endpoint := self.properties.get(GCS_SERVICE_HOST):\n            url_parts = urlparse(endpoint)\n            gcs_kwargs[\"scheme\"] = url_parts.scheme\n            gcs_kwargs[\"endpoint_override\"] = url_parts.netloc\n\n        return GcsFileSystem(**gcs_kwargs)\n\n    def _initialize_local_fs(self) -&gt; FileSystem:\n        return PyArrowLocalFileSystem()\n\n    def new_input(self, location: str) -&gt; PyArrowFile:\n        \"\"\"Get a PyArrowFile instance to read bytes from the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n\n        Returns:\n            PyArrowFile: A PyArrowFile instance for the given location.\n        \"\"\"\n        scheme, netloc, path = self.parse_location(location, self.properties)\n        return PyArrowFile(\n            fs=self.fs_by_scheme(scheme, netloc),\n            location=location,\n            path=path,\n            buffer_size=int(self.properties.get(BUFFER_SIZE, ONE_MEGABYTE)),\n        )\n\n    def new_output(self, location: str) -&gt; PyArrowFile:\n        \"\"\"Get a PyArrowFile instance to write bytes to the file at the given location.\n\n        Args:\n            location (str): A URI or a path to a local file.\n\n        Returns:\n            PyArrowFile: A PyArrowFile instance for the given location.\n        \"\"\"\n        scheme, netloc, path = self.parse_location(location, self.properties)\n        return PyArrowFile(\n            fs=self.fs_by_scheme(scheme, netloc),\n            location=location,\n            path=path,\n            buffer_size=int(self.properties.get(BUFFER_SIZE, ONE_MEGABYTE)),\n        )\n\n    def delete(self, location: str | InputFile | OutputFile) -&gt; None:\n        \"\"\"Delete the file at the given location.\n\n        Args:\n            location (Union[str, InputFile, OutputFile]): The URI to the file--if an InputFile instance or\n                an OutputFile instance is provided, the location attribute for that instance is used as\n                the location to delete.\n\n        Raises:\n            FileNotFoundError: When the file at the provided location does not exist.\n            PermissionError: If the file at the provided location cannot be accessed due to a permission error such as\n                an AWS error code 15.\n        \"\"\"\n        str_location = location.location if isinstance(location, (InputFile, OutputFile)) else location\n        scheme, netloc, path = self.parse_location(str_location, self.properties)\n        fs = self.fs_by_scheme(scheme, netloc)\n\n        try:\n            fs.delete_file(path)\n        except FileNotFoundError:\n            raise\n        except PermissionError:\n            raise\n        except OSError as e:\n            if e.errno == 2 or \"Path does not exist\" in str(e):\n                raise FileNotFoundError(f\"Cannot delete file, does not exist: {location}\") from e\n            elif e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n                raise PermissionError(f\"Cannot delete file, access denied: {location}\") from e\n            raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n\n    def __getstate__(self) -&gt; dict[str, Any]:\n        \"\"\"Create a dictionary of the PyArrowFileIO fields used when pickling.\"\"\"\n        fileio_copy = copy(self.__dict__)\n        fileio_copy[\"fs_by_scheme\"] = None\n        return fileio_copy\n\n    def __setstate__(self, state: dict[str, Any]) -&gt; None:\n        \"\"\"Deserialize the state into a PyArrowFileIO instance.\"\"\"\n        self.__dict__ = state\n        self.fs_by_scheme = lru_cache(self._initialize_fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Create a dictionary of the PyArrowFileIO fields used when pickling.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def __getstate__(self) -&gt; dict[str, Any]:\n    \"\"\"Create a dictionary of the PyArrowFileIO fields used when pickling.\"\"\"\n    fileio_copy = copy(self.__dict__)\n    fileio_copy[\"fs_by_scheme\"] = None\n    return fileio_copy\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Deserialize the state into a PyArrowFileIO instance.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def __setstate__(self, state: dict[str, Any]) -&gt; None:\n    \"\"\"Deserialize the state into a PyArrowFileIO instance.\"\"\"\n    self.__dict__ = state\n    self.fs_by_scheme = lru_cache(self._initialize_fs)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.delete","title":"<code>delete(location)</code>","text":"<p>Delete the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Union[str, InputFile, OutputFile]</code> <p>The URI to the file--if an InputFile instance or an OutputFile instance is provided, the location attribute for that instance is used as the location to delete.</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>When the file at the provided location does not exist.</p> <code>PermissionError</code> <p>If the file at the provided location cannot be accessed due to a permission error such as an AWS error code 15.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def delete(self, location: str | InputFile | OutputFile) -&gt; None:\n    \"\"\"Delete the file at the given location.\n\n    Args:\n        location (Union[str, InputFile, OutputFile]): The URI to the file--if an InputFile instance or\n            an OutputFile instance is provided, the location attribute for that instance is used as\n            the location to delete.\n\n    Raises:\n        FileNotFoundError: When the file at the provided location does not exist.\n        PermissionError: If the file at the provided location cannot be accessed due to a permission error such as\n            an AWS error code 15.\n    \"\"\"\n    str_location = location.location if isinstance(location, (InputFile, OutputFile)) else location\n    scheme, netloc, path = self.parse_location(str_location, self.properties)\n    fs = self.fs_by_scheme(scheme, netloc)\n\n    try:\n        fs.delete_file(path)\n    except FileNotFoundError:\n        raise\n    except PermissionError:\n        raise\n    except OSError as e:\n        if e.errno == 2 or \"Path does not exist\" in str(e):\n            raise FileNotFoundError(f\"Cannot delete file, does not exist: {location}\") from e\n        elif e.errno == 13 or \"AWS Error [code 15]\" in str(e):\n            raise PermissionError(f\"Cannot delete file, access denied: {location}\") from e\n        raise  # pragma: no cover - If some other kind of OSError, raise the raw error\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.new_input","title":"<code>new_input(location)</code>","text":"<p>Get a PyArrowFile instance to read bytes from the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Returns:</p> Name Type Description <code>PyArrowFile</code> <code>PyArrowFile</code> <p>A PyArrowFile instance for the given location.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def new_input(self, location: str) -&gt; PyArrowFile:\n    \"\"\"Get a PyArrowFile instance to read bytes from the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Returns:\n        PyArrowFile: A PyArrowFile instance for the given location.\n    \"\"\"\n    scheme, netloc, path = self.parse_location(location, self.properties)\n    return PyArrowFile(\n        fs=self.fs_by_scheme(scheme, netloc),\n        location=location,\n        path=path,\n        buffer_size=int(self.properties.get(BUFFER_SIZE, ONE_MEGABYTE)),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.new_output","title":"<code>new_output(location)</code>","text":"<p>Get a PyArrowFile instance to write bytes to the file at the given location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>A URI or a path to a local file.</p> required <p>Returns:</p> Name Type Description <code>PyArrowFile</code> <code>PyArrowFile</code> <p>A PyArrowFile instance for the given location.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def new_output(self, location: str) -&gt; PyArrowFile:\n    \"\"\"Get a PyArrowFile instance to write bytes to the file at the given location.\n\n    Args:\n        location (str): A URI or a path to a local file.\n\n    Returns:\n        PyArrowFile: A PyArrowFile instance for the given location.\n    \"\"\"\n    scheme, netloc, path = self.parse_location(location, self.properties)\n    return PyArrowFile(\n        fs=self.fs_by_scheme(scheme, netloc),\n        location=location,\n        path=path,\n        buffer_size=int(self.properties.get(BUFFER_SIZE, ONE_MEGABYTE)),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowFileIO.parse_location","title":"<code>parse_location(location, properties=EMPTY_DICT)</code>  <code>staticmethod</code>","text":"<p>Return (scheme, netloc, path) for the given location.</p> <p>Uses DEFAULT_SCHEME and DEFAULT_NETLOC if scheme/netloc are missing.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@staticmethod\ndef parse_location(location: str, properties: Properties = EMPTY_DICT) -&gt; tuple[str, str, str]:\n    \"\"\"Return (scheme, netloc, path) for the given location.\n\n    Uses DEFAULT_SCHEME and DEFAULT_NETLOC if scheme/netloc are missing.\n    \"\"\"\n    uri = urlparse(location)\n\n    if not uri.scheme:\n        default_scheme = properties.get(\"DEFAULT_SCHEME\", \"file\")\n        default_netloc = properties.get(\"DEFAULT_NETLOC\", \"\")\n        return default_scheme, default_netloc, os.path.abspath(location)\n    elif uri.scheme in (\"hdfs\", \"viewfs\"):\n        return uri.scheme, uri.netloc, uri.path\n    else:\n        return uri.scheme, uri.netloc, f\"{uri.netloc}{uri.path}\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor","title":"<code>PyArrowSchemaVisitor</code>","text":"<p>               Bases: <code>Generic[T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class PyArrowSchemaVisitor(Generic[T], ABC):\n    def before_field(self, field: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n\n    def after_field(self, field: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n\n    def before_list_element(self, element: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n\n    def after_list_element(self, element: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n\n    def before_map_key(self, key: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n\n    def after_map_key(self, key: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n\n    def before_map_value(self, value: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n\n    def after_map_value(self, value: pa.Field) -&gt; None:\n        \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n\n    @abstractmethod\n    def schema(self, schema: pa.Schema, struct_result: T) -&gt; T:\n        \"\"\"Visit a schema.\"\"\"\n\n    @abstractmethod\n    def struct(self, struct: pa.StructType, field_results: builtins.list[T]) -&gt; T:\n        \"\"\"Visit a struct.\"\"\"\n\n    @abstractmethod\n    def field(self, field: pa.Field, field_result: T) -&gt; T:\n        \"\"\"Visit a field.\"\"\"\n\n    @abstractmethod\n    def list(self, list_type: pa.ListType, element_result: T) -&gt; T:\n        \"\"\"Visit a list.\"\"\"\n\n    @abstractmethod\n    def map(self, map_type: pa.MapType, key_result: T, value_result: T) -&gt; T:\n        \"\"\"Visit a map.\"\"\"\n\n    @abstractmethod\n    def primitive(self, primitive: pa.DataType) -&gt; T:\n        \"\"\"Visit a primitive type.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.after_field","title":"<code>after_field(field)</code>","text":"<p>Override this method to perform an action immediately after visiting a field.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def after_field(self, field: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.after_list_element","title":"<code>after_list_element(element)</code>","text":"<p>Override this method to perform an action immediately after visiting an element within a ListType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def after_list_element(self, element: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting an element within a ListType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.after_map_key","title":"<code>after_map_key(key)</code>","text":"<p>Override this method to perform an action immediately after visiting a key within a MapType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def after_map_key(self, key: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a key within a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.after_map_value","title":"<code>after_map_value(value)</code>","text":"<p>Override this method to perform an action immediately after visiting a value within a MapType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def after_map_value(self, value: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately after visiting a value within a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.before_field","title":"<code>before_field(field)</code>","text":"<p>Override this method to perform an action immediately before visiting a field.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def before_field(self, field: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.before_list_element","title":"<code>before_list_element(element)</code>","text":"<p>Override this method to perform an action immediately before visiting an element within a ListType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def before_list_element(self, element: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting an element within a ListType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.before_map_key","title":"<code>before_map_key(key)</code>","text":"<p>Override this method to perform an action immediately before visiting a key within a MapType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def before_map_key(self, key: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a key within a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.before_map_value","title":"<code>before_map_value(value)</code>","text":"<p>Override this method to perform an action immediately before visiting a value within a MapType.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def before_map_value(self, value: pa.Field) -&gt; None:\n    \"\"\"Override this method to perform an action immediately before visiting a value within a MapType.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.field","title":"<code>field(field, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a field.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef field(self, field: pa.Field, field_result: T) -&gt; T:\n    \"\"\"Visit a field.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.list","title":"<code>list(list_type, element_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a list.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef list(self, list_type: pa.ListType, element_result: T) -&gt; T:\n    \"\"\"Visit a list.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.map","title":"<code>map(map_type, key_result, value_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a map.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef map(self, map_type: pa.MapType, key_result: T, value_result: T) -&gt; T:\n    \"\"\"Visit a map.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.primitive","title":"<code>primitive(primitive)</code>  <code>abstractmethod</code>","text":"<p>Visit a primitive type.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef primitive(self, primitive: pa.DataType) -&gt; T:\n    \"\"\"Visit a primitive type.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.schema","title":"<code>schema(schema, struct_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a schema.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef schema(self, schema: pa.Schema, struct_result: T) -&gt; T:\n    \"\"\"Visit a schema.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.PyArrowSchemaVisitor.struct","title":"<code>struct(struct, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a struct.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@abstractmethod\ndef struct(self, struct: pa.StructType, field_results: builtins.list[T]) -&gt; T:\n    \"\"\"Visit a struct.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.UnsupportedPyArrowTypeException","title":"<code>UnsupportedPyArrowTypeException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Cannot convert PyArrow type to corresponding Iceberg type.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>class UnsupportedPyArrowTypeException(Exception):\n    \"\"\"Cannot convert PyArrow type to corresponding Iceberg type.\"\"\"\n\n    def __init__(self, field: pa.Field, *args: Any):\n        self.field = field\n        super().__init__(*args)\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.compute_statistics_plan","title":"<code>compute_statistics_plan(schema, table_properties)</code>","text":"<p>Compute the statistics plan for all columns.</p> <p>The resulting list is assumed to have the same length and same order as the columns in the pyarrow table. This allows the list to map from the column index to the Iceberg column ID. For each element, the desired metrics collection that was provided by the user in the configuration is computed and then adjusted according to the data type of the column. For nested columns the minimum and maximum values are not computed. And truncation is only applied to text of binary strings.</p> <p>Parameters:</p> Name Type Description Default <code>table_properties</code> <code>from pyiceberg.table.metadata.TableMetadata</code> <p>The Iceberg table metadata properties. They are required to compute the mapping of column position to iceberg schema type id. It's also used to set the mode for column metrics collection</p> required Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def compute_statistics_plan(\n    schema: Schema,\n    table_properties: dict[str, str],\n) -&gt; dict[int, StatisticsCollector]:\n    \"\"\"\n    Compute the statistics plan for all columns.\n\n    The resulting list is assumed to have the same length and same order as the columns in the pyarrow table.\n    This allows the list to map from the column index to the Iceberg column ID.\n    For each element, the desired metrics collection that was provided by the user in the configuration\n    is computed and then adjusted according to the data type of the column. For nested columns the minimum\n    and maximum values are not computed. And truncation is only applied to text of binary strings.\n\n    Args:\n        table_properties (from pyiceberg.table.metadata.TableMetadata): The Iceberg table metadata properties.\n            They are required to compute the mapping of column position to iceberg schema type id. It's also\n            used to set the mode for column metrics collection\n    \"\"\"\n    stats_cols = pre_order_visit(schema, PyArrowStatisticsCollector(schema, table_properties))\n    result: dict[int, StatisticsCollector] = {}\n    for stats_col in stats_cols:\n        result[stats_col.field_id] = stats_col\n    return result\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.data_file_statistics_from_parquet_metadata","title":"<code>data_file_statistics_from_parquet_metadata(parquet_metadata, stats_columns, parquet_column_mapping)</code>","text":"<p>Compute and return DataFileStatistics that includes the following.</p> <ul> <li>record_count</li> <li>column_sizes</li> <li>value_counts</li> <li>null_value_counts</li> <li>nan_value_counts</li> <li>column_aggregates</li> <li>split_offsets</li> </ul> <p>Parameters:</p> Name Type Description Default <code>parquet_metadata</code> <code>FileMetaData</code> <p>A pyarrow metadata object.</p> required <code>stats_columns</code> <code>Dict[int, StatisticsCollector]</code> <p>The statistics gathering plan. It is required to set the mode for column metrics collection</p> required <code>parquet_column_mapping</code> <code>Dict[str, int]</code> <p>The mapping of the parquet file name to the field ID</p> required Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def data_file_statistics_from_parquet_metadata(\n    parquet_metadata: pq.FileMetaData,\n    stats_columns: dict[int, StatisticsCollector],\n    parquet_column_mapping: dict[str, int],\n) -&gt; DataFileStatistics:\n    \"\"\"\n    Compute and return DataFileStatistics that includes the following.\n\n    - record_count\n    - column_sizes\n    - value_counts\n    - null_value_counts\n    - nan_value_counts\n    - column_aggregates\n    - split_offsets\n\n    Args:\n        parquet_metadata (pyarrow.parquet.FileMetaData): A pyarrow metadata object.\n        stats_columns (Dict[int, StatisticsCollector]): The statistics gathering plan. It is required to\n            set the mode for column metrics collection\n        parquet_column_mapping (Dict[str, int]): The mapping of the parquet file name to the field ID\n    \"\"\"\n    column_sizes: dict[int, int] = {}\n    value_counts: dict[int, int] = {}\n    split_offsets: list[int] = []\n\n    null_value_counts: dict[int, int] = {}\n    nan_value_counts: dict[int, int] = {}\n\n    col_aggs = {}\n\n    invalidate_col: set[int] = set()\n    for r in range(parquet_metadata.num_row_groups):\n        # References:\n        # https://github.com/apache/iceberg/blob/fc381a81a1fdb8f51a0637ca27cd30673bd7aad3/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java#L232\n        # https://github.com/apache/parquet-mr/blob/ac29db4611f86a07cc6877b416aa4b183e09b353/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/ColumnChunkMetaData.java#L184\n\n        row_group = parquet_metadata.row_group(r)\n\n        data_offset = row_group.column(0).data_page_offset\n        dictionary_offset = row_group.column(0).dictionary_page_offset\n\n        if row_group.column(0).has_dictionary_page and dictionary_offset &lt; data_offset:\n            split_offsets.append(dictionary_offset)\n        else:\n            split_offsets.append(data_offset)\n\n        for pos in range(parquet_metadata.num_columns):\n            column = row_group.column(pos)\n            field_id = parquet_column_mapping[column.path_in_schema]\n\n            stats_col = stats_columns[field_id]\n\n            column_sizes.setdefault(field_id, 0)\n            column_sizes[field_id] += column.total_compressed_size\n\n            if stats_col.mode == MetricsMode(MetricModeTypes.NONE):\n                continue\n\n            value_counts[field_id] = value_counts.get(field_id, 0) + column.num_values\n\n            if column.is_stats_set:\n                try:\n                    statistics = column.statistics\n\n                    if statistics.has_null_count:\n                        null_value_counts[field_id] = null_value_counts.get(field_id, 0) + statistics.null_count\n\n                    if stats_col.mode == MetricsMode(MetricModeTypes.COUNTS):\n                        continue\n\n                    if field_id not in col_aggs:\n                        try:\n                            col_aggs[field_id] = StatsAggregator(\n                                stats_col.iceberg_type, statistics.physical_type, stats_col.mode.length\n                            )\n                        except ValueError as e:\n                            raise ValueError(f\"{e} for column '{stats_col.column_name}'\") from e\n\n                    if isinstance(stats_col.iceberg_type, DecimalType) and statistics.physical_type != \"FIXED_LEN_BYTE_ARRAY\":\n                        scale = stats_col.iceberg_type.scale\n                        (\n                            col_aggs[field_id].update_min(unscaled_to_decimal(statistics.min_raw, scale))\n                            if statistics.min_raw is not None\n                            else None\n                        )\n                        (\n                            col_aggs[field_id].update_max(unscaled_to_decimal(statistics.max_raw, scale))\n                            if statistics.max_raw is not None\n                            else None\n                        )\n                    else:\n                        col_aggs[field_id].update_min(statistics.min)\n                        col_aggs[field_id].update_max(statistics.max)\n\n                except pyarrow.lib.ArrowNotImplementedError as e:\n                    invalidate_col.add(field_id)\n                    logger.warning(e)\n            else:\n                invalidate_col.add(field_id)\n                logger.warning(\"PyArrow statistics missing for column %d when writing file\", pos)\n\n    split_offsets.sort()\n\n    for field_id in invalidate_col:\n        col_aggs.pop(field_id, None)\n        null_value_counts.pop(field_id, None)\n\n    return DataFileStatistics(\n        record_count=parquet_metadata.num_rows,\n        column_sizes=column_sizes,\n        value_counts=value_counts,\n        null_value_counts=null_value_counts,\n        nan_value_counts=nan_value_counts,\n        column_aggregates=col_aggs,\n        split_offsets=split_offsets,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.expression_to_pyarrow","title":"<code>expression_to_pyarrow(expr, schema=None)</code>","text":"<p>Convert an Iceberg boolean expression to a PyArrow expression.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>BooleanExpression</code> <p>The Iceberg boolean expression to convert.</p> required <code>schema</code> <code>Schema | None</code> <p>Optional Iceberg schema to resolve full field paths for nested fields.     If provided, nested struct fields will use dotted paths (e.g., \"parent.child\").</p> <code>None</code> <p>Returns:</p> Type Description <code>Expression</code> <p>A PyArrow compute expression.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def expression_to_pyarrow(expr: BooleanExpression, schema: Schema | None = None) -&gt; pc.Expression:\n    \"\"\"Convert an Iceberg boolean expression to a PyArrow expression.\n\n    Args:\n        expr: The Iceberg boolean expression to convert.\n        schema: Optional Iceberg schema to resolve full field paths for nested fields.\n                If provided, nested struct fields will use dotted paths (e.g., \"parent.child\").\n\n    Returns:\n        A PyArrow compute expression.\n    \"\"\"\n    return boolean_expression_visit(expr, _ConvertToArrowExpression(schema))\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.parquet_path_to_id_mapping","title":"<code>parquet_path_to_id_mapping(schema)</code>","text":"<p>Compute the mapping of parquet column path to Iceberg ID.</p> <p>For each column, the parquet file metadata has a path_in_schema attribute that follows a specific naming scheme for nested columns. This function computes a mapping of the full paths to the corresponding Iceberg IDs.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Schema</code> <p>The current table schema.</p> required Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>def parquet_path_to_id_mapping(\n    schema: Schema,\n) -&gt; dict[str, int]:\n    \"\"\"\n    Compute the mapping of parquet column path to Iceberg ID.\n\n    For each column, the parquet file metadata has a path_in_schema attribute that follows\n    a specific naming scheme for nested columns. This function computes a mapping of\n    the full paths to the corresponding Iceberg IDs.\n\n    Args:\n        schema (pyiceberg.schema.Schema): The current table schema.\n    \"\"\"\n    result: dict[str, int] = {}\n    for pair in pre_order_visit(schema, ID2ParquetPathVisitor()):\n        result[pair.parquet_path] = pair.field_id\n    return result\n</code></pre>"},{"location":"reference/pyiceberg/io/pyarrow/#pyiceberg.io.pyarrow.visit_pyarrow","title":"<code>visit_pyarrow(obj, visitor)</code>","text":"<p>Apply a pyarrow schema visitor to any point within a schema.</p> <p>The function traverses the schema in post-order fashion.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[DataType, Schema]</code> <p>An instance of a Schema or an IcebergType.</p> required <code>visitor</code> <code>PyArrowSchemaVisitor[T]</code> <p>An instance of an implementation of the generic PyarrowSchemaVisitor base class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If attempting to visit an unrecognized object type.</p> Source code in <code>pyiceberg/io/pyarrow.py</code> <pre><code>@singledispatch\ndef visit_pyarrow(obj: pa.DataType | pa.Schema, visitor: PyArrowSchemaVisitor[T]) -&gt; T:\n    \"\"\"Apply a pyarrow schema visitor to any point within a schema.\n\n    The function traverses the schema in post-order fashion.\n\n    Args:\n        obj (Union[pa.DataType, pa.Schema]): An instance of a Schema or an IcebergType.\n        visitor (PyArrowSchemaVisitor[T]): An instance of an implementation of the generic PyarrowSchemaVisitor base class.\n\n    Raises:\n        NotImplementedError: If attempting to visit an unrecognized object type.\n    \"\"\"\n    raise NotImplementedError(f\"Cannot visit non-type: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/","title":"table","text":""},{"location":"reference/pyiceberg/table/#pyiceberg.table.CommitTableRequest","title":"<code>CommitTableRequest</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>A pydantic BaseModel for a table commit request.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class CommitTableRequest(IcebergBaseModel):\n    \"\"\"A pydantic BaseModel for a table commit request.\"\"\"\n\n    identifier: TableIdentifier = Field()\n    requirements: tuple[TableRequirement, ...] = Field(default_factory=tuple)\n    updates: tuple[TableUpdate, ...] = Field(default_factory=tuple)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.CommitTableResponse","title":"<code>CommitTableResponse</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>A pydantic BaseModel for a table commit response.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class CommitTableResponse(IcebergBaseModel):\n    \"\"\"A pydantic BaseModel for a table commit response.\"\"\"\n\n    metadata: TableMetadata\n    metadata_location: str = Field(alias=\"metadata-location\")\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.CreateTableTransaction","title":"<code>CreateTableTransaction</code>","text":"<p>               Bases: <code>Transaction</code></p> <p>A transaction that involves the creation of a new table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class CreateTableTransaction(Transaction):\n    \"\"\"A transaction that involves the creation of a new table.\"\"\"\n\n    def _initial_changes(self, table_metadata: TableMetadata) -&gt; None:\n        \"\"\"Set the initial changes that can reconstruct the initial table metadata when creating the CreateTableTransaction.\"\"\"\n        self._updates += (\n            AssignUUIDUpdate(uuid=table_metadata.table_uuid),\n            UpgradeFormatVersionUpdate(format_version=table_metadata.format_version),\n        )\n\n        schema: Schema = table_metadata.schema()\n        self._updates += (\n            AddSchemaUpdate(schema_=schema),\n            SetCurrentSchemaUpdate(schema_id=-1),\n        )\n\n        spec: PartitionSpec = table_metadata.spec()\n        if spec.is_unpartitioned():\n            self._updates += (AddPartitionSpecUpdate(spec=UNPARTITIONED_PARTITION_SPEC),)\n        else:\n            self._updates += (AddPartitionSpecUpdate(spec=spec),)\n        self._updates += (SetDefaultSpecUpdate(spec_id=-1),)\n\n        sort_order: SortOrder | None = table_metadata.sort_order_by_id(table_metadata.default_sort_order_id)\n        if sort_order is None or sort_order.is_unsorted:\n            self._updates += (AddSortOrderUpdate(sort_order=UNSORTED_SORT_ORDER),)\n        else:\n            self._updates += (AddSortOrderUpdate(sort_order=sort_order),)\n        self._updates += (SetDefaultSortOrderUpdate(sort_order_id=-1),)\n\n        self._updates += (\n            SetLocationUpdate(location=table_metadata.location),\n            SetPropertiesUpdate(updates=table_metadata.properties),\n        )\n\n    def __init__(self, table: StagedTable):\n        super().__init__(table, autocommit=False)\n        self._initial_changes(table.metadata)\n\n    def commit_transaction(self) -&gt; Table:\n        \"\"\"Commit the changes to the catalog.\n\n        In the case of a CreateTableTransaction, the only requirement is AssertCreate.\n        Returns:\n            The table with the updates applied.\n        \"\"\"\n        if len(self._updates) &gt; 0:\n            self._table._do_commit(  # pylint: disable=W0212\n                updates=self._updates,\n                requirements=(AssertCreate(),),\n            )\n\n        self._updates = ()\n        self._requirements = ()\n\n        return self._table\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.CreateTableTransaction.commit_transaction","title":"<code>commit_transaction()</code>","text":"<p>Commit the changes to the catalog.</p> <p>In the case of a CreateTableTransaction, the only requirement is AssertCreate. Returns:     The table with the updates applied.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def commit_transaction(self) -&gt; Table:\n    \"\"\"Commit the changes to the catalog.\n\n    In the case of a CreateTableTransaction, the only requirement is AssertCreate.\n    Returns:\n        The table with the updates applied.\n    \"\"\"\n    if len(self._updates) &gt; 0:\n        self._table._do_commit(  # pylint: disable=W0212\n            updates=self._updates,\n            requirements=(AssertCreate(),),\n        )\n\n    self._updates = ()\n    self._requirements = ()\n\n    return self._table\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan","title":"<code>DataScan</code>","text":"<p>               Bases: <code>TableScan</code></p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class DataScan(TableScan):\n    def _build_partition_projection(self, spec_id: int) -&gt; BooleanExpression:\n        project = inclusive_projection(self.table_metadata.schema(), self.table_metadata.specs()[spec_id], self.case_sensitive)\n        return project(self.row_filter)\n\n    @cached_property\n    def partition_filters(self) -&gt; KeyDefaultDict[int, BooleanExpression]:\n        return KeyDefaultDict(self._build_partition_projection)\n\n    def _build_manifest_evaluator(self, spec_id: int) -&gt; Callable[[ManifestFile], bool]:\n        spec = self.table_metadata.specs()[spec_id]\n        return manifest_evaluator(spec, self.table_metadata.schema(), self.partition_filters[spec_id], self.case_sensitive)\n\n    def _build_partition_evaluator(self, spec_id: int) -&gt; Callable[[DataFile], bool]:\n        spec = self.table_metadata.specs()[spec_id]\n        partition_type = spec.partition_type(self.table_metadata.schema())\n        partition_schema = Schema(*partition_type.fields)\n        partition_expr = self.partition_filters[spec_id]\n\n        # The lambda created here is run in multiple threads.\n        # So we avoid creating _EvaluatorExpression methods bound to a single\n        # shared instance across multiple threads.\n        return lambda data_file: expression_evaluator(partition_schema, partition_expr, self.case_sensitive)(data_file.partition)\n\n    def _build_metrics_evaluator(self) -&gt; Callable[[DataFile], bool]:\n        schema = self.table_metadata.schema()\n        include_empty_files = strtobool(self.options.get(\"include_empty_files\", \"false\"))\n\n        # The lambda created here is run in multiple threads.\n        # So we avoid creating _InclusiveMetricsEvaluator methods bound to a single\n        # shared instance across multiple threads.\n        return lambda data_file: _InclusiveMetricsEvaluator(\n            schema,\n            self.row_filter,\n            self.case_sensitive,\n            include_empty_files,\n        ).eval(data_file)\n\n    def _build_residual_evaluator(self, spec_id: int) -&gt; Callable[[DataFile], ResidualEvaluator]:\n        spec = self.table_metadata.specs()[spec_id]\n\n        from pyiceberg.expressions.visitors import residual_evaluator_of\n\n        # The lambda created here is run in multiple threads.\n        # So we avoid creating _EvaluatorExpression methods bound to a single\n        # shared instance across multiple threads.\n        return lambda datafile: (\n            residual_evaluator_of(\n                spec=spec,\n                expr=self.row_filter,\n                case_sensitive=self.case_sensitive,\n                schema=self.table_metadata.schema(),\n            )\n        )\n\n    @staticmethod\n    def _check_sequence_number(min_sequence_number: int, manifest: ManifestFile) -&gt; bool:\n        \"\"\"Ensure that no manifests are loaded that contain deletes that are older than the data.\n\n        Args:\n            min_sequence_number (int): The minimal sequence number.\n            manifest (ManifestFile): A ManifestFile that can be either data or deletes.\n\n        Returns:\n            Boolean indicating if it is either a data file, or a relevant delete file.\n        \"\"\"\n        return manifest.content == ManifestContent.DATA or (\n            # Not interested in deletes that are older than the data\n            manifest.content == ManifestContent.DELETES\n            and (manifest.sequence_number or INITIAL_SEQUENCE_NUMBER) &gt;= min_sequence_number\n        )\n\n    def scan_plan_helper(self) -&gt; Iterator[list[ManifestEntry]]:\n        \"\"\"Filter and return manifest entries based on partition and metrics evaluators.\n\n        Returns:\n            Iterator of ManifestEntry objects that match the scan's partition filter.\n        \"\"\"\n        snapshot = self.snapshot()\n        if not snapshot:\n            return iter([])\n\n        # step 1: filter manifests using partition summaries\n        # the filter depends on the partition spec used to write the manifest file, so create a cache of filters for each spec id\n\n        manifest_evaluators: dict[int, Callable[[ManifestFile], bool]] = KeyDefaultDict(self._build_manifest_evaluator)\n\n        manifests = [\n            manifest_file\n            for manifest_file in snapshot.manifests(self.io)\n            if manifest_evaluators[manifest_file.partition_spec_id](manifest_file)\n        ]\n\n        # step 2: filter the data files in each manifest\n        # this filter depends on the partition spec used to write the manifest file\n\n        partition_evaluators: dict[int, Callable[[DataFile], bool]] = KeyDefaultDict(self._build_partition_evaluator)\n\n        min_sequence_number = _min_sequence_number(manifests)\n\n        executor = ExecutorFactory.get_or_create()\n\n        return executor.map(\n            lambda args: _open_manifest(*args),\n            [\n                (\n                    self.io,\n                    manifest,\n                    partition_evaluators[manifest.partition_spec_id],\n                    self._build_metrics_evaluator(),\n                )\n                for manifest in manifests\n                if self._check_sequence_number(min_sequence_number, manifest)\n            ],\n        )\n\n    def _should_use_server_side_planning(self) -&gt; bool:\n        \"\"\"Check if server-side scan planning should be used for this scan.\"\"\"\n        if not self.catalog:\n            return False\n        return self.catalog.supports_server_side_planning()\n\n    def _plan_files_server_side(self) -&gt; Iterable[FileScanTask]:\n        \"\"\"Plan files using REST server-side scan planning.\"\"\"\n        from pyiceberg.catalog.rest import RestCatalog\n        from pyiceberg.catalog.rest.scan_planning import PlanTableScanRequest\n\n        if not isinstance(self.catalog, RestCatalog):\n            raise TypeError(\"REST scan planning requires a RestCatalog\")\n        if self.table_identifier is None:\n            raise ValueError(\"REST scan planning requires a table identifier\")\n\n        request = PlanTableScanRequest(\n            snapshot_id=self.snapshot_id,\n            select=list(self.selected_fields) if self.selected_fields != (\"*\",) else None,\n            filter=self.row_filter if self.row_filter != ALWAYS_TRUE else None,\n            case_sensitive=self.case_sensitive,\n        )\n\n        return self.catalog.plan_scan(self.table_identifier, request)\n\n    def _plan_files_local(self) -&gt; Iterable[FileScanTask]:\n        \"\"\"Plan files locally by reading manifests.\"\"\"\n        data_entries: list[ManifestEntry] = []\n        delete_index = DeleteFileIndex()\n\n        residual_evaluators: dict[int, Callable[[DataFile], ResidualEvaluator]] = KeyDefaultDict(self._build_residual_evaluator)\n\n        for manifest_entry in chain.from_iterable(self.scan_plan_helper()):\n            data_file = manifest_entry.data_file\n            if data_file.content == DataFileContent.DATA:\n                data_entries.append(manifest_entry)\n            elif data_file.content == DataFileContent.POSITION_DELETES:\n                delete_index.add_delete_file(manifest_entry, partition_key=data_file.partition)\n            elif data_file.content == DataFileContent.EQUALITY_DELETES:\n                raise ValueError(\"PyIceberg does not yet support equality deletes: https://github.com/apache/iceberg/issues/6568\")\n            else:\n                raise ValueError(f\"Unknown DataFileContent ({data_file.content}): {manifest_entry}\")\n        return [\n            FileScanTask(\n                data_entry.data_file,\n                delete_files=delete_index.for_data_file(\n                    data_entry.sequence_number or INITIAL_SEQUENCE_NUMBER,\n                    data_entry.data_file,\n                    partition_key=data_entry.data_file.partition,\n                ),\n                residual=residual_evaluators[data_entry.data_file.spec_id](data_entry.data_file).residual_for(\n                    data_entry.data_file.partition\n                ),\n            )\n            for data_entry in data_entries\n        ]\n\n    def plan_files(self) -&gt; Iterable[FileScanTask]:\n        \"\"\"Plans the relevant files by filtering on the PartitionSpecs.\n\n        If the table comes from a REST catalog with scan planning enabled,\n        this will use server-side scan planning. Otherwise, it falls back\n        to local planning.\n\n        Returns:\n            List of FileScanTasks that contain both data and delete files.\n        \"\"\"\n        if self._should_use_server_side_planning():\n            return self._plan_files_server_side()\n        return self._plan_files_local()\n\n    def to_arrow(self) -&gt; pa.Table:\n        \"\"\"Read an Arrow table eagerly from this DataScan.\n\n        All rows will be loaded into memory at once.\n\n        Returns:\n            pa.Table: Materialized Arrow Table from the Iceberg table's DataScan\n        \"\"\"\n        from pyiceberg.io.pyarrow import ArrowScan\n\n        return ArrowScan(\n            self.table_metadata, self.io, self.projection(), self.row_filter, self.case_sensitive, self.limit\n        ).to_table(self.plan_files())\n\n    def to_arrow_batch_reader(self) -&gt; pa.RecordBatchReader:\n        \"\"\"Return an Arrow RecordBatchReader from this DataScan.\n\n        For large results, using a RecordBatchReader requires less memory than\n        loading an Arrow Table for the same DataScan, because a RecordBatch\n        is read one at a time.\n\n        Returns:\n            pa.RecordBatchReader: Arrow RecordBatchReader from the Iceberg table's DataScan\n                which can be used to read a stream of record batches one by one.\n        \"\"\"\n        import pyarrow as pa\n\n        from pyiceberg.io.pyarrow import ArrowScan, schema_to_pyarrow\n\n        target_schema = schema_to_pyarrow(self.projection())\n        batches = ArrowScan(\n            self.table_metadata, self.io, self.projection(), self.row_filter, self.case_sensitive, self.limit\n        ).to_record_batches(self.plan_files())\n\n        return pa.RecordBatchReader.from_batches(\n            target_schema,\n            batches,\n        ).cast(target_schema)\n\n    def to_pandas(self, **kwargs: Any) -&gt; pd.DataFrame:\n        \"\"\"Read a Pandas DataFrame eagerly from this Iceberg table.\n\n        Returns:\n            pd.DataFrame: Materialized Pandas Dataframe from the Iceberg table\n        \"\"\"\n        return self.to_arrow().to_pandas(**kwargs)\n\n    def to_duckdb(self, table_name: str, connection: DuckDBPyConnection | None = None) -&gt; DuckDBPyConnection:\n        \"\"\"Shorthand for loading the Iceberg Table in DuckDB.\n\n        Returns:\n            DuckDBPyConnection: In memory DuckDB connection with the Iceberg table.\n        \"\"\"\n        import duckdb\n\n        con = connection or duckdb.connect(database=\":memory:\")\n        con.register(table_name, self.to_arrow())\n\n        return con\n\n    def to_ray(self) -&gt; ray.data.dataset.Dataset:\n        \"\"\"Read a Ray Dataset eagerly from this Iceberg table.\n\n        Returns:\n            ray.data.dataset.Dataset: Materialized Ray Dataset from the Iceberg table\n        \"\"\"\n        import ray\n\n        return ray.data.from_arrow(self.to_arrow())\n\n    def to_polars(self) -&gt; pl.DataFrame:\n        \"\"\"Read a Polars DataFrame from this Iceberg table.\n\n        Returns:\n            pl.DataFrame: Materialized Polars Dataframe from the Iceberg table\n        \"\"\"\n        import polars as pl\n\n        result = pl.from_arrow(self.to_arrow())\n        if isinstance(result, pl.Series):\n            result = result.to_frame()\n\n        return result\n\n    def count(self) -&gt; int:\n        from pyiceberg.io.pyarrow import ArrowScan\n\n        # Usage: Calculates the total number of records in a Scan that haven't had positional deletes.\n        res = 0\n        # every task is a FileScanTask\n        tasks = self.plan_files()\n\n        for task in tasks:\n            # task.residual is a Boolean Expression if the filter condition is fully satisfied by the\n            # partition value and task.delete_files represents that positional delete haven't been merged yet\n            # hence those files have to read as a pyarrow table applying the filter and deletes\n            if task.residual == AlwaysTrue() and len(task.delete_files) == 0:\n                # Every File has a metadata stat that stores the file record count\n                res += task.file.record_count\n            else:\n                arrow_scan = ArrowScan(\n                    table_metadata=self.table_metadata,\n                    io=self.io,\n                    projected_schema=self.projection(),\n                    row_filter=self.row_filter,\n                    case_sensitive=self.case_sensitive,\n                )\n                tbl = arrow_scan.to_table([task])\n                res += len(tbl)\n        return res\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.plan_files","title":"<code>plan_files()</code>","text":"<p>Plans the relevant files by filtering on the PartitionSpecs.</p> <p>If the table comes from a REST catalog with scan planning enabled, this will use server-side scan planning. Otherwise, it falls back to local planning.</p> <p>Returns:</p> Type Description <code>Iterable[FileScanTask]</code> <p>List of FileScanTasks that contain both data and delete files.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def plan_files(self) -&gt; Iterable[FileScanTask]:\n    \"\"\"Plans the relevant files by filtering on the PartitionSpecs.\n\n    If the table comes from a REST catalog with scan planning enabled,\n    this will use server-side scan planning. Otherwise, it falls back\n    to local planning.\n\n    Returns:\n        List of FileScanTasks that contain both data and delete files.\n    \"\"\"\n    if self._should_use_server_side_planning():\n        return self._plan_files_server_side()\n    return self._plan_files_local()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.scan_plan_helper","title":"<code>scan_plan_helper()</code>","text":"<p>Filter and return manifest entries based on partition and metrics evaluators.</p> <p>Returns:</p> Type Description <code>Iterator[list[ManifestEntry]]</code> <p>Iterator of ManifestEntry objects that match the scan's partition filter.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def scan_plan_helper(self) -&gt; Iterator[list[ManifestEntry]]:\n    \"\"\"Filter and return manifest entries based on partition and metrics evaluators.\n\n    Returns:\n        Iterator of ManifestEntry objects that match the scan's partition filter.\n    \"\"\"\n    snapshot = self.snapshot()\n    if not snapshot:\n        return iter([])\n\n    # step 1: filter manifests using partition summaries\n    # the filter depends on the partition spec used to write the manifest file, so create a cache of filters for each spec id\n\n    manifest_evaluators: dict[int, Callable[[ManifestFile], bool]] = KeyDefaultDict(self._build_manifest_evaluator)\n\n    manifests = [\n        manifest_file\n        for manifest_file in snapshot.manifests(self.io)\n        if manifest_evaluators[manifest_file.partition_spec_id](manifest_file)\n    ]\n\n    # step 2: filter the data files in each manifest\n    # this filter depends on the partition spec used to write the manifest file\n\n    partition_evaluators: dict[int, Callable[[DataFile], bool]] = KeyDefaultDict(self._build_partition_evaluator)\n\n    min_sequence_number = _min_sequence_number(manifests)\n\n    executor = ExecutorFactory.get_or_create()\n\n    return executor.map(\n        lambda args: _open_manifest(*args),\n        [\n            (\n                self.io,\n                manifest,\n                partition_evaluators[manifest.partition_spec_id],\n                self._build_metrics_evaluator(),\n            )\n            for manifest in manifests\n            if self._check_sequence_number(min_sequence_number, manifest)\n        ],\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_arrow","title":"<code>to_arrow()</code>","text":"<p>Read an Arrow table eagerly from this DataScan.</p> <p>All rows will be loaded into memory at once.</p> <p>Returns:</p> Type Description <code>Table</code> <p>pa.Table: Materialized Arrow Table from the Iceberg table's DataScan</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_arrow(self) -&gt; pa.Table:\n    \"\"\"Read an Arrow table eagerly from this DataScan.\n\n    All rows will be loaded into memory at once.\n\n    Returns:\n        pa.Table: Materialized Arrow Table from the Iceberg table's DataScan\n    \"\"\"\n    from pyiceberg.io.pyarrow import ArrowScan\n\n    return ArrowScan(\n        self.table_metadata, self.io, self.projection(), self.row_filter, self.case_sensitive, self.limit\n    ).to_table(self.plan_files())\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_arrow_batch_reader","title":"<code>to_arrow_batch_reader()</code>","text":"<p>Return an Arrow RecordBatchReader from this DataScan.</p> <p>For large results, using a RecordBatchReader requires less memory than loading an Arrow Table for the same DataScan, because a RecordBatch is read one at a time.</p> <p>Returns:</p> Type Description <code>RecordBatchReader</code> <p>pa.RecordBatchReader: Arrow RecordBatchReader from the Iceberg table's DataScan which can be used to read a stream of record batches one by one.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_arrow_batch_reader(self) -&gt; pa.RecordBatchReader:\n    \"\"\"Return an Arrow RecordBatchReader from this DataScan.\n\n    For large results, using a RecordBatchReader requires less memory than\n    loading an Arrow Table for the same DataScan, because a RecordBatch\n    is read one at a time.\n\n    Returns:\n        pa.RecordBatchReader: Arrow RecordBatchReader from the Iceberg table's DataScan\n            which can be used to read a stream of record batches one by one.\n    \"\"\"\n    import pyarrow as pa\n\n    from pyiceberg.io.pyarrow import ArrowScan, schema_to_pyarrow\n\n    target_schema = schema_to_pyarrow(self.projection())\n    batches = ArrowScan(\n        self.table_metadata, self.io, self.projection(), self.row_filter, self.case_sensitive, self.limit\n    ).to_record_batches(self.plan_files())\n\n    return pa.RecordBatchReader.from_batches(\n        target_schema,\n        batches,\n    ).cast(target_schema)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_duckdb","title":"<code>to_duckdb(table_name, connection=None)</code>","text":"<p>Shorthand for loading the Iceberg Table in DuckDB.</p> <p>Returns:</p> Name Type Description <code>DuckDBPyConnection</code> <code>DuckDBPyConnection</code> <p>In memory DuckDB connection with the Iceberg table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_duckdb(self, table_name: str, connection: DuckDBPyConnection | None = None) -&gt; DuckDBPyConnection:\n    \"\"\"Shorthand for loading the Iceberg Table in DuckDB.\n\n    Returns:\n        DuckDBPyConnection: In memory DuckDB connection with the Iceberg table.\n    \"\"\"\n    import duckdb\n\n    con = connection or duckdb.connect(database=\":memory:\")\n    con.register(table_name, self.to_arrow())\n\n    return con\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_pandas","title":"<code>to_pandas(**kwargs)</code>","text":"<p>Read a Pandas DataFrame eagerly from this Iceberg table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Materialized Pandas Dataframe from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_pandas(self, **kwargs: Any) -&gt; pd.DataFrame:\n    \"\"\"Read a Pandas DataFrame eagerly from this Iceberg table.\n\n    Returns:\n        pd.DataFrame: Materialized Pandas Dataframe from the Iceberg table\n    \"\"\"\n    return self.to_arrow().to_pandas(**kwargs)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_polars","title":"<code>to_polars()</code>","text":"<p>Read a Polars DataFrame from this Iceberg table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Materialized Polars Dataframe from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_polars(self) -&gt; pl.DataFrame:\n    \"\"\"Read a Polars DataFrame from this Iceberg table.\n\n    Returns:\n        pl.DataFrame: Materialized Polars Dataframe from the Iceberg table\n    \"\"\"\n    import polars as pl\n\n    result = pl.from_arrow(self.to_arrow())\n    if isinstance(result, pl.Series):\n        result = result.to_frame()\n\n    return result\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.DataScan.to_ray","title":"<code>to_ray()</code>","text":"<p>Read a Ray Dataset eagerly from this Iceberg table.</p> <p>Returns:</p> Type Description <code>Dataset</code> <p>ray.data.dataset.Dataset: Materialized Ray Dataset from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_ray(self) -&gt; ray.data.dataset.Dataset:\n    \"\"\"Read a Ray Dataset eagerly from this Iceberg table.\n\n    Returns:\n        ray.data.dataset.Dataset: Materialized Ray Dataset from the Iceberg table\n    \"\"\"\n    import ray\n\n    return ray.data.from_arrow(self.to_arrow())\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.FileScanTask","title":"<code>FileScanTask</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ScanTask</code></p> <p>Task representing a data file and its corresponding delete files.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>@dataclass(init=False)\nclass FileScanTask(ScanTask):\n    \"\"\"Task representing a data file and its corresponding delete files.\"\"\"\n\n    file: DataFile\n    delete_files: set[DataFile]\n    residual: BooleanExpression\n\n    def __init__(\n        self,\n        data_file: DataFile,\n        delete_files: set[DataFile] | None = None,\n        residual: BooleanExpression = ALWAYS_TRUE,\n    ) -&gt; None:\n        self.file = data_file\n        self.delete_files = delete_files or set()\n        self.residual = residual\n\n    @staticmethod\n    def from_rest_response(\n        rest_task: RESTFileScanTask,\n        delete_files: list[RESTDeleteFile],\n    ) -&gt; FileScanTask:\n        \"\"\"Convert a RESTFileScanTask to a FileScanTask.\n\n        Args:\n            rest_task: The REST file scan task.\n            delete_files: The list of delete files from the ScanTasks response.\n\n        Returns:\n            A FileScanTask with the converted data and delete files.\n\n        Raises:\n            NotImplementedError: If equality delete files are encountered.\n        \"\"\"\n        from pyiceberg.catalog.rest.scan_planning import RESTEqualityDeleteFile\n\n        data_file = _rest_file_to_data_file(rest_task.data_file)\n\n        resolved_deletes: set[DataFile] = set()\n        if rest_task.delete_file_references:\n            for idx in rest_task.delete_file_references:\n                delete_file = delete_files[idx]\n                if isinstance(delete_file, RESTEqualityDeleteFile):\n                    raise NotImplementedError(f\"PyIceberg does not yet support equality deletes: {delete_file.file_path}\")\n                resolved_deletes.add(_rest_file_to_data_file(delete_file))\n\n        return FileScanTask(\n            data_file=data_file,\n            delete_files=resolved_deletes,\n            residual=rest_task.residual_filter if rest_task.residual_filter else ALWAYS_TRUE,\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.FileScanTask.from_rest_response","title":"<code>from_rest_response(rest_task, delete_files)</code>  <code>staticmethod</code>","text":"<p>Convert a RESTFileScanTask to a FileScanTask.</p> <p>Parameters:</p> Name Type Description Default <code>rest_task</code> <code>RESTFileScanTask</code> <p>The REST file scan task.</p> required <code>delete_files</code> <code>list[RESTDeleteFile]</code> <p>The list of delete files from the ScanTasks response.</p> required <p>Returns:</p> Type Description <code>FileScanTask</code> <p>A FileScanTask with the converted data and delete files.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If equality delete files are encountered.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>@staticmethod\ndef from_rest_response(\n    rest_task: RESTFileScanTask,\n    delete_files: list[RESTDeleteFile],\n) -&gt; FileScanTask:\n    \"\"\"Convert a RESTFileScanTask to a FileScanTask.\n\n    Args:\n        rest_task: The REST file scan task.\n        delete_files: The list of delete files from the ScanTasks response.\n\n    Returns:\n        A FileScanTask with the converted data and delete files.\n\n    Raises:\n        NotImplementedError: If equality delete files are encountered.\n    \"\"\"\n    from pyiceberg.catalog.rest.scan_planning import RESTEqualityDeleteFile\n\n    data_file = _rest_file_to_data_file(rest_task.data_file)\n\n    resolved_deletes: set[DataFile] = set()\n    if rest_task.delete_file_references:\n        for idx in rest_task.delete_file_references:\n            delete_file = delete_files[idx]\n            if isinstance(delete_file, RESTEqualityDeleteFile):\n                raise NotImplementedError(f\"PyIceberg does not yet support equality deletes: {delete_file.file_path}\")\n            resolved_deletes.add(_rest_file_to_data_file(delete_file))\n\n    return FileScanTask(\n        data_file=data_file,\n        delete_files=resolved_deletes,\n        residual=rest_task.residual_filter if rest_task.residual_filter else ALWAYS_TRUE,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Namespace","title":"<code>Namespace</code>","text":"<p>               Bases: <code>IcebergRootModel[list[str]]</code></p> <p>Reference to one or more levels of a namespace.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class Namespace(IcebergRootModel[list[str]]):\n    \"\"\"Reference to one or more levels of a namespace.\"\"\"\n\n    root: list[str] = Field(\n        ...,\n        description=\"Reference to one or more levels of a namespace\",\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.StaticTable","title":"<code>StaticTable</code>","text":"<p>               Bases: <code>Table</code></p> <p>Load a table directly from a metadata file (i.e., without using a catalog).</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class StaticTable(Table):\n    \"\"\"Load a table directly from a metadata file (i.e., without using a catalog).\"\"\"\n\n    def refresh(self) -&gt; Table:\n        \"\"\"Refresh the current table metadata.\"\"\"\n        raise NotImplementedError(\"To be implemented\")\n\n    @classmethod\n    def _metadata_location_from_version_hint(cls, metadata_location: str, properties: Properties = EMPTY_DICT) -&gt; str:\n        version_hint_location = os.path.join(metadata_location, \"metadata\", \"version-hint.text\")\n        io = load_file_io(properties=properties, location=version_hint_location)\n        file = io.new_input(version_hint_location)\n\n        with file.open() as stream:\n            content = stream.read().decode(\"utf-8\")\n\n        if content.endswith(\".metadata.json\"):\n            return os.path.join(metadata_location, \"metadata\", content)\n        elif content.isnumeric():\n            return os.path.join(metadata_location, \"metadata\", f\"v{content}.metadata.json\")\n        else:\n            return os.path.join(metadata_location, \"metadata\", f\"{content}.metadata.json\")\n\n    @classmethod\n    def from_metadata(cls, metadata_location: str, properties: Properties = EMPTY_DICT) -&gt; StaticTable:\n        if not metadata_location.endswith(\".metadata.json\"):\n            metadata_location = StaticTable._metadata_location_from_version_hint(metadata_location, properties)\n\n        io = load_file_io(properties=properties, location=metadata_location)\n        file = io.new_input(metadata_location)\n\n        from pyiceberg.serializers import FromInputFile\n\n        metadata = FromInputFile.table_metadata(file)\n\n        from pyiceberg.catalog.noop import NoopCatalog\n\n        return cls(\n            identifier=(\"static-table\", metadata_location),\n            metadata_location=metadata_location,\n            metadata=metadata,\n            io=load_file_io({**properties, **metadata.properties}),\n            catalog=NoopCatalog(\"static-table\"),\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.StaticTable.refresh","title":"<code>refresh()</code>","text":"<p>Refresh the current table metadata.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def refresh(self) -&gt; Table:\n    \"\"\"Refresh the current table metadata.\"\"\"\n    raise NotImplementedError(\"To be implemented\")\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table","title":"<code>Table</code>","text":"<p>An Iceberg table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class Table:\n    \"\"\"An Iceberg table.\"\"\"\n\n    _identifier: Identifier = Field()\n    metadata: TableMetadata\n    metadata_location: str = Field()\n    io: FileIO\n    catalog: Catalog\n    config: dict[str, str]\n\n    def __init__(\n        self,\n        identifier: Identifier,\n        metadata: TableMetadata,\n        metadata_location: str,\n        io: FileIO,\n        catalog: Catalog,\n        config: dict[str, str] = EMPTY_DICT,\n    ) -&gt; None:\n        self._identifier = identifier\n        self.metadata = metadata\n        self.metadata_location = metadata_location\n        self.io = io\n        self.catalog = catalog\n        self.config = config\n\n    def transaction(self) -&gt; Transaction:\n        \"\"\"Create a new transaction object to first stage the changes, and then commit them to the catalog.\n\n        Returns:\n            The transaction object\n        \"\"\"\n        return Transaction(self)\n\n    @property\n    def inspect(self) -&gt; InspectTable:\n        \"\"\"Return the InspectTable object to browse the table metadata.\n\n        Returns:\n            InspectTable object based on this Table.\n        \"\"\"\n        return InspectTable(self)\n\n    @property\n    def maintenance(self) -&gt; MaintenanceTable:\n        \"\"\"Return the MaintenanceTable object for maintenance.\n\n        Returns:\n            MaintenanceTable object based on this Table.\n        \"\"\"\n        return MaintenanceTable(self)\n\n    def refresh(self) -&gt; Table:\n        \"\"\"Refresh the current table metadata.\n\n        Returns:\n            An updated instance of the same Iceberg table\n        \"\"\"\n        fresh = self.catalog.load_table(self._identifier)\n        self._check_uuid(self.metadata, fresh.metadata)\n        self.metadata = fresh.metadata\n        self.io = fresh.io\n        self.metadata_location = fresh.metadata_location\n        return self\n\n    def name(self) -&gt; Identifier:\n        \"\"\"Return the identifier of this table.\n\n        Returns:\n            An Identifier tuple of the table name\n        \"\"\"\n        return self._identifier\n\n    def scan(\n        self,\n        row_filter: str | BooleanExpression = ALWAYS_TRUE,\n        selected_fields: tuple[str, ...] = (\"*\",),\n        case_sensitive: bool = True,\n        snapshot_id: int | None = None,\n        options: Properties = EMPTY_DICT,\n        limit: int | None = None,\n    ) -&gt; DataScan:\n        \"\"\"Fetch a DataScan based on the table's current metadata.\n\n            The data scan can be used to project the table's data\n            that matches the provided row_filter onto the table's\n            current schema.\n\n        Args:\n            row_filter:\n                A string or BooleanExpression that describes the\n                desired rows\n            selected_fields:\n                A tuple of strings representing the column names\n                to return in the output dataframe.\n            case_sensitive:\n                If True column matching is case sensitive\n            snapshot_id:\n                Optional Snapshot ID to time travel to. If None,\n                scans the table as of the current snapshot ID.\n            options:\n                Additional Table properties as a dictionary of\n                string key value pairs to use for this scan.\n            limit:\n                An integer representing the number of rows to\n                return in the scan result. If None, fetches all\n                matching rows.\n\n        Returns:\n            A DataScan based on the table's current metadata.\n        \"\"\"\n        return DataScan(\n            table_metadata=self.metadata,\n            io=self.io,\n            row_filter=row_filter,\n            selected_fields=selected_fields,\n            case_sensitive=case_sensitive,\n            snapshot_id=snapshot_id,\n            options=options,\n            limit=limit,\n            catalog=self.catalog,\n            table_identifier=self._identifier,\n        )\n\n    @property\n    def format_version(self) -&gt; TableVersion:\n        return self.metadata.format_version\n\n    def schema(self) -&gt; Schema:\n        \"\"\"Return the schema for this table.\"\"\"\n        return next(schema for schema in self.metadata.schemas if schema.schema_id == self.metadata.current_schema_id)\n\n    def schemas(self) -&gt; dict[int, Schema]:\n        \"\"\"Return a dict of the schema of this table.\"\"\"\n        return {schema.schema_id: schema for schema in self.metadata.schemas}\n\n    def spec(self) -&gt; PartitionSpec:\n        \"\"\"Return the partition spec of this table.\"\"\"\n        return next(spec for spec in self.metadata.partition_specs if spec.spec_id == self.metadata.default_spec_id)\n\n    def specs(self) -&gt; dict[int, PartitionSpec]:\n        \"\"\"Return a dict the partition specs this table.\"\"\"\n        return {spec.spec_id: spec for spec in self.metadata.partition_specs}\n\n    def sort_order(self) -&gt; SortOrder:\n        \"\"\"Return the sort order of this table.\"\"\"\n        return next(\n            sort_order for sort_order in self.metadata.sort_orders if sort_order.order_id == self.metadata.default_sort_order_id\n        )\n\n    def sort_orders(self) -&gt; dict[int, SortOrder]:\n        \"\"\"Return a dict of the sort orders of this table.\"\"\"\n        return {sort_order.order_id: sort_order for sort_order in self.metadata.sort_orders}\n\n    def last_partition_id(self) -&gt; int:\n        \"\"\"Return the highest assigned partition field ID across all specs or 999 if only the unpartitioned spec exists.\"\"\"\n        if self.metadata.last_partition_id:\n            return self.metadata.last_partition_id\n        return PARTITION_FIELD_ID_START - 1\n\n    @property\n    def properties(self) -&gt; dict[str, str]:\n        \"\"\"Properties of the table.\"\"\"\n        return self.metadata.properties\n\n    def location(self) -&gt; str:\n        \"\"\"Return the table's base location.\"\"\"\n        return self.metadata.location\n\n    def location_provider(self) -&gt; LocationProvider:\n        \"\"\"Return the table's location provider.\"\"\"\n        return load_location_provider(table_location=self.metadata.location, table_properties=self.metadata.properties)\n\n    @property\n    def last_sequence_number(self) -&gt; int:\n        return self.metadata.last_sequence_number\n\n    def current_snapshot(self) -&gt; Snapshot | None:\n        \"\"\"Get the current snapshot for this table, or None if there is no current snapshot.\"\"\"\n        if self.metadata.current_snapshot_id is not None:\n            return self.snapshot_by_id(self.metadata.current_snapshot_id)\n        return None\n\n    def snapshots(self) -&gt; list[Snapshot]:\n        return self.metadata.snapshots\n\n    def snapshot_by_id(self, snapshot_id: int) -&gt; Snapshot | None:\n        \"\"\"Get the snapshot of this table with the given id, or None if there is no matching snapshot.\"\"\"\n        return self.metadata.snapshot_by_id(snapshot_id)\n\n    def snapshot_by_name(self, name: str) -&gt; Snapshot | None:\n        \"\"\"Return the snapshot referenced by the given name or null if no such reference exists.\"\"\"\n        if ref := self.metadata.refs.get(name):\n            return self.snapshot_by_id(ref.snapshot_id)\n        return None\n\n    def snapshot_as_of_timestamp(self, timestamp_ms: int, inclusive: bool = True) -&gt; Snapshot | None:\n        \"\"\"Get the snapshot that was current as of or right before the given timestamp, or None if there is no matching snapshot.\n\n        Args:\n            timestamp_ms: Find snapshot that was current at/before this timestamp\n            inclusive: Includes timestamp_ms in search when True. Excludes timestamp_ms when False\n        \"\"\"\n        for log_entry in reversed(self.history()):\n            if (inclusive and log_entry.timestamp_ms &lt;= timestamp_ms) or log_entry.timestamp_ms &lt; timestamp_ms:\n                return self.snapshot_by_id(log_entry.snapshot_id)\n        return None\n\n    def history(self) -&gt; list[SnapshotLogEntry]:\n        \"\"\"Get the snapshot history of this table.\"\"\"\n        return self.metadata.snapshot_log\n\n    def manage_snapshots(self) -&gt; ManageSnapshots:\n        \"\"\"\n        Shorthand to run snapshot management operations like create branch, create tag, etc.\n\n        Use table.manage_snapshots().&lt;operation&gt;().commit() to run a specific operation.\n        Use table.manage_snapshots().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n        Pending changes are applied on commit.\n\n        We can also use context managers to make more changes. For example,\n\n        with table.manage_snapshots() as ms:\n           ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")\n        \"\"\"\n        return ManageSnapshots(transaction=Transaction(self, autocommit=True))\n\n    def update_statistics(self) -&gt; UpdateStatistics:\n        \"\"\"\n        Shorthand to run statistics management operations like add statistics and remove statistics.\n\n        Use table.update_statistics().&lt;operation&gt;().commit() to run a specific operation.\n        Use table.update_statistics().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n\n        Pending changes are applied on commit.\n\n        We can also use context managers to make more changes. For example:\n\n        with table.update_statistics() as update:\n            update.set_statistics(statistics_file=statistics_file)\n            update.remove_statistics(snapshot_id=2)\n        \"\"\"\n        return UpdateStatistics(transaction=Transaction(self, autocommit=True))\n\n    def update_schema(self, allow_incompatible_changes: bool = False, case_sensitive: bool = True) -&gt; UpdateSchema:\n        \"\"\"Create a new UpdateSchema to alter the columns of this table.\n\n        Args:\n            allow_incompatible_changes: If changes are allowed that might break downstream consumers.\n            case_sensitive: If field names are case-sensitive.\n\n        Returns:\n            A new UpdateSchema.\n        \"\"\"\n        return UpdateSchema(\n            transaction=Transaction(self, autocommit=True),\n            allow_incompatible_changes=allow_incompatible_changes,\n            case_sensitive=case_sensitive,\n            name_mapping=self.name_mapping(),\n        )\n\n    def update_sort_order(self, case_sensitive: bool = True) -&gt; UpdateSortOrder:\n        \"\"\"Create a new UpdateSortOrder to update the sort order of this table.\n\n        Returns:\n            A new UpdateSortOrder.\n        \"\"\"\n        return UpdateSortOrder(transaction=Transaction(self, autocommit=True), case_sensitive=case_sensitive)\n\n    def name_mapping(self) -&gt; NameMapping | None:\n        \"\"\"Return the table's field-id NameMapping.\"\"\"\n        return self.metadata.name_mapping()\n\n    def upsert(\n        self,\n        df: pa.Table,\n        join_cols: list[str] | None = None,\n        when_matched_update_all: bool = True,\n        when_not_matched_insert_all: bool = True,\n        case_sensitive: bool = True,\n        branch: str | None = MAIN_BRANCH,\n        snapshot_properties: dict[str, str] = EMPTY_DICT,\n    ) -&gt; UpsertResult:\n        \"\"\"Shorthand API for performing an upsert to an iceberg table.\n\n        Args:\n\n            df: The input dataframe to upsert with the table's data.\n            join_cols: Columns to join on, if not provided, it will use the identifier-field-ids.\n            when_matched_update_all: Bool indicating to update rows that are matched but require an update\n                due to a value in a non-key column changing\n            when_not_matched_insert_all: Bool indicating new rows to be inserted that do not match any\n                existing rows in the table\n            case_sensitive: Bool indicating if the match should be case-sensitive\n            branch: Branch Reference to run the upsert operation\n            snapshot_properties: Custom properties to be added to the snapshot summary\n\n            To learn more about the identifier-field-ids: https://iceberg.apache.org/spec/#identifier-field-ids\n\n                Example Use Cases:\n                    Case 1: Both Parameters = True (Full Upsert)\n                    Existing row found \u2192 Update it\n                    New row found \u2192 Insert it\n\n                    Case 2: when_matched_update_all = False, when_not_matched_insert_all = True\n                    Existing row found \u2192 Do nothing (no updates)\n                    New row found \u2192 Insert it\n\n                    Case 3: when_matched_update_all = True, when_not_matched_insert_all = False\n                    Existing row found \u2192 Update it\n                    New row found \u2192 Do nothing (no inserts)\n\n                    Case 4: Both Parameters = False (No Merge Effect)\n                    Existing row found \u2192 Do nothing\n                    New row found \u2192 Do nothing\n                    (Function effectively does nothing)\n\n\n        Returns:\n            An UpsertResult class (contains details of rows updated and inserted)\n        \"\"\"\n        with self.transaction() as tx:\n            return tx.upsert(\n                df=df,\n                join_cols=join_cols,\n                when_matched_update_all=when_matched_update_all,\n                when_not_matched_insert_all=when_not_matched_insert_all,\n                case_sensitive=case_sensitive,\n                branch=branch,\n                snapshot_properties=snapshot_properties,\n            )\n\n    def append(self, df: pa.Table, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH) -&gt; None:\n        \"\"\"\n        Shorthand API for appending a PyArrow table to the table.\n\n        Args:\n            df: The Arrow dataframe that will be appended to overwrite the table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            branch: Branch Reference to run the append operation\n        \"\"\"\n        with self.transaction() as tx:\n            tx.append(df=df, snapshot_properties=snapshot_properties, branch=branch)\n\n    def dynamic_partition_overwrite(\n        self, df: pa.Table, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH\n    ) -&gt; None:\n        \"\"\"Shorthand for dynamic overwriting the table with a PyArrow table.\n\n        Old partitions are auto detected and replaced with data files created for input arrow table.\n        Args:\n            df: The Arrow dataframe that will be used to overwrite the table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            branch: Branch Reference to run the dynamic partition overwrite operation\n        \"\"\"\n        with self.transaction() as tx:\n            tx.dynamic_partition_overwrite(df=df, snapshot_properties=snapshot_properties, branch=branch)\n\n    def overwrite(\n        self,\n        df: pa.Table,\n        overwrite_filter: BooleanExpression | str = ALWAYS_TRUE,\n        snapshot_properties: dict[str, str] = EMPTY_DICT,\n        case_sensitive: bool = True,\n        branch: str | None = MAIN_BRANCH,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand for overwriting the table with a PyArrow table.\n\n        An overwrite may produce zero or more snapshots based on the operation:\n\n            - DELETE: In case existing Parquet files can be dropped completely.\n            - OVERWRITE: In case existing Parquet files need to be rewritten to drop rows that match the overwrite filter..\n            - APPEND: In case new data is being inserted into the table.\n\n        Args:\n            df: The Arrow dataframe that will be used to overwrite the table\n            overwrite_filter: ALWAYS_TRUE when you overwrite all the data,\n                              or a boolean expression in case of a partial overwrite\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            case_sensitive: A bool determine if the provided `overwrite_filter` is case-sensitive\n            branch: Branch Reference to run the overwrite operation\n        \"\"\"\n        with self.transaction() as tx:\n            tx.overwrite(\n                df=df,\n                overwrite_filter=overwrite_filter,\n                case_sensitive=case_sensitive,\n                snapshot_properties=snapshot_properties,\n                branch=branch,\n            )\n\n    def delete(\n        self,\n        delete_filter: BooleanExpression | str = ALWAYS_TRUE,\n        snapshot_properties: dict[str, str] = EMPTY_DICT,\n        case_sensitive: bool = True,\n        branch: str | None = MAIN_BRANCH,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand for deleting rows from the table.\n\n        Args:\n            delete_filter: The predicate that used to remove rows\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            case_sensitive: A bool determine if the provided `delete_filter` is case-sensitive\n            branch: Branch Reference to run the delete operation\n        \"\"\"\n        with self.transaction() as tx:\n            tx.delete(\n                delete_filter=delete_filter, case_sensitive=case_sensitive, snapshot_properties=snapshot_properties, branch=branch\n            )\n\n    def add_files(\n        self,\n        file_paths: list[str],\n        snapshot_properties: dict[str, str] = EMPTY_DICT,\n        check_duplicate_files: bool = True,\n        branch: str | None = MAIN_BRANCH,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand API for adding files as data files to the table.\n\n        Args:\n            file_paths: The list of full file paths to be added as data files to the table\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n        \"\"\"\n        with self.transaction() as tx:\n            tx.add_files(\n                file_paths=file_paths,\n                snapshot_properties=snapshot_properties,\n                check_duplicate_files=check_duplicate_files,\n                branch=branch,\n            )\n\n    def update_spec(self, case_sensitive: bool = True) -&gt; UpdateSpec:\n        return UpdateSpec(Transaction(self, autocommit=True), case_sensitive=case_sensitive)\n\n    def refs(self) -&gt; dict[str, SnapshotRef]:\n        \"\"\"Return the snapshot references in the table.\"\"\"\n        return self.metadata.refs\n\n    @staticmethod\n    def _check_uuid(current_metadata: TableMetadata, new_metadata: TableMetadata) -&gt; None:\n        \"\"\"Validate that the table UUID matches after refresh.\"\"\"\n        current = current_metadata.table_uuid\n        refreshed = new_metadata.table_uuid\n\n        if current != refreshed:\n            raise ValueError(f\"Table UUID does not match: current={current} != refreshed={refreshed}\")\n\n    def _do_commit(self, updates: tuple[TableUpdate, ...], requirements: tuple[TableRequirement, ...]) -&gt; None:\n        response = self.catalog.commit_table(self, requirements, updates)\n\n        # Ensure table uuid has not changed\n        self._check_uuid(self.metadata, response.metadata)\n\n        # https://github.com/apache/iceberg/blob/f6faa58/core/src/main/java/org/apache/iceberg/CatalogUtil.java#L527\n        # delete old metadata if METADATA_DELETE_AFTER_COMMIT_ENABLED is set to true and uses\n        # TableProperties.METADATA_PREVIOUS_VERSIONS_MAX to determine how many previous versions to keep -\n        # everything else will be removed.\n        try:\n            self.catalog._delete_old_metadata(self.io, self.metadata, response.metadata)\n        except Exception as e:\n            warnings.warn(f\"Failed to delete old metadata after commit: {e}\", stacklevel=2)\n\n        self.metadata = response.metadata\n        self.metadata_location = response.metadata_location\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Return the equality of two instances of the Table class.\"\"\"\n        return (\n            self.name() == other.name() and self.metadata == other.metadata and self.metadata_location == other.metadata_location\n            if isinstance(other, Table)\n            else False\n        )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Table class.\"\"\"\n        table_name = self.catalog.table_name_from(self._identifier)\n        schema_str = \",\\n  \".join(str(column) for column in self.schema().columns if self.schema())\n        partition_str = f\"partition by: [{', '.join(field.name for field in self.spec().fields if self.spec())}]\"\n        sort_order_str = f\"sort order: [{', '.join(str(field) for field in self.sort_order().fields if self.sort_order())}]\"\n        snapshot_str = f\"snapshot: {str(self.current_snapshot()) if self.current_snapshot() else 'null'}\"\n        result_str = f\"{table_name}(\\n  {schema_str}\\n),\\n{partition_str},\\n{sort_order_str},\\n{snapshot_str}\"\n        return result_str\n\n    def to_daft(self) -&gt; daft.DataFrame:\n        \"\"\"Read a Daft DataFrame lazily from this Iceberg table.\n\n        Returns:\n            daft.DataFrame: Unmaterialized Daft Dataframe created from the Iceberg table\n        \"\"\"\n        import daft\n\n        return daft.read_iceberg(self)\n\n    def to_bodo(self) -&gt; bd.DataFrame:\n        \"\"\"Read a bodo DataFrame lazily from this Iceberg table.\n\n        Returns:\n            bd.DataFrame: Unmaterialized Bodo Dataframe created from the Iceberg table\n        \"\"\"\n        import bodo.pandas as bd\n\n        return bd.read_iceberg_table(self)\n\n    def to_polars(self) -&gt; pl.LazyFrame:\n        \"\"\"Lazily read from this Apache Iceberg table.\n\n        Returns:\n            pl.LazyFrame: Unmaterialized Polars LazyFrame created from the Iceberg table\n        \"\"\"\n        import polars as pl\n\n        return pl.scan_iceberg(self)\n\n    def __datafusion_table_provider__(self) -&gt; IcebergDataFusionTable:\n        \"\"\"Return the DataFusion table provider PyCapsule interface.\n\n        To support DataFusion features such as push down filtering, this function will return a PyCapsule\n        interface that conforms to the FFI Table Provider required by DataFusion. From an end user perspective\n        you should not need to call this function directly. Instead you can use ``register_table`` in\n        the DataFusion SessionContext.\n\n        Returns:\n            A PyCapsule DataFusion TableProvider interface.\n\n        Example:\n            ```python\n            from datafusion import SessionContext\n            from pyiceberg.catalog import load_catalog\n            import pyarrow as pa\n            catalog = load_catalog(\"catalog\", type=\"in-memory\")\n            catalog.create_namespace_if_not_exists(\"default\")\n            data = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n            iceberg_table = catalog.create_table(\"default.test\", schema=data.schema)\n            iceberg_table.append(data)\n            ctx = SessionContext()\n            ctx.register_table(\"test\", iceberg_table)\n            ctx.table(\"test\").show()\n            ```\n            Results in\n            ```\n            DataFrame()\n            +---+---+\n            | x | y |\n            +---+---+\n            | 1 | 4 |\n            | 2 | 5 |\n            | 3 | 6 |\n            +---+---+\n            ```\n        \"\"\"\n        from pyiceberg_core.datafusion import IcebergDataFusionTable\n\n        return IcebergDataFusionTable(\n            identifier=self.name(),\n            metadata_location=self.metadata_location,\n            file_io_properties=self.io.properties,\n        ).__datafusion_table_provider__()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.inspect","title":"<code>inspect</code>  <code>property</code>","text":"<p>Return the InspectTable object to browse the table metadata.</p> <p>Returns:</p> Type Description <code>InspectTable</code> <p>InspectTable object based on this Table.</p>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.maintenance","title":"<code>maintenance</code>  <code>property</code>","text":"<p>Return the MaintenanceTable object for maintenance.</p> <p>Returns:</p> Type Description <code>MaintenanceTable</code> <p>MaintenanceTable object based on this Table.</p>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.properties","title":"<code>properties</code>  <code>property</code>","text":"<p>Properties of the table.</p>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.__datafusion_table_provider__","title":"<code>__datafusion_table_provider__()</code>","text":"<p>Return the DataFusion table provider PyCapsule interface.</p> <p>To support DataFusion features such as push down filtering, this function will return a PyCapsule interface that conforms to the FFI Table Provider required by DataFusion. From an end user perspective you should not need to call this function directly. Instead you can use <code>register_table</code> in the DataFusion SessionContext.</p> <p>Returns:</p> Type Description <code>IcebergDataFusionTable</code> <p>A PyCapsule DataFusion TableProvider interface.</p> Example <p><pre><code>from datafusion import SessionContext\nfrom pyiceberg.catalog import load_catalog\nimport pyarrow as pa\ncatalog = load_catalog(\"catalog\", type=\"in-memory\")\ncatalog.create_namespace_if_not_exists(\"default\")\ndata = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\niceberg_table = catalog.create_table(\"default.test\", schema=data.schema)\niceberg_table.append(data)\nctx = SessionContext()\nctx.register_table(\"test\", iceberg_table)\nctx.table(\"test\").show()\n</code></pre> Results in <pre><code>DataFrame()\n+---+---+\n| x | y |\n+---+---+\n| 1 | 4 |\n| 2 | 5 |\n| 3 | 6 |\n+---+---+\n</code></pre></p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __datafusion_table_provider__(self) -&gt; IcebergDataFusionTable:\n    \"\"\"Return the DataFusion table provider PyCapsule interface.\n\n    To support DataFusion features such as push down filtering, this function will return a PyCapsule\n    interface that conforms to the FFI Table Provider required by DataFusion. From an end user perspective\n    you should not need to call this function directly. Instead you can use ``register_table`` in\n    the DataFusion SessionContext.\n\n    Returns:\n        A PyCapsule DataFusion TableProvider interface.\n\n    Example:\n        ```python\n        from datafusion import SessionContext\n        from pyiceberg.catalog import load_catalog\n        import pyarrow as pa\n        catalog = load_catalog(\"catalog\", type=\"in-memory\")\n        catalog.create_namespace_if_not_exists(\"default\")\n        data = pa.table({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n        iceberg_table = catalog.create_table(\"default.test\", schema=data.schema)\n        iceberg_table.append(data)\n        ctx = SessionContext()\n        ctx.register_table(\"test\", iceberg_table)\n        ctx.table(\"test\").show()\n        ```\n        Results in\n        ```\n        DataFrame()\n        +---+---+\n        | x | y |\n        +---+---+\n        | 1 | 4 |\n        | 2 | 5 |\n        | 3 | 6 |\n        +---+---+\n        ```\n    \"\"\"\n    from pyiceberg_core.datafusion import IcebergDataFusionTable\n\n    return IcebergDataFusionTable(\n        identifier=self.name(),\n        metadata_location=self.metadata_location,\n        file_io_properties=self.io.properties,\n    ).__datafusion_table_provider__()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Return the equality of two instances of the Table class.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Return the equality of two instances of the Table class.\"\"\"\n    return (\n        self.name() == other.name() and self.metadata == other.metadata and self.metadata_location == other.metadata_location\n        if isinstance(other, Table)\n        else False\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Table class.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Table class.\"\"\"\n    table_name = self.catalog.table_name_from(self._identifier)\n    schema_str = \",\\n  \".join(str(column) for column in self.schema().columns if self.schema())\n    partition_str = f\"partition by: [{', '.join(field.name for field in self.spec().fields if self.spec())}]\"\n    sort_order_str = f\"sort order: [{', '.join(str(field) for field in self.sort_order().fields if self.sort_order())}]\"\n    snapshot_str = f\"snapshot: {str(self.current_snapshot()) if self.current_snapshot() else 'null'}\"\n    result_str = f\"{table_name}(\\n  {schema_str}\\n),\\n{partition_str},\\n{sort_order_str},\\n{snapshot_str}\"\n    return result_str\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.add_files","title":"<code>add_files(file_paths, snapshot_properties=EMPTY_DICT, check_duplicate_files=True, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand API for adding files as data files to the table.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>The list of full file paths to be added as data files to the table</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def add_files(\n    self,\n    file_paths: list[str],\n    snapshot_properties: dict[str, str] = EMPTY_DICT,\n    check_duplicate_files: bool = True,\n    branch: str | None = MAIN_BRANCH,\n) -&gt; None:\n    \"\"\"\n    Shorthand API for adding files as data files to the table.\n\n    Args:\n        file_paths: The list of full file paths to be added as data files to the table\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n    \"\"\"\n    with self.transaction() as tx:\n        tx.add_files(\n            file_paths=file_paths,\n            snapshot_properties=snapshot_properties,\n            check_duplicate_files=check_duplicate_files,\n            branch=branch,\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.append","title":"<code>append(df, snapshot_properties=EMPTY_DICT, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand API for appending a PyArrow table to the table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be appended to overwrite the table</p> required <code>snapshot_properties</code> <code>dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>branch</code> <code>str | None</code> <p>Branch Reference to run the append operation</p> <code>MAIN_BRANCH</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def append(self, df: pa.Table, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH) -&gt; None:\n    \"\"\"\n    Shorthand API for appending a PyArrow table to the table.\n\n    Args:\n        df: The Arrow dataframe that will be appended to overwrite the table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        branch: Branch Reference to run the append operation\n    \"\"\"\n    with self.transaction() as tx:\n        tx.append(df=df, snapshot_properties=snapshot_properties, branch=branch)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.current_snapshot","title":"<code>current_snapshot()</code>","text":"<p>Get the current snapshot for this table, or None if there is no current snapshot.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def current_snapshot(self) -&gt; Snapshot | None:\n    \"\"\"Get the current snapshot for this table, or None if there is no current snapshot.\"\"\"\n    if self.metadata.current_snapshot_id is not None:\n        return self.snapshot_by_id(self.metadata.current_snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.delete","title":"<code>delete(delete_filter=ALWAYS_TRUE, snapshot_properties=EMPTY_DICT, case_sensitive=True, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand for deleting rows from the table.</p> <p>Parameters:</p> Name Type Description Default <code>delete_filter</code> <code>BooleanExpression | str</code> <p>The predicate that used to remove rows</p> <code>ALWAYS_TRUE</code> <code>snapshot_properties</code> <code>dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>case_sensitive</code> <code>bool</code> <p>A bool determine if the provided <code>delete_filter</code> is case-sensitive</p> <code>True</code> <code>branch</code> <code>str | None</code> <p>Branch Reference to run the delete operation</p> <code>MAIN_BRANCH</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def delete(\n    self,\n    delete_filter: BooleanExpression | str = ALWAYS_TRUE,\n    snapshot_properties: dict[str, str] = EMPTY_DICT,\n    case_sensitive: bool = True,\n    branch: str | None = MAIN_BRANCH,\n) -&gt; None:\n    \"\"\"\n    Shorthand for deleting rows from the table.\n\n    Args:\n        delete_filter: The predicate that used to remove rows\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        case_sensitive: A bool determine if the provided `delete_filter` is case-sensitive\n        branch: Branch Reference to run the delete operation\n    \"\"\"\n    with self.transaction() as tx:\n        tx.delete(\n            delete_filter=delete_filter, case_sensitive=case_sensitive, snapshot_properties=snapshot_properties, branch=branch\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.dynamic_partition_overwrite","title":"<code>dynamic_partition_overwrite(df, snapshot_properties=EMPTY_DICT, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand for dynamic overwriting the table with a PyArrow table.</p> <p>Old partitions are auto detected and replaced with data files created for input arrow table. Args:     df: The Arrow dataframe that will be used to overwrite the table     snapshot_properties: Custom properties to be added to the snapshot summary     branch: Branch Reference to run the dynamic partition overwrite operation</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def dynamic_partition_overwrite(\n    self, df: pa.Table, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH\n) -&gt; None:\n    \"\"\"Shorthand for dynamic overwriting the table with a PyArrow table.\n\n    Old partitions are auto detected and replaced with data files created for input arrow table.\n    Args:\n        df: The Arrow dataframe that will be used to overwrite the table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        branch: Branch Reference to run the dynamic partition overwrite operation\n    \"\"\"\n    with self.transaction() as tx:\n        tx.dynamic_partition_overwrite(df=df, snapshot_properties=snapshot_properties, branch=branch)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.history","title":"<code>history()</code>","text":"<p>Get the snapshot history of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def history(self) -&gt; list[SnapshotLogEntry]:\n    \"\"\"Get the snapshot history of this table.\"\"\"\n    return self.metadata.snapshot_log\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.last_partition_id","title":"<code>last_partition_id()</code>","text":"<p>Return the highest assigned partition field ID across all specs or 999 if only the unpartitioned spec exists.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def last_partition_id(self) -&gt; int:\n    \"\"\"Return the highest assigned partition field ID across all specs or 999 if only the unpartitioned spec exists.\"\"\"\n    if self.metadata.last_partition_id:\n        return self.metadata.last_partition_id\n    return PARTITION_FIELD_ID_START - 1\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.location","title":"<code>location()</code>","text":"<p>Return the table's base location.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def location(self) -&gt; str:\n    \"\"\"Return the table's base location.\"\"\"\n    return self.metadata.location\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.location_provider","title":"<code>location_provider()</code>","text":"<p>Return the table's location provider.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def location_provider(self) -&gt; LocationProvider:\n    \"\"\"Return the table's location provider.\"\"\"\n    return load_location_provider(table_location=self.metadata.location, table_properties=self.metadata.properties)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.manage_snapshots","title":"<code>manage_snapshots()</code>","text":"<p>Shorthand to run snapshot management operations like create branch, create tag, etc.</p> <p>Use table.manage_snapshots().().commit() to run a specific operation. Use table.manage_snapshots().().().commit() to run multiple operations. Pending changes are applied on commit. <p>We can also use context managers to make more changes. For example,</p> <p>with table.manage_snapshots() as ms:    ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def manage_snapshots(self) -&gt; ManageSnapshots:\n    \"\"\"\n    Shorthand to run snapshot management operations like create branch, create tag, etc.\n\n    Use table.manage_snapshots().&lt;operation&gt;().commit() to run a specific operation.\n    Use table.manage_snapshots().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n    Pending changes are applied on commit.\n\n    We can also use context managers to make more changes. For example,\n\n    with table.manage_snapshots() as ms:\n       ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")\n    \"\"\"\n    return ManageSnapshots(transaction=Transaction(self, autocommit=True))\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.name","title":"<code>name()</code>","text":"<p>Return the identifier of this table.</p> <p>Returns:</p> Type Description <code>Identifier</code> <p>An Identifier tuple of the table name</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def name(self) -&gt; Identifier:\n    \"\"\"Return the identifier of this table.\n\n    Returns:\n        An Identifier tuple of the table name\n    \"\"\"\n    return self._identifier\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.name_mapping","title":"<code>name_mapping()</code>","text":"<p>Return the table's field-id NameMapping.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def name_mapping(self) -&gt; NameMapping | None:\n    \"\"\"Return the table's field-id NameMapping.\"\"\"\n    return self.metadata.name_mapping()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.overwrite","title":"<code>overwrite(df, overwrite_filter=ALWAYS_TRUE, snapshot_properties=EMPTY_DICT, case_sensitive=True, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand for overwriting the table with a PyArrow table.</p> <p>An overwrite may produce zero or more snapshots based on the operation:</p> <pre><code>- DELETE: In case existing Parquet files can be dropped completely.\n- OVERWRITE: In case existing Parquet files need to be rewritten to drop rows that match the overwrite filter..\n- APPEND: In case new data is being inserted into the table.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be used to overwrite the table</p> required <code>overwrite_filter</code> <code>BooleanExpression | str</code> <p>ALWAYS_TRUE when you overwrite all the data,               or a boolean expression in case of a partial overwrite</p> <code>ALWAYS_TRUE</code> <code>snapshot_properties</code> <code>dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>case_sensitive</code> <code>bool</code> <p>A bool determine if the provided <code>overwrite_filter</code> is case-sensitive</p> <code>True</code> <code>branch</code> <code>str | None</code> <p>Branch Reference to run the overwrite operation</p> <code>MAIN_BRANCH</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def overwrite(\n    self,\n    df: pa.Table,\n    overwrite_filter: BooleanExpression | str = ALWAYS_TRUE,\n    snapshot_properties: dict[str, str] = EMPTY_DICT,\n    case_sensitive: bool = True,\n    branch: str | None = MAIN_BRANCH,\n) -&gt; None:\n    \"\"\"\n    Shorthand for overwriting the table with a PyArrow table.\n\n    An overwrite may produce zero or more snapshots based on the operation:\n\n        - DELETE: In case existing Parquet files can be dropped completely.\n        - OVERWRITE: In case existing Parquet files need to be rewritten to drop rows that match the overwrite filter..\n        - APPEND: In case new data is being inserted into the table.\n\n    Args:\n        df: The Arrow dataframe that will be used to overwrite the table\n        overwrite_filter: ALWAYS_TRUE when you overwrite all the data,\n                          or a boolean expression in case of a partial overwrite\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        case_sensitive: A bool determine if the provided `overwrite_filter` is case-sensitive\n        branch: Branch Reference to run the overwrite operation\n    \"\"\"\n    with self.transaction() as tx:\n        tx.overwrite(\n            df=df,\n            overwrite_filter=overwrite_filter,\n            case_sensitive=case_sensitive,\n            snapshot_properties=snapshot_properties,\n            branch=branch,\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.refresh","title":"<code>refresh()</code>","text":"<p>Refresh the current table metadata.</p> <p>Returns:</p> Type Description <code>Table</code> <p>An updated instance of the same Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def refresh(self) -&gt; Table:\n    \"\"\"Refresh the current table metadata.\n\n    Returns:\n        An updated instance of the same Iceberg table\n    \"\"\"\n    fresh = self.catalog.load_table(self._identifier)\n    self._check_uuid(self.metadata, fresh.metadata)\n    self.metadata = fresh.metadata\n    self.io = fresh.io\n    self.metadata_location = fresh.metadata_location\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.refs","title":"<code>refs()</code>","text":"<p>Return the snapshot references in the table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def refs(self) -&gt; dict[str, SnapshotRef]:\n    \"\"\"Return the snapshot references in the table.\"\"\"\n    return self.metadata.refs\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.scan","title":"<code>scan(row_filter=ALWAYS_TRUE, selected_fields=('*',), case_sensitive=True, snapshot_id=None, options=EMPTY_DICT, limit=None)</code>","text":"<p>Fetch a DataScan based on the table's current metadata.</p> <pre><code>The data scan can be used to project the table's data\nthat matches the provided row_filter onto the table's\ncurrent schema.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>row_filter</code> <code>str | BooleanExpression</code> <p>A string or BooleanExpression that describes the desired rows</p> <code>ALWAYS_TRUE</code> <code>selected_fields</code> <code>tuple[str, ...]</code> <p>A tuple of strings representing the column names to return in the output dataframe.</p> <code>('*',)</code> <code>case_sensitive</code> <code>bool</code> <p>If True column matching is case sensitive</p> <code>True</code> <code>snapshot_id</code> <code>int | None</code> <p>Optional Snapshot ID to time travel to. If None, scans the table as of the current snapshot ID.</p> <code>None</code> <code>options</code> <code>Properties</code> <p>Additional Table properties as a dictionary of string key value pairs to use for this scan.</p> <code>EMPTY_DICT</code> <code>limit</code> <code>int | None</code> <p>An integer representing the number of rows to return in the scan result. If None, fetches all matching rows.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataScan</code> <p>A DataScan based on the table's current metadata.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def scan(\n    self,\n    row_filter: str | BooleanExpression = ALWAYS_TRUE,\n    selected_fields: tuple[str, ...] = (\"*\",),\n    case_sensitive: bool = True,\n    snapshot_id: int | None = None,\n    options: Properties = EMPTY_DICT,\n    limit: int | None = None,\n) -&gt; DataScan:\n    \"\"\"Fetch a DataScan based on the table's current metadata.\n\n        The data scan can be used to project the table's data\n        that matches the provided row_filter onto the table's\n        current schema.\n\n    Args:\n        row_filter:\n            A string or BooleanExpression that describes the\n            desired rows\n        selected_fields:\n            A tuple of strings representing the column names\n            to return in the output dataframe.\n        case_sensitive:\n            If True column matching is case sensitive\n        snapshot_id:\n            Optional Snapshot ID to time travel to. If None,\n            scans the table as of the current snapshot ID.\n        options:\n            Additional Table properties as a dictionary of\n            string key value pairs to use for this scan.\n        limit:\n            An integer representing the number of rows to\n            return in the scan result. If None, fetches all\n            matching rows.\n\n    Returns:\n        A DataScan based on the table's current metadata.\n    \"\"\"\n    return DataScan(\n        table_metadata=self.metadata,\n        io=self.io,\n        row_filter=row_filter,\n        selected_fields=selected_fields,\n        case_sensitive=case_sensitive,\n        snapshot_id=snapshot_id,\n        options=options,\n        limit=limit,\n        catalog=self.catalog,\n        table_identifier=self._identifier,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.schema","title":"<code>schema()</code>","text":"<p>Return the schema for this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def schema(self) -&gt; Schema:\n    \"\"\"Return the schema for this table.\"\"\"\n    return next(schema for schema in self.metadata.schemas if schema.schema_id == self.metadata.current_schema_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.schemas","title":"<code>schemas()</code>","text":"<p>Return a dict of the schema of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def schemas(self) -&gt; dict[int, Schema]:\n    \"\"\"Return a dict of the schema of this table.\"\"\"\n    return {schema.schema_id: schema for schema in self.metadata.schemas}\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.snapshot_as_of_timestamp","title":"<code>snapshot_as_of_timestamp(timestamp_ms, inclusive=True)</code>","text":"<p>Get the snapshot that was current as of or right before the given timestamp, or None if there is no matching snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>timestamp_ms</code> <code>int</code> <p>Find snapshot that was current at/before this timestamp</p> required <code>inclusive</code> <code>bool</code> <p>Includes timestamp_ms in search when True. Excludes timestamp_ms when False</p> <code>True</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def snapshot_as_of_timestamp(self, timestamp_ms: int, inclusive: bool = True) -&gt; Snapshot | None:\n    \"\"\"Get the snapshot that was current as of or right before the given timestamp, or None if there is no matching snapshot.\n\n    Args:\n        timestamp_ms: Find snapshot that was current at/before this timestamp\n        inclusive: Includes timestamp_ms in search when True. Excludes timestamp_ms when False\n    \"\"\"\n    for log_entry in reversed(self.history()):\n        if (inclusive and log_entry.timestamp_ms &lt;= timestamp_ms) or log_entry.timestamp_ms &lt; timestamp_ms:\n            return self.snapshot_by_id(log_entry.snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.snapshot_by_id","title":"<code>snapshot_by_id(snapshot_id)</code>","text":"<p>Get the snapshot of this table with the given id, or None if there is no matching snapshot.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def snapshot_by_id(self, snapshot_id: int) -&gt; Snapshot | None:\n    \"\"\"Get the snapshot of this table with the given id, or None if there is no matching snapshot.\"\"\"\n    return self.metadata.snapshot_by_id(snapshot_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.snapshot_by_name","title":"<code>snapshot_by_name(name)</code>","text":"<p>Return the snapshot referenced by the given name or null if no such reference exists.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def snapshot_by_name(self, name: str) -&gt; Snapshot | None:\n    \"\"\"Return the snapshot referenced by the given name or null if no such reference exists.\"\"\"\n    if ref := self.metadata.refs.get(name):\n        return self.snapshot_by_id(ref.snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.sort_order","title":"<code>sort_order()</code>","text":"<p>Return the sort order of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def sort_order(self) -&gt; SortOrder:\n    \"\"\"Return the sort order of this table.\"\"\"\n    return next(\n        sort_order for sort_order in self.metadata.sort_orders if sort_order.order_id == self.metadata.default_sort_order_id\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.sort_orders","title":"<code>sort_orders()</code>","text":"<p>Return a dict of the sort orders of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def sort_orders(self) -&gt; dict[int, SortOrder]:\n    \"\"\"Return a dict of the sort orders of this table.\"\"\"\n    return {sort_order.order_id: sort_order for sort_order in self.metadata.sort_orders}\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.spec","title":"<code>spec()</code>","text":"<p>Return the partition spec of this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def spec(self) -&gt; PartitionSpec:\n    \"\"\"Return the partition spec of this table.\"\"\"\n    return next(spec for spec in self.metadata.partition_specs if spec.spec_id == self.metadata.default_spec_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.specs","title":"<code>specs()</code>","text":"<p>Return a dict the partition specs this table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def specs(self) -&gt; dict[int, PartitionSpec]:\n    \"\"\"Return a dict the partition specs this table.\"\"\"\n    return {spec.spec_id: spec for spec in self.metadata.partition_specs}\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.to_bodo","title":"<code>to_bodo()</code>","text":"<p>Read a bodo DataFrame lazily from this Iceberg table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>bd.DataFrame: Unmaterialized Bodo Dataframe created from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_bodo(self) -&gt; bd.DataFrame:\n    \"\"\"Read a bodo DataFrame lazily from this Iceberg table.\n\n    Returns:\n        bd.DataFrame: Unmaterialized Bodo Dataframe created from the Iceberg table\n    \"\"\"\n    import bodo.pandas as bd\n\n    return bd.read_iceberg_table(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.to_daft","title":"<code>to_daft()</code>","text":"<p>Read a Daft DataFrame lazily from this Iceberg table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>daft.DataFrame: Unmaterialized Daft Dataframe created from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_daft(self) -&gt; daft.DataFrame:\n    \"\"\"Read a Daft DataFrame lazily from this Iceberg table.\n\n    Returns:\n        daft.DataFrame: Unmaterialized Daft Dataframe created from the Iceberg table\n    \"\"\"\n    import daft\n\n    return daft.read_iceberg(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.to_polars","title":"<code>to_polars()</code>","text":"<p>Lazily read from this Apache Iceberg table.</p> <p>Returns:</p> Type Description <code>LazyFrame</code> <p>pl.LazyFrame: Unmaterialized Polars LazyFrame created from the Iceberg table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def to_polars(self) -&gt; pl.LazyFrame:\n    \"\"\"Lazily read from this Apache Iceberg table.\n\n    Returns:\n        pl.LazyFrame: Unmaterialized Polars LazyFrame created from the Iceberg table\n    \"\"\"\n    import polars as pl\n\n    return pl.scan_iceberg(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.transaction","title":"<code>transaction()</code>","text":"<p>Create a new transaction object to first stage the changes, and then commit them to the catalog.</p> <p>Returns:</p> Type Description <code>Transaction</code> <p>The transaction object</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def transaction(self) -&gt; Transaction:\n    \"\"\"Create a new transaction object to first stage the changes, and then commit them to the catalog.\n\n    Returns:\n        The transaction object\n    \"\"\"\n    return Transaction(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.update_schema","title":"<code>update_schema(allow_incompatible_changes=False, case_sensitive=True)</code>","text":"<p>Create a new UpdateSchema to alter the columns of this table.</p> <p>Parameters:</p> Name Type Description Default <code>allow_incompatible_changes</code> <code>bool</code> <p>If changes are allowed that might break downstream consumers.</p> <code>False</code> <code>case_sensitive</code> <code>bool</code> <p>If field names are case-sensitive.</p> <code>True</code> <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>A new UpdateSchema.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_schema(self, allow_incompatible_changes: bool = False, case_sensitive: bool = True) -&gt; UpdateSchema:\n    \"\"\"Create a new UpdateSchema to alter the columns of this table.\n\n    Args:\n        allow_incompatible_changes: If changes are allowed that might break downstream consumers.\n        case_sensitive: If field names are case-sensitive.\n\n    Returns:\n        A new UpdateSchema.\n    \"\"\"\n    return UpdateSchema(\n        transaction=Transaction(self, autocommit=True),\n        allow_incompatible_changes=allow_incompatible_changes,\n        case_sensitive=case_sensitive,\n        name_mapping=self.name_mapping(),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.update_sort_order","title":"<code>update_sort_order(case_sensitive=True)</code>","text":"<p>Create a new UpdateSortOrder to update the sort order of this table.</p> <p>Returns:</p> Type Description <code>UpdateSortOrder</code> <p>A new UpdateSortOrder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_sort_order(self, case_sensitive: bool = True) -&gt; UpdateSortOrder:\n    \"\"\"Create a new UpdateSortOrder to update the sort order of this table.\n\n    Returns:\n        A new UpdateSortOrder.\n    \"\"\"\n    return UpdateSortOrder(transaction=Transaction(self, autocommit=True), case_sensitive=case_sensitive)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.update_statistics","title":"<code>update_statistics()</code>","text":"<p>Shorthand to run statistics management operations like add statistics and remove statistics.</p> <p>Use table.update_statistics().().commit() to run a specific operation. Use table.update_statistics().().().commit() to run multiple operations. <p>Pending changes are applied on commit.</p> <p>We can also use context managers to make more changes. For example:</p> <p>with table.update_statistics() as update:     update.set_statistics(statistics_file=statistics_file)     update.remove_statistics(snapshot_id=2)</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_statistics(self) -&gt; UpdateStatistics:\n    \"\"\"\n    Shorthand to run statistics management operations like add statistics and remove statistics.\n\n    Use table.update_statistics().&lt;operation&gt;().commit() to run a specific operation.\n    Use table.update_statistics().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n\n    Pending changes are applied on commit.\n\n    We can also use context managers to make more changes. For example:\n\n    with table.update_statistics() as update:\n        update.set_statistics(statistics_file=statistics_file)\n        update.remove_statistics(snapshot_id=2)\n    \"\"\"\n    return UpdateStatistics(transaction=Transaction(self, autocommit=True))\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Table.upsert","title":"<code>upsert(df, join_cols=None, when_matched_update_all=True, when_not_matched_insert_all=True, case_sensitive=True, branch=MAIN_BRANCH, snapshot_properties=EMPTY_DICT)</code>","text":"<p>Shorthand API for performing an upsert to an iceberg table.</p> <p>Args:</p> <pre><code>df: The input dataframe to upsert with the table's data.\njoin_cols: Columns to join on, if not provided, it will use the identifier-field-ids.\nwhen_matched_update_all: Bool indicating to update rows that are matched but require an update\n    due to a value in a non-key column changing\nwhen_not_matched_insert_all: Bool indicating new rows to be inserted that do not match any\n    existing rows in the table\ncase_sensitive: Bool indicating if the match should be case-sensitive\nbranch: Branch Reference to run the upsert operation\nsnapshot_properties: Custom properties to be added to the snapshot summary\n\nTo learn more about the identifier-field-ids: https://iceberg.apache.org/spec/#identifier-field-ids\n\n    Example Use Cases:\n        Case 1: Both Parameters = True (Full Upsert)\n        Existing row found \u2192 Update it\n        New row found \u2192 Insert it\n\n        Case 2: when_matched_update_all = False, when_not_matched_insert_all = True\n        Existing row found \u2192 Do nothing (no updates)\n        New row found \u2192 Insert it\n\n        Case 3: when_matched_update_all = True, when_not_matched_insert_all = False\n        Existing row found \u2192 Update it\n        New row found \u2192 Do nothing (no inserts)\n\n        Case 4: Both Parameters = False (No Merge Effect)\n        Existing row found \u2192 Do nothing\n        New row found \u2192 Do nothing\n        (Function effectively does nothing)\n</code></pre> <p>Returns:</p> Type Description <code>UpsertResult</code> <p>An UpsertResult class (contains details of rows updated and inserted)</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def upsert(\n    self,\n    df: pa.Table,\n    join_cols: list[str] | None = None,\n    when_matched_update_all: bool = True,\n    when_not_matched_insert_all: bool = True,\n    case_sensitive: bool = True,\n    branch: str | None = MAIN_BRANCH,\n    snapshot_properties: dict[str, str] = EMPTY_DICT,\n) -&gt; UpsertResult:\n    \"\"\"Shorthand API for performing an upsert to an iceberg table.\n\n    Args:\n\n        df: The input dataframe to upsert with the table's data.\n        join_cols: Columns to join on, if not provided, it will use the identifier-field-ids.\n        when_matched_update_all: Bool indicating to update rows that are matched but require an update\n            due to a value in a non-key column changing\n        when_not_matched_insert_all: Bool indicating new rows to be inserted that do not match any\n            existing rows in the table\n        case_sensitive: Bool indicating if the match should be case-sensitive\n        branch: Branch Reference to run the upsert operation\n        snapshot_properties: Custom properties to be added to the snapshot summary\n\n        To learn more about the identifier-field-ids: https://iceberg.apache.org/spec/#identifier-field-ids\n\n            Example Use Cases:\n                Case 1: Both Parameters = True (Full Upsert)\n                Existing row found \u2192 Update it\n                New row found \u2192 Insert it\n\n                Case 2: when_matched_update_all = False, when_not_matched_insert_all = True\n                Existing row found \u2192 Do nothing (no updates)\n                New row found \u2192 Insert it\n\n                Case 3: when_matched_update_all = True, when_not_matched_insert_all = False\n                Existing row found \u2192 Update it\n                New row found \u2192 Do nothing (no inserts)\n\n                Case 4: Both Parameters = False (No Merge Effect)\n                Existing row found \u2192 Do nothing\n                New row found \u2192 Do nothing\n                (Function effectively does nothing)\n\n\n    Returns:\n        An UpsertResult class (contains details of rows updated and inserted)\n    \"\"\"\n    with self.transaction() as tx:\n        return tx.upsert(\n            df=df,\n            join_cols=join_cols,\n            when_matched_update_all=when_matched_update_all,\n            when_not_matched_insert_all=when_not_matched_insert_all,\n            case_sensitive=case_sensitive,\n            branch=branch,\n            snapshot_properties=snapshot_properties,\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.TableIdentifier","title":"<code>TableIdentifier</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Fully Qualified identifier to a table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class TableIdentifier(IcebergBaseModel):\n    \"\"\"Fully Qualified identifier to a table.\"\"\"\n\n    namespace: Namespace\n    name: str\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.TableScan","title":"<code>TableScan</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class TableScan(ABC):\n    table_metadata: TableMetadata\n    io: FileIO\n    row_filter: BooleanExpression\n    selected_fields: tuple[str, ...]\n    case_sensitive: bool\n    snapshot_id: int | None\n    options: Properties\n    limit: int | None\n    catalog: Catalog | None\n    table_identifier: Identifier | None\n\n    def __init__(\n        self,\n        table_metadata: TableMetadata,\n        io: FileIO,\n        row_filter: str | BooleanExpression = ALWAYS_TRUE,\n        selected_fields: tuple[str, ...] = (\"*\",),\n        case_sensitive: bool = True,\n        snapshot_id: int | None = None,\n        options: Properties = EMPTY_DICT,\n        limit: int | None = None,\n        catalog: Catalog | None = None,\n        table_identifier: Identifier | None = None,\n    ):\n        self.table_metadata = table_metadata\n        self.io = io\n        self.row_filter = _parse_row_filter(row_filter)\n        self.selected_fields = selected_fields\n        self.case_sensitive = case_sensitive\n        self.snapshot_id = snapshot_id\n        self.options = options\n        self.limit = limit\n        self.catalog = catalog\n        self.table_identifier = table_identifier\n\n    def snapshot(self) -&gt; Snapshot | None:\n        if self.snapshot_id:\n            return self.table_metadata.snapshot_by_id(self.snapshot_id)\n        return self.table_metadata.current_snapshot()\n\n    def projection(self) -&gt; Schema:\n        current_schema = self.table_metadata.schema()\n        if self.snapshot_id is not None:\n            snapshot = self.table_metadata.snapshot_by_id(self.snapshot_id)\n            if snapshot is not None:\n                if snapshot.schema_id is not None:\n                    try:\n                        current_schema = next(\n                            schema for schema in self.table_metadata.schemas if schema.schema_id == snapshot.schema_id\n                        )\n                    except StopIteration:\n                        warnings.warn(f\"Metadata does not contain schema with id: {snapshot.schema_id}\", stacklevel=2)\n            else:\n                raise ValueError(f\"Snapshot not found: {self.snapshot_id}\")\n\n        if \"*\" in self.selected_fields:\n            return current_schema\n\n        return current_schema.select(*self.selected_fields, case_sensitive=self.case_sensitive)\n\n    @abstractmethod\n    def plan_files(self) -&gt; Iterable[ScanTask]: ...\n\n    @abstractmethod\n    def to_arrow(self) -&gt; pa.Table: ...\n\n    @abstractmethod\n    def to_pandas(self, **kwargs: Any) -&gt; pd.DataFrame: ...\n\n    @abstractmethod\n    def to_polars(self) -&gt; pl.DataFrame: ...\n\n    def update(self: S, **overrides: Any) -&gt; S:\n        \"\"\"Create a copy of this table scan with updated fields.\"\"\"\n        from inspect import signature\n\n        # Extract those attributes that are constructor parameters. We don't use self.__dict__ as the kwargs to the\n        # constructors because it may contain additional attributes that are not part of the constructor signature.\n        params = signature(type(self).__init__).parameters.keys() - {\"self\"}  # Skip \"self\" parameter\n        kwargs = {param: getattr(self, param) for param in params}  # Assume parameters are attributes\n\n        return type(self)(**{**kwargs, **overrides})\n\n    def use_ref(self: S, name: str) -&gt; S:\n        if self.snapshot_id:\n            raise ValueError(f\"Cannot override ref, already set snapshot id={self.snapshot_id}\")\n        if snapshot := self.table_metadata.snapshot_by_name(name):\n            return self.update(snapshot_id=snapshot.snapshot_id)\n\n        raise ValueError(f\"Cannot scan unknown ref={name}\")\n\n    def select(self: S, *field_names: str) -&gt; S:\n        if \"*\" in self.selected_fields:\n            return self.update(selected_fields=field_names)\n        return self.update(selected_fields=tuple(set(self.selected_fields).intersection(set(field_names))))\n\n    def filter(self: S, expr: str | BooleanExpression) -&gt; S:\n        return self.update(row_filter=And(self.row_filter, _parse_row_filter(expr)))\n\n    def with_case_sensitive(self: S, case_sensitive: bool = True) -&gt; S:\n        return self.update(case_sensitive=case_sensitive)\n\n    @abstractmethod\n    def count(self) -&gt; int: ...\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.TableScan.update","title":"<code>update(**overrides)</code>","text":"<p>Create a copy of this table scan with updated fields.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update(self: S, **overrides: Any) -&gt; S:\n    \"\"\"Create a copy of this table scan with updated fields.\"\"\"\n    from inspect import signature\n\n    # Extract those attributes that are constructor parameters. We don't use self.__dict__ as the kwargs to the\n    # constructors because it may contain additional attributes that are not part of the constructor signature.\n    params = signature(type(self).__init__).parameters.keys() - {\"self\"}  # Skip \"self\" parameter\n    kwargs = {param: getattr(self, param) for param in params}  # Assume parameters are attributes\n\n    return type(self)(**{**kwargs, **overrides})\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction","title":"<code>Transaction</code>","text":"Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>class Transaction:\n    _table: Table\n    _autocommit: bool\n    _updates: tuple[TableUpdate, ...]\n    _requirements: tuple[TableRequirement, ...]\n\n    def __init__(self, table: Table, autocommit: bool = False):\n        \"\"\"Open a transaction to stage and commit changes to a table.\n\n        Args:\n            table: The table that will be altered.\n            autocommit: Option to automatically commit the changes when they are staged.\n        \"\"\"\n        self._table = table\n        self._autocommit = autocommit\n        self._updates = ()\n        self._requirements = ()\n\n    @property\n    def table_metadata(self) -&gt; TableMetadata:\n        return update_table_metadata(self._table.metadata, self._updates)\n\n    def __enter__(self) -&gt; Transaction:\n        \"\"\"Start a transaction to update the table.\"\"\"\n        return self\n\n    def __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n        \"\"\"Close and commit the transaction if no exceptions have been raised.\"\"\"\n        if exctype is None and excinst is None and exctb is None:\n            self.commit_transaction()\n\n    def _stage(\n        self,\n        updates: tuple[TableUpdate, ...],\n        requirements: tuple[TableRequirement, ...] = (),\n    ) -&gt; Transaction:\n        \"\"\"Stage updates to the transaction state without committing to the catalog.\n\n        Args:\n            updates: The updates to stage.\n            requirements: The requirements that must be met.\n\n        Returns:\n            This transaction for method chaining.\n        \"\"\"\n        for requirement in requirements:\n            requirement.validate(self.table_metadata)\n\n        self._updates += updates\n\n        # For the requirements, it does not make sense to add a requirement more than once\n        # For example, you cannot assert that the current schema has two different IDs\n        existing_requirements = {type(requirement) for requirement in self._requirements}\n        for new_requirement in requirements:\n            if type(new_requirement) not in existing_requirements:\n                self._requirements = self._requirements + (new_requirement,)\n\n        return self\n\n    def _apply(\n        self,\n        updates: tuple[TableUpdate, ...],\n        requirements: tuple[TableRequirement, ...] = (),\n    ) -&gt; Transaction:\n        \"\"\"Check if the requirements are met, and applies the updates to the metadata.\"\"\"\n        self._stage(updates, requirements)\n\n        if self._autocommit:\n            self.commit_transaction()\n\n        return self\n\n    def _scan(self, row_filter: str | BooleanExpression = ALWAYS_TRUE, case_sensitive: bool = True) -&gt; DataScan:\n        \"\"\"Minimal data scan of the table with the current state of the transaction.\"\"\"\n        return DataScan(\n            table_metadata=self.table_metadata, io=self._table.io, row_filter=row_filter, case_sensitive=case_sensitive\n        )\n\n    def upgrade_table_version(self, format_version: TableVersion) -&gt; Transaction:\n        \"\"\"Set the table to a certain version.\n\n        Args:\n            format_version: The newly set version.\n\n        Returns:\n            The alter table builder.\n        \"\"\"\n        if format_version not in {1, 2}:\n            raise ValueError(f\"Unsupported table format version: {format_version}\")\n\n        if format_version &lt; self.table_metadata.format_version:\n            raise ValueError(f\"Cannot downgrade v{self.table_metadata.format_version} table to v{format_version}\")\n\n        if format_version &gt; self.table_metadata.format_version:\n            return self._apply((UpgradeFormatVersionUpdate(format_version=format_version),))\n\n        return self\n\n    def set_properties(self, properties: Properties = EMPTY_DICT, **kwargs: Any) -&gt; Transaction:\n        \"\"\"Set properties.\n\n        When a property is already set, it will be overwritten.\n\n        Args:\n            properties: The properties set on the table.\n            kwargs: properties can also be pass as kwargs.\n\n        Returns:\n            The alter table builder.\n        \"\"\"\n        if properties and kwargs:\n            raise ValueError(\"Cannot pass both properties and kwargs\")\n        updates = properties or kwargs\n        return self._apply((SetPropertiesUpdate(updates=updates),))\n\n    def _set_ref_snapshot(\n        self,\n        snapshot_id: int,\n        ref_name: str,\n        type: str,\n        max_ref_age_ms: int | None = None,\n        max_snapshot_age_ms: int | None = None,\n        min_snapshots_to_keep: int | None = None,\n    ) -&gt; UpdatesAndRequirements:\n        \"\"\"Update a ref to a snapshot.\n\n        Returns:\n            The updates and requirements for the set-snapshot-ref staged\n        \"\"\"\n        updates = (\n            SetSnapshotRefUpdate(\n                snapshot_id=snapshot_id,\n                ref_name=ref_name,\n                type=type,\n                max_ref_age_ms=max_ref_age_ms,\n                max_snapshot_age_ms=max_snapshot_age_ms,\n                min_snapshots_to_keep=min_snapshots_to_keep,\n            ),\n        )\n        requirements = (\n            AssertRefSnapshotId(\n                snapshot_id=self.table_metadata.refs[ref_name].snapshot_id if ref_name in self.table_metadata.refs else None,\n                ref=ref_name,\n            ),\n        )\n\n        return updates, requirements\n\n    def _build_partition_predicate(self, partition_records: set[Record]) -&gt; BooleanExpression:\n        \"\"\"Build a filter predicate matching any of the input partition records.\n\n        Args:\n            partition_records: A set of partition records to match\n        Returns:\n            A predicate matching any of the input partition records.\n        \"\"\"\n        partition_spec = self.table_metadata.spec()\n        schema = self.table_metadata.schema()\n        partition_fields = [schema.find_field(field.source_id).name for field in partition_spec.fields]\n\n        expr: BooleanExpression = AlwaysFalse()\n        for partition_record in partition_records:\n            match_partition_expression: BooleanExpression = AlwaysTrue()\n\n            for pos, partition_field in enumerate(partition_fields):\n                predicate = (\n                    EqualTo(Reference(partition_field), partition_record[pos])\n                    if partition_record[pos] is not None\n                    else IsNull(Reference(partition_field))\n                )\n                match_partition_expression = And(match_partition_expression, predicate)\n            expr = Or(expr, match_partition_expression)\n        return expr\n\n    def _append_snapshot_producer(\n        self, snapshot_properties: dict[str, str], branch: str | None = MAIN_BRANCH\n    ) -&gt; _FastAppendFiles:\n        \"\"\"Determine the append type based on table properties.\n\n        Args:\n            snapshot_properties: Custom properties to be added to the snapshot summary\n        Returns:\n            Either a fast-append or a merge-append snapshot producer.\n        \"\"\"\n        manifest_merge_enabled = property_as_bool(\n            self.table_metadata.properties,\n            TableProperties.MANIFEST_MERGE_ENABLED,\n            TableProperties.MANIFEST_MERGE_ENABLED_DEFAULT,\n        )\n        update_snapshot = self.update_snapshot(snapshot_properties=snapshot_properties, branch=branch)\n        return update_snapshot.merge_append() if manifest_merge_enabled else update_snapshot.fast_append()\n\n    def update_schema(self, allow_incompatible_changes: bool = False, case_sensitive: bool = True) -&gt; UpdateSchema:\n        \"\"\"Create a new UpdateSchema to alter the columns of this table.\n\n        Args:\n            allow_incompatible_changes: If changes are allowed that might break downstream consumers.\n            case_sensitive: If field names are case-sensitive.\n\n        Returns:\n            A new UpdateSchema.\n        \"\"\"\n        return UpdateSchema(\n            self,\n            allow_incompatible_changes=allow_incompatible_changes,\n            case_sensitive=case_sensitive,\n            name_mapping=self.table_metadata.name_mapping(),\n        )\n\n    def update_sort_order(self, case_sensitive: bool = True) -&gt; UpdateSortOrder:\n        \"\"\"Create a new UpdateSortOrder to update the sort order of this table.\n\n        Args:\n            case_sensitive: If field names are case-sensitive.\n\n        Returns:\n            A new UpdateSortOrder.\n        \"\"\"\n        return UpdateSortOrder(\n            self,\n            case_sensitive=case_sensitive,\n        )\n\n    def update_snapshot(\n        self, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH\n    ) -&gt; UpdateSnapshot:\n        \"\"\"Create a new UpdateSnapshot to produce a new snapshot for the table.\n\n        Returns:\n            A new UpdateSnapshot\n        \"\"\"\n        return UpdateSnapshot(self, io=self._table.io, branch=branch, snapshot_properties=snapshot_properties)\n\n    def update_statistics(self) -&gt; UpdateStatistics:\n        \"\"\"\n        Create a new UpdateStatistics to update the statistics of the table.\n\n        Returns:\n            A new UpdateStatistics\n        \"\"\"\n        return UpdateStatistics(transaction=self)\n\n    def append(self, df: pa.Table, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH) -&gt; None:\n        \"\"\"\n        Shorthand API for appending a PyArrow table to a table transaction.\n\n        Args:\n            df: The Arrow dataframe that will be appended to overwrite the table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            branch: Branch Reference to run the append operation\n        \"\"\"\n        try:\n            import pyarrow as pa\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n        from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n        if not isinstance(df, pa.Table):\n            raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n        downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n        _check_pyarrow_schema_compatible(\n            self.table_metadata.schema(),\n            provided_schema=df.schema,\n            downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us,\n            format_version=self.table_metadata.format_version,\n        )\n\n        with self._append_snapshot_producer(snapshot_properties, branch=branch) as append_files:\n            # skip writing data files if the dataframe is empty\n            if df.shape[0] &gt; 0:\n                data_files = list(\n                    _dataframe_to_data_files(\n                        table_metadata=self.table_metadata, write_uuid=append_files.commit_uuid, df=df, io=self._table.io\n                    )\n                )\n                for data_file in data_files:\n                    append_files.append_data_file(data_file)\n\n    def dynamic_partition_overwrite(\n        self, df: pa.Table, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH\n    ) -&gt; None:\n        \"\"\"\n        Shorthand for overwriting existing partitions with a PyArrow table.\n\n        The function detects partition values in the provided arrow table using the current\n        partition spec, and deletes existing partitions matching these values. Finally, the\n        data in the table is appended to the table.\n\n        Args:\n            df: The Arrow dataframe that will be used to overwrite the table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            branch: Branch Reference to run the dynamic partition overwrite operation\n        \"\"\"\n        try:\n            import pyarrow as pa\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n        from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n        if not isinstance(df, pa.Table):\n            raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n        if self.table_metadata.spec().is_unpartitioned():\n            raise ValueError(\"Cannot apply dynamic overwrite on an unpartitioned table.\")\n\n        for field in self.table_metadata.spec().fields:\n            if not isinstance(field.transform, IdentityTransform):\n                raise ValueError(\n                    f\"For now dynamic overwrite does not support a table with non-identity-transform field \"\n                    f\"in the latest partition spec: {field}\"\n                )\n\n        downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n        _check_pyarrow_schema_compatible(\n            self.table_metadata.schema(),\n            provided_schema=df.schema,\n            downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us,\n            format_version=self.table_metadata.format_version,\n        )\n\n        # If dataframe does not have data, there is no need to overwrite\n        if df.shape[0] == 0:\n            return\n\n        append_snapshot_commit_uuid = uuid.uuid4()\n        data_files: list[DataFile] = list(\n            _dataframe_to_data_files(\n                table_metadata=self._table.metadata, write_uuid=append_snapshot_commit_uuid, df=df, io=self._table.io\n            )\n        )\n\n        partitions_to_overwrite = {data_file.partition for data_file in data_files}\n        delete_filter = self._build_partition_predicate(partition_records=partitions_to_overwrite)\n        self.delete(delete_filter=delete_filter, snapshot_properties=snapshot_properties, branch=branch)\n\n        with self._append_snapshot_producer(snapshot_properties, branch=branch) as append_files:\n            append_files.commit_uuid = append_snapshot_commit_uuid\n            for data_file in data_files:\n                append_files.append_data_file(data_file)\n\n    def overwrite(\n        self,\n        df: pa.Table,\n        overwrite_filter: BooleanExpression | str = ALWAYS_TRUE,\n        snapshot_properties: dict[str, str] = EMPTY_DICT,\n        case_sensitive: bool = True,\n        branch: str | None = MAIN_BRANCH,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand for adding a table overwrite with a PyArrow table to the transaction.\n\n        An overwrite may produce zero or more snapshots based on the operation:\n\n            - DELETE: In case existing Parquet files can be dropped completely.\n            - OVERWRITE: In case existing Parquet files need to be rewritten to drop rows that match the overwrite filter.\n            - APPEND: In case new data is being inserted into the table.\n\n        Args:\n            df: The Arrow dataframe that will be used to overwrite the table\n            overwrite_filter: ALWAYS_TRUE when you overwrite all the data,\n                              or a boolean expression in case of a partial overwrite\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            case_sensitive: A bool determine if the provided `overwrite_filter` is case-sensitive\n            branch: Branch Reference to run the overwrite operation\n        \"\"\"\n        try:\n            import pyarrow as pa\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n        from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n        if not isinstance(df, pa.Table):\n            raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n        downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n        _check_pyarrow_schema_compatible(\n            self.table_metadata.schema(),\n            provided_schema=df.schema,\n            downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us,\n            format_version=self.table_metadata.format_version,\n        )\n\n        if overwrite_filter != AlwaysFalse():\n            # Only delete when the filter is != AlwaysFalse\n            self.delete(\n                delete_filter=overwrite_filter,\n                case_sensitive=case_sensitive,\n                snapshot_properties=snapshot_properties,\n                branch=branch,\n            )\n\n        with self._append_snapshot_producer(snapshot_properties, branch=branch) as append_files:\n            # skip writing data files if the dataframe is empty\n            if df.shape[0] &gt; 0:\n                data_files = _dataframe_to_data_files(\n                    table_metadata=self.table_metadata, write_uuid=append_files.commit_uuid, df=df, io=self._table.io\n                )\n                for data_file in data_files:\n                    append_files.append_data_file(data_file)\n\n    def delete(\n        self,\n        delete_filter: str | BooleanExpression,\n        snapshot_properties: dict[str, str] = EMPTY_DICT,\n        case_sensitive: bool = True,\n        branch: str | None = MAIN_BRANCH,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand for deleting record from a table.\n\n        A delete may produce zero or more snapshots based on the operation:\n\n            - DELETE: In case existing Parquet files can be dropped completely.\n            - OVERWRITE: In case existing Parquet files need to be rewritten to drop rows that match the delete filter.\n\n        Args:\n            delete_filter: A boolean expression to delete rows from a table\n            snapshot_properties: Custom properties to be added to the snapshot summary\n            case_sensitive: A bool determine if the provided `delete_filter` is case-sensitive\n            branch: Branch Reference to run the delete operation\n        \"\"\"\n        from pyiceberg.io.pyarrow import (\n            ArrowScan,\n            _dataframe_to_data_files,\n            _expression_to_complementary_pyarrow,\n        )\n\n        if (\n            self.table_metadata.properties.get(TableProperties.DELETE_MODE, TableProperties.DELETE_MODE_DEFAULT)\n            == TableProperties.DELETE_MODE_MERGE_ON_READ\n        ):\n            warnings.warn(\"Merge on read is not yet supported, falling back to copy-on-write\", stacklevel=2)\n\n        if isinstance(delete_filter, str):\n            delete_filter = _parse_row_filter(delete_filter)\n\n        with self.update_snapshot(snapshot_properties=snapshot_properties, branch=branch).delete() as delete_snapshot:\n            delete_snapshot.delete_by_predicate(delete_filter, case_sensitive)\n\n        # Check if there are any files that require an actual rewrite of a data file\n        if delete_snapshot.rewrites_needed is True:\n            bound_delete_filter = bind(self.table_metadata.schema(), delete_filter, case_sensitive)\n            preserve_row_filter = _expression_to_complementary_pyarrow(bound_delete_filter, self.table_metadata.schema())\n\n            file_scan = self._scan(row_filter=delete_filter, case_sensitive=case_sensitive)\n            if branch is not None:\n                file_scan = file_scan.use_ref(branch)\n            files = file_scan.plan_files()\n\n            commit_uuid = uuid.uuid4()\n            counter = itertools.count(0)\n\n            replaced_files: list[tuple[DataFile, list[DataFile]]] = []\n            # This will load the Parquet file into memory, including:\n            #   - Filter out the rows based on the delete filter\n            #   - Projecting it to the current schema\n            #   - Applying the positional deletes if they are there\n            # When writing\n            #   - Apply the latest partition-spec\n            #   - And sort order when added\n            for original_file in files:\n                df = ArrowScan(\n                    table_metadata=self.table_metadata,\n                    io=self._table.io,\n                    projected_schema=self.table_metadata.schema(),\n                    row_filter=AlwaysTrue(),\n                ).to_table(tasks=[original_file])\n                filtered_df = df.filter(preserve_row_filter)\n\n                # Only rewrite if there are records being deleted\n                if len(filtered_df) == 0:\n                    replaced_files.append((original_file.file, []))\n                elif len(df) != len(filtered_df):\n                    replaced_files.append(\n                        (\n                            original_file.file,\n                            list(\n                                _dataframe_to_data_files(\n                                    io=self._table.io,\n                                    df=filtered_df,\n                                    table_metadata=self.table_metadata,\n                                    write_uuid=commit_uuid,\n                                    counter=counter,\n                                )\n                            ),\n                        )\n                    )\n\n            if len(replaced_files) &gt; 0:\n                with self.update_snapshot(\n                    snapshot_properties=snapshot_properties, branch=branch\n                ).overwrite() as overwrite_snapshot:\n                    overwrite_snapshot.commit_uuid = commit_uuid\n                    for original_data_file, replaced_data_files in replaced_files:\n                        overwrite_snapshot.delete_data_file(original_data_file)\n                        for replaced_data_file in replaced_data_files:\n                            overwrite_snapshot.append_data_file(replaced_data_file)\n\n        if not delete_snapshot.files_affected and not delete_snapshot.rewrites_needed:\n            warnings.warn(\"Delete operation did not match any records\", stacklevel=2)\n\n    def upsert(\n        self,\n        df: pa.Table,\n        join_cols: list[str] | None = None,\n        when_matched_update_all: bool = True,\n        when_not_matched_insert_all: bool = True,\n        case_sensitive: bool = True,\n        branch: str | None = MAIN_BRANCH,\n        snapshot_properties: dict[str, str] = EMPTY_DICT,\n    ) -&gt; UpsertResult:\n        \"\"\"Shorthand API for performing an upsert to an iceberg table.\n\n        Args:\n\n            df: The input dataframe to upsert with the table's data.\n            join_cols: Columns to join on, if not provided, it will use the identifier-field-ids.\n            when_matched_update_all: Bool indicating to update rows that are matched but require an update\n                due to a value in a non-key column changing\n            when_not_matched_insert_all: Bool indicating new rows to be inserted that do not match any\n                existing rows in the table\n            case_sensitive: Bool indicating if the match should be case-sensitive\n            branch: Branch Reference to run the upsert operation\n            snapshot_properties: Custom properties to be added to the snapshot summary\n\n            To learn more about the identifier-field-ids: https://iceberg.apache.org/spec/#identifier-field-ids\n\n                Example Use Cases:\n                    Case 1: Both Parameters = True (Full Upsert)\n                    Existing row found \u2192 Update it\n                    New row found \u2192 Insert it\n\n                    Case 2: when_matched_update_all = False, when_not_matched_insert_all = True\n                    Existing row found \u2192 Do nothing (no updates)\n                    New row found \u2192 Insert it\n\n                    Case 3: when_matched_update_all = True, when_not_matched_insert_all = False\n                    Existing row found \u2192 Update it\n                    New row found \u2192 Do nothing (no inserts)\n\n                    Case 4: Both Parameters = False (No Merge Effect)\n                    Existing row found \u2192 Do nothing\n                    New row found \u2192 Do nothing\n                    (Function effectively does nothing)\n\n\n        Returns:\n            An UpsertResult class (contains details of rows updated and inserted)\n        \"\"\"\n        try:\n            import pyarrow as pa  # noqa: F401\n        except ModuleNotFoundError as e:\n            raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n        from pyiceberg.io.pyarrow import expression_to_pyarrow\n        from pyiceberg.table import upsert_util\n\n        if join_cols is None:\n            join_cols = []\n            for field_id in self.table_metadata.schema().identifier_field_ids:\n                col = self.table_metadata.schema().find_column_name(field_id)\n                if col is not None:\n                    join_cols.append(col)\n                else:\n                    raise ValueError(f\"Field-ID could not be found: {join_cols}\")\n\n        if len(join_cols) == 0:\n            raise ValueError(\"Join columns could not be found, please set identifier-field-ids or pass in explicitly.\")\n\n        if not when_matched_update_all and not when_not_matched_insert_all:\n            raise ValueError(\"no upsert options selected...exiting\")\n\n        if upsert_util.has_duplicate_rows(df, join_cols):\n            raise ValueError(\"Duplicate rows found in source dataset based on the key columns. No upsert executed\")\n\n        from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible\n\n        downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n        _check_pyarrow_schema_compatible(\n            self.table_metadata.schema(),\n            provided_schema=df.schema,\n            downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us,\n            format_version=self.table_metadata.format_version,\n        )\n\n        # get list of rows that exist so we don't have to load the entire target table\n        matched_predicate = upsert_util.create_match_filter(df, join_cols)\n\n        # We must use Transaction.table_metadata for the scan. This includes all uncommitted - but relevant - changes.\n\n        matched_iceberg_record_batches_scan = DataScan(\n            table_metadata=self.table_metadata,\n            io=self._table.io,\n            row_filter=matched_predicate,\n            case_sensitive=case_sensitive,\n        )\n\n        if branch in self.table_metadata.refs:\n            matched_iceberg_record_batches_scan = matched_iceberg_record_batches_scan.use_ref(branch)\n\n        matched_iceberg_record_batches = matched_iceberg_record_batches_scan.to_arrow_batch_reader()\n\n        batches_to_overwrite = []\n        overwrite_predicates = []\n        rows_to_insert = df\n\n        for batch in matched_iceberg_record_batches:\n            rows = pa.Table.from_batches([batch])\n\n            if when_matched_update_all:\n                # function get_rows_to_update is doing a check on non-key columns to see if any of the\n                # values have actually changed. We don't want to do just a blanket overwrite for matched\n                # rows if the actual non-key column data hasn't changed.\n                # this extra step avoids unnecessary IO and writes\n                rows_to_update = upsert_util.get_rows_to_update(df, rows, join_cols)\n\n                if len(rows_to_update) &gt; 0:\n                    # build the match predicate filter\n                    overwrite_mask_predicate = upsert_util.create_match_filter(rows_to_update, join_cols)\n\n                    batches_to_overwrite.append(rows_to_update)\n                    overwrite_predicates.append(overwrite_mask_predicate)\n\n            if when_not_matched_insert_all:\n                expr_match = upsert_util.create_match_filter(rows, join_cols)\n                expr_match_bound = bind(self.table_metadata.schema(), expr_match, case_sensitive=case_sensitive)\n                expr_match_arrow = expression_to_pyarrow(expr_match_bound)\n\n                # Filter rows per batch.\n                rows_to_insert = rows_to_insert.filter(~expr_match_arrow)\n\n        update_row_cnt = 0\n        insert_row_cnt = 0\n\n        if batches_to_overwrite:\n            rows_to_update = pa.concat_tables(batches_to_overwrite)\n            update_row_cnt = len(rows_to_update)\n            self.overwrite(\n                rows_to_update,\n                overwrite_filter=Or(*overwrite_predicates) if len(overwrite_predicates) &gt; 1 else overwrite_predicates[0],\n                branch=branch,\n                snapshot_properties=snapshot_properties,\n            )\n\n        if when_not_matched_insert_all:\n            insert_row_cnt = len(rows_to_insert)\n            if rows_to_insert:\n                self.append(rows_to_insert, branch=branch, snapshot_properties=snapshot_properties)\n\n        return UpsertResult(rows_updated=update_row_cnt, rows_inserted=insert_row_cnt)\n\n    def add_files(\n        self,\n        file_paths: list[str],\n        snapshot_properties: dict[str, str] = EMPTY_DICT,\n        check_duplicate_files: bool = True,\n        branch: str | None = MAIN_BRANCH,\n    ) -&gt; None:\n        \"\"\"\n        Shorthand API for adding files as data files to the table transaction.\n\n        Args:\n            file_paths: The list of full file paths to be added as data files to the table\n\n        Raises:\n            FileNotFoundError: If the file does not exist.\n            ValueError: Raises a ValueError given file_paths contains duplicate files\n            ValueError: Raises a ValueError given file_paths already referenced by table\n        \"\"\"\n        if len(file_paths) != len(set(file_paths)):\n            raise ValueError(\"File paths must be unique\")\n\n        if check_duplicate_files:\n            import pyarrow.compute as pc\n\n            expr = pc.field(\"file_path\").isin(file_paths)\n            referenced_files = [file[\"file_path\"] for file in self._table.inspect.data_files().filter(expr).to_pylist()]\n\n            if referenced_files:\n                raise ValueError(f\"Cannot add files that are already referenced by table, files: {', '.join(referenced_files)}\")\n\n        if self.table_metadata.name_mapping() is None:\n            self.set_properties(\n                **{TableProperties.DEFAULT_NAME_MAPPING: self.table_metadata.schema().name_mapping.model_dump_json()}\n            )\n        with self._append_snapshot_producer(snapshot_properties, branch=branch) as append_files:\n            data_files = _parquet_files_to_data_files(\n                table_metadata=self.table_metadata, file_paths=file_paths, io=self._table.io\n            )\n            for data_file in data_files:\n                append_files.append_data_file(data_file)\n\n    def update_spec(self) -&gt; UpdateSpec:\n        \"\"\"Create a new UpdateSpec to update the partitioning of the table.\n\n        Returns:\n            A new UpdateSpec.\n        \"\"\"\n        return UpdateSpec(self)\n\n    def remove_properties(self, *removals: str) -&gt; Transaction:\n        \"\"\"Remove properties.\n\n        Args:\n            removals: Properties to be removed.\n\n        Returns:\n            The alter table builder.\n        \"\"\"\n        return self._apply((RemovePropertiesUpdate(removals=removals),))\n\n    def update_location(self, location: str) -&gt; Transaction:\n        \"\"\"Set the new table location.\n\n        Args:\n            location: The new location of the table.\n\n        Returns:\n            The alter table builder.\n        \"\"\"\n        raise NotImplementedError(\"Not yet implemented\")\n\n    def commit_transaction(self) -&gt; Table:\n        \"\"\"Commit the changes to the catalog.\n\n        Returns:\n            The table with the updates applied.\n        \"\"\"\n        if len(self._updates) &gt; 0:\n            self._requirements += (AssertTableUUID(uuid=self.table_metadata.table_uuid),)\n            self._table._do_commit(  # pylint: disable=W0212\n                updates=self._updates,\n                requirements=self._requirements,\n            )\n\n        self._updates = ()\n        self._requirements = ()\n\n        return self._table\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.__enter__","title":"<code>__enter__()</code>","text":"<p>Start a transaction to update the table.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __enter__(self) -&gt; Transaction:\n    \"\"\"Start a transaction to update the table.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.__exit__","title":"<code>__exit__(exctype, excinst, exctb)</code>","text":"<p>Close and commit the transaction if no exceptions have been raised.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __exit__(self, exctype: type[BaseException] | None, excinst: BaseException | None, exctb: TracebackType | None) -&gt; None:\n    \"\"\"Close and commit the transaction if no exceptions have been raised.\"\"\"\n    if exctype is None and excinst is None and exctb is None:\n        self.commit_transaction()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.__init__","title":"<code>__init__(table, autocommit=False)</code>","text":"<p>Open a transaction to stage and commit changes to a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table that will be altered.</p> required <code>autocommit</code> <code>bool</code> <p>Option to automatically commit the changes when they are staged.</p> <code>False</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def __init__(self, table: Table, autocommit: bool = False):\n    \"\"\"Open a transaction to stage and commit changes to a table.\n\n    Args:\n        table: The table that will be altered.\n        autocommit: Option to automatically commit the changes when they are staged.\n    \"\"\"\n    self._table = table\n    self._autocommit = autocommit\n    self._updates = ()\n    self._requirements = ()\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.add_files","title":"<code>add_files(file_paths, snapshot_properties=EMPTY_DICT, check_duplicate_files=True, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand API for adding files as data files to the table transaction.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>list[str]</code> <p>The list of full file paths to be added as data files to the table</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p> <code>ValueError</code> <p>Raises a ValueError given file_paths contains duplicate files</p> <code>ValueError</code> <p>Raises a ValueError given file_paths already referenced by table</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def add_files(\n    self,\n    file_paths: list[str],\n    snapshot_properties: dict[str, str] = EMPTY_DICT,\n    check_duplicate_files: bool = True,\n    branch: str | None = MAIN_BRANCH,\n) -&gt; None:\n    \"\"\"\n    Shorthand API for adding files as data files to the table transaction.\n\n    Args:\n        file_paths: The list of full file paths to be added as data files to the table\n\n    Raises:\n        FileNotFoundError: If the file does not exist.\n        ValueError: Raises a ValueError given file_paths contains duplicate files\n        ValueError: Raises a ValueError given file_paths already referenced by table\n    \"\"\"\n    if len(file_paths) != len(set(file_paths)):\n        raise ValueError(\"File paths must be unique\")\n\n    if check_duplicate_files:\n        import pyarrow.compute as pc\n\n        expr = pc.field(\"file_path\").isin(file_paths)\n        referenced_files = [file[\"file_path\"] for file in self._table.inspect.data_files().filter(expr).to_pylist()]\n\n        if referenced_files:\n            raise ValueError(f\"Cannot add files that are already referenced by table, files: {', '.join(referenced_files)}\")\n\n    if self.table_metadata.name_mapping() is None:\n        self.set_properties(\n            **{TableProperties.DEFAULT_NAME_MAPPING: self.table_metadata.schema().name_mapping.model_dump_json()}\n        )\n    with self._append_snapshot_producer(snapshot_properties, branch=branch) as append_files:\n        data_files = _parquet_files_to_data_files(\n            table_metadata=self.table_metadata, file_paths=file_paths, io=self._table.io\n        )\n        for data_file in data_files:\n            append_files.append_data_file(data_file)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.append","title":"<code>append(df, snapshot_properties=EMPTY_DICT, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand API for appending a PyArrow table to a table transaction.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be appended to overwrite the table</p> required <code>snapshot_properties</code> <code>dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>branch</code> <code>str | None</code> <p>Branch Reference to run the append operation</p> <code>MAIN_BRANCH</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def append(self, df: pa.Table, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH) -&gt; None:\n    \"\"\"\n    Shorthand API for appending a PyArrow table to a table transaction.\n\n    Args:\n        df: The Arrow dataframe that will be appended to overwrite the table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        branch: Branch Reference to run the append operation\n    \"\"\"\n    try:\n        import pyarrow as pa\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n    from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n    if not isinstance(df, pa.Table):\n        raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n    downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n    _check_pyarrow_schema_compatible(\n        self.table_metadata.schema(),\n        provided_schema=df.schema,\n        downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us,\n        format_version=self.table_metadata.format_version,\n    )\n\n    with self._append_snapshot_producer(snapshot_properties, branch=branch) as append_files:\n        # skip writing data files if the dataframe is empty\n        if df.shape[0] &gt; 0:\n            data_files = list(\n                _dataframe_to_data_files(\n                    table_metadata=self.table_metadata, write_uuid=append_files.commit_uuid, df=df, io=self._table.io\n                )\n            )\n            for data_file in data_files:\n                append_files.append_data_file(data_file)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.commit_transaction","title":"<code>commit_transaction()</code>","text":"<p>Commit the changes to the catalog.</p> <p>Returns:</p> Type Description <code>Table</code> <p>The table with the updates applied.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def commit_transaction(self) -&gt; Table:\n    \"\"\"Commit the changes to the catalog.\n\n    Returns:\n        The table with the updates applied.\n    \"\"\"\n    if len(self._updates) &gt; 0:\n        self._requirements += (AssertTableUUID(uuid=self.table_metadata.table_uuid),)\n        self._table._do_commit(  # pylint: disable=W0212\n            updates=self._updates,\n            requirements=self._requirements,\n        )\n\n    self._updates = ()\n    self._requirements = ()\n\n    return self._table\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.delete","title":"<code>delete(delete_filter, snapshot_properties=EMPTY_DICT, case_sensitive=True, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand for deleting record from a table.</p> <p>A delete may produce zero or more snapshots based on the operation:</p> <pre><code>- DELETE: In case existing Parquet files can be dropped completely.\n- OVERWRITE: In case existing Parquet files need to be rewritten to drop rows that match the delete filter.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>delete_filter</code> <code>str | BooleanExpression</code> <p>A boolean expression to delete rows from a table</p> required <code>snapshot_properties</code> <code>dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>case_sensitive</code> <code>bool</code> <p>A bool determine if the provided <code>delete_filter</code> is case-sensitive</p> <code>True</code> <code>branch</code> <code>str | None</code> <p>Branch Reference to run the delete operation</p> <code>MAIN_BRANCH</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def delete(\n    self,\n    delete_filter: str | BooleanExpression,\n    snapshot_properties: dict[str, str] = EMPTY_DICT,\n    case_sensitive: bool = True,\n    branch: str | None = MAIN_BRANCH,\n) -&gt; None:\n    \"\"\"\n    Shorthand for deleting record from a table.\n\n    A delete may produce zero or more snapshots based on the operation:\n\n        - DELETE: In case existing Parquet files can be dropped completely.\n        - OVERWRITE: In case existing Parquet files need to be rewritten to drop rows that match the delete filter.\n\n    Args:\n        delete_filter: A boolean expression to delete rows from a table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        case_sensitive: A bool determine if the provided `delete_filter` is case-sensitive\n        branch: Branch Reference to run the delete operation\n    \"\"\"\n    from pyiceberg.io.pyarrow import (\n        ArrowScan,\n        _dataframe_to_data_files,\n        _expression_to_complementary_pyarrow,\n    )\n\n    if (\n        self.table_metadata.properties.get(TableProperties.DELETE_MODE, TableProperties.DELETE_MODE_DEFAULT)\n        == TableProperties.DELETE_MODE_MERGE_ON_READ\n    ):\n        warnings.warn(\"Merge on read is not yet supported, falling back to copy-on-write\", stacklevel=2)\n\n    if isinstance(delete_filter, str):\n        delete_filter = _parse_row_filter(delete_filter)\n\n    with self.update_snapshot(snapshot_properties=snapshot_properties, branch=branch).delete() as delete_snapshot:\n        delete_snapshot.delete_by_predicate(delete_filter, case_sensitive)\n\n    # Check if there are any files that require an actual rewrite of a data file\n    if delete_snapshot.rewrites_needed is True:\n        bound_delete_filter = bind(self.table_metadata.schema(), delete_filter, case_sensitive)\n        preserve_row_filter = _expression_to_complementary_pyarrow(bound_delete_filter, self.table_metadata.schema())\n\n        file_scan = self._scan(row_filter=delete_filter, case_sensitive=case_sensitive)\n        if branch is not None:\n            file_scan = file_scan.use_ref(branch)\n        files = file_scan.plan_files()\n\n        commit_uuid = uuid.uuid4()\n        counter = itertools.count(0)\n\n        replaced_files: list[tuple[DataFile, list[DataFile]]] = []\n        # This will load the Parquet file into memory, including:\n        #   - Filter out the rows based on the delete filter\n        #   - Projecting it to the current schema\n        #   - Applying the positional deletes if they are there\n        # When writing\n        #   - Apply the latest partition-spec\n        #   - And sort order when added\n        for original_file in files:\n            df = ArrowScan(\n                table_metadata=self.table_metadata,\n                io=self._table.io,\n                projected_schema=self.table_metadata.schema(),\n                row_filter=AlwaysTrue(),\n            ).to_table(tasks=[original_file])\n            filtered_df = df.filter(preserve_row_filter)\n\n            # Only rewrite if there are records being deleted\n            if len(filtered_df) == 0:\n                replaced_files.append((original_file.file, []))\n            elif len(df) != len(filtered_df):\n                replaced_files.append(\n                    (\n                        original_file.file,\n                        list(\n                            _dataframe_to_data_files(\n                                io=self._table.io,\n                                df=filtered_df,\n                                table_metadata=self.table_metadata,\n                                write_uuid=commit_uuid,\n                                counter=counter,\n                            )\n                        ),\n                    )\n                )\n\n        if len(replaced_files) &gt; 0:\n            with self.update_snapshot(\n                snapshot_properties=snapshot_properties, branch=branch\n            ).overwrite() as overwrite_snapshot:\n                overwrite_snapshot.commit_uuid = commit_uuid\n                for original_data_file, replaced_data_files in replaced_files:\n                    overwrite_snapshot.delete_data_file(original_data_file)\n                    for replaced_data_file in replaced_data_files:\n                        overwrite_snapshot.append_data_file(replaced_data_file)\n\n    if not delete_snapshot.files_affected and not delete_snapshot.rewrites_needed:\n        warnings.warn(\"Delete operation did not match any records\", stacklevel=2)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.dynamic_partition_overwrite","title":"<code>dynamic_partition_overwrite(df, snapshot_properties=EMPTY_DICT, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand for overwriting existing partitions with a PyArrow table.</p> <p>The function detects partition values in the provided arrow table using the current partition spec, and deletes existing partitions matching these values. Finally, the data in the table is appended to the table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be used to overwrite the table</p> required <code>snapshot_properties</code> <code>dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>branch</code> <code>str | None</code> <p>Branch Reference to run the dynamic partition overwrite operation</p> <code>MAIN_BRANCH</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def dynamic_partition_overwrite(\n    self, df: pa.Table, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH\n) -&gt; None:\n    \"\"\"\n    Shorthand for overwriting existing partitions with a PyArrow table.\n\n    The function detects partition values in the provided arrow table using the current\n    partition spec, and deletes existing partitions matching these values. Finally, the\n    data in the table is appended to the table.\n\n    Args:\n        df: The Arrow dataframe that will be used to overwrite the table\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        branch: Branch Reference to run the dynamic partition overwrite operation\n    \"\"\"\n    try:\n        import pyarrow as pa\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n    from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n    if not isinstance(df, pa.Table):\n        raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n    if self.table_metadata.spec().is_unpartitioned():\n        raise ValueError(\"Cannot apply dynamic overwrite on an unpartitioned table.\")\n\n    for field in self.table_metadata.spec().fields:\n        if not isinstance(field.transform, IdentityTransform):\n            raise ValueError(\n                f\"For now dynamic overwrite does not support a table with non-identity-transform field \"\n                f\"in the latest partition spec: {field}\"\n            )\n\n    downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n    _check_pyarrow_schema_compatible(\n        self.table_metadata.schema(),\n        provided_schema=df.schema,\n        downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us,\n        format_version=self.table_metadata.format_version,\n    )\n\n    # If dataframe does not have data, there is no need to overwrite\n    if df.shape[0] == 0:\n        return\n\n    append_snapshot_commit_uuid = uuid.uuid4()\n    data_files: list[DataFile] = list(\n        _dataframe_to_data_files(\n            table_metadata=self._table.metadata, write_uuid=append_snapshot_commit_uuid, df=df, io=self._table.io\n        )\n    )\n\n    partitions_to_overwrite = {data_file.partition for data_file in data_files}\n    delete_filter = self._build_partition_predicate(partition_records=partitions_to_overwrite)\n    self.delete(delete_filter=delete_filter, snapshot_properties=snapshot_properties, branch=branch)\n\n    with self._append_snapshot_producer(snapshot_properties, branch=branch) as append_files:\n        append_files.commit_uuid = append_snapshot_commit_uuid\n        for data_file in data_files:\n            append_files.append_data_file(data_file)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.overwrite","title":"<code>overwrite(df, overwrite_filter=ALWAYS_TRUE, snapshot_properties=EMPTY_DICT, case_sensitive=True, branch=MAIN_BRANCH)</code>","text":"<p>Shorthand for adding a table overwrite with a PyArrow table to the transaction.</p> <p>An overwrite may produce zero or more snapshots based on the operation:</p> <pre><code>- DELETE: In case existing Parquet files can be dropped completely.\n- OVERWRITE: In case existing Parquet files need to be rewritten to drop rows that match the overwrite filter.\n- APPEND: In case new data is being inserted into the table.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Table</code> <p>The Arrow dataframe that will be used to overwrite the table</p> required <code>overwrite_filter</code> <code>BooleanExpression | str</code> <p>ALWAYS_TRUE when you overwrite all the data,               or a boolean expression in case of a partial overwrite</p> <code>ALWAYS_TRUE</code> <code>snapshot_properties</code> <code>dict[str, str]</code> <p>Custom properties to be added to the snapshot summary</p> <code>EMPTY_DICT</code> <code>case_sensitive</code> <code>bool</code> <p>A bool determine if the provided <code>overwrite_filter</code> is case-sensitive</p> <code>True</code> <code>branch</code> <code>str | None</code> <p>Branch Reference to run the overwrite operation</p> <code>MAIN_BRANCH</code> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def overwrite(\n    self,\n    df: pa.Table,\n    overwrite_filter: BooleanExpression | str = ALWAYS_TRUE,\n    snapshot_properties: dict[str, str] = EMPTY_DICT,\n    case_sensitive: bool = True,\n    branch: str | None = MAIN_BRANCH,\n) -&gt; None:\n    \"\"\"\n    Shorthand for adding a table overwrite with a PyArrow table to the transaction.\n\n    An overwrite may produce zero or more snapshots based on the operation:\n\n        - DELETE: In case existing Parquet files can be dropped completely.\n        - OVERWRITE: In case existing Parquet files need to be rewritten to drop rows that match the overwrite filter.\n        - APPEND: In case new data is being inserted into the table.\n\n    Args:\n        df: The Arrow dataframe that will be used to overwrite the table\n        overwrite_filter: ALWAYS_TRUE when you overwrite all the data,\n                          or a boolean expression in case of a partial overwrite\n        snapshot_properties: Custom properties to be added to the snapshot summary\n        case_sensitive: A bool determine if the provided `overwrite_filter` is case-sensitive\n        branch: Branch Reference to run the overwrite operation\n    \"\"\"\n    try:\n        import pyarrow as pa\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n    from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible, _dataframe_to_data_files\n\n    if not isinstance(df, pa.Table):\n        raise ValueError(f\"Expected PyArrow table, got: {df}\")\n\n    downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n    _check_pyarrow_schema_compatible(\n        self.table_metadata.schema(),\n        provided_schema=df.schema,\n        downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us,\n        format_version=self.table_metadata.format_version,\n    )\n\n    if overwrite_filter != AlwaysFalse():\n        # Only delete when the filter is != AlwaysFalse\n        self.delete(\n            delete_filter=overwrite_filter,\n            case_sensitive=case_sensitive,\n            snapshot_properties=snapshot_properties,\n            branch=branch,\n        )\n\n    with self._append_snapshot_producer(snapshot_properties, branch=branch) as append_files:\n        # skip writing data files if the dataframe is empty\n        if df.shape[0] &gt; 0:\n            data_files = _dataframe_to_data_files(\n                table_metadata=self.table_metadata, write_uuid=append_files.commit_uuid, df=df, io=self._table.io\n            )\n            for data_file in data_files:\n                append_files.append_data_file(data_file)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.remove_properties","title":"<code>remove_properties(*removals)</code>","text":"<p>Remove properties.</p> <p>Parameters:</p> Name Type Description Default <code>removals</code> <code>str</code> <p>Properties to be removed.</p> <code>()</code> <p>Returns:</p> Type Description <code>Transaction</code> <p>The alter table builder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def remove_properties(self, *removals: str) -&gt; Transaction:\n    \"\"\"Remove properties.\n\n    Args:\n        removals: Properties to be removed.\n\n    Returns:\n        The alter table builder.\n    \"\"\"\n    return self._apply((RemovePropertiesUpdate(removals=removals),))\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.set_properties","title":"<code>set_properties(properties=EMPTY_DICT, **kwargs)</code>","text":"<p>Set properties.</p> <p>When a property is already set, it will be overwritten.</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <code>Properties</code> <p>The properties set on the table.</p> <code>EMPTY_DICT</code> <code>kwargs</code> <code>Any</code> <p>properties can also be pass as kwargs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Transaction</code> <p>The alter table builder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def set_properties(self, properties: Properties = EMPTY_DICT, **kwargs: Any) -&gt; Transaction:\n    \"\"\"Set properties.\n\n    When a property is already set, it will be overwritten.\n\n    Args:\n        properties: The properties set on the table.\n        kwargs: properties can also be pass as kwargs.\n\n    Returns:\n        The alter table builder.\n    \"\"\"\n    if properties and kwargs:\n        raise ValueError(\"Cannot pass both properties and kwargs\")\n    updates = properties or kwargs\n    return self._apply((SetPropertiesUpdate(updates=updates),))\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_location","title":"<code>update_location(location)</code>","text":"<p>Set the new table location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>The new location of the table.</p> required <p>Returns:</p> Type Description <code>Transaction</code> <p>The alter table builder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_location(self, location: str) -&gt; Transaction:\n    \"\"\"Set the new table location.\n\n    Args:\n        location: The new location of the table.\n\n    Returns:\n        The alter table builder.\n    \"\"\"\n    raise NotImplementedError(\"Not yet implemented\")\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_schema","title":"<code>update_schema(allow_incompatible_changes=False, case_sensitive=True)</code>","text":"<p>Create a new UpdateSchema to alter the columns of this table.</p> <p>Parameters:</p> Name Type Description Default <code>allow_incompatible_changes</code> <code>bool</code> <p>If changes are allowed that might break downstream consumers.</p> <code>False</code> <code>case_sensitive</code> <code>bool</code> <p>If field names are case-sensitive.</p> <code>True</code> <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>A new UpdateSchema.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_schema(self, allow_incompatible_changes: bool = False, case_sensitive: bool = True) -&gt; UpdateSchema:\n    \"\"\"Create a new UpdateSchema to alter the columns of this table.\n\n    Args:\n        allow_incompatible_changes: If changes are allowed that might break downstream consumers.\n        case_sensitive: If field names are case-sensitive.\n\n    Returns:\n        A new UpdateSchema.\n    \"\"\"\n    return UpdateSchema(\n        self,\n        allow_incompatible_changes=allow_incompatible_changes,\n        case_sensitive=case_sensitive,\n        name_mapping=self.table_metadata.name_mapping(),\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_snapshot","title":"<code>update_snapshot(snapshot_properties=EMPTY_DICT, branch=MAIN_BRANCH)</code>","text":"<p>Create a new UpdateSnapshot to produce a new snapshot for the table.</p> <p>Returns:</p> Type Description <code>UpdateSnapshot</code> <p>A new UpdateSnapshot</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_snapshot(\n    self, snapshot_properties: dict[str, str] = EMPTY_DICT, branch: str | None = MAIN_BRANCH\n) -&gt; UpdateSnapshot:\n    \"\"\"Create a new UpdateSnapshot to produce a new snapshot for the table.\n\n    Returns:\n        A new UpdateSnapshot\n    \"\"\"\n    return UpdateSnapshot(self, io=self._table.io, branch=branch, snapshot_properties=snapshot_properties)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_sort_order","title":"<code>update_sort_order(case_sensitive=True)</code>","text":"<p>Create a new UpdateSortOrder to update the sort order of this table.</p> <p>Parameters:</p> Name Type Description Default <code>case_sensitive</code> <code>bool</code> <p>If field names are case-sensitive.</p> <code>True</code> <p>Returns:</p> Type Description <code>UpdateSortOrder</code> <p>A new UpdateSortOrder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_sort_order(self, case_sensitive: bool = True) -&gt; UpdateSortOrder:\n    \"\"\"Create a new UpdateSortOrder to update the sort order of this table.\n\n    Args:\n        case_sensitive: If field names are case-sensitive.\n\n    Returns:\n        A new UpdateSortOrder.\n    \"\"\"\n    return UpdateSortOrder(\n        self,\n        case_sensitive=case_sensitive,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_spec","title":"<code>update_spec()</code>","text":"<p>Create a new UpdateSpec to update the partitioning of the table.</p> <p>Returns:</p> Type Description <code>UpdateSpec</code> <p>A new UpdateSpec.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_spec(self) -&gt; UpdateSpec:\n    \"\"\"Create a new UpdateSpec to update the partitioning of the table.\n\n    Returns:\n        A new UpdateSpec.\n    \"\"\"\n    return UpdateSpec(self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.update_statistics","title":"<code>update_statistics()</code>","text":"<p>Create a new UpdateStatistics to update the statistics of the table.</p> <p>Returns:</p> Type Description <code>UpdateStatistics</code> <p>A new UpdateStatistics</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def update_statistics(self) -&gt; UpdateStatistics:\n    \"\"\"\n    Create a new UpdateStatistics to update the statistics of the table.\n\n    Returns:\n        A new UpdateStatistics\n    \"\"\"\n    return UpdateStatistics(transaction=self)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.upgrade_table_version","title":"<code>upgrade_table_version(format_version)</code>","text":"<p>Set the table to a certain version.</p> <p>Parameters:</p> Name Type Description Default <code>format_version</code> <code>TableVersion</code> <p>The newly set version.</p> required <p>Returns:</p> Type Description <code>Transaction</code> <p>The alter table builder.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def upgrade_table_version(self, format_version: TableVersion) -&gt; Transaction:\n    \"\"\"Set the table to a certain version.\n\n    Args:\n        format_version: The newly set version.\n\n    Returns:\n        The alter table builder.\n    \"\"\"\n    if format_version not in {1, 2}:\n        raise ValueError(f\"Unsupported table format version: {format_version}\")\n\n    if format_version &lt; self.table_metadata.format_version:\n        raise ValueError(f\"Cannot downgrade v{self.table_metadata.format_version} table to v{format_version}\")\n\n    if format_version &gt; self.table_metadata.format_version:\n        return self._apply((UpgradeFormatVersionUpdate(format_version=format_version),))\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.Transaction.upsert","title":"<code>upsert(df, join_cols=None, when_matched_update_all=True, when_not_matched_insert_all=True, case_sensitive=True, branch=MAIN_BRANCH, snapshot_properties=EMPTY_DICT)</code>","text":"<p>Shorthand API for performing an upsert to an iceberg table.</p> <p>Args:</p> <pre><code>df: The input dataframe to upsert with the table's data.\njoin_cols: Columns to join on, if not provided, it will use the identifier-field-ids.\nwhen_matched_update_all: Bool indicating to update rows that are matched but require an update\n    due to a value in a non-key column changing\nwhen_not_matched_insert_all: Bool indicating new rows to be inserted that do not match any\n    existing rows in the table\ncase_sensitive: Bool indicating if the match should be case-sensitive\nbranch: Branch Reference to run the upsert operation\nsnapshot_properties: Custom properties to be added to the snapshot summary\n\nTo learn more about the identifier-field-ids: https://iceberg.apache.org/spec/#identifier-field-ids\n\n    Example Use Cases:\n        Case 1: Both Parameters = True (Full Upsert)\n        Existing row found \u2192 Update it\n        New row found \u2192 Insert it\n\n        Case 2: when_matched_update_all = False, when_not_matched_insert_all = True\n        Existing row found \u2192 Do nothing (no updates)\n        New row found \u2192 Insert it\n\n        Case 3: when_matched_update_all = True, when_not_matched_insert_all = False\n        Existing row found \u2192 Update it\n        New row found \u2192 Do nothing (no inserts)\n\n        Case 4: Both Parameters = False (No Merge Effect)\n        Existing row found \u2192 Do nothing\n        New row found \u2192 Do nothing\n        (Function effectively does nothing)\n</code></pre> <p>Returns:</p> Type Description <code>UpsertResult</code> <p>An UpsertResult class (contains details of rows updated and inserted)</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>def upsert(\n    self,\n    df: pa.Table,\n    join_cols: list[str] | None = None,\n    when_matched_update_all: bool = True,\n    when_not_matched_insert_all: bool = True,\n    case_sensitive: bool = True,\n    branch: str | None = MAIN_BRANCH,\n    snapshot_properties: dict[str, str] = EMPTY_DICT,\n) -&gt; UpsertResult:\n    \"\"\"Shorthand API for performing an upsert to an iceberg table.\n\n    Args:\n\n        df: The input dataframe to upsert with the table's data.\n        join_cols: Columns to join on, if not provided, it will use the identifier-field-ids.\n        when_matched_update_all: Bool indicating to update rows that are matched but require an update\n            due to a value in a non-key column changing\n        when_not_matched_insert_all: Bool indicating new rows to be inserted that do not match any\n            existing rows in the table\n        case_sensitive: Bool indicating if the match should be case-sensitive\n        branch: Branch Reference to run the upsert operation\n        snapshot_properties: Custom properties to be added to the snapshot summary\n\n        To learn more about the identifier-field-ids: https://iceberg.apache.org/spec/#identifier-field-ids\n\n            Example Use Cases:\n                Case 1: Both Parameters = True (Full Upsert)\n                Existing row found \u2192 Update it\n                New row found \u2192 Insert it\n\n                Case 2: when_matched_update_all = False, when_not_matched_insert_all = True\n                Existing row found \u2192 Do nothing (no updates)\n                New row found \u2192 Insert it\n\n                Case 3: when_matched_update_all = True, when_not_matched_insert_all = False\n                Existing row found \u2192 Update it\n                New row found \u2192 Do nothing (no inserts)\n\n                Case 4: Both Parameters = False (No Merge Effect)\n                Existing row found \u2192 Do nothing\n                New row found \u2192 Do nothing\n                (Function effectively does nothing)\n\n\n    Returns:\n        An UpsertResult class (contains details of rows updated and inserted)\n    \"\"\"\n    try:\n        import pyarrow as pa  # noqa: F401\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\"For writes PyArrow needs to be installed\") from e\n\n    from pyiceberg.io.pyarrow import expression_to_pyarrow\n    from pyiceberg.table import upsert_util\n\n    if join_cols is None:\n        join_cols = []\n        for field_id in self.table_metadata.schema().identifier_field_ids:\n            col = self.table_metadata.schema().find_column_name(field_id)\n            if col is not None:\n                join_cols.append(col)\n            else:\n                raise ValueError(f\"Field-ID could not be found: {join_cols}\")\n\n    if len(join_cols) == 0:\n        raise ValueError(\"Join columns could not be found, please set identifier-field-ids or pass in explicitly.\")\n\n    if not when_matched_update_all and not when_not_matched_insert_all:\n        raise ValueError(\"no upsert options selected...exiting\")\n\n    if upsert_util.has_duplicate_rows(df, join_cols):\n        raise ValueError(\"Duplicate rows found in source dataset based on the key columns. No upsert executed\")\n\n    from pyiceberg.io.pyarrow import _check_pyarrow_schema_compatible\n\n    downcast_ns_timestamp_to_us = Config().get_bool(DOWNCAST_NS_TIMESTAMP_TO_US_ON_WRITE) or False\n    _check_pyarrow_schema_compatible(\n        self.table_metadata.schema(),\n        provided_schema=df.schema,\n        downcast_ns_timestamp_to_us=downcast_ns_timestamp_to_us,\n        format_version=self.table_metadata.format_version,\n    )\n\n    # get list of rows that exist so we don't have to load the entire target table\n    matched_predicate = upsert_util.create_match_filter(df, join_cols)\n\n    # We must use Transaction.table_metadata for the scan. This includes all uncommitted - but relevant - changes.\n\n    matched_iceberg_record_batches_scan = DataScan(\n        table_metadata=self.table_metadata,\n        io=self._table.io,\n        row_filter=matched_predicate,\n        case_sensitive=case_sensitive,\n    )\n\n    if branch in self.table_metadata.refs:\n        matched_iceberg_record_batches_scan = matched_iceberg_record_batches_scan.use_ref(branch)\n\n    matched_iceberg_record_batches = matched_iceberg_record_batches_scan.to_arrow_batch_reader()\n\n    batches_to_overwrite = []\n    overwrite_predicates = []\n    rows_to_insert = df\n\n    for batch in matched_iceberg_record_batches:\n        rows = pa.Table.from_batches([batch])\n\n        if when_matched_update_all:\n            # function get_rows_to_update is doing a check on non-key columns to see if any of the\n            # values have actually changed. We don't want to do just a blanket overwrite for matched\n            # rows if the actual non-key column data hasn't changed.\n            # this extra step avoids unnecessary IO and writes\n            rows_to_update = upsert_util.get_rows_to_update(df, rows, join_cols)\n\n            if len(rows_to_update) &gt; 0:\n                # build the match predicate filter\n                overwrite_mask_predicate = upsert_util.create_match_filter(rows_to_update, join_cols)\n\n                batches_to_overwrite.append(rows_to_update)\n                overwrite_predicates.append(overwrite_mask_predicate)\n\n        if when_not_matched_insert_all:\n            expr_match = upsert_util.create_match_filter(rows, join_cols)\n            expr_match_bound = bind(self.table_metadata.schema(), expr_match, case_sensitive=case_sensitive)\n            expr_match_arrow = expression_to_pyarrow(expr_match_bound)\n\n            # Filter rows per batch.\n            rows_to_insert = rows_to_insert.filter(~expr_match_arrow)\n\n    update_row_cnt = 0\n    insert_row_cnt = 0\n\n    if batches_to_overwrite:\n        rows_to_update = pa.concat_tables(batches_to_overwrite)\n        update_row_cnt = len(rows_to_update)\n        self.overwrite(\n            rows_to_update,\n            overwrite_filter=Or(*overwrite_predicates) if len(overwrite_predicates) &gt; 1 else overwrite_predicates[0],\n            branch=branch,\n            snapshot_properties=snapshot_properties,\n        )\n\n    if when_not_matched_insert_all:\n        insert_row_cnt = len(rows_to_insert)\n        if rows_to_insert:\n            self.append(rows_to_insert, branch=branch, snapshot_properties=snapshot_properties)\n\n    return UpsertResult(rows_updated=update_row_cnt, rows_inserted=insert_row_cnt)\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.UpsertResult","title":"<code>UpsertResult</code>  <code>dataclass</code>","text":"<p>Summary the upsert operation.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>@dataclass()\nclass UpsertResult:\n    \"\"\"Summary the upsert operation.\"\"\"\n\n    rows_updated: int = 0\n    rows_inserted: int = 0\n</code></pre>"},{"location":"reference/pyiceberg/table/#pyiceberg.table.WriteTask","title":"<code>WriteTask</code>  <code>dataclass</code>","text":"<p>Task with the parameters for writing a DataFile.</p> Source code in <code>pyiceberg/table/__init__.py</code> <pre><code>@dataclass(frozen=True)\nclass WriteTask:\n    \"\"\"Task with the parameters for writing a DataFile.\"\"\"\n\n    write_uuid: uuid.UUID\n    task_id: int\n    schema: Schema\n    record_batches: list[pa.RecordBatch]\n    sort_order_id: int | None = None\n    partition_key: PartitionKey | None = None\n\n    def generate_data_file_filename(self, extension: str) -&gt; str:\n        # Mimics the behavior in the Java API:\n        # https://github.com/apache/iceberg/blob/a582968975dd30ff4917fbbe999f1be903efac02/core/src/main/java/org/apache/iceberg/io/OutputFileFactory.java#L92-L101\n        return f\"00000-{self.task_id}-{self.write_uuid}.{extension}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/delete_file_index/","title":"delete_file_index","text":""},{"location":"reference/pyiceberg/table/delete_file_index/#pyiceberg.table.delete_file_index.DeleteFileIndex","title":"<code>DeleteFileIndex</code>","text":"<p>Indexes position delete files by partition and by exact data file path.</p> Source code in <code>pyiceberg/table/delete_file_index.py</code> <pre><code>class DeleteFileIndex:\n    \"\"\"Indexes position delete files by partition and by exact data file path.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._by_partition: dict[tuple[int, Record], PositionDeletes] = {}\n        self._by_path: dict[str, PositionDeletes] = {}\n\n    def is_empty(self) -&gt; bool:\n        return not self._by_partition and not self._by_path\n\n    def add_delete_file(self, manifest_entry: ManifestEntry, partition_key: Record | None = None) -&gt; None:\n        delete_file = manifest_entry.data_file\n        seq = manifest_entry.sequence_number or INITIAL_SEQUENCE_NUMBER\n        target_path = _referenced_data_file_path(delete_file)\n\n        if target_path:\n            deletes = self._by_path.setdefault(target_path, PositionDeletes())\n            deletes.add(delete_file, seq)\n        else:\n            key = _partition_key(delete_file.spec_id or 0, partition_key)\n            deletes = self._by_partition.setdefault(key, PositionDeletes())\n            deletes.add(delete_file, seq)\n\n    def for_data_file(self, seq_num: int, data_file: DataFile, partition_key: Record | None = None) -&gt; set[DataFile]:\n        if self.is_empty():\n            return set()\n\n        deletes: set[DataFile] = set()\n        spec_id = data_file.spec_id or 0\n\n        key = _partition_key(spec_id, partition_key)\n        partition_deletes = self._by_partition.get(key)\n        if partition_deletes:\n            for delete_file in partition_deletes.filter_by_seq(seq_num):\n                if _applies_to_data_file(delete_file, data_file):\n                    deletes.add(delete_file)\n\n        path_deletes = self._by_path.get(data_file.file_path)\n        if path_deletes:\n            deletes.update(path_deletes.filter_by_seq(seq_num))\n\n        return deletes\n</code></pre>"},{"location":"reference/pyiceberg/table/delete_file_index/#pyiceberg.table.delete_file_index.PositionDeletes","title":"<code>PositionDeletes</code>","text":"<p>Collects position delete files and indexes them by sequence number.</p> Source code in <code>pyiceberg/table/delete_file_index.py</code> <pre><code>class PositionDeletes:\n    \"\"\"Collects position delete files and indexes them by sequence number.\"\"\"\n\n    __slots__ = (\"_buffer\", \"_seqs\", \"_files\")\n\n    def __init__(self) -&gt; None:\n        self._buffer: list[tuple[DataFile, int]] | None = []\n        self._seqs: list[int] = []\n        self._files: list[tuple[DataFile, int]] = []\n\n    def add(self, delete_file: DataFile, seq_num: int) -&gt; None:\n        if self._buffer is None:\n            raise ValueError(\"Cannot add files after indexing\")\n        self._buffer.append((delete_file, seq_num))\n\n    def _ensure_indexed(self) -&gt; None:\n        if self._buffer is not None:\n            self._files = sorted(self._buffer, key=lambda file: file[1])\n            self._seqs = [seq for _, seq in self._files]\n            self._buffer = None\n\n    def filter_by_seq(self, seq: int) -&gt; list[DataFile]:\n        self._ensure_indexed()\n        if not self._files:\n            return []\n        start_idx = bisect_left(self._seqs, seq)\n        return [delete_file for delete_file, _ in self._files[start_idx:]]\n</code></pre>"},{"location":"reference/pyiceberg/table/inspect/","title":"inspect","text":""},{"location":"reference/pyiceberg/table/locations/","title":"locations","text":""},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.LocationProvider","title":"<code>LocationProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A base class for location providers, that provide file locations for a table's write tasks.</p> <p>Parameters:</p> Name Type Description Default <code>table_location</code> <code>str</code> <p>The table's base storage location.</p> required <code>table_properties</code> <code>Properties</code> <p>The table's properties.</p> required Source code in <code>pyiceberg/table/locations.py</code> <pre><code>class LocationProvider(ABC):\n    \"\"\"A base class for location providers, that provide file locations for a table's write tasks.\n\n    Args:\n        table_location (str): The table's base storage location.\n        table_properties (Properties): The table's properties.\n    \"\"\"\n\n    table_location: str\n    table_properties: Properties\n\n    data_path: str\n    metadata_path: str\n\n    def __init__(self, table_location: str, table_properties: Properties):\n        self.table_location = table_location\n        self.table_properties = table_properties\n\n        from pyiceberg.table import TableProperties\n\n        if path := table_properties.get(TableProperties.WRITE_DATA_PATH):\n            self.data_path = path.rstrip(\"/\")\n        else:\n            self.data_path = f\"{self.table_location.rstrip('/')}/data\"\n\n        if path := table_properties.get(TableProperties.WRITE_METADATA_PATH):\n            self.metadata_path = path.rstrip(\"/\")\n        else:\n            self.metadata_path = f\"{self.table_location.rstrip('/')}/metadata\"\n\n    @abstractmethod\n    def new_data_location(self, data_file_name: str, partition_key: PartitionKey | None = None) -&gt; str:\n        \"\"\"Return a fully-qualified data file location for the given filename.\n\n        Args:\n            data_file_name (str): The name of the data file.\n            partition_key (Optional[PartitionKey]): The data file's partition key. If None, the data is not partitioned.\n\n        Returns:\n            str: A fully-qualified location URI for the data file.\n        \"\"\"\n\n    def new_table_metadata_file_location(self, new_version: int = 0) -&gt; str:\n        \"\"\"Return a fully-qualified metadata file location for a new table version.\n\n        Args:\n            new_version (int): Version number of the metadata file.\n\n        Returns:\n            str: fully-qualified URI for the new table metadata file.\n\n        Raises:\n            ValueError: If the version is negative.\n        \"\"\"\n        if new_version &lt; 0:\n            raise ValueError(f\"Table metadata version: `{new_version}` must be a non-negative integer\")\n\n        file_name = f\"{new_version:05d}-{uuid.uuid4()}.metadata.json\"\n        return self.new_metadata_location(file_name)\n\n    def new_metadata_location(self, metadata_file_name: str) -&gt; str:\n        \"\"\"Return a fully-qualified metadata file location for the given filename.\n\n        Args:\n            metadata_file_name (str): Name of the metadata file.\n\n        Returns:\n            str: A fully-qualified location URI for the metadata file.\n        \"\"\"\n        return f\"{self.metadata_path}/{metadata_file_name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.LocationProvider.new_data_location","title":"<code>new_data_location(data_file_name, partition_key=None)</code>  <code>abstractmethod</code>","text":"<p>Return a fully-qualified data file location for the given filename.</p> <p>Parameters:</p> Name Type Description Default <code>data_file_name</code> <code>str</code> <p>The name of the data file.</p> required <code>partition_key</code> <code>Optional[PartitionKey]</code> <p>The data file's partition key. If None, the data is not partitioned.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A fully-qualified location URI for the data file.</p> Source code in <code>pyiceberg/table/locations.py</code> <pre><code>@abstractmethod\ndef new_data_location(self, data_file_name: str, partition_key: PartitionKey | None = None) -&gt; str:\n    \"\"\"Return a fully-qualified data file location for the given filename.\n\n    Args:\n        data_file_name (str): The name of the data file.\n        partition_key (Optional[PartitionKey]): The data file's partition key. If None, the data is not partitioned.\n\n    Returns:\n        str: A fully-qualified location URI for the data file.\n    \"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.LocationProvider.new_metadata_location","title":"<code>new_metadata_location(metadata_file_name)</code>","text":"<p>Return a fully-qualified metadata file location for the given filename.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_file_name</code> <code>str</code> <p>Name of the metadata file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A fully-qualified location URI for the metadata file.</p> Source code in <code>pyiceberg/table/locations.py</code> <pre><code>def new_metadata_location(self, metadata_file_name: str) -&gt; str:\n    \"\"\"Return a fully-qualified metadata file location for the given filename.\n\n    Args:\n        metadata_file_name (str): Name of the metadata file.\n\n    Returns:\n        str: A fully-qualified location URI for the metadata file.\n    \"\"\"\n    return f\"{self.metadata_path}/{metadata_file_name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.LocationProvider.new_table_metadata_file_location","title":"<code>new_table_metadata_file_location(new_version=0)</code>","text":"<p>Return a fully-qualified metadata file location for a new table version.</p> <p>Parameters:</p> Name Type Description Default <code>new_version</code> <code>int</code> <p>Version number of the metadata file.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>fully-qualified URI for the new table metadata file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the version is negative.</p> Source code in <code>pyiceberg/table/locations.py</code> <pre><code>def new_table_metadata_file_location(self, new_version: int = 0) -&gt; str:\n    \"\"\"Return a fully-qualified metadata file location for a new table version.\n\n    Args:\n        new_version (int): Version number of the metadata file.\n\n    Returns:\n        str: fully-qualified URI for the new table metadata file.\n\n    Raises:\n        ValueError: If the version is negative.\n    \"\"\"\n    if new_version &lt; 0:\n        raise ValueError(f\"Table metadata version: `{new_version}` must be a non-negative integer\")\n\n    file_name = f\"{new_version:05d}-{uuid.uuid4()}.metadata.json\"\n    return self.new_metadata_location(file_name)\n</code></pre>"},{"location":"reference/pyiceberg/table/locations/#pyiceberg.table.locations.ObjectStoreLocationProvider","title":"<code>ObjectStoreLocationProvider</code>","text":"<p>               Bases: <code>LocationProvider</code></p> Source code in <code>pyiceberg/table/locations.py</code> <pre><code>class ObjectStoreLocationProvider(LocationProvider):\n    HASH_BINARY_STRING_BITS = 20\n    ENTROPY_DIR_LENGTH = 4\n    ENTROPY_DIR_DEPTH = 3\n\n    _include_partition_paths: bool\n\n    def __init__(self, table_location: str, table_properties: Properties):\n        super().__init__(table_location, table_properties)\n        from pyiceberg.table import TableProperties\n\n        self._include_partition_paths = property_as_bool(\n            self.table_properties,\n            TableProperties.WRITE_OBJECT_STORE_PARTITIONED_PATHS,\n            TableProperties.WRITE_OBJECT_STORE_PARTITIONED_PATHS_DEFAULT,\n        )\n\n    def new_data_location(self, data_file_name: str, partition_key: PartitionKey | None = None) -&gt; str:\n        if self._include_partition_paths and partition_key:\n            return self.new_data_location(f\"{partition_key.to_path()}/{data_file_name}\")\n\n        hashed_path = self._compute_hash(data_file_name)\n\n        return (\n            f\"{self.data_path}/{hashed_path}/{data_file_name}\"\n            if self._include_partition_paths\n            else f\"{self.data_path}/{hashed_path}-{data_file_name}\"\n        )\n\n    @staticmethod\n    def _compute_hash(data_file_name: str) -&gt; str:\n        # Bitwise AND to combat sign-extension; bitwise OR to preserve leading zeroes that `bin` would otherwise strip.\n        top_mask = 1 &lt;&lt; ObjectStoreLocationProvider.HASH_BINARY_STRING_BITS\n        hash_code = mmh3.hash(data_file_name) &amp; (top_mask - 1) | top_mask\n        return ObjectStoreLocationProvider._dirs_from_hash(bin(hash_code)[-ObjectStoreLocationProvider.HASH_BINARY_STRING_BITS :])\n\n    @staticmethod\n    def _dirs_from_hash(file_hash: str) -&gt; str:\n        \"\"\"Divides hash into directories for optimized orphan removal operation using ENTROPY_DIR_DEPTH and ENTROPY_DIR_LENGTH.\"\"\"\n        total_entropy_length = ObjectStoreLocationProvider.ENTROPY_DIR_DEPTH * ObjectStoreLocationProvider.ENTROPY_DIR_LENGTH\n\n        hash_with_dirs = []\n        for i in range(0, total_entropy_length, ObjectStoreLocationProvider.ENTROPY_DIR_LENGTH):\n            hash_with_dirs.append(file_hash[i : i + ObjectStoreLocationProvider.ENTROPY_DIR_LENGTH])\n\n        if len(file_hash) &gt; total_entropy_length:\n            hash_with_dirs.append(file_hash[total_entropy_length:])\n\n        return \"/\".join(hash_with_dirs)\n</code></pre>"},{"location":"reference/pyiceberg/table/maintenance/","title":"maintenance","text":""},{"location":"reference/pyiceberg/table/maintenance/#pyiceberg.table.maintenance.MaintenanceTable","title":"<code>MaintenanceTable</code>","text":"Source code in <code>pyiceberg/table/maintenance.py</code> <pre><code>class MaintenanceTable:\n    tbl: Table\n\n    def __init__(self, tbl: Table) -&gt; None:\n        self.tbl = tbl\n\n    def expire_snapshots(self) -&gt; ExpireSnapshots:\n        \"\"\"Return an ExpireSnapshots builder for snapshot expiration operations.\n\n        Returns:\n            ExpireSnapshots builder for configuring and executing snapshot expiration.\n        \"\"\"\n        from pyiceberg.table import Transaction\n        from pyiceberg.table.update.snapshot import ExpireSnapshots\n\n        return ExpireSnapshots(transaction=Transaction(self.tbl, autocommit=True))\n</code></pre>"},{"location":"reference/pyiceberg/table/maintenance/#pyiceberg.table.maintenance.MaintenanceTable.expire_snapshots","title":"<code>expire_snapshots()</code>","text":"<p>Return an ExpireSnapshots builder for snapshot expiration operations.</p> <p>Returns:</p> Type Description <code>ExpireSnapshots</code> <p>ExpireSnapshots builder for configuring and executing snapshot expiration.</p> Source code in <code>pyiceberg/table/maintenance.py</code> <pre><code>def expire_snapshots(self) -&gt; ExpireSnapshots:\n    \"\"\"Return an ExpireSnapshots builder for snapshot expiration operations.\n\n    Returns:\n        ExpireSnapshots builder for configuring and executing snapshot expiration.\n    \"\"\"\n    from pyiceberg.table import Transaction\n    from pyiceberg.table.update.snapshot import ExpireSnapshots\n\n    return ExpireSnapshots(transaction=Transaction(self.tbl, autocommit=True))\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/","title":"metadata","text":""},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields","title":"<code>TableMetadataCommonFields</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Metadata for an Iceberg table as specified in the Apache Iceberg spec.</p> <p>https://iceberg.apache.org/spec/#iceberg-table-spec</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataCommonFields(IcebergBaseModel):\n    \"\"\"Metadata for an Iceberg table as specified in the Apache Iceberg spec.\n\n    https://iceberg.apache.org/spec/#iceberg-table-spec\n    \"\"\"\n\n    location: str = Field()\n    \"\"\"The table\u2019s base location. This is used by writers to determine where\n    to store data files, manifest files, and table metadata files.\"\"\"\n\n    table_uuid: uuid.UUID = Field(alias=\"table-uuid\", default_factory=uuid.uuid4)\n    \"\"\"A UUID that identifies the table, generated when the table is created.\n    Implementations must throw an exception if a table\u2019s UUID does not match\n    the expected UUID after refreshing metadata.\"\"\"\n\n    last_updated_ms: int = Field(\n        alias=\"last-updated-ms\", default_factory=lambda: datetime_to_millis(datetime.datetime.now().astimezone())\n    )\n    \"\"\"Timestamp in milliseconds from the unix epoch when the table\n    was last updated. Each table metadata file should update this\n    field just before writing.\"\"\"\n\n    last_column_id: int = Field(alias=\"last-column-id\")\n    \"\"\"An integer; the highest assigned column ID for the table.\n    This is used to ensure fields are always assigned an unused ID\n    when evolving schemas.\"\"\"\n\n    schemas: list[Schema] = Field(default_factory=list)\n    \"\"\"A list of schemas, stored as objects with schema-id.\"\"\"\n\n    current_schema_id: int = Field(alias=\"current-schema-id\", default=DEFAULT_SCHEMA_ID)\n    \"\"\"ID of the table\u2019s current schema.\"\"\"\n\n    partition_specs: list[PartitionSpec] = Field(alias=\"partition-specs\", default_factory=list)\n    \"\"\"A list of partition specs, stored as full partition spec objects.\"\"\"\n\n    default_spec_id: int = Field(alias=\"default-spec-id\", default=INITIAL_SPEC_ID)\n    \"\"\"ID of the \u201ccurrent\u201d spec that writers should use by default.\"\"\"\n\n    last_partition_id: int | None = Field(alias=\"last-partition-id\", default=None)\n    \"\"\"An integer; the highest assigned partition field ID across all\n    partition specs for the table. This is used to ensure partition fields\n    are always assigned an unused ID when evolving specs.\"\"\"\n\n    properties: dict[str, str] = Field(default_factory=dict)\n    \"\"\"A string to string map of table properties. This is used to\n    control settings that affect reading and writing and is not intended\n    to be used for arbitrary metadata. For example, commit.retry.num-retries\n    is used to control the number of commit retries.\"\"\"\n\n    current_snapshot_id: int | None = Field(alias=\"current-snapshot-id\", default=None)\n    \"\"\"ID of the current table snapshot.\"\"\"\n\n    snapshots: list[Snapshot] = Field(default_factory=list)\n    \"\"\"A list of valid snapshots. Valid snapshots are snapshots for which\n    all data files exist in the file system. A data file must not be\n    deleted from the file system until the last snapshot in which it was\n    listed is garbage collected.\"\"\"\n\n    snapshot_log: list[SnapshotLogEntry] = Field(alias=\"snapshot-log\", default_factory=list)\n    \"\"\"A list (optional) of timestamp and snapshot ID pairs that encodes\n    changes to the current snapshot for the table. Each time the\n    current-snapshot-id is changed, a new entry should be added with the\n    last-updated-ms and the new current-snapshot-id. When snapshots are\n    expired from the list of valid snapshots, all entries before a snapshot\n    that has expired should be removed.\"\"\"\n\n    metadata_log: list[MetadataLogEntry] = Field(alias=\"metadata-log\", default_factory=list)\n    \"\"\"A list (optional) of timestamp and metadata file location pairs that\n    encodes changes to the previous metadata files for the table. Each time\n    a new metadata file is created, a new entry of the previous metadata\n    file location should be added to the list. Tables can be configured to\n    remove oldest metadata log entries and keep a fixed-size log of the most\n    recent entries after a commit.\"\"\"\n\n    sort_orders: list[SortOrder] = Field(alias=\"sort-orders\", default_factory=list)\n    \"\"\"A list of sort orders, stored as full sort order objects.\"\"\"\n\n    default_sort_order_id: int = Field(alias=\"default-sort-order-id\", default=UNSORTED_SORT_ORDER_ID)\n    \"\"\"Default sort order id of the table. Note that this could be used by\n    writers, but is not used when reading because reads use the specs stored\n     in manifest files.\"\"\"\n\n    refs: dict[str, SnapshotRef] = Field(default_factory=dict)\n    \"\"\"A map of snapshot references.\n    The map keys are the unique snapshot reference names in the table,\n    and the map values are snapshot reference objects.\n    There is always a main branch reference pointing to the\n    current-snapshot-id even if the refs map is null.\"\"\"\n\n    statistics: list[StatisticsFile] = Field(default_factory=list)\n    \"\"\"A optional list of table statistics files.\n    Table statistics files are valid Puffin files. Statistics are\n    informational. A reader can choose to ignore statistics\n    information. Statistics support is not required to read the\n    table correctly. A table can contain many statistics files\n    associated with different table snapshots.\"\"\"\n\n    partition_statistics: list[PartitionStatisticsFile] = Field(alias=\"partition-statistics\", default_factory=list)\n    \"\"\"A optional list of partition statistics files.\n    Partition statistics are not required for reading or planning\n    and readers may ignore them. Each table snapshot may be associated\n    with at most one partition statistics file. A writer can optionally\n    write the partition statistics file during each write operation,\n    or it can also be computed on demand.\"\"\"\n\n    # validators\n    @field_validator(\"properties\", mode=\"before\")\n    def transform_properties_dict_value_to_str(cls, properties: Properties) -&gt; dict[str, str]:\n        return transform_dict_value_to_str(properties)\n\n    def snapshot_by_id(self, snapshot_id: int) -&gt; Snapshot | None:\n        \"\"\"Get the snapshot by snapshot_id.\"\"\"\n        return next((snapshot for snapshot in self.snapshots if snapshot.snapshot_id == snapshot_id), None)\n\n    def schema_by_id(self, schema_id: int) -&gt; Schema | None:\n        \"\"\"Get the schema by schema_id.\"\"\"\n        return next((schema for schema in self.schemas if schema.schema_id == schema_id), None)\n\n    def schema(self) -&gt; Schema:\n        \"\"\"Return the schema for this table.\"\"\"\n        return next(schema for schema in self.schemas if schema.schema_id == self.current_schema_id)\n\n    def name_mapping(self) -&gt; NameMapping | None:\n        \"\"\"Return the table's field-id NameMapping.\"\"\"\n        if name_mapping_json := self.properties.get(\"schema.name-mapping.default\"):\n            return parse_mapping_from_json(name_mapping_json)\n        else:\n            return None\n\n    def spec(self) -&gt; PartitionSpec:\n        \"\"\"Return the partition spec of this table.\"\"\"\n        return next(spec for spec in self.partition_specs if spec.spec_id == self.default_spec_id)\n\n    def specs(self) -&gt; dict[int, PartitionSpec]:\n        \"\"\"Return a dict the partition specs this table.\"\"\"\n        return {spec.spec_id: spec for spec in self.partition_specs}\n\n    def specs_struct(self, spec_ids: Iterable[int] | None = None) -&gt; StructType:\n        \"\"\"Produce a struct of the combined PartitionSpecs.\n\n        The partition fields should be optional: Partition fields may be added later,\n        in which case not all files would have the result field, and it may be null.\n\n        Args:\n            spec_ids: Optional iterable of spec IDs to include. When not provided,\n                all table specs are used.\n\n        :return: A StructType that represents the combined PartitionSpecs of the table\n        \"\"\"\n        specs = self.specs()\n        selected_specs = specs.values() if spec_ids is None else [specs[spec_id] for spec_id in spec_ids if spec_id in specs]\n\n        # Collect all the fields\n        struct_fields = {field.field_id: field for spec in selected_specs for field in spec.fields}\n\n        schema = self.schema()\n\n        nested_fields = []\n        # Sort them by field_id in order to get a deterministic output\n        for field_id in sorted(struct_fields):\n            field = struct_fields[field_id]\n            source_type = schema.find_type(field.source_id)\n            result_type = field.transform.result_type(source_type)\n            nested_fields.append(NestedField(field_id=field.field_id, name=field.name, type=result_type, required=False))\n\n        return StructType(*nested_fields)\n\n    def new_snapshot_id(self) -&gt; int:\n        \"\"\"Generate a new snapshot-id that's not in use.\"\"\"\n        snapshot_id = _generate_snapshot_id()\n        while self.snapshot_by_id(snapshot_id) is not None:\n            snapshot_id = _generate_snapshot_id()\n\n        return snapshot_id\n\n    def snapshot_by_name(self, name: str | None) -&gt; Snapshot | None:\n        \"\"\"Return the snapshot referenced by the given name or null if no such reference exists.\"\"\"\n        if name is None:\n            name = MAIN_BRANCH\n        if ref := self.refs.get(name):\n            return self.snapshot_by_id(ref.snapshot_id)\n        return None\n\n    def current_snapshot(self) -&gt; Snapshot | None:\n        \"\"\"Get the current snapshot for this table, or None if there is no current snapshot.\"\"\"\n        if self.current_snapshot_id is not None:\n            return self.snapshot_by_id(self.current_snapshot_id)\n        return None\n\n    def next_sequence_number(self) -&gt; int:\n        return self.last_sequence_number + 1 if self.format_version &gt; 1 else INITIAL_SEQUENCE_NUMBER\n\n    def sort_order(self) -&gt; SortOrder:\n        \"\"\"Get the current sort order for this table, or UNSORTED_SORT_ORDER if there is no sort order.\"\"\"\n        return self.sort_order_by_id(self.default_sort_order_id) or UNSORTED_SORT_ORDER\n\n    def sort_order_by_id(self, sort_order_id: int) -&gt; SortOrder | None:\n        \"\"\"Get the sort order by sort_order_id.\"\"\"\n        return next((sort_order for sort_order in self.sort_orders if sort_order.order_id == sort_order_id), None)\n\n    @field_serializer(\"current_snapshot_id\")\n    def serialize_current_snapshot_id(self, current_snapshot_id: int | None) -&gt; int | None:\n        if current_snapshot_id is None and Config().get_bool(\"legacy-current-snapshot-id\"):\n            return -1\n        return current_snapshot_id\n\n    @field_serializer(\"snapshots\")\n    def serialize_snapshots(self, snapshots: list[Snapshot]) -&gt; list[Snapshot]:\n        # Snapshot field `sequence-number` should not be written for v1 metadata\n        if self.format_version == 1:\n            return [snapshot.model_copy(update={\"sequence_number\": None}) for snapshot in snapshots]\n        return snapshots\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.current_schema_id","title":"<code>current_schema_id = Field(alias='current-schema-id', default=DEFAULT_SCHEMA_ID)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ID of the table\u2019s current schema.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.current_snapshot_id","title":"<code>current_snapshot_id = Field(alias='current-snapshot-id', default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ID of the current table snapshot.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.default_sort_order_id","title":"<code>default_sort_order_id = Field(alias='default-sort-order-id', default=UNSORTED_SORT_ORDER_ID)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default sort order id of the table. Note that this could be used by writers, but is not used when reading because reads use the specs stored  in manifest files.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.default_spec_id","title":"<code>default_spec_id = Field(alias='default-spec-id', default=INITIAL_SPEC_ID)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ID of the \u201ccurrent\u201d spec that writers should use by default.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.last_column_id","title":"<code>last_column_id = Field(alias='last-column-id')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer; the highest assigned column ID for the table. This is used to ensure fields are always assigned an unused ID when evolving schemas.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.last_partition_id","title":"<code>last_partition_id = Field(alias='last-partition-id', default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer; the highest assigned partition field ID across all partition specs for the table. This is used to ensure partition fields are always assigned an unused ID when evolving specs.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.last_updated_ms","title":"<code>last_updated_ms = Field(alias='last-updated-ms', default_factory=(lambda: datetime_to_millis(datetime.datetime.now().astimezone())))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Timestamp in milliseconds from the unix epoch when the table was last updated. Each table metadata file should update this field just before writing.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.location","title":"<code>location = Field()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s base location. This is used by writers to determine where to store data files, manifest files, and table metadata files.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.metadata_log","title":"<code>metadata_log = Field(alias='metadata-log', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list (optional) of timestamp and metadata file location pairs that encodes changes to the previous metadata files for the table. Each time a new metadata file is created, a new entry of the previous metadata file location should be added to the list. Tables can be configured to remove oldest metadata log entries and keep a fixed-size log of the most recent entries after a commit.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.partition_specs","title":"<code>partition_specs = Field(alias='partition-specs', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of partition specs, stored as full partition spec objects.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.partition_statistics","title":"<code>partition_statistics = Field(alias='partition-statistics', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A optional list of partition statistics files. Partition statistics are not required for reading or planning and readers may ignore them. Each table snapshot may be associated with at most one partition statistics file. A writer can optionally write the partition statistics file during each write operation, or it can also be computed on demand.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.properties","title":"<code>properties = Field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A string to string map of table properties. This is used to control settings that affect reading and writing and is not intended to be used for arbitrary metadata. For example, commit.retry.num-retries is used to control the number of commit retries.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.refs","title":"<code>refs = Field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A map of snapshot references. The map keys are the unique snapshot reference names in the table, and the map values are snapshot reference objects. There is always a main branch reference pointing to the current-snapshot-id even if the refs map is null.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.schemas","title":"<code>schemas = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of schemas, stored as objects with schema-id.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.snapshot_log","title":"<code>snapshot_log = Field(alias='snapshot-log', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list (optional) of timestamp and snapshot ID pairs that encodes changes to the current snapshot for the table. Each time the current-snapshot-id is changed, a new entry should be added with the last-updated-ms and the new current-snapshot-id. When snapshots are expired from the list of valid snapshots, all entries before a snapshot that has expired should be removed.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.snapshots","title":"<code>snapshots = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of valid snapshots. Valid snapshots are snapshots for which all data files exist in the file system. A data file must not be deleted from the file system until the last snapshot in which it was listed is garbage collected.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.sort_orders","title":"<code>sort_orders = Field(alias='sort-orders', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A list of sort orders, stored as full sort order objects.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.statistics","title":"<code>statistics = Field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A optional list of table statistics files. Table statistics files are valid Puffin files. Statistics are informational. A reader can choose to ignore statistics information. Statistics support is not required to read the table correctly. A table can contain many statistics files associated with different table snapshots.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.table_uuid","title":"<code>table_uuid = Field(alias='table-uuid', default_factory=(uuid.uuid4))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A UUID that identifies the table, generated when the table is created. Implementations must throw an exception if a table\u2019s UUID does not match the expected UUID after refreshing metadata.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.current_snapshot","title":"<code>current_snapshot()</code>","text":"<p>Get the current snapshot for this table, or None if there is no current snapshot.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def current_snapshot(self) -&gt; Snapshot | None:\n    \"\"\"Get the current snapshot for this table, or None if there is no current snapshot.\"\"\"\n    if self.current_snapshot_id is not None:\n        return self.snapshot_by_id(self.current_snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.name_mapping","title":"<code>name_mapping()</code>","text":"<p>Return the table's field-id NameMapping.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def name_mapping(self) -&gt; NameMapping | None:\n    \"\"\"Return the table's field-id NameMapping.\"\"\"\n    if name_mapping_json := self.properties.get(\"schema.name-mapping.default\"):\n        return parse_mapping_from_json(name_mapping_json)\n    else:\n        return None\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.new_snapshot_id","title":"<code>new_snapshot_id()</code>","text":"<p>Generate a new snapshot-id that's not in use.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def new_snapshot_id(self) -&gt; int:\n    \"\"\"Generate a new snapshot-id that's not in use.\"\"\"\n    snapshot_id = _generate_snapshot_id()\n    while self.snapshot_by_id(snapshot_id) is not None:\n        snapshot_id = _generate_snapshot_id()\n\n    return snapshot_id\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.schema","title":"<code>schema()</code>","text":"<p>Return the schema for this table.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def schema(self) -&gt; Schema:\n    \"\"\"Return the schema for this table.\"\"\"\n    return next(schema for schema in self.schemas if schema.schema_id == self.current_schema_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.schema_by_id","title":"<code>schema_by_id(schema_id)</code>","text":"<p>Get the schema by schema_id.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def schema_by_id(self, schema_id: int) -&gt; Schema | None:\n    \"\"\"Get the schema by schema_id.\"\"\"\n    return next((schema for schema in self.schemas if schema.schema_id == schema_id), None)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.snapshot_by_id","title":"<code>snapshot_by_id(snapshot_id)</code>","text":"<p>Get the snapshot by snapshot_id.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def snapshot_by_id(self, snapshot_id: int) -&gt; Snapshot | None:\n    \"\"\"Get the snapshot by snapshot_id.\"\"\"\n    return next((snapshot for snapshot in self.snapshots if snapshot.snapshot_id == snapshot_id), None)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.snapshot_by_name","title":"<code>snapshot_by_name(name)</code>","text":"<p>Return the snapshot referenced by the given name or null if no such reference exists.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def snapshot_by_name(self, name: str | None) -&gt; Snapshot | None:\n    \"\"\"Return the snapshot referenced by the given name or null if no such reference exists.\"\"\"\n    if name is None:\n        name = MAIN_BRANCH\n    if ref := self.refs.get(name):\n        return self.snapshot_by_id(ref.snapshot_id)\n    return None\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.sort_order","title":"<code>sort_order()</code>","text":"<p>Get the current sort order for this table, or UNSORTED_SORT_ORDER if there is no sort order.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def sort_order(self) -&gt; SortOrder:\n    \"\"\"Get the current sort order for this table, or UNSORTED_SORT_ORDER if there is no sort order.\"\"\"\n    return self.sort_order_by_id(self.default_sort_order_id) or UNSORTED_SORT_ORDER\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.sort_order_by_id","title":"<code>sort_order_by_id(sort_order_id)</code>","text":"<p>Get the sort order by sort_order_id.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def sort_order_by_id(self, sort_order_id: int) -&gt; SortOrder | None:\n    \"\"\"Get the sort order by sort_order_id.\"\"\"\n    return next((sort_order for sort_order in self.sort_orders if sort_order.order_id == sort_order_id), None)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.spec","title":"<code>spec()</code>","text":"<p>Return the partition spec of this table.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def spec(self) -&gt; PartitionSpec:\n    \"\"\"Return the partition spec of this table.\"\"\"\n    return next(spec for spec in self.partition_specs if spec.spec_id == self.default_spec_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.specs","title":"<code>specs()</code>","text":"<p>Return a dict the partition specs this table.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def specs(self) -&gt; dict[int, PartitionSpec]:\n    \"\"\"Return a dict the partition specs this table.\"\"\"\n    return {spec.spec_id: spec for spec in self.partition_specs}\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataCommonFields.specs_struct","title":"<code>specs_struct(spec_ids=None)</code>","text":"<p>Produce a struct of the combined PartitionSpecs.</p> <p>The partition fields should be optional: Partition fields may be added later, in which case not all files would have the result field, and it may be null.</p> <p>Parameters:</p> Name Type Description Default <code>spec_ids</code> <code>Iterable[int] | None</code> <p>Optional iterable of spec IDs to include. When not provided, all table specs are used.</p> <code>None</code> <p>:return: A StructType that represents the combined PartitionSpecs of the table</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def specs_struct(self, spec_ids: Iterable[int] | None = None) -&gt; StructType:\n    \"\"\"Produce a struct of the combined PartitionSpecs.\n\n    The partition fields should be optional: Partition fields may be added later,\n    in which case not all files would have the result field, and it may be null.\n\n    Args:\n        spec_ids: Optional iterable of spec IDs to include. When not provided,\n            all table specs are used.\n\n    :return: A StructType that represents the combined PartitionSpecs of the table\n    \"\"\"\n    specs = self.specs()\n    selected_specs = specs.values() if spec_ids is None else [specs[spec_id] for spec_id in spec_ids if spec_id in specs]\n\n    # Collect all the fields\n    struct_fields = {field.field_id: field for spec in selected_specs for field in spec.fields}\n\n    schema = self.schema()\n\n    nested_fields = []\n    # Sort them by field_id in order to get a deterministic output\n    for field_id in sorted(struct_fields):\n        field = struct_fields[field_id]\n        source_type = schema.find_type(field.source_id)\n        result_type = field.transform.result_type(source_type)\n        nested_fields.append(NestedField(field_id=field.field_id, name=field.name, type=result_type, required=False))\n\n    return StructType(*nested_fields)\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataUtil","title":"<code>TableMetadataUtil</code>","text":"<p>Helper class for parsing TableMetadata.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataUtil:\n    \"\"\"Helper class for parsing TableMetadata.\"\"\"\n\n    @staticmethod\n    def parse_raw(data: str) -&gt; TableMetadata:\n        try:\n            return TableMetadataWrapper.model_validate_json(data).root\n        except PydanticValidationError as e:\n            raise ValidationError(e) from e\n\n    @staticmethod\n    def parse_obj(data: dict[str, Any]) -&gt; TableMetadata:\n        if \"format-version\" not in data:\n            raise ValidationError(f\"Missing format-version in TableMetadata: {data}\")\n        format_version = data[\"format-version\"]\n\n        if format_version == 1:\n            return TableMetadataV1(**data)\n        elif format_version == 2:\n            return TableMetadataV2(**data)\n        elif format_version == 3:\n            return TableMetadataV3(**data)\n        else:\n            raise ValidationError(f\"Unknown format version: {format_version}\")\n\n    @staticmethod\n    def _construct_without_validation(table_metadata: TableMetadata) -&gt; TableMetadata:\n        \"\"\"Construct table metadata from an existing table without performing validation.\n\n        This method is useful during a sequence of table updates when the model needs to be\n        re-constructed but is not yet ready for validation.\n        \"\"\"\n        if table_metadata.format_version is None:\n            raise ValidationError(f\"Missing format-version in TableMetadata: {table_metadata}\")\n\n        if table_metadata.format_version == 1:\n            return TableMetadataV1.model_construct(**dict(table_metadata))\n        elif table_metadata.format_version == 2:\n            return TableMetadataV2.model_construct(**dict(table_metadata))\n        elif table_metadata.format_version == 3:\n            return TableMetadataV3.model_construct(**dict(table_metadata))\n        else:\n            raise ValidationError(f\"Unknown format version: {table_metadata.format_version}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1","title":"<code>TableMetadataV1</code>","text":"<p>               Bases: <code>TableMetadataCommonFields</code>, <code>IcebergBaseModel</code></p> <p>Represents version 1 of the Table Metadata.</p> <p>More information about the specification: https://iceberg.apache.org/spec/#version-1-analytic-data-tables</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataV1(TableMetadataCommonFields, IcebergBaseModel):\n    \"\"\"Represents version 1 of the Table Metadata.\n\n    More information about the specification:\n    https://iceberg.apache.org/spec/#version-1-analytic-data-tables\n    \"\"\"\n\n    # When we read a V1 format-version, we'll make sure to populate the fields\n    # for V2 as well. This makes it easier downstream because we can just\n    # assume that everything is a TableMetadataV2.\n    # When writing, we should stick to the same version that it was,\n    # because bumping the version should be an explicit operation that is up\n    # to the owner of the table.\n\n    @model_validator(mode=\"before\")\n    def cleanup_snapshot_id(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n        return cleanup_snapshot_id(data)\n\n    @model_validator(mode=\"after\")\n    def construct_refs(self) -&gt; TableMetadataV1:\n        return construct_refs(self)\n\n    @model_validator(mode=\"before\")\n    def set_v2_compatible_defaults(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Set default values to be compatible with the format v2.\n\n        Args:\n            data: The raw arguments when initializing a V1 TableMetadata.\n\n        Returns:\n            The TableMetadata with the defaults applied.\n        \"\"\"\n        # When the schema doesn't have an ID\n        schema = data.get(\"schema\")\n        if isinstance(schema, dict):\n            if \"schema_id\" not in schema and \"schema-id\" not in schema:\n                schema[\"schema_id\"] = DEFAULT_SCHEMA_ID\n\n        return data\n\n    @model_validator(mode=\"before\")\n    def construct_schemas(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Convert the schema into schemas.\n\n        For V1 schemas is optional, and if they aren't set, we'll set them\n        in this validator. This was we can always use the schemas when reading\n        table metadata, and we don't have to worry if it is a v1 or v2 format.\n\n        Args:\n            data: The raw data after validation, meaning that the aliases are applied.\n\n        Returns:\n            The TableMetadata with the schemas set, if not provided.\n        \"\"\"\n        if not data.get(\"schemas\"):\n            schema = data[\"schema\"]\n            data[\"schemas\"] = [schema]\n        return data\n\n    @model_validator(mode=\"before\")\n    def construct_partition_specs(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Convert the partition_spec into partition_specs.\n\n        For V1 partition_specs is optional, and if they aren't set, we'll set them\n        in this validator. This was we can always use the partition_specs when reading\n        table metadata, and we don't have to worry if it is a v1 or v2 format.\n\n        Args:\n            data: The raw data after validation, meaning that the aliases are applied.\n\n        Returns:\n            The TableMetadata with the partition_specs set, if not provided.\n        \"\"\"\n        if not data.get(PARTITION_SPECS):\n            if data.get(PARTITION_SPEC) is not None:\n                # Promote the spec from partition-spec to partition-specs\n                fields = data[PARTITION_SPEC]\n                data[PARTITION_SPECS] = [{SPEC_ID: INITIAL_SPEC_ID, FIELDS: fields}]\n                data[DEFAULT_SPEC_ID] = INITIAL_SPEC_ID\n            elif data.get(\"partition_spec\") is not None:\n                # Promote the spec from partition_spec to partition-specs\n                fields = data[\"partition_spec\"]\n                data[PARTITION_SPECS] = [{SPEC_ID: INITIAL_SPEC_ID, FIELDS: fields}]\n                data[DEFAULT_SPEC_ID] = INITIAL_SPEC_ID\n            else:\n                data[PARTITION_SPECS] = [{\"field-id\": 0, \"fields\": ()}]\n\n        data[LAST_PARTITION_ID] = max(\n            [field.get(FIELD_ID) for spec in data[PARTITION_SPECS] for field in spec[FIELDS]],\n            default=PARTITION_FIELD_ID_START - 1,\n        )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    def set_sort_orders(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"Set the sort_orders if not provided.\n\n        For V1 sort_orders is optional, and if they aren't set, we'll set them\n        in this validator.\n\n        Args:\n            data: The raw data after validation, meaning that the aliases are applied.\n\n        Returns:\n            The TableMetadata with the sort_orders set, if not provided.\n        \"\"\"\n        if not data.get(SORT_ORDERS) and not data.get(\"sort_orders\"):\n            data[SORT_ORDERS] = [UNSORTED_SORT_ORDER]\n        return data\n\n    def to_v2(self) -&gt; TableMetadataV2:\n        metadata = copy(self.model_dump())\n        metadata[\"format-version\"] = 2\n        return TableMetadataV2.model_validate(metadata)\n\n    format_version: Literal[1] = Field(alias=\"format-version\", default=1)\n    \"\"\"An integer version number for the format. Implementations must throw\n    an exception if a table\u2019s version is higher than the supported version.\"\"\"\n\n    schema_: Schema = Field(alias=\"schema\")\n    \"\"\"The table\u2019s current schema. (Deprecated: use schemas and\n    current-schema-id instead).\"\"\"\n\n    partition_spec: list[dict[str, Any]] = Field(alias=\"partition-spec\", default_factory=list)\n    \"\"\"The table\u2019s current partition spec, stored as only fields.\n    Note that this is used by writers to partition data, but is\n    not used when reading because reads use the specs stored in\n    manifest files. (Deprecated: use partition-specs and default-spec-id\n    instead).\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.format_version","title":"<code>format_version = Field(alias='format-version', default=1)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer version number for the format. Implementations must throw an exception if a table\u2019s version is higher than the supported version.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.partition_spec","title":"<code>partition_spec = Field(alias='partition-spec', default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s current partition spec, stored as only fields. Note that this is used by writers to partition data, but is not used when reading because reads use the specs stored in manifest files. (Deprecated: use partition-specs and default-spec-id instead).</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.schema_","title":"<code>schema_ = Field(alias='schema')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s current schema. (Deprecated: use schemas and current-schema-id instead).</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.construct_partition_specs","title":"<code>construct_partition_specs(data)</code>","text":"<p>Convert the partition_spec into partition_specs.</p> <p>For V1 partition_specs is optional, and if they aren't set, we'll set them in this validator. This was we can always use the partition_specs when reading table metadata, and we don't have to worry if it is a v1 or v2 format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>The raw data after validation, meaning that the aliases are applied.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The TableMetadata with the partition_specs set, if not provided.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\ndef construct_partition_specs(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Convert the partition_spec into partition_specs.\n\n    For V1 partition_specs is optional, and if they aren't set, we'll set them\n    in this validator. This was we can always use the partition_specs when reading\n    table metadata, and we don't have to worry if it is a v1 or v2 format.\n\n    Args:\n        data: The raw data after validation, meaning that the aliases are applied.\n\n    Returns:\n        The TableMetadata with the partition_specs set, if not provided.\n    \"\"\"\n    if not data.get(PARTITION_SPECS):\n        if data.get(PARTITION_SPEC) is not None:\n            # Promote the spec from partition-spec to partition-specs\n            fields = data[PARTITION_SPEC]\n            data[PARTITION_SPECS] = [{SPEC_ID: INITIAL_SPEC_ID, FIELDS: fields}]\n            data[DEFAULT_SPEC_ID] = INITIAL_SPEC_ID\n        elif data.get(\"partition_spec\") is not None:\n            # Promote the spec from partition_spec to partition-specs\n            fields = data[\"partition_spec\"]\n            data[PARTITION_SPECS] = [{SPEC_ID: INITIAL_SPEC_ID, FIELDS: fields}]\n            data[DEFAULT_SPEC_ID] = INITIAL_SPEC_ID\n        else:\n            data[PARTITION_SPECS] = [{\"field-id\": 0, \"fields\": ()}]\n\n    data[LAST_PARTITION_ID] = max(\n        [field.get(FIELD_ID) for spec in data[PARTITION_SPECS] for field in spec[FIELDS]],\n        default=PARTITION_FIELD_ID_START - 1,\n    )\n\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.construct_schemas","title":"<code>construct_schemas(data)</code>","text":"<p>Convert the schema into schemas.</p> <p>For V1 schemas is optional, and if they aren't set, we'll set them in this validator. This was we can always use the schemas when reading table metadata, and we don't have to worry if it is a v1 or v2 format.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>The raw data after validation, meaning that the aliases are applied.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The TableMetadata with the schemas set, if not provided.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\ndef construct_schemas(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Convert the schema into schemas.\n\n    For V1 schemas is optional, and if they aren't set, we'll set them\n    in this validator. This was we can always use the schemas when reading\n    table metadata, and we don't have to worry if it is a v1 or v2 format.\n\n    Args:\n        data: The raw data after validation, meaning that the aliases are applied.\n\n    Returns:\n        The TableMetadata with the schemas set, if not provided.\n    \"\"\"\n    if not data.get(\"schemas\"):\n        schema = data[\"schema\"]\n        data[\"schemas\"] = [schema]\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.set_sort_orders","title":"<code>set_sort_orders(data)</code>","text":"<p>Set the sort_orders if not provided.</p> <p>For V1 sort_orders is optional, and if they aren't set, we'll set them in this validator.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>The raw data after validation, meaning that the aliases are applied.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The TableMetadata with the sort_orders set, if not provided.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\ndef set_sort_orders(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Set the sort_orders if not provided.\n\n    For V1 sort_orders is optional, and if they aren't set, we'll set them\n    in this validator.\n\n    Args:\n        data: The raw data after validation, meaning that the aliases are applied.\n\n    Returns:\n        The TableMetadata with the sort_orders set, if not provided.\n    \"\"\"\n    if not data.get(SORT_ORDERS) and not data.get(\"sort_orders\"):\n        data[SORT_ORDERS] = [UNSORTED_SORT_ORDER]\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV1.set_v2_compatible_defaults","title":"<code>set_v2_compatible_defaults(data)</code>","text":"<p>Set default values to be compatible with the format v2.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>The raw arguments when initializing a V1 TableMetadata.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The TableMetadata with the defaults applied.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>@model_validator(mode=\"before\")\ndef set_v2_compatible_defaults(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Set default values to be compatible with the format v2.\n\n    Args:\n        data: The raw arguments when initializing a V1 TableMetadata.\n\n    Returns:\n        The TableMetadata with the defaults applied.\n    \"\"\"\n    # When the schema doesn't have an ID\n    schema = data.get(\"schema\")\n    if isinstance(schema, dict):\n        if \"schema_id\" not in schema and \"schema-id\" not in schema:\n            schema[\"schema_id\"] = DEFAULT_SCHEMA_ID\n\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV2","title":"<code>TableMetadataV2</code>","text":"<p>               Bases: <code>TableMetadataCommonFields</code>, <code>IcebergBaseModel</code></p> <p>Represents version 2 of the Table Metadata.</p> <p>This extends Version 1 with row-level deletes, and adds some additional information to the schema, such as all the historical schemas, partition-specs, sort-orders.</p> <p>For more information: https://iceberg.apache.org/spec/#version-2-row-level-deletes</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataV2(TableMetadataCommonFields, IcebergBaseModel):\n    \"\"\"Represents version 2 of the Table Metadata.\n\n    This extends Version 1 with row-level deletes, and adds some additional\n    information to the schema, such as all the historical schemas, partition-specs,\n    sort-orders.\n\n    For more information:\n    https://iceberg.apache.org/spec/#version-2-row-level-deletes\n    \"\"\"\n\n    @model_validator(mode=\"before\")\n    def cleanup_snapshot_id(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n        return cleanup_snapshot_id(data)\n\n    @model_validator(mode=\"after\")\n    def check_schemas(self) -&gt; TableMetadata:\n        return check_schemas(self)\n\n    @model_validator(mode=\"after\")\n    def check_partition_specs(self) -&gt; TableMetadata:\n        return check_partition_specs(self)\n\n    @model_validator(mode=\"after\")\n    def check_sort_orders(self) -&gt; TableMetadata:\n        return check_sort_orders(self)\n\n    @model_validator(mode=\"after\")\n    def construct_refs(self) -&gt; TableMetadata:\n        return construct_refs(self)\n\n    format_version: Literal[2] = Field(alias=\"format-version\", default=2)\n    \"\"\"An integer version number for the format. Implementations must throw\n    an exception if a table\u2019s version is higher than the supported version.\"\"\"\n\n    last_sequence_number: int = Field(alias=\"last-sequence-number\", default=INITIAL_SEQUENCE_NUMBER)\n    \"\"\"The table\u2019s highest assigned sequence number, a monotonically\n    increasing long that tracks the order of snapshots in a table.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV2.format_version","title":"<code>format_version = Field(alias='format-version', default=2)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer version number for the format. Implementations must throw an exception if a table\u2019s version is higher than the supported version.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV2.last_sequence_number","title":"<code>last_sequence_number = Field(alias='last-sequence-number', default=INITIAL_SEQUENCE_NUMBER)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s highest assigned sequence number, a monotonically increasing long that tracks the order of snapshots in a table.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV3","title":"<code>TableMetadataV3</code>","text":"<p>               Bases: <code>TableMetadataCommonFields</code>, <code>IcebergBaseModel</code></p> <p>Represents version 3 of the Table Metadata.</p> <p>Version 3 of the Iceberg spec extends data types and existing metadata structures to add new capabilities:</p> <pre><code>- New data types: nanosecond timestamp(tz), unknown\n- Default value support for columns\n- Multi-argument transforms for partitioning and sorting\n- Row Lineage tracking\n- Binary deletion vectors\n</code></pre> <p>For more information: https://iceberg.apache.org/spec/?column-projection#version-3-extended-types-and-capabilities</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>class TableMetadataV3(TableMetadataCommonFields, IcebergBaseModel):\n    \"\"\"Represents version 3 of the Table Metadata.\n\n    Version 3 of the Iceberg spec extends data types and existing metadata structures to add new capabilities:\n\n        - New data types: nanosecond timestamp(tz), unknown\n        - Default value support for columns\n        - Multi-argument transforms for partitioning and sorting\n        - Row Lineage tracking\n        - Binary deletion vectors\n\n    For more information:\n    https://iceberg.apache.org/spec/?column-projection#version-3-extended-types-and-capabilities\n    \"\"\"\n\n    @model_validator(mode=\"before\")\n    def cleanup_snapshot_id(cls, data: dict[str, Any]) -&gt; dict[str, Any]:\n        return cleanup_snapshot_id(data)\n\n    @model_validator(mode=\"after\")\n    def check_schemas(self) -&gt; TableMetadata:\n        return check_schemas(self)\n\n    @model_validator(mode=\"after\")\n    def check_partition_specs(self) -&gt; TableMetadata:\n        return check_partition_specs(self)\n\n    @model_validator(mode=\"after\")\n    def check_sort_orders(self) -&gt; TableMetadata:\n        return check_sort_orders(self)\n\n    @model_validator(mode=\"after\")\n    def construct_refs(self) -&gt; TableMetadata:\n        return construct_refs(self)\n\n    format_version: Literal[3] = Field(alias=\"format-version\", default=3)\n    \"\"\"An integer version number for the format. Implementations must throw\n    an exception if a table\u2019s version is higher than the supported version.\"\"\"\n\n    last_sequence_number: int = Field(alias=\"last-sequence-number\", default=INITIAL_SEQUENCE_NUMBER)\n    \"\"\"The table\u2019s highest assigned sequence number, a monotonically\n    increasing long that tracks the order of snapshots in a table.\"\"\"\n\n    next_row_id: int | None = Field(alias=\"next-row-id\", default=None)\n    \"\"\"A long higher than all assigned row IDs; the next snapshot's `first-row-id`.\"\"\"\n\n    def model_dump_json(self, exclude_none: bool = True, exclude: Any | None = None, by_alias: bool = True, **kwargs: Any) -&gt; str:\n        raise NotImplementedError(\"Writing V3 is not yet supported, see: https://github.com/apache/iceberg-python/issues/1551\")\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV3.format_version","title":"<code>format_version = Field(alias='format-version', default=3)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An integer version number for the format. Implementations must throw an exception if a table\u2019s version is higher than the supported version.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV3.last_sequence_number","title":"<code>last_sequence_number = Field(alias='last-sequence-number', default=INITIAL_SEQUENCE_NUMBER)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The table\u2019s highest assigned sequence number, a monotonically increasing long that tracks the order of snapshots in a table.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.TableMetadataV3.next_row_id","title":"<code>next_row_id = Field(alias='next-row-id', default=None)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A long higher than all assigned row IDs; the next snapshot's <code>first-row-id</code>.</p>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.check_partition_specs","title":"<code>check_partition_specs(table_metadata)</code>","text":"<p>Check if the default-spec-id is present in partition-specs.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def check_partition_specs(table_metadata: TableMetadata) -&gt; TableMetadata:\n    \"\"\"Check if the default-spec-id is present in partition-specs.\"\"\"\n    default_spec_id = table_metadata.default_spec_id\n\n    partition_specs: list[PartitionSpec] = table_metadata.partition_specs\n    for spec in partition_specs:\n        if spec.spec_id == default_spec_id:\n            return table_metadata\n\n    raise ValidationError(f\"default-spec-id {default_spec_id} can't be found\")\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.check_schemas","title":"<code>check_schemas(table_metadata)</code>","text":"<p>Check if the current-schema-id is actually present in schemas.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def check_schemas(table_metadata: TableMetadata) -&gt; TableMetadata:\n    \"\"\"Check if the current-schema-id is actually present in schemas.\"\"\"\n    current_schema_id = table_metadata.current_schema_id\n\n    for schema in table_metadata.schemas:\n        if schema.schema_id == current_schema_id:\n            return table_metadata\n\n    raise ValidationError(f\"current-schema-id {current_schema_id} can't be found in the schemas\")\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.check_sort_orders","title":"<code>check_sort_orders(table_metadata)</code>","text":"<p>Check if the default_sort_order_id is present in sort-orders.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def check_sort_orders(table_metadata: TableMetadata) -&gt; TableMetadata:\n    \"\"\"Check if the default_sort_order_id is present in sort-orders.\"\"\"\n    default_sort_order_id: int = table_metadata.default_sort_order_id\n\n    if default_sort_order_id != UNSORTED_SORT_ORDER_ID:\n        sort_orders: list[SortOrder] = table_metadata.sort_orders\n        for sort_order in sort_orders:\n            if sort_order.order_id == default_sort_order_id:\n                return table_metadata\n\n        raise ValidationError(f\"default-sort-order-id {default_sort_order_id} can't be found in {sort_orders}\")\n    return table_metadata\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.cleanup_snapshot_id","title":"<code>cleanup_snapshot_id(data)</code>","text":"<p>Run before validation.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def cleanup_snapshot_id(data: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Run before validation.\"\"\"\n    if CURRENT_SNAPSHOT_ID in data and data[CURRENT_SNAPSHOT_ID] == -1:\n        # We treat -1 and None the same, by cleaning this up\n        # in a pre-validator, we can simplify the logic later on\n        data[CURRENT_SNAPSHOT_ID] = None\n    return data\n</code></pre>"},{"location":"reference/pyiceberg/table/metadata/#pyiceberg.table.metadata.construct_refs","title":"<code>construct_refs(table_metadata)</code>","text":"<p>Set the main branch if missing.</p> Source code in <code>pyiceberg/table/metadata.py</code> <pre><code>def construct_refs(table_metadata: TableMetadata) -&gt; TableMetadata:\n    \"\"\"Set the main branch if missing.\"\"\"\n    if table_metadata.current_snapshot_id is not None:\n        if MAIN_BRANCH not in table_metadata.refs:\n            table_metadata.refs[MAIN_BRANCH] = SnapshotRef(\n                snapshot_id=table_metadata.current_snapshot_id, snapshot_ref_type=SnapshotRefType.BRANCH\n            )\n    return table_metadata\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/","title":"name_mapping","text":"<p>Contains everything around the name mapping.</p> <p>More information can be found on here: https://iceberg.apache.org/spec/#name-mapping-serialization</p>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.MappedField","title":"<code>MappedField</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>class MappedField(IcebergBaseModel):\n    field_id: int | None = Field(alias=\"field-id\", default=None)\n    names: list[str] = conlist(str)\n    fields: list[MappedField] = Field(default_factory=list)\n\n    @field_validator(\"fields\", mode=\"before\")\n    @classmethod\n    def convert_null_to_empty_List(cls, v: Any) -&gt; Any:\n        return v or []\n\n    @model_serializer\n    def ser_model(self) -&gt; dict[str, Any]:\n        \"\"\"Set custom serializer to leave out the field when it is empty.\"\"\"\n        serialized: dict[str, Any] = {\"names\": self.names}\n        if self.field_id is not None:\n            serialized[\"field-id\"] = self.field_id\n        if len(self.fields) &gt; 0:\n            serialized[\"fields\"] = self.fields\n        return serialized\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of fields.\"\"\"\n        return len(self.fields)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Convert the mapped-field into a nicely formatted string.\"\"\"\n        # Otherwise the UTs fail because the order of the set can change\n        fields_str = \", \".join([str(e) for e in self.fields]) or \"\"\n        fields_str = \" \" + fields_str if fields_str else \"\"\n        field_id = \"?\" if self.field_id is None else (str(self.field_id) or \"?\")\n        return \"([\" + \", \".join(self.names) + \"] -&gt; \" + field_id + fields_str + \")\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.MappedField.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of fields.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of fields.\"\"\"\n    return len(self.fields)\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.MappedField.__str__","title":"<code>__str__()</code>","text":"<p>Convert the mapped-field into a nicely formatted string.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Convert the mapped-field into a nicely formatted string.\"\"\"\n    # Otherwise the UTs fail because the order of the set can change\n    fields_str = \", \".join([str(e) for e in self.fields]) or \"\"\n    fields_str = \" \" + fields_str if fields_str else \"\"\n    field_id = \"?\" if self.field_id is None else (str(self.field_id) or \"?\")\n    return \"([\" + \", \".join(self.names) + \"] -&gt; \" + field_id + fields_str + \")\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.MappedField.ser_model","title":"<code>ser_model()</code>","text":"<p>Set custom serializer to leave out the field when it is empty.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@model_serializer\ndef ser_model(self) -&gt; dict[str, Any]:\n    \"\"\"Set custom serializer to leave out the field when it is empty.\"\"\"\n    serialized: dict[str, Any] = {\"names\": self.names}\n    if self.field_id is not None:\n        serialized[\"field-id\"] = self.field_id\n    if len(self.fields) &gt; 0:\n        serialized[\"fields\"] = self.fields\n    return serialized\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMapping","title":"<code>NameMapping</code>","text":"<p>               Bases: <code>IcebergRootModel[list[MappedField]]</code></p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>class NameMapping(IcebergRootModel[list[MappedField]]):\n    root: list[MappedField]\n\n    @cached_property\n    def _field_by_name(self) -&gt; dict[str, MappedField]:\n        return visit_name_mapping(self, _IndexByName())\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of mappings.\"\"\"\n        return len(self.root)\n\n    def __iter__(self) -&gt; Iterator[MappedField]:\n        \"\"\"Iterate over the mapped fields.\"\"\"\n        return iter(self.root)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Convert the name-mapping into a nicely formatted string.\"\"\"\n        if len(self.root) == 0:\n            return \"[]\"\n        else:\n            return \"[\\n  \" + \"\\n  \".join([str(e) for e in self.root]) + \"\\n]\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMapping.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the mapped fields.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __iter__(self) -&gt; Iterator[MappedField]:\n    \"\"\"Iterate over the mapped fields.\"\"\"\n    return iter(self.root)\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMapping.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of mappings.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of mappings.\"\"\"\n    return len(self.root)\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMapping.__str__","title":"<code>__str__()</code>","text":"<p>Convert the name-mapping into a nicely formatted string.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Convert the name-mapping into a nicely formatted string.\"\"\"\n    if len(self.root) == 0:\n        return \"[]\"\n    else:\n        return \"[\\n  \" + \"\\n  \".join([str(e) for e in self.root]) + \"\\n]\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMappingVisitor","title":"<code>NameMappingVisitor</code>","text":"<p>               Bases: <code>Generic[S, T]</code>, <code>ABC</code></p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>class NameMappingVisitor(Generic[S, T], ABC):\n    @abstractmethod\n    def mapping(self, nm: NameMapping, field_results: S) -&gt; S:\n        \"\"\"Visit a NameMapping.\"\"\"\n\n    @abstractmethod\n    def fields(self, struct: list[MappedField], field_results: list[T]) -&gt; S:\n        \"\"\"Visit a List[MappedField].\"\"\"\n\n    @abstractmethod\n    def field(self, field: MappedField, field_result: S) -&gt; T:\n        \"\"\"Visit a MappedField.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMappingVisitor.field","title":"<code>field(field, field_result)</code>  <code>abstractmethod</code>","text":"<p>Visit a MappedField.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@abstractmethod\ndef field(self, field: MappedField, field_result: S) -&gt; T:\n    \"\"\"Visit a MappedField.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMappingVisitor.fields","title":"<code>fields(struct, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a List[MappedField].</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@abstractmethod\ndef fields(self, struct: list[MappedField], field_results: list[T]) -&gt; S:\n    \"\"\"Visit a List[MappedField].\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.NameMappingVisitor.mapping","title":"<code>mapping(nm, field_results)</code>  <code>abstractmethod</code>","text":"<p>Visit a NameMapping.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@abstractmethod\ndef mapping(self, nm: NameMapping, field_results: S) -&gt; S:\n    \"\"\"Visit a NameMapping.\"\"\"\n</code></pre>"},{"location":"reference/pyiceberg/table/name_mapping/#pyiceberg.table.name_mapping.visit_name_mapping","title":"<code>visit_name_mapping(obj, visitor)</code>","text":"<p>Traverse the name mapping in post-order traversal.</p> Source code in <code>pyiceberg/table/name_mapping.py</code> <pre><code>@singledispatch\ndef visit_name_mapping(obj: NameMapping | list[MappedField] | MappedField, visitor: NameMappingVisitor[S, T]) -&gt; S:\n    \"\"\"Traverse the name mapping in post-order traversal.\"\"\"\n    raise NotImplementedError(f\"Cannot visit non-type: {obj}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/puffin/","title":"puffin","text":""},{"location":"reference/pyiceberg/table/refs/","title":"refs","text":""},{"location":"reference/pyiceberg/table/refs/#pyiceberg.table.refs.SnapshotRefType","title":"<code>SnapshotRefType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>pyiceberg/table/refs.py</code> <pre><code>class SnapshotRefType(str, Enum):\n    BRANCH = \"branch\"\n    TAG = \"tag\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the SnapshotRefType class.\"\"\"\n        return f\"SnapshotRefType.{self.name}\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SnapshotRefType class.\"\"\"\n        return self.value\n</code></pre>"},{"location":"reference/pyiceberg/table/refs/#pyiceberg.table.refs.SnapshotRefType.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the SnapshotRefType class.</p> Source code in <code>pyiceberg/table/refs.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the SnapshotRefType class.\"\"\"\n    return f\"SnapshotRefType.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/refs/#pyiceberg.table.refs.SnapshotRefType.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SnapshotRefType class.</p> Source code in <code>pyiceberg/table/refs.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SnapshotRefType class.\"\"\"\n    return self.value\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/","title":"snapshots","text":""},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Operation","title":"<code>Operation</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Describes the operation.</p> Possible operation values are <ul> <li>append: Only data files were added and no files were removed.</li> <li>replace: Data and delete files were added and removed without changing table data;     i.e., compaction, changing the data file format, or relocating data files.</li> <li>overwrite: Data and delete files were added and removed in a logical overwrite operation.</li> <li>delete: Data files were removed and their contents logically deleted and/or delete files     were added to delete rows.</li> </ul> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>class Operation(Enum):\n    \"\"\"Describes the operation.\n\n    Possible operation values are:\n        - append: Only data files were added and no files were removed.\n        - replace: Data and delete files were added and removed without changing table data;\n            i.e., compaction, changing the data file format, or relocating data files.\n        - overwrite: Data and delete files were added and removed in a logical overwrite operation.\n        - delete: Data files were removed and their contents logically deleted and/or delete files\n            were added to delete rows.\n    \"\"\"\n\n    APPEND = \"append\"\n    REPLACE = \"replace\"\n    OVERWRITE = \"overwrite\"\n    DELETE = \"delete\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Operation class.\"\"\"\n        return f\"Operation.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Operation.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Operation class.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Operation class.\"\"\"\n    return f\"Operation.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Snapshot","title":"<code>Snapshot</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>class Snapshot(IcebergBaseModel):\n    snapshot_id: int = Field(alias=\"snapshot-id\")\n    parent_snapshot_id: int | None = Field(alias=\"parent-snapshot-id\", default=None)\n    sequence_number: int | None = Field(alias=\"sequence-number\", default=INITIAL_SEQUENCE_NUMBER)\n    timestamp_ms: int = Field(alias=\"timestamp-ms\", default_factory=lambda: int(time.time() * 1000))\n    manifest_list: str = Field(alias=\"manifest-list\", description=\"Location of the snapshot's manifest list file\")\n    summary: Summary | None = Field(default=None)\n    schema_id: int | None = Field(alias=\"schema-id\", default=None)\n    first_row_id: int | None = Field(\n        alias=\"first-row-id\", default=None, description=\"assigned to the first row in the first data file in the first manifest\"\n    )\n    added_rows: int | None = Field(\n        alias=\"added-rows\", default=None, description=\"The upper bound of the number of rows with assigned row IDs\"\n    )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the Snapshot class.\"\"\"\n        operation = f\"{self.summary.operation}: \" if self.summary else \"\"\n        parent_id = f\", parent_id={self.parent_snapshot_id}\" if self.parent_snapshot_id else \"\"\n        schema_id = f\", schema_id={self.schema_id}\" if self.schema_id is not None else \"\"\n        result_str = f\"{operation}id={self.snapshot_id}{parent_id}{schema_id}\"\n        return result_str\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Snapshot class.\"\"\"\n        fields = [\n            f\"snapshot_id={self.snapshot_id}\",\n            f\"parent_snapshot_id={self.parent_snapshot_id}\",\n            f\"sequence_number={self.sequence_number}\",\n            f\"timestamp_ms={self.timestamp_ms}\",\n            f\"manifest_list='{self.manifest_list}'\",\n            f\"summary={repr(self.summary)}\" if self.summary else None,\n            f\"schema_id={self.schema_id}\" if self.schema_id is not None else None,\n            f\"first_row_id={self.first_row_id}\" if self.first_row_id is not None else None,\n            f\"added_rows={self.added_rows}\" if self.added_rows is not None else None,\n        ]\n        filtered_fields = [field for field in fields if field is not None]\n        return f\"Snapshot({', '.join(filtered_fields)})\"\n\n    def manifests(self, io: FileIO) -&gt; list[ManifestFile]:\n        \"\"\"Return the manifests for the given snapshot.\"\"\"\n        return list(_manifests(io, self.manifest_list))\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Snapshot.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Snapshot class.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Snapshot class.\"\"\"\n    fields = [\n        f\"snapshot_id={self.snapshot_id}\",\n        f\"parent_snapshot_id={self.parent_snapshot_id}\",\n        f\"sequence_number={self.sequence_number}\",\n        f\"timestamp_ms={self.timestamp_ms}\",\n        f\"manifest_list='{self.manifest_list}'\",\n        f\"summary={repr(self.summary)}\" if self.summary else None,\n        f\"schema_id={self.schema_id}\" if self.schema_id is not None else None,\n        f\"first_row_id={self.first_row_id}\" if self.first_row_id is not None else None,\n        f\"added_rows={self.added_rows}\" if self.added_rows is not None else None,\n    ]\n    filtered_fields = [field for field in fields if field is not None]\n    return f\"Snapshot({', '.join(filtered_fields)})\"\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Snapshot.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the Snapshot class.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the Snapshot class.\"\"\"\n    operation = f\"{self.summary.operation}: \" if self.summary else \"\"\n    parent_id = f\", parent_id={self.parent_snapshot_id}\" if self.parent_snapshot_id else \"\"\n    schema_id = f\", schema_id={self.schema_id}\" if self.schema_id is not None else \"\"\n    result_str = f\"{operation}id={self.snapshot_id}{parent_id}{schema_id}\"\n    return result_str\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Snapshot.manifests","title":"<code>manifests(io)</code>","text":"<p>Return the manifests for the given snapshot.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def manifests(self, io: FileIO) -&gt; list[ManifestFile]:\n    \"\"\"Return the manifests for the given snapshot.\"\"\"\n    return list(_manifests(io, self.manifest_list))\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary","title":"<code>Summary</code>","text":"<p>               Bases: <code>IcebergBaseModel</code>, <code>Mapping[str, str]</code></p> <p>A class that stores the summary information for a Snapshot.</p> <p>The snapshot summary\u2019s operation field is used by some operations, like snapshot expiration, to skip processing certain snapshots.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>class Summary(IcebergBaseModel, Mapping[str, str]):\n    \"\"\"A class that stores the summary information for a Snapshot.\n\n    The snapshot summary\u2019s operation field is used by some operations,\n    like snapshot expiration, to skip processing certain snapshots.\n    \"\"\"\n\n    operation: Operation = Field()\n    _additional_properties: dict[str, str] = PrivateAttr()\n\n    def __init__(self, operation: Operation | None = None, **data: Any) -&gt; None:\n        if operation is None:\n            warnings.warn(\"Encountered invalid snapshot summary: operation is missing, defaulting to overwrite\", stacklevel=2)\n            operation = Operation.OVERWRITE\n        super().__init__(operation=operation, **data)\n        self._additional_properties = data\n\n    def __getitem__(self, __key: str) -&gt; Any | None:  # type: ignore\n        \"\"\"Return a key as it is a map.\"\"\"\n        if __key.lower() == \"operation\":\n            return self.operation\n        else:\n            return self._additional_properties.get(__key)\n\n    def __setitem__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Set a key as it is a map.\"\"\"\n        if key.lower() == \"operation\":\n            self.operation = value\n        else:\n            self._additional_properties[key] = value\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of keys in the summary.\"\"\"\n        # Operation is required\n        return 1 + len(self._additional_properties)\n\n    @model_serializer\n    def ser_model(self) -&gt; dict[str, str]:\n        return {\n            \"operation\": str(self.operation.value),\n            **self._additional_properties,\n        }\n\n    @property\n    def additional_properties(self) -&gt; dict[str, str]:\n        return self._additional_properties\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the Summary class.\"\"\"\n        repr_properties = f\", **{repr(self._additional_properties)}\" if self._additional_properties else \"\"\n        return f\"Summary({repr(self.operation)}{repr_properties})\"\n\n    def __eq__(self, other: Any) -&gt; bool:\n        \"\"\"Compare if the summary is equal to another summary.\"\"\"\n        return (\n            self.operation == other.operation and self.additional_properties == other.additional_properties\n            if isinstance(other, Summary)\n            else False\n        )\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Compare if the summary is equal to another summary.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __eq__(self, other: Any) -&gt; bool:\n    \"\"\"Compare if the summary is equal to another summary.\"\"\"\n    return (\n        self.operation == other.operation and self.additional_properties == other.additional_properties\n        if isinstance(other, Summary)\n        else False\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__getitem__","title":"<code>__getitem__(__key)</code>","text":"<p>Return a key as it is a map.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __getitem__(self, __key: str) -&gt; Any | None:  # type: ignore\n    \"\"\"Return a key as it is a map.\"\"\"\n    if __key.lower() == \"operation\":\n        return self.operation\n    else:\n        return self._additional_properties.get(__key)\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of keys in the summary.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of keys in the summary.\"\"\"\n    # Operation is required\n    return 1 + len(self._additional_properties)\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the Summary class.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the Summary class.\"\"\"\n    repr_properties = f\", **{repr(self._additional_properties)}\" if self._additional_properties else \"\"\n    return f\"Summary({repr(self.operation)}{repr_properties})\"\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.Summary.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set a key as it is a map.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def __setitem__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Set a key as it is a map.\"\"\"\n    if key.lower() == \"operation\":\n        self.operation = value\n    else:\n        self._additional_properties[key] = value\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.ancestors_between","title":"<code>ancestors_between(from_snapshot, to_snapshot, table_metadata)</code>","text":"<p>Get the ancestors of and including the given snapshot between the to and from snapshots.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def ancestors_between(from_snapshot: Snapshot | None, to_snapshot: Snapshot, table_metadata: TableMetadata) -&gt; Iterable[Snapshot]:\n    \"\"\"Get the ancestors of and including the given snapshot between the to and from snapshots.\"\"\"\n    if from_snapshot is not None:\n        for snapshot in ancestors_of(to_snapshot, table_metadata):\n            yield snapshot\n            if snapshot == from_snapshot:\n                break\n    else:\n        yield from ancestors_of(to_snapshot, table_metadata)\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.ancestors_of","title":"<code>ancestors_of(current_snapshot, table_metadata)</code>","text":"<p>Get the ancestors of and including the given snapshot.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def ancestors_of(current_snapshot: Snapshot | None, table_metadata: TableMetadata) -&gt; Iterable[Snapshot]:\n    \"\"\"Get the ancestors of and including the given snapshot.\"\"\"\n    snapshot = current_snapshot\n    while snapshot is not None:\n        yield snapshot\n        if snapshot.parent_snapshot_id is None:\n            break\n        snapshot = table_metadata.snapshot_by_id(snapshot.parent_snapshot_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/snapshots/#pyiceberg.table.snapshots.latest_ancestor_before_timestamp","title":"<code>latest_ancestor_before_timestamp(table_metadata, timestamp_ms)</code>","text":"<p>Find the latest ancestor snapshot whose timestamp is before the provided timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>table_metadata</code> <code>TableMetadata</code> <p>The table metadata for a table</p> required <code>timestamp_ms</code> <code>int</code> <p>lookup snapshots strictly before this timestamp</p> required <p>Returns:</p> Type Description <code>Snapshot | None</code> <p>The latest ancestor snapshot older than the timestamp, or None if not found.</p> Source code in <code>pyiceberg/table/snapshots.py</code> <pre><code>def latest_ancestor_before_timestamp(table_metadata: TableMetadata, timestamp_ms: int) -&gt; Snapshot | None:\n    \"\"\"Find the latest ancestor snapshot whose timestamp is before the provided timestamp.\n\n    Args:\n        table_metadata: The table metadata for a table\n        timestamp_ms: lookup snapshots strictly before this timestamp\n\n    Returns:\n        The latest ancestor snapshot older than the timestamp, or None if not found.\n    \"\"\"\n    result: Snapshot | None = None\n    result_timestamp: int = 0\n\n    for ancestor in ancestors_of(table_metadata.current_snapshot(), table_metadata):\n        if timestamp_ms &gt; ancestor.timestamp_ms &gt; result_timestamp:\n            result = ancestor\n            result_timestamp = ancestor.timestamp_ms\n\n    return result\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/","title":"sorting","text":""},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.NullOrder","title":"<code>NullOrder</code>","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>class NullOrder(Enum):\n    NULLS_FIRST = \"nulls-first\"\n    NULLS_LAST = \"nulls-last\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the NullOrder class.\"\"\"\n        return self.name.replace(\"_\", \" \")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the NullOrder class.\"\"\"\n        return f\"NullOrder.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.NullOrder.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the NullOrder class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the NullOrder class.\"\"\"\n    return f\"NullOrder.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.NullOrder.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the NullOrder class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the NullOrder class.\"\"\"\n    return self.name.replace(\"_\", \" \")\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortDirection","title":"<code>SortDirection</code>","text":"<p>               Bases: <code>Enum</code></p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>class SortDirection(Enum):\n    ASC = \"asc\"\n    DESC = \"desc\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortDirection class.\"\"\"\n        return self.name\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortDirection class.\"\"\"\n        return f\"SortDirection.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortDirection.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the SortDirection class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortDirection class.\"\"\"\n    return f\"SortDirection.{self.name}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortDirection.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SortDirection class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortDirection class.\"\"\"\n    return self.name\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortField","title":"<code>SortField</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Sort order field.</p> <p>Parameters:</p> Name Type Description Default <code>source_id</code> <code>int</code> <p>Source column id from the table\u2019s schema.</p> <code>None</code> <code>transform</code> <code>str</code> <p>Transform that is used to produce values to be sorted on from the source column.                This is the same transform as described in partition transforms.</p> <code>None</code> <code>direction</code> <code>SortDirection</code> <p>Sort direction, that can only be either asc or desc.</p> <code>None</code> <code>null_order</code> <code>NullOrder</code> <p>Null order that describes the order of null values when sorted.                       Can only be either nulls-first or nulls-last.</p> <code>None</code> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>class SortField(IcebergBaseModel):\n    \"\"\"Sort order field.\n\n    Args:\n      source_id (int): Source column id from the table\u2019s schema.\n      transform (str): Transform that is used to produce values to be sorted on from the source column.\n                       This is the same transform as described in partition transforms.\n      direction (SortDirection): Sort direction, that can only be either asc or desc.\n      null_order (NullOrder): Null order that describes the order of null values when sorted.\n                              Can only be either nulls-first or nulls-last.\n    \"\"\"\n\n    def __init__(\n        self,\n        source_id: int | None = None,\n        transform: Transform[Any, Any] | Callable[[IcebergType], Transform[Any, Any]] | None = None,\n        direction: SortDirection | None = None,\n        null_order: NullOrder | None = None,\n        **data: Any,\n    ):\n        if source_id is not None:\n            data[\"source-id\"] = source_id\n        if transform is not None:\n            data[\"transform\"] = transform\n        if direction is not None:\n            data[\"direction\"] = direction\n        if null_order is not None:\n            data[\"null-order\"] = null_order\n        super().__init__(**data)\n\n    @model_validator(mode=\"before\")\n    def set_null_order(cls, values: dict[str, Any]) -&gt; dict[str, Any]:\n        values[\"direction\"] = values[\"direction\"] if values.get(\"direction\") else SortDirection.ASC\n        if not values.get(\"null-order\"):\n            values[\"null-order\"] = NullOrder.NULLS_FIRST if values[\"direction\"] == SortDirection.ASC else NullOrder.NULLS_LAST\n        return values\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def map_source_ids_onto_source_id(cls, data: Any) -&gt; Any:\n        if isinstance(data, dict):\n            if \"source-id\" not in data and (source_ids := data[\"source-ids\"]):\n                if isinstance(source_ids, list):\n                    if len(source_ids) == 0:\n                        raise ValueError(\"Empty source-ids is not allowed\")\n                    if len(source_ids) &gt; 1:\n                        raise ValueError(\"Multi argument transforms are not yet supported\")\n                    data[\"source-id\"] = source_ids[0]\n        return data\n\n    source_id: int = Field(alias=\"source-id\")\n    transform: Annotated[  # type: ignore\n        Transform,\n        BeforeValidator(parse_transform),\n        PlainSerializer(lambda c: str(c), return_type=str),  # pylint: disable=W0108\n        WithJsonSchema({\"type\": \"string\"}, mode=\"serialization\"),\n    ] = Field(default=IdentityTransform())\n    direction: SortDirection = Field()\n    null_order: NullOrder = Field(alias=\"null-order\")\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortField class.\"\"\"\n        if isinstance(self.transform, IdentityTransform):\n            # In the case of an identity transform, we can omit the transform\n            return f\"{self.source_id} {self.direction} {self.null_order}\"\n        else:\n            return f\"{self.transform}({self.source_id}) {self.direction} {self.null_order}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortField.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SortField class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortField class.\"\"\"\n    if isinstance(self.transform, IdentityTransform):\n        # In the case of an identity transform, we can omit the transform\n        return f\"{self.source_id} {self.direction} {self.null_order}\"\n    else:\n        return f\"{self.transform}({self.source_id}) {self.direction} {self.null_order}\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortOrder","title":"<code>SortOrder</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Describes how the data is sorted within the table.</p> <p>Users can sort their data within partitions by columns to gain performance.</p> <p>The order of the sort fields within the list defines the order in which the sort is applied to the data.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>List[SortField]</code> <p>The fields how the table is sorted.</p> <code>()</code> <p>Other Parameters:</p> Name Type Description <code>order_id</code> <code>int</code> <p>An unique id of the sort-order of a table.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>class SortOrder(IcebergBaseModel):\n    \"\"\"Describes how the data is sorted within the table.\n\n    Users can sort their data within partitions by columns to gain performance.\n\n    The order of the sort fields within the list defines the order in which the sort is applied to the data.\n\n    Args:\n      fields (List[SortField]): The fields how the table is sorted.\n\n    Keyword Args:\n      order_id (int): An unique id of the sort-order of a table.\n    \"\"\"\n\n    order_id: int = Field(alias=\"order-id\", default=INITIAL_SORT_ORDER_ID)\n    fields: list[SortField] = Field(default_factory=list)\n\n    def __init__(self, *fields: SortField, **data: Any):\n        if fields:\n            data[\"fields\"] = fields\n        super().__init__(**data)\n\n    @property\n    def is_unsorted(self) -&gt; bool:\n        return len(self.fields) == 0\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortOrder class.\"\"\"\n        result_str = \"[\"\n        if self.fields:\n            result_str += \"\\n  \" + \"\\n  \".join([str(field) for field in self.fields]) + \"\\n\"\n        result_str += \"]\"\n        return result_str\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the SortOrder class.\"\"\"\n        fields = f\"{', '.join(repr(column) for column in self.fields)}, \" if self.fields else \"\"\n        return f\"SortOrder({fields}order_id={self.order_id})\"\n\n    def check_compatible(self, schema: Schema) -&gt; None:\n        for field in self.fields:\n            source_field = schema._lazy_id_to_field.get(field.source_id)\n            if source_field is None:\n                raise ValidationError(f\"Cannot find source column for sort field: {field}\")\n            if not source_field.field_type.is_primitive:\n                raise ValidationError(f\"Cannot sort by non-primitive source field: {source_field}\")\n            if not field.transform.can_transform(source_field.field_type):\n                raise ValidationError(\n                    f\"Invalid source field {source_field.name} with type {source_field.field_type} \"\n                    + f\"for transform: {field.transform}\"\n                )\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortOrder.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the SortOrder class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortOrder class.\"\"\"\n    fields = f\"{', '.join(repr(column) for column in self.fields)}, \" if self.fields else \"\"\n    return f\"SortOrder({fields}order_id={self.order_id})\"\n</code></pre>"},{"location":"reference/pyiceberg/table/sorting/#pyiceberg.table.sorting.SortOrder.__str__","title":"<code>__str__()</code>","text":"<p>Return the string representation of the SortOrder class.</p> Source code in <code>pyiceberg/table/sorting.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the string representation of the SortOrder class.\"\"\"\n    result_str = \"[\"\n    if self.fields:\n        result_str += \"\\n  \" + \"\\n  \".join([str(field) for field in self.fields]) + \"\\n\"\n    result_str += \"]\"\n    return result_str\n</code></pre>"},{"location":"reference/pyiceberg/table/statistics/","title":"statistics","text":""},{"location":"reference/pyiceberg/table/statistics/#pyiceberg.table.statistics.StatisticsCommonFields","title":"<code>StatisticsCommonFields</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> <p>Common fields between table and partition statistics structs found on metadata.</p> Source code in <code>pyiceberg/table/statistics.py</code> <pre><code>class StatisticsCommonFields(IcebergBaseModel):\n    \"\"\"Common fields between table and partition statistics structs found on metadata.\"\"\"\n\n    snapshot_id: int = Field(alias=\"snapshot-id\")\n    statistics_path: str = Field(alias=\"statistics-path\")\n    file_size_in_bytes: int = Field(alias=\"file-size-in-bytes\")\n</code></pre>"},{"location":"reference/pyiceberg/table/upsert_util/","title":"upsert_util","text":""},{"location":"reference/pyiceberg/table/upsert_util/#pyiceberg.table.upsert_util.get_rows_to_update","title":"<code>get_rows_to_update(source_table, target_table, join_cols)</code>","text":"<p>Return a table with rows that need to be updated in the target table based on the join columns.</p> <p>The table is joined on the identifier columns, and then checked if there are any updated rows. Those are selected and everything is renamed correctly.</p> Source code in <code>pyiceberg/table/upsert_util.py</code> <pre><code>def get_rows_to_update(source_table: pa.Table, target_table: pa.Table, join_cols: list[str]) -&gt; pa.Table:\n    \"\"\"\n    Return a table with rows that need to be updated in the target table based on the join columns.\n\n    The table is joined on the identifier columns, and then checked if there are any updated rows.\n    Those are selected and everything is renamed correctly.\n    \"\"\"\n    all_columns = set(source_table.column_names)\n    join_cols_set = set(join_cols)\n\n    non_key_cols = list(all_columns - join_cols_set)\n\n    if has_duplicate_rows(target_table, join_cols):\n        raise ValueError(\"Target table has duplicate rows, aborting upsert\")\n\n    if len(target_table) == 0:\n        # When the target table is empty, there is nothing to update :)\n        return source_table.schema.empty_table()\n\n    # We need to compare non_key_cols in Python as PyArrow\n    # 1. Cannot do a join when non-join columns have complex types\n    # 2. Cannot compare columns with complex types\n    # See: https://github.com/apache/arrow/issues/35785\n    SOURCE_INDEX_COLUMN_NAME = \"__source_index\"\n    TARGET_INDEX_COLUMN_NAME = \"__target_index\"\n\n    if SOURCE_INDEX_COLUMN_NAME in join_cols or TARGET_INDEX_COLUMN_NAME in join_cols:\n        raise ValueError(\n            f\"{SOURCE_INDEX_COLUMN_NAME} and {TARGET_INDEX_COLUMN_NAME} are reserved for joining \"\n            f\"DataFrames, and cannot be used as column names\"\n        ) from None\n\n    # Step 1: Prepare source index with join keys and a marker index\n    # Cast to target table schema, so we can do the join\n    # See: https://github.com/apache/arrow/issues/37542\n    source_index = (\n        source_table.cast(target_table.schema)\n        .select(join_cols_set)\n        .append_column(SOURCE_INDEX_COLUMN_NAME, pa.array(range(len(source_table))))\n    )\n\n    # Step 2: Prepare target index with join keys and a marker\n    target_index = target_table.select(join_cols_set).append_column(TARGET_INDEX_COLUMN_NAME, pa.array(range(len(target_table))))\n\n    # Step 3: Perform an inner join to find which rows from source exist in target\n    matching_indices = source_index.join(target_index, keys=list(join_cols_set), join_type=\"inner\")\n\n    # Step 4: Compare all rows using Python\n    to_update_indices = []\n    for source_idx, target_idx in zip(\n        matching_indices[SOURCE_INDEX_COLUMN_NAME].to_pylist(),\n        matching_indices[TARGET_INDEX_COLUMN_NAME].to_pylist(),\n        strict=True,\n    ):\n        source_row = source_table.slice(source_idx, 1)\n        target_row = target_table.slice(target_idx, 1)\n\n        for key in non_key_cols:\n            source_val = source_row.column(key)[0].as_py()\n            target_val = target_row.column(key)[0].as_py()\n            if source_val != target_val:\n                to_update_indices.append(source_idx)\n                break\n\n    # Step 5: Take rows from source table using the indices and cast to target schema\n    if to_update_indices:\n        return source_table.take(to_update_indices)\n    else:\n        return source_table.schema.empty_table()\n</code></pre>"},{"location":"reference/pyiceberg/table/upsert_util/#pyiceberg.table.upsert_util.has_duplicate_rows","title":"<code>has_duplicate_rows(df, join_cols)</code>","text":"<p>Check for duplicate rows in a PyArrow table based on the join columns.</p> Source code in <code>pyiceberg/table/upsert_util.py</code> <pre><code>def has_duplicate_rows(df: pyarrow_table, join_cols: list[str]) -&gt; bool:\n    \"\"\"Check for duplicate rows in a PyArrow table based on the join columns.\"\"\"\n    return len(df.select(join_cols).group_by(join_cols).aggregate([([], \"count_all\")]).filter(pc.field(\"count_all\") &gt; 1)) &gt; 0\n</code></pre>"},{"location":"reference/pyiceberg/table/update/","title":"update","text":""},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertCreate","title":"<code>AssertCreate</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table must not already exist; used for create transactions.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertCreate(ValidatableTableRequirement):\n    \"\"\"The table must not already exist; used for create transactions.\"\"\"\n\n    type: Literal[\"assert-create\"] = Field(default=\"assert-create\")\n\n    def validate(self, base_metadata: TableMetadata | None) -&gt; None:\n        if base_metadata is not None:\n            raise CommitFailedException(\"Table already exists\")\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertCurrentSchemaId","title":"<code>AssertCurrentSchemaId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's current schema id must match the requirement's <code>current-schema-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertCurrentSchemaId(ValidatableTableRequirement):\n    \"\"\"The table's current schema id must match the requirement's `current-schema-id`.\"\"\"\n\n    type: Literal[\"assert-current-schema-id\"] = Field(default=\"assert-current-schema-id\")\n    current_schema_id: int = Field(..., alias=\"current-schema-id\")\n\n    def validate(self, base_metadata: TableMetadata | None) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif self.current_schema_id != base_metadata.current_schema_id:\n            raise CommitFailedException(\n                f\"Requirement failed: current schema id has changed: \"\n                f\"expected {self.current_schema_id}, found {base_metadata.current_schema_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertDefaultSortOrderId","title":"<code>AssertDefaultSortOrderId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's default sort order id must match the requirement's <code>default-sort-order-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertDefaultSortOrderId(ValidatableTableRequirement):\n    \"\"\"The table's default sort order id must match the requirement's `default-sort-order-id`.\"\"\"\n\n    type: Literal[\"assert-default-sort-order-id\"] = Field(default=\"assert-default-sort-order-id\")\n    default_sort_order_id: int = Field(..., alias=\"default-sort-order-id\")\n\n    def validate(self, base_metadata: TableMetadata | None) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif self.default_sort_order_id != base_metadata.default_sort_order_id:\n            raise CommitFailedException(\n                f\"Requirement failed: default sort order id has changed: \"\n                f\"expected {self.default_sort_order_id}, found {base_metadata.default_sort_order_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertDefaultSpecId","title":"<code>AssertDefaultSpecId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's default spec id must match the requirement's <code>default-spec-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertDefaultSpecId(ValidatableTableRequirement):\n    \"\"\"The table's default spec id must match the requirement's `default-spec-id`.\"\"\"\n\n    type: Literal[\"assert-default-spec-id\"] = Field(default=\"assert-default-spec-id\")\n    default_spec_id: int = Field(..., alias=\"default-spec-id\")\n\n    def validate(self, base_metadata: TableMetadata | None) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif self.default_spec_id != base_metadata.default_spec_id:\n            raise CommitFailedException(\n                f\"Requirement failed: default spec id has changed: \"\n                f\"expected {self.default_spec_id}, found {base_metadata.default_spec_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertLastAssignedFieldId","title":"<code>AssertLastAssignedFieldId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's last assigned column id must match the requirement's <code>last-assigned-field-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertLastAssignedFieldId(ValidatableTableRequirement):\n    \"\"\"The table's last assigned column id must match the requirement's `last-assigned-field-id`.\"\"\"\n\n    type: Literal[\"assert-last-assigned-field-id\"] = Field(default=\"assert-last-assigned-field-id\")\n    last_assigned_field_id: int = Field(..., alias=\"last-assigned-field-id\")\n\n    def validate(self, base_metadata: TableMetadata | None) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif base_metadata.last_column_id != self.last_assigned_field_id:\n            raise CommitFailedException(\n                f\"Requirement failed: last assigned field id has changed: \"\n                f\"expected {self.last_assigned_field_id}, found {base_metadata.last_column_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertLastAssignedPartitionId","title":"<code>AssertLastAssignedPartitionId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table's last assigned partition id must match the requirement's <code>last-assigned-partition-id</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertLastAssignedPartitionId(ValidatableTableRequirement):\n    \"\"\"The table's last assigned partition id must match the requirement's `last-assigned-partition-id`.\"\"\"\n\n    type: Literal[\"assert-last-assigned-partition-id\"] = Field(default=\"assert-last-assigned-partition-id\")\n    last_assigned_partition_id: int | None = Field(..., alias=\"last-assigned-partition-id\")\n\n    def validate(self, base_metadata: TableMetadata | None) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif base_metadata.last_partition_id != self.last_assigned_partition_id:\n            raise CommitFailedException(\n                f\"Requirement failed: last assigned partition id has changed: \"\n                f\"expected {self.last_assigned_partition_id}, found {base_metadata.last_partition_id}\"\n            )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertRefSnapshotId","title":"<code>AssertRefSnapshotId</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table branch or tag identified by the requirement's <code>ref</code> must reference the requirement's <code>snapshot-id</code>.</p> <p>if <code>snapshot-id</code> is <code>null</code> or missing, the ref must not already exist.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertRefSnapshotId(ValidatableTableRequirement):\n    \"\"\"The table branch or tag identified by the requirement's `ref` must reference the requirement's `snapshot-id`.\n\n    if `snapshot-id` is `null` or missing, the ref must not already exist.\n    \"\"\"\n\n    type: Literal[\"assert-ref-snapshot-id\"] = Field(default=\"assert-ref-snapshot-id\")\n    ref: str = Field(...)\n    snapshot_id: int | None = Field(default=None, alias=\"snapshot-id\")\n\n    @model_serializer(mode=\"wrap\")\n    def serialize_model(self, handler: ModelWrapSerializerWithoutInfo) -&gt; dict[str, Any]:\n        partial_result = handler(self)\n        # Ensure \"snapshot-id\" is always present, even if value is None\n        return {**partial_result, \"snapshot-id\": self.snapshot_id}\n\n    def validate(self, base_metadata: TableMetadata | None) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif len(base_metadata.snapshots) == 0 and self.ref != MAIN_BRANCH:\n            raise CommitFailedException(\n                f\"Requirement failed: Table has no snapshots and can only be written to the {MAIN_BRANCH} BRANCH.\"\n            )\n        elif snapshot_ref := base_metadata.refs.get(self.ref):\n            ref_type = snapshot_ref.snapshot_ref_type\n            if self.snapshot_id is None:\n                raise CommitFailedException(f\"Requirement failed: {ref_type} {self.ref} was created concurrently\")\n            elif self.snapshot_id != snapshot_ref.snapshot_id:\n                raise CommitFailedException(\n                    f\"Requirement failed: {ref_type} {self.ref} has changed: \"\n                    f\"expected id {self.snapshot_id}, found {snapshot_ref.snapshot_id}\"\n                )\n        elif self.snapshot_id is not None:\n            raise CommitFailedException(f\"Requirement failed: branch or tag {self.ref} is missing, expected {self.snapshot_id}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.AssertTableUUID","title":"<code>AssertTableUUID</code>","text":"<p>               Bases: <code>ValidatableTableRequirement</code></p> <p>The table UUID must match the requirement's <code>uuid</code>.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class AssertTableUUID(ValidatableTableRequirement):\n    \"\"\"The table UUID must match the requirement's `uuid`.\"\"\"\n\n    type: Literal[\"assert-table-uuid\"] = Field(default=\"assert-table-uuid\")\n    uuid: uuid.UUID\n\n    def validate(self, base_metadata: TableMetadata | None) -&gt; None:\n        if base_metadata is None:\n            raise CommitFailedException(\"Requirement failed: current table metadata is missing\")\n        elif self.uuid != base_metadata.table_uuid:\n            raise CommitFailedException(f\"Table UUID does not match: {self.uuid} != {base_metadata.table_uuid}\")\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.UpdateTableMetadata","title":"<code>UpdateTableMetadata</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[U]</code></p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class UpdateTableMetadata(ABC, Generic[U]):\n    _transaction: Transaction\n\n    def __init__(self, transaction: Transaction) -&gt; None:\n        self._transaction = transaction\n\n    @abstractmethod\n    def _commit(self) -&gt; UpdatesAndRequirements: ...\n\n    def commit(self) -&gt; None:\n        self._transaction._apply(*self._commit())\n\n    def __exit__(self, _: Any, value: Any, traceback: Any) -&gt; None:\n        \"\"\"Close and commit the change.\"\"\"\n        self.commit()\n\n    def __enter__(self) -&gt; U:\n        \"\"\"Update the table.\"\"\"\n        return self  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.UpdateTableMetadata.__enter__","title":"<code>__enter__()</code>","text":"<p>Update the table.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>def __enter__(self) -&gt; U:\n    \"\"\"Update the table.\"\"\"\n    return self  # type: ignore\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.UpdateTableMetadata.__exit__","title":"<code>__exit__(_, value, traceback)</code>","text":"<p>Close and commit the change.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>def __exit__(self, _: Any, value: Any, traceback: Any) -&gt; None:\n    \"\"\"Close and commit the change.\"\"\"\n    self.commit()\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.ValidatableTableRequirement","title":"<code>ValidatableTableRequirement</code>","text":"<p>               Bases: <code>IcebergBaseModel</code></p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>class ValidatableTableRequirement(IcebergBaseModel):\n    type: str\n\n    @abstractmethod\n    def validate(self, base_metadata: TableMetadata | None) -&gt; None:\n        \"\"\"Validate the requirement against the base metadata.\n\n        Args:\n            base_metadata: The base metadata to be validated against.\n\n        Raises:\n            CommitFailedException: When the requirement is not met.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.ValidatableTableRequirement.validate","title":"<code>validate(base_metadata)</code>  <code>abstractmethod</code>","text":"<p>Validate the requirement against the base metadata.</p> <p>Parameters:</p> Name Type Description Default <code>base_metadata</code> <code>TableMetadata | None</code> <p>The base metadata to be validated against.</p> required <p>Raises:</p> Type Description <code>CommitFailedException</code> <p>When the requirement is not met.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>@abstractmethod\ndef validate(self, base_metadata: TableMetadata | None) -&gt; None:\n    \"\"\"Validate the requirement against the base metadata.\n\n    Args:\n        base_metadata: The base metadata to be validated against.\n\n    Raises:\n        CommitFailedException: When the requirement is not met.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/pyiceberg/table/update/#pyiceberg.table.update.update_table_metadata","title":"<code>update_table_metadata(base_metadata, updates, enforce_validation=False, metadata_location=None)</code>","text":"<p>Update the table metadata with the given updates in one transaction.</p> <p>Parameters:</p> Name Type Description Default <code>base_metadata</code> <code>TableMetadata</code> <p>The base metadata to be updated.</p> required <code>updates</code> <code>tuple[TableUpdate, ...]</code> <p>The updates in one transaction.</p> required <code>enforce_validation</code> <code>bool</code> <p>Whether to trigger validation after applying the updates.</p> <code>False</code> <code>metadata_location</code> <code>str | None</code> <p>Current metadata location of the table</p> <code>None</code> <p>Returns:</p> Type Description <code>TableMetadata</code> <p>The metadata with the updates applied.</p> Source code in <code>pyiceberg/table/update/__init__.py</code> <pre><code>def update_table_metadata(\n    base_metadata: TableMetadata,\n    updates: tuple[TableUpdate, ...],\n    enforce_validation: bool = False,\n    metadata_location: str | None = None,\n) -&gt; TableMetadata:\n    \"\"\"Update the table metadata with the given updates in one transaction.\n\n    Args:\n        base_metadata: The base metadata to be updated.\n        updates: The updates in one transaction.\n        enforce_validation: Whether to trigger validation after applying the updates.\n        metadata_location: Current metadata location of the table\n\n    Returns:\n        The metadata with the updates applied.\n    \"\"\"\n    context = _TableMetadataUpdateContext()\n    new_metadata = base_metadata\n\n    for update in updates:\n        new_metadata = _apply_table_update(update, new_metadata, context)\n\n    # Update last_updated_ms if it was not updated by update operations\n    if context.has_changes():\n        if metadata_location:\n            new_metadata = _update_table_metadata_log(new_metadata, metadata_location, base_metadata.last_updated_ms)\n        if base_metadata.last_updated_ms == new_metadata.last_updated_ms:\n            new_metadata = new_metadata.model_copy(update={\"last_updated_ms\": datetime_to_millis(datetime.now().astimezone())})\n\n    # Check correctness of partition spec, and sort order\n    new_metadata.spec().check_compatible(new_metadata.schema())\n    new_metadata.sort_order().check_compatible(new_metadata.schema())\n\n    if enforce_validation:\n        return TableMetadataUtil.parse_obj(new_metadata.model_dump())\n    else:\n        return new_metadata.model_copy(deep=True)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/","title":"schema","text":""},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema","title":"<code>UpdateSchema</code>","text":"<p>               Bases: <code>UpdateTableMetadata['UpdateSchema']</code></p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>class UpdateSchema(UpdateTableMetadata[\"UpdateSchema\"]):\n    _schema: Schema\n    _last_column_id: itertools.count[int]\n    _identifier_field_names: set[str]\n\n    _adds: dict[int, list[NestedField]] = {}\n    _updates: dict[int, NestedField] = {}\n    _deletes: set[int] = set()\n    _moves: dict[int, list[_Move]] = {}\n\n    _added_name_to_id: dict[str, int] = {}\n    # Part of https://github.com/apache/iceberg/pull/8393\n    _id_to_parent: dict[int, str] = {}\n    _allow_incompatible_changes: bool\n    _case_sensitive: bool\n\n    def __init__(\n        self,\n        transaction: Transaction,\n        allow_incompatible_changes: bool = False,\n        case_sensitive: bool = True,\n        schema: Schema | None = None,\n        name_mapping: NameMapping | None = None,\n    ) -&gt; None:\n        super().__init__(transaction)\n\n        if isinstance(schema, Schema):\n            self._schema = schema\n            self._last_column_id = itertools.count(1 + schema.highest_field_id)\n        else:\n            self._schema = self._transaction.table_metadata.schema()\n            self._last_column_id = itertools.count(1 + self._transaction.table_metadata.last_column_id)\n\n        self._name_mapping = name_mapping\n        self._identifier_field_names = self._schema.identifier_field_names()\n\n        self._adds = {}\n        self._updates = {}\n        self._deletes = set()\n        self._moves = {}\n\n        self._added_name_to_id = {}\n\n        def get_column_name(field_id: int) -&gt; str:\n            column_name = self._schema.find_column_name(column_id=field_id)\n            if column_name is None:\n                raise ValueError(f\"Could not find field-id: {field_id}\")\n            return column_name\n\n        self._id_to_parent = {\n            field_id: get_column_name(parent_field_id) for field_id, parent_field_id in self._schema._lazy_id_to_parent.items()\n        }\n\n        self._allow_incompatible_changes = allow_incompatible_changes\n        self._case_sensitive = case_sensitive\n        self._transaction = transaction\n\n    def case_sensitive(self, case_sensitive: bool) -&gt; UpdateSchema:\n        \"\"\"Determine if the case of schema needs to be considered when comparing column names.\n\n        Args:\n            case_sensitive: When false case is not considered in column name comparisons.\n\n        Returns:\n            This for method chaining\n        \"\"\"\n        self._case_sensitive = case_sensitive\n        return self\n\n    def union_by_name(\n        # TODO: Move TableProperties.DEFAULT_FORMAT_VERSION to separate file and set that as format_version default.\n        self,\n        new_schema: Schema | pa.Schema,\n        format_version: TableVersion = 2,\n    ) -&gt; UpdateSchema:\n        from pyiceberg.catalog import Catalog\n\n        visit_with_partner(\n            Catalog._convert_schema_if_needed(new_schema, format_version=format_version),\n            -1,\n            _UnionByNameVisitor(update_schema=self, existing_schema=self._schema, case_sensitive=self._case_sensitive),\n            # type: ignore\n            PartnerIdByNameAccessor(partner_schema=self._schema, case_sensitive=self._case_sensitive),\n        )\n        return self\n\n    def add_column(\n        self,\n        path: str | tuple[str, ...],\n        field_type: IcebergType,\n        doc: str | None = None,\n        required: bool = False,\n        default_value: L | None = None,\n    ) -&gt; UpdateSchema:\n        \"\"\"Add a new column to a nested struct or Add a new top-level column.\n\n        Because \".\" may be interpreted as a column path separator or may be used in field names, it\n        is not allowed to add nested column by passing in a string. To add to nested structures or\n        to add fields with names that contain \".\" use a tuple instead to indicate the path.\n\n        If type is a nested type, its field IDs are reassigned when added to the existing schema.\n\n        Args:\n            path: Name for the new column.\n            field_type: Type for the new column.\n            doc: Documentation string for the new column.\n            required: Whether the new column is required.\n            default_value: Default value for the new column.\n\n        Returns:\n            This for method chaining.\n        \"\"\"\n        if isinstance(path, str):\n            if \".\" in path:\n                raise ValueError(f\"Cannot add column with ambiguous name: {path}, provide a tuple instead\")\n            path = (path,)\n\n        name = path[-1]\n        parent = path[:-1]\n\n        full_name = \".\".join(path)\n        parent_full_path = \".\".join(parent)\n        parent_id: int = TABLE_ROOT_ID\n\n        if len(parent) &gt; 0:\n            parent_field = self._schema.find_field(parent_full_path, self._case_sensitive)\n            parent_type = parent_field.field_type\n            if isinstance(parent_type, MapType):\n                parent_field = parent_type.value_field\n            elif isinstance(parent_type, ListType):\n                parent_field = parent_type.element_field\n\n            if not parent_field.field_type.is_struct:\n                raise ValueError(f\"Cannot add column '{name}' to non-struct type: {parent_full_path}\")\n\n            parent_id = parent_field.field_id\n\n        existing_field = None\n        try:\n            existing_field = self._schema.find_field(full_name, self._case_sensitive)\n        except ValueError:\n            pass\n\n        if existing_field is not None and existing_field.field_id not in self._deletes:\n            raise ValueError(f\"Cannot add column, name already exists: {full_name}\")\n\n        # assign new IDs in order\n        new_id = self.assign_new_column_id()\n        new_type = assign_fresh_schema_ids(field_type, self.assign_new_column_id)\n\n        if default_value is not None:\n            try:\n                # To make sure that the value is valid for the type\n                initial_default = literal(default_value).to(new_type).value\n            except ValueError as e:\n                raise ValueError(f\"Invalid default value: {e}\") from e\n        else:\n            initial_default = default_value  # type: ignore\n\n        if (required and initial_default is None) and not self._allow_incompatible_changes:\n            # Table format version 1 and 2 cannot add required column because there is no initial value\n            raise ValueError(f\"Incompatible change: cannot add required column: {'.'.join(path)}\")\n\n        # update tracking for moves\n        self._added_name_to_id[full_name] = new_id\n        self._id_to_parent[new_id] = parent_full_path\n\n        field = NestedField(\n            field_id=new_id,\n            name=name,\n            field_type=new_type,\n            required=required,\n            doc=doc,\n            initial_default=initial_default,\n            write_default=initial_default,\n        )\n\n        if parent_id in self._adds:\n            self._adds[parent_id].append(field)\n        else:\n            self._adds[parent_id] = [field]\n\n        return self\n\n    def delete_column(self, path: str | tuple[str, ...]) -&gt; UpdateSchema:\n        \"\"\"Delete a column from a table.\n\n        Args:\n            path: The path to the column.\n\n        Returns:\n            The UpdateSchema with the delete operation staged.\n        \"\"\"\n        name = (path,) if isinstance(path, str) else path\n        full_name = \".\".join(name)\n\n        field = self._schema.find_field(full_name, case_sensitive=self._case_sensitive)\n\n        if field.field_id in self._adds:\n            raise ValueError(f\"Cannot delete a column that has additions: {full_name}\")\n        if field.field_id in self._updates:\n            raise ValueError(f\"Cannot delete a column that has updates: {full_name}\")\n\n        self._deletes.add(field.field_id)\n\n        return self\n\n    def set_default_value(self, path: str | tuple[str, ...], default_value: L | None) -&gt; UpdateSchema:\n        \"\"\"Set the default value of a column.\n\n        Args:\n            path: The path to the column.\n\n        Returns:\n            The UpdateSchema with the delete operation staged.\n        \"\"\"\n        self._set_column_default_value(path, default_value)\n\n        return self\n\n    def rename_column(self, path_from: str | tuple[str, ...], new_name: str) -&gt; UpdateSchema:\n        \"\"\"Update the name of a column.\n\n        Args:\n            path_from: The path to the column to be renamed.\n            new_name: The new path of the column.\n\n        Returns:\n            The UpdateSchema with the rename operation staged.\n        \"\"\"\n        path_from = \".\".join(path_from) if isinstance(path_from, tuple) else path_from\n        field_from = self._schema.find_field(path_from, self._case_sensitive)\n\n        if field_from.field_id in self._deletes:\n            raise ValueError(f\"Cannot rename a column that will be deleted: {path_from}\")\n\n        if updated := self._updates.get(field_from.field_id):\n            self._updates[field_from.field_id] = NestedField(\n                field_id=updated.field_id,\n                name=new_name,\n                field_type=updated.field_type,\n                doc=updated.doc,\n                required=updated.required,\n                initial_default=updated.initial_default,\n                write_default=updated.write_default,\n            )\n        else:\n            self._updates[field_from.field_id] = NestedField(\n                field_id=field_from.field_id,\n                name=new_name,\n                field_type=field_from.field_type,\n                doc=field_from.doc,\n                required=field_from.required,\n                initial_default=field_from.initial_default,\n                write_default=field_from.write_default,\n            )\n\n        # Lookup the field because of casing\n        from_field_correct_casing = self._schema.find_column_name(field_from.field_id)\n        if from_field_correct_casing in self._identifier_field_names:\n            self._identifier_field_names.remove(from_field_correct_casing)\n            new_identifier_path = f\"{from_field_correct_casing[: -len(field_from.name)]}{new_name}\"\n            self._identifier_field_names.add(new_identifier_path)\n\n        return self\n\n    def make_column_optional(self, path: str | tuple[str, ...]) -&gt; UpdateSchema:\n        \"\"\"Make a column optional.\n\n        Args:\n            path: The path to the field.\n\n        Returns:\n            The UpdateSchema with the requirement change staged.\n        \"\"\"\n        self._set_column_requirement(path, required=False)\n        return self\n\n    def set_identifier_fields(self, *fields: str) -&gt; None:\n        self._identifier_field_names = set(fields)\n\n    def _set_column_requirement(self, path: str | tuple[str, ...], required: bool) -&gt; None:\n        path = (path,) if isinstance(path, str) else path\n        name = \".\".join(path)\n\n        field = self._schema.find_field(name, self._case_sensitive)\n\n        if (field.required and required) or (field.optional and not required):\n            # if the change is a noop, allow it even if allowIncompatibleChanges is false\n            return\n\n        if not self._allow_incompatible_changes and required:\n            raise ValueError(f\"Cannot change column nullability: {name}: optional -&gt; required\")\n\n        if field.field_id in self._deletes:\n            raise ValueError(f\"Cannot update a column that will be deleted: {name}\")\n\n        if updated := self._updates.get(field.field_id):\n            self._updates[field.field_id] = NestedField(\n                field_id=updated.field_id,\n                name=updated.name,\n                field_type=updated.field_type,\n                doc=updated.doc,\n                required=required,\n                initial_default=updated.initial_default,\n                write_default=updated.write_default,\n            )\n        else:\n            self._updates[field.field_id] = NestedField(\n                field_id=field.field_id,\n                name=field.name,\n                field_type=field.field_type,\n                doc=field.doc,\n                required=required,\n                initial_default=field.initial_default,\n                write_default=field.write_default,\n            )\n\n    def _set_column_default_value(self, path: str | tuple[str, ...], default_value: Any) -&gt; None:\n        path = (path,) if isinstance(path, str) else path\n        name = \".\".join(path)\n\n        field = self._schema.find_field(name, self._case_sensitive)\n\n        if default_value is not None:\n            try:\n                # To make sure that the value is valid for the type\n                default_value = literal(default_value).to(field.field_type).value\n            except ValueError as e:\n                raise ValueError(f\"Invalid default value: {e}\") from e\n\n        if field.required and default_value == field.write_default:\n            # if the change is a noop, allow it even if allowIncompatibleChanges is false\n            return\n\n        if not self._allow_incompatible_changes and field.required and default_value is None:\n            raise ValueError(\"Cannot change change default-value of a required column to None\")\n\n        if field.field_id in self._deletes:\n            raise ValueError(f\"Cannot update a column that will be deleted: {name}\")\n\n        if updated := self._updates.get(field.field_id):\n            self._updates[field.field_id] = NestedField(\n                field_id=updated.field_id,\n                name=updated.name,\n                field_type=updated.field_type,\n                doc=updated.doc,\n                required=updated.required,\n                initial_default=updated.initial_default,\n                write_default=default_value,\n            )\n        else:\n            self._updates[field.field_id] = NestedField(\n                field_id=field.field_id,\n                name=field.name,\n                field_type=field.field_type,\n                doc=field.doc,\n                required=field.required,\n                initial_default=field.initial_default,\n                write_default=default_value,\n            )\n\n    def update_column(\n        self,\n        path: str | tuple[str, ...],\n        field_type: IcebergType | None = None,\n        required: bool | None = None,\n        doc: str | None = None,\n    ) -&gt; UpdateSchema:\n        \"\"\"Update the type of column.\n\n        Args:\n            path: The path to the field.\n            field_type: The new type\n            required: If the field should be required\n            doc: Documentation describing the column\n\n        Returns:\n            The UpdateSchema with the type update staged.\n        \"\"\"\n        path = (path,) if isinstance(path, str) else path\n        full_name = \".\".join(path)\n\n        if field_type is None and required is None and doc is None:\n            return self\n\n        field = self._schema.find_field(full_name, self._case_sensitive)\n\n        if field.field_id in self._deletes:\n            raise ValueError(f\"Cannot update a column that will be deleted: {full_name}\")\n\n        if field_type is not None:\n            if not field.field_type.is_primitive:\n                raise ValidationError(f\"Cannot change column type: {field.field_type} is not a primitive\")\n\n            if not self._allow_incompatible_changes and field.field_type != field_type:\n                try:\n                    promote(field.field_type, field_type)\n                except ResolveError as e:\n                    raise ValidationError(f\"Cannot change column type: {full_name}: {field.field_type} -&gt; {field_type}\") from e\n\n        # if other updates for the same field exist in one transaction:\n        if updated := self._updates.get(field.field_id):\n            self._updates[field.field_id] = NestedField(\n                field_id=updated.field_id,\n                name=updated.name,\n                field_type=field_type or updated.field_type,\n                doc=doc if doc is not None else updated.doc,\n                required=updated.required,\n                initial_default=updated.initial_default,\n                write_default=updated.write_default,\n            )\n        else:\n            self._updates[field.field_id] = NestedField(\n                field_id=field.field_id,\n                name=field.name,\n                field_type=field_type or field.field_type,\n                doc=doc if doc is not None else field.doc,\n                required=field.required,\n                initial_default=field.initial_default,\n                write_default=field.write_default,\n            )\n\n        if required is not None:\n            self._set_column_requirement(path, required=required)\n\n        return self\n\n    def _find_for_move(self, name: str) -&gt; int | None:\n        try:\n            return self._schema.find_field(name, self._case_sensitive).field_id\n        except ValueError:\n            pass\n\n        return self._added_name_to_id.get(name)\n\n    def _move(self, move: _Move) -&gt; None:\n        if parent_name := self._id_to_parent.get(move.field_id):\n            parent_field = self._schema.find_field(parent_name, case_sensitive=self._case_sensitive)\n            if not parent_field.field_type.is_struct:\n                raise ValueError(f\"Cannot move fields in non-struct type: {parent_field.field_type}\")\n\n            if move.op == _MoveOperation.After or move.op == _MoveOperation.Before:\n                if move.other_field_id is None:\n                    raise ValueError(\"Expected other field when performing before/after move\")\n\n                if self._id_to_parent.get(move.field_id) != self._id_to_parent.get(move.other_field_id):\n                    raise ValueError(f\"Cannot move field {move.full_name} to a different struct\")\n\n            self._moves[parent_field.field_id] = self._moves.get(parent_field.field_id, []) + [move]\n        else:\n            # In the top level field\n            if move.op == _MoveOperation.After or move.op == _MoveOperation.Before:\n                if move.other_field_id is None:\n                    raise ValueError(\"Expected other field when performing before/after move\")\n\n                if other_struct := self._id_to_parent.get(move.other_field_id):\n                    raise ValueError(f\"Cannot move field {move.full_name} to a different struct: {other_struct}\")\n\n            self._moves[TABLE_ROOT_ID] = self._moves.get(TABLE_ROOT_ID, []) + [move]\n\n    def move_first(self, path: str | tuple[str, ...]) -&gt; UpdateSchema:\n        \"\"\"Move the field to the first position of the parent struct.\n\n        Args:\n            path: The path to the field.\n\n        Returns:\n            The UpdateSchema with the move operation staged.\n        \"\"\"\n        full_name = \".\".join(path) if isinstance(path, tuple) else path\n\n        field_id = self._find_for_move(full_name)\n\n        if field_id is None:\n            raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n        self._move(_Move(field_id=field_id, full_name=full_name, op=_MoveOperation.First))\n\n        return self\n\n    def move_before(self, path: str | tuple[str, ...], before_path: str | tuple[str, ...]) -&gt; UpdateSchema:\n        \"\"\"Move the field to before another field.\n\n        Args:\n            path: The path to the field.\n\n        Returns:\n            The UpdateSchema with the move operation staged.\n        \"\"\"\n        full_name = \".\".join(path) if isinstance(path, tuple) else path\n        field_id = self._find_for_move(full_name)\n\n        if field_id is None:\n            raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n        before_full_name = (\n            \".\".join(\n                before_path,\n            )\n            if isinstance(before_path, tuple)\n            else before_path\n        )\n        before_field_id = self._find_for_move(before_full_name)\n\n        if before_field_id is None:\n            raise ValueError(f\"Cannot move {full_name} before missing column: {before_full_name}\")\n\n        if field_id == before_field_id:\n            raise ValueError(f\"Cannot move {full_name} before itself\")\n\n        self._move(_Move(field_id=field_id, full_name=full_name, other_field_id=before_field_id, op=_MoveOperation.Before))\n\n        return self\n\n    def move_after(self, path: str | tuple[str, ...], after_name: str | tuple[str, ...]) -&gt; UpdateSchema:\n        \"\"\"Move the field to after another field.\n\n        Args:\n            path: The path to the field.\n\n        Returns:\n            The UpdateSchema with the move operation staged.\n        \"\"\"\n        full_name = \".\".join(path) if isinstance(path, tuple) else path\n\n        field_id = self._find_for_move(full_name)\n\n        if field_id is None:\n            raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n        after_path = \".\".join(after_name) if isinstance(after_name, tuple) else after_name\n        after_field_id = self._find_for_move(after_path)\n\n        if after_field_id is None:\n            raise ValueError(f\"Cannot move {full_name} after missing column: {after_path}\")\n\n        if field_id == after_field_id:\n            raise ValueError(f\"Cannot move {full_name} after itself\")\n\n        self._move(_Move(field_id=field_id, full_name=full_name, other_field_id=after_field_id, op=_MoveOperation.After))\n\n        return self\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        \"\"\"Apply the pending changes and commit.\"\"\"\n        from pyiceberg.table import TableProperties\n\n        new_schema = self._apply()\n\n        existing_schema_id = next(\n            (schema.schema_id for schema in self._transaction.table_metadata.schemas if schema == new_schema), None\n        )\n\n        requirements: tuple[TableRequirement, ...] = ()\n        updates: tuple[TableUpdate, ...] = ()\n\n        # Check if it is different current schema ID\n        if existing_schema_id != self._schema.schema_id:\n            requirements += (AssertCurrentSchemaId(current_schema_id=self._schema.schema_id),)\n            if existing_schema_id is None:\n                updates += (\n                    AddSchemaUpdate(schema=new_schema),\n                    SetCurrentSchemaUpdate(schema_id=-1),\n                )\n            else:\n                updates += (SetCurrentSchemaUpdate(schema_id=existing_schema_id),)\n\n            if name_mapping := self._name_mapping:\n                updated_name_mapping = update_mapping(name_mapping, self._updates, self._adds)\n                updates += (\n                    SetPropertiesUpdate(updates={TableProperties.DEFAULT_NAME_MAPPING: updated_name_mapping.model_dump_json()}),\n                )\n\n        return updates, requirements\n\n    def _apply(self) -&gt; Schema:\n        \"\"\"Apply the pending changes to the original schema and returns the result.\n\n        Returns:\n            the result Schema when all pending updates are applied\n        \"\"\"\n        struct = visit(self._schema, _ApplyChanges(self._adds, self._updates, self._deletes, self._moves))\n        if struct is None:\n            # Should never happen\n            raise ValueError(\"Could not apply changes\")\n\n        # Check the field-ids\n        new_schema = Schema(*struct.fields)\n        from pyiceberg.partitioning import validate_partition_name\n\n        for spec in self._transaction.table_metadata.partition_specs:\n            for partition_field in spec.fields:\n                validate_partition_name(\n                    partition_field.name, partition_field.transform, partition_field.source_id, new_schema, set()\n                )\n        field_ids = set()\n        for name in self._identifier_field_names:\n            try:\n                field = new_schema.find_field(name, case_sensitive=self._case_sensitive)\n            except ValueError as e:\n                raise ValueError(\n                    f\"Cannot find identifier field {name}. In case of deletion, update the identifier fields first.\"\n                ) from e\n\n            field_ids.add(field.field_id)\n\n        if txn := self._transaction:\n            next_schema_id = 1 + (\n                max(schema.schema_id for schema in txn.table_metadata.schemas) if txn.table_metadata is not None else 0\n            )\n        else:\n            next_schema_id = 0\n\n        return Schema(*struct.fields, schema_id=next_schema_id, identifier_field_ids=field_ids)\n\n    def assign_new_column_id(self) -&gt; int:\n        return next(self._last_column_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.add_column","title":"<code>add_column(path, field_type, doc=None, required=False, default_value=None)</code>","text":"<p>Add a new column to a nested struct or Add a new top-level column.</p> <p>Because \".\" may be interpreted as a column path separator or may be used in field names, it is not allowed to add nested column by passing in a string. To add to nested structures or to add fields with names that contain \".\" use a tuple instead to indicate the path.</p> <p>If type is a nested type, its field IDs are reassigned when added to the existing schema.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | tuple[str, ...]</code> <p>Name for the new column.</p> required <code>field_type</code> <code>IcebergType</code> <p>Type for the new column.</p> required <code>doc</code> <code>str | None</code> <p>Documentation string for the new column.</p> <code>None</code> <code>required</code> <code>bool</code> <p>Whether the new column is required.</p> <code>False</code> <code>default_value</code> <code>L | None</code> <p>Default value for the new column.</p> <code>None</code> <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>This for method chaining.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def add_column(\n    self,\n    path: str | tuple[str, ...],\n    field_type: IcebergType,\n    doc: str | None = None,\n    required: bool = False,\n    default_value: L | None = None,\n) -&gt; UpdateSchema:\n    \"\"\"Add a new column to a nested struct or Add a new top-level column.\n\n    Because \".\" may be interpreted as a column path separator or may be used in field names, it\n    is not allowed to add nested column by passing in a string. To add to nested structures or\n    to add fields with names that contain \".\" use a tuple instead to indicate the path.\n\n    If type is a nested type, its field IDs are reassigned when added to the existing schema.\n\n    Args:\n        path: Name for the new column.\n        field_type: Type for the new column.\n        doc: Documentation string for the new column.\n        required: Whether the new column is required.\n        default_value: Default value for the new column.\n\n    Returns:\n        This for method chaining.\n    \"\"\"\n    if isinstance(path, str):\n        if \".\" in path:\n            raise ValueError(f\"Cannot add column with ambiguous name: {path}, provide a tuple instead\")\n        path = (path,)\n\n    name = path[-1]\n    parent = path[:-1]\n\n    full_name = \".\".join(path)\n    parent_full_path = \".\".join(parent)\n    parent_id: int = TABLE_ROOT_ID\n\n    if len(parent) &gt; 0:\n        parent_field = self._schema.find_field(parent_full_path, self._case_sensitive)\n        parent_type = parent_field.field_type\n        if isinstance(parent_type, MapType):\n            parent_field = parent_type.value_field\n        elif isinstance(parent_type, ListType):\n            parent_field = parent_type.element_field\n\n        if not parent_field.field_type.is_struct:\n            raise ValueError(f\"Cannot add column '{name}' to non-struct type: {parent_full_path}\")\n\n        parent_id = parent_field.field_id\n\n    existing_field = None\n    try:\n        existing_field = self._schema.find_field(full_name, self._case_sensitive)\n    except ValueError:\n        pass\n\n    if existing_field is not None and existing_field.field_id not in self._deletes:\n        raise ValueError(f\"Cannot add column, name already exists: {full_name}\")\n\n    # assign new IDs in order\n    new_id = self.assign_new_column_id()\n    new_type = assign_fresh_schema_ids(field_type, self.assign_new_column_id)\n\n    if default_value is not None:\n        try:\n            # To make sure that the value is valid for the type\n            initial_default = literal(default_value).to(new_type).value\n        except ValueError as e:\n            raise ValueError(f\"Invalid default value: {e}\") from e\n    else:\n        initial_default = default_value  # type: ignore\n\n    if (required and initial_default is None) and not self._allow_incompatible_changes:\n        # Table format version 1 and 2 cannot add required column because there is no initial value\n        raise ValueError(f\"Incompatible change: cannot add required column: {'.'.join(path)}\")\n\n    # update tracking for moves\n    self._added_name_to_id[full_name] = new_id\n    self._id_to_parent[new_id] = parent_full_path\n\n    field = NestedField(\n        field_id=new_id,\n        name=name,\n        field_type=new_type,\n        required=required,\n        doc=doc,\n        initial_default=initial_default,\n        write_default=initial_default,\n    )\n\n    if parent_id in self._adds:\n        self._adds[parent_id].append(field)\n    else:\n        self._adds[parent_id] = [field]\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.case_sensitive","title":"<code>case_sensitive(case_sensitive)</code>","text":"<p>Determine if the case of schema needs to be considered when comparing column names.</p> <p>Parameters:</p> Name Type Description Default <code>case_sensitive</code> <code>bool</code> <p>When false case is not considered in column name comparisons.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>This for method chaining</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def case_sensitive(self, case_sensitive: bool) -&gt; UpdateSchema:\n    \"\"\"Determine if the case of schema needs to be considered when comparing column names.\n\n    Args:\n        case_sensitive: When false case is not considered in column name comparisons.\n\n    Returns:\n        This for method chaining\n    \"\"\"\n    self._case_sensitive = case_sensitive\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.delete_column","title":"<code>delete_column(path)</code>","text":"<p>Delete a column from a table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | tuple[str, ...]</code> <p>The path to the column.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the delete operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def delete_column(self, path: str | tuple[str, ...]) -&gt; UpdateSchema:\n    \"\"\"Delete a column from a table.\n\n    Args:\n        path: The path to the column.\n\n    Returns:\n        The UpdateSchema with the delete operation staged.\n    \"\"\"\n    name = (path,) if isinstance(path, str) else path\n    full_name = \".\".join(name)\n\n    field = self._schema.find_field(full_name, case_sensitive=self._case_sensitive)\n\n    if field.field_id in self._adds:\n        raise ValueError(f\"Cannot delete a column that has additions: {full_name}\")\n    if field.field_id in self._updates:\n        raise ValueError(f\"Cannot delete a column that has updates: {full_name}\")\n\n    self._deletes.add(field.field_id)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.make_column_optional","title":"<code>make_column_optional(path)</code>","text":"<p>Make a column optional.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | tuple[str, ...]</code> <p>The path to the field.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the requirement change staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def make_column_optional(self, path: str | tuple[str, ...]) -&gt; UpdateSchema:\n    \"\"\"Make a column optional.\n\n    Args:\n        path: The path to the field.\n\n    Returns:\n        The UpdateSchema with the requirement change staged.\n    \"\"\"\n    self._set_column_requirement(path, required=False)\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.move_after","title":"<code>move_after(path, after_name)</code>","text":"<p>Move the field to after another field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | tuple[str, ...]</code> <p>The path to the field.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the move operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def move_after(self, path: str | tuple[str, ...], after_name: str | tuple[str, ...]) -&gt; UpdateSchema:\n    \"\"\"Move the field to after another field.\n\n    Args:\n        path: The path to the field.\n\n    Returns:\n        The UpdateSchema with the move operation staged.\n    \"\"\"\n    full_name = \".\".join(path) if isinstance(path, tuple) else path\n\n    field_id = self._find_for_move(full_name)\n\n    if field_id is None:\n        raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n    after_path = \".\".join(after_name) if isinstance(after_name, tuple) else after_name\n    after_field_id = self._find_for_move(after_path)\n\n    if after_field_id is None:\n        raise ValueError(f\"Cannot move {full_name} after missing column: {after_path}\")\n\n    if field_id == after_field_id:\n        raise ValueError(f\"Cannot move {full_name} after itself\")\n\n    self._move(_Move(field_id=field_id, full_name=full_name, other_field_id=after_field_id, op=_MoveOperation.After))\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.move_before","title":"<code>move_before(path, before_path)</code>","text":"<p>Move the field to before another field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | tuple[str, ...]</code> <p>The path to the field.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the move operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def move_before(self, path: str | tuple[str, ...], before_path: str | tuple[str, ...]) -&gt; UpdateSchema:\n    \"\"\"Move the field to before another field.\n\n    Args:\n        path: The path to the field.\n\n    Returns:\n        The UpdateSchema with the move operation staged.\n    \"\"\"\n    full_name = \".\".join(path) if isinstance(path, tuple) else path\n    field_id = self._find_for_move(full_name)\n\n    if field_id is None:\n        raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n    before_full_name = (\n        \".\".join(\n            before_path,\n        )\n        if isinstance(before_path, tuple)\n        else before_path\n    )\n    before_field_id = self._find_for_move(before_full_name)\n\n    if before_field_id is None:\n        raise ValueError(f\"Cannot move {full_name} before missing column: {before_full_name}\")\n\n    if field_id == before_field_id:\n        raise ValueError(f\"Cannot move {full_name} before itself\")\n\n    self._move(_Move(field_id=field_id, full_name=full_name, other_field_id=before_field_id, op=_MoveOperation.Before))\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.move_first","title":"<code>move_first(path)</code>","text":"<p>Move the field to the first position of the parent struct.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | tuple[str, ...]</code> <p>The path to the field.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the move operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def move_first(self, path: str | tuple[str, ...]) -&gt; UpdateSchema:\n    \"\"\"Move the field to the first position of the parent struct.\n\n    Args:\n        path: The path to the field.\n\n    Returns:\n        The UpdateSchema with the move operation staged.\n    \"\"\"\n    full_name = \".\".join(path) if isinstance(path, tuple) else path\n\n    field_id = self._find_for_move(full_name)\n\n    if field_id is None:\n        raise ValueError(f\"Cannot move missing column: {full_name}\")\n\n    self._move(_Move(field_id=field_id, full_name=full_name, op=_MoveOperation.First))\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.rename_column","title":"<code>rename_column(path_from, new_name)</code>","text":"<p>Update the name of a column.</p> <p>Parameters:</p> Name Type Description Default <code>path_from</code> <code>str | tuple[str, ...]</code> <p>The path to the column to be renamed.</p> required <code>new_name</code> <code>str</code> <p>The new path of the column.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the rename operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def rename_column(self, path_from: str | tuple[str, ...], new_name: str) -&gt; UpdateSchema:\n    \"\"\"Update the name of a column.\n\n    Args:\n        path_from: The path to the column to be renamed.\n        new_name: The new path of the column.\n\n    Returns:\n        The UpdateSchema with the rename operation staged.\n    \"\"\"\n    path_from = \".\".join(path_from) if isinstance(path_from, tuple) else path_from\n    field_from = self._schema.find_field(path_from, self._case_sensitive)\n\n    if field_from.field_id in self._deletes:\n        raise ValueError(f\"Cannot rename a column that will be deleted: {path_from}\")\n\n    if updated := self._updates.get(field_from.field_id):\n        self._updates[field_from.field_id] = NestedField(\n            field_id=updated.field_id,\n            name=new_name,\n            field_type=updated.field_type,\n            doc=updated.doc,\n            required=updated.required,\n            initial_default=updated.initial_default,\n            write_default=updated.write_default,\n        )\n    else:\n        self._updates[field_from.field_id] = NestedField(\n            field_id=field_from.field_id,\n            name=new_name,\n            field_type=field_from.field_type,\n            doc=field_from.doc,\n            required=field_from.required,\n            initial_default=field_from.initial_default,\n            write_default=field_from.write_default,\n        )\n\n    # Lookup the field because of casing\n    from_field_correct_casing = self._schema.find_column_name(field_from.field_id)\n    if from_field_correct_casing in self._identifier_field_names:\n        self._identifier_field_names.remove(from_field_correct_casing)\n        new_identifier_path = f\"{from_field_correct_casing[: -len(field_from.name)]}{new_name}\"\n        self._identifier_field_names.add(new_identifier_path)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.set_default_value","title":"<code>set_default_value(path, default_value)</code>","text":"<p>Set the default value of a column.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | tuple[str, ...]</code> <p>The path to the column.</p> required <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the delete operation staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def set_default_value(self, path: str | tuple[str, ...], default_value: L | None) -&gt; UpdateSchema:\n    \"\"\"Set the default value of a column.\n\n    Args:\n        path: The path to the column.\n\n    Returns:\n        The UpdateSchema with the delete operation staged.\n    \"\"\"\n    self._set_column_default_value(path, default_value)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/schema/#pyiceberg.table.update.schema.UpdateSchema.update_column","title":"<code>update_column(path, field_type=None, required=None, doc=None)</code>","text":"<p>Update the type of column.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | tuple[str, ...]</code> <p>The path to the field.</p> required <code>field_type</code> <code>IcebergType | None</code> <p>The new type</p> <code>None</code> <code>required</code> <code>bool | None</code> <p>If the field should be required</p> <code>None</code> <code>doc</code> <code>str | None</code> <p>Documentation describing the column</p> <code>None</code> <p>Returns:</p> Type Description <code>UpdateSchema</code> <p>The UpdateSchema with the type update staged.</p> Source code in <code>pyiceberg/table/update/schema.py</code> <pre><code>def update_column(\n    self,\n    path: str | tuple[str, ...],\n    field_type: IcebergType | None = None,\n    required: bool | None = None,\n    doc: str | None = None,\n) -&gt; UpdateSchema:\n    \"\"\"Update the type of column.\n\n    Args:\n        path: The path to the field.\n        field_type: The new type\n        required: If the field should be required\n        doc: Documentation describing the column\n\n    Returns:\n        The UpdateSchema with the type update staged.\n    \"\"\"\n    path = (path,) if isinstance(path, str) else path\n    full_name = \".\".join(path)\n\n    if field_type is None and required is None and doc is None:\n        return self\n\n    field = self._schema.find_field(full_name, self._case_sensitive)\n\n    if field.field_id in self._deletes:\n        raise ValueError(f\"Cannot update a column that will be deleted: {full_name}\")\n\n    if field_type is not None:\n        if not field.field_type.is_primitive:\n            raise ValidationError(f\"Cannot change column type: {field.field_type} is not a primitive\")\n\n        if not self._allow_incompatible_changes and field.field_type != field_type:\n            try:\n                promote(field.field_type, field_type)\n            except ResolveError as e:\n                raise ValidationError(f\"Cannot change column type: {full_name}: {field.field_type} -&gt; {field_type}\") from e\n\n    # if other updates for the same field exist in one transaction:\n    if updated := self._updates.get(field.field_id):\n        self._updates[field.field_id] = NestedField(\n            field_id=updated.field_id,\n            name=updated.name,\n            field_type=field_type or updated.field_type,\n            doc=doc if doc is not None else updated.doc,\n            required=updated.required,\n            initial_default=updated.initial_default,\n            write_default=updated.write_default,\n        )\n    else:\n        self._updates[field.field_id] = NestedField(\n            field_id=field.field_id,\n            name=field.name,\n            field_type=field_type or field.field_type,\n            doc=doc if doc is not None else field.doc,\n            required=field.required,\n            initial_default=field.initial_default,\n            write_default=field.write_default,\n        )\n\n    if required is not None:\n        self._set_column_requirement(path, required=required)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/","title":"snapshot","text":""},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ExpireSnapshots","title":"<code>ExpireSnapshots</code>","text":"<p>               Bases: <code>UpdateTableMetadata['ExpireSnapshots']</code></p> <p>Expire snapshots by ID.</p> <p>Use table.expire_snapshots().().commit() to run a specific operation. Use table.expire_snapshots().().().commit() to run multiple operations. Pending changes are applied on commit. Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>class ExpireSnapshots(UpdateTableMetadata[\"ExpireSnapshots\"]):\n    \"\"\"Expire snapshots by ID.\n\n    Use table.expire_snapshots().&lt;operation&gt;().commit() to run a specific operation.\n    Use table.expire_snapshots().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n    Pending changes are applied on commit.\n    \"\"\"\n\n    _updates: tuple[TableUpdate, ...]\n    _requirements: tuple[TableRequirement, ...]\n    _snapshot_ids_to_expire: set[int]\n\n    def __init__(self, transaction: Transaction) -&gt; None:\n        super().__init__(transaction)\n        self._updates = ()\n        self._requirements = ()\n        self._snapshot_ids_to_expire = set()\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        \"\"\"\n        Commit the staged updates and requirements.\n\n        This will remove the snapshots with the given IDs, but will always skip protected snapshots (branch/tag heads).\n\n        Returns:\n            Tuple of updates and requirements to be committed,\n            as required by the calling parent apply functions.\n        \"\"\"\n        # Remove any protected snapshot IDs from the set to expire, just in case\n        protected_ids = self._get_protected_snapshot_ids()\n        self._snapshot_ids_to_expire -= protected_ids\n        update = RemoveSnapshotsUpdate(snapshot_ids=self._snapshot_ids_to_expire)\n        self._updates += (update,)\n        return self._updates, self._requirements\n\n    def _get_protected_snapshot_ids(self) -&gt; set[int]:\n        \"\"\"\n        Get the IDs of protected snapshots.\n\n        These are the HEAD snapshots of all branches and all tagged snapshots.  These ids are to be excluded from expiration.\n\n        Returns:\n            Set of protected snapshot IDs to exclude from expiration.\n        \"\"\"\n        return {\n            ref.snapshot_id\n            for ref in self._transaction.table_metadata.refs.values()\n            if ref.snapshot_ref_type in [SnapshotRefType.TAG, SnapshotRefType.BRANCH]\n        }\n\n    def by_id(self, snapshot_id: int) -&gt; ExpireSnapshots:\n        \"\"\"\n        Expire a snapshot by its ID.\n\n        This will mark the snapshot for expiration.\n\n        Args:\n            snapshot_id (int): The ID of the snapshot to expire.\n        Returns:\n            This for method chaining.\n        \"\"\"\n        if self._transaction.table_metadata.snapshot_by_id(snapshot_id) is None:\n            raise ValueError(f\"Snapshot with ID {snapshot_id} does not exist.\")\n\n        if snapshot_id in self._get_protected_snapshot_ids():\n            raise ValueError(f\"Snapshot with ID {snapshot_id} is protected and cannot be expired.\")\n\n        self._snapshot_ids_to_expire.add(snapshot_id)\n\n        return self\n\n    def by_ids(self, snapshot_ids: list[int]) -&gt; ExpireSnapshots:\n        \"\"\"\n        Expire multiple snapshots by their IDs.\n\n        This will mark the snapshots for expiration.\n\n        Args:\n            snapshot_ids (List[int]): List of snapshot IDs to expire.\n        Returns:\n            This for method chaining.\n        \"\"\"\n        for snapshot_id in snapshot_ids:\n            self.by_id(snapshot_id)\n        return self\n\n    def older_than(self, dt: datetime) -&gt; ExpireSnapshots:\n        \"\"\"\n        Expire all unprotected snapshots with a timestamp older than a given value.\n\n        Args:\n            dt (datetime): Only snapshots with datetime &lt; this value will be expired.\n\n        Returns:\n            This for method chaining.\n        \"\"\"\n        protected_ids = self._get_protected_snapshot_ids()\n        expire_from = datetime_to_millis(dt)\n        for snapshot in self._transaction.table_metadata.snapshots:\n            if snapshot.timestamp_ms &lt; expire_from and snapshot.snapshot_id not in protected_ids:\n                self._snapshot_ids_to_expire.add(snapshot.snapshot_id)\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ExpireSnapshots.by_id","title":"<code>by_id(snapshot_id)</code>","text":"<p>Expire a snapshot by its ID.</p> <p>This will mark the snapshot for expiration.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_id</code> <code>int</code> <p>The ID of the snapshot to expire.</p> required <p>Returns:     This for method chaining.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def by_id(self, snapshot_id: int) -&gt; ExpireSnapshots:\n    \"\"\"\n    Expire a snapshot by its ID.\n\n    This will mark the snapshot for expiration.\n\n    Args:\n        snapshot_id (int): The ID of the snapshot to expire.\n    Returns:\n        This for method chaining.\n    \"\"\"\n    if self._transaction.table_metadata.snapshot_by_id(snapshot_id) is None:\n        raise ValueError(f\"Snapshot with ID {snapshot_id} does not exist.\")\n\n    if snapshot_id in self._get_protected_snapshot_ids():\n        raise ValueError(f\"Snapshot with ID {snapshot_id} is protected and cannot be expired.\")\n\n    self._snapshot_ids_to_expire.add(snapshot_id)\n\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ExpireSnapshots.by_ids","title":"<code>by_ids(snapshot_ids)</code>","text":"<p>Expire multiple snapshots by their IDs.</p> <p>This will mark the snapshots for expiration.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_ids</code> <code>List[int]</code> <p>List of snapshot IDs to expire.</p> required <p>Returns:     This for method chaining.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def by_ids(self, snapshot_ids: list[int]) -&gt; ExpireSnapshots:\n    \"\"\"\n    Expire multiple snapshots by their IDs.\n\n    This will mark the snapshots for expiration.\n\n    Args:\n        snapshot_ids (List[int]): List of snapshot IDs to expire.\n    Returns:\n        This for method chaining.\n    \"\"\"\n    for snapshot_id in snapshot_ids:\n        self.by_id(snapshot_id)\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ExpireSnapshots.older_than","title":"<code>older_than(dt)</code>","text":"<p>Expire all unprotected snapshots with a timestamp older than a given value.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>datetime</code> <p>Only snapshots with datetime &lt; this value will be expired.</p> required <p>Returns:</p> Type Description <code>ExpireSnapshots</code> <p>This for method chaining.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def older_than(self, dt: datetime) -&gt; ExpireSnapshots:\n    \"\"\"\n    Expire all unprotected snapshots with a timestamp older than a given value.\n\n    Args:\n        dt (datetime): Only snapshots with datetime &lt; this value will be expired.\n\n    Returns:\n        This for method chaining.\n    \"\"\"\n    protected_ids = self._get_protected_snapshot_ids()\n    expire_from = datetime_to_millis(dt)\n    for snapshot in self._transaction.table_metadata.snapshots:\n        if snapshot.timestamp_ms &lt; expire_from and snapshot.snapshot_id not in protected_ids:\n            self._snapshot_ids_to_expire.add(snapshot.snapshot_id)\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots","title":"<code>ManageSnapshots</code>","text":"<p>               Bases: <code>UpdateTableMetadata['ManageSnapshots']</code></p> <p>Run snapshot management operations using APIs.</p> <p>APIs include create branch, create tag, etc.</p> <p>Use table.manage_snapshots().().commit() to run a specific operation. Use table.manage_snapshots().().().commit() to run multiple operations. Pending changes are applied on commit. <p>We can also use context managers to make more changes. For example,</p> <p>with table.manage_snapshots() as ms:    ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>class ManageSnapshots(UpdateTableMetadata[\"ManageSnapshots\"]):\n    \"\"\"\n    Run snapshot management operations using APIs.\n\n    APIs include create branch, create tag, etc.\n\n    Use table.manage_snapshots().&lt;operation&gt;().commit() to run a specific operation.\n    Use table.manage_snapshots().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n    Pending changes are applied on commit.\n\n    We can also use context managers to make more changes. For example,\n\n    with table.manage_snapshots() as ms:\n       ms.create_tag(snapshot_id1, \"Tag_A\").create_tag(snapshot_id2, \"Tag_B\")\n    \"\"\"\n\n    _updates: tuple[TableUpdate, ...]\n    _requirements: tuple[TableRequirement, ...]\n\n    def __init__(self, transaction: Transaction) -&gt; None:\n        super().__init__(transaction)\n        self._updates = ()\n        self._requirements = ()\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        \"\"\"Apply the pending changes and commit.\"\"\"\n        return self._updates, self._requirements\n\n    def _commit_if_ref_updates_exist(self) -&gt; None:\n        \"\"\"Stage any pending ref updates to the transaction state.\"\"\"\n        if self._updates:\n            self._transaction._stage(*self._commit())\n            self._updates = ()\n            self._requirements = ()\n\n    def _remove_ref_snapshot(self, ref_name: str) -&gt; ManageSnapshots:\n        \"\"\"Remove a snapshot ref.\n\n        Args:\n            ref_name: branch / tag name to remove\n        Stages the updates and requirements for the remove-snapshot-ref.\n        Returns\n            This method for chaining\n        \"\"\"\n        updates = (RemoveSnapshotRefUpdate(ref_name=ref_name),)\n        requirements = (\n            AssertRefSnapshotId(\n                snapshot_id=self._transaction.table_metadata.refs[ref_name].snapshot_id\n                if ref_name in self._transaction.table_metadata.refs\n                else None,\n                ref=ref_name,\n            ),\n        )\n        self._updates += updates\n        self._requirements += requirements\n        return self\n\n    def create_tag(self, snapshot_id: int, tag_name: str, max_ref_age_ms: int | None = None) -&gt; ManageSnapshots:\n        \"\"\"\n        Create a new tag pointing to the given snapshot id.\n\n        Args:\n            snapshot_id (int): snapshot id of the existing snapshot to tag\n            tag_name (str): name of the tag\n            max_ref_age_ms (Optional[int]): max ref age in milliseconds\n\n        Returns:\n            This for method chaining\n        \"\"\"\n        update, requirement = self._transaction._set_ref_snapshot(\n            snapshot_id=snapshot_id,\n            ref_name=tag_name,\n            type=SnapshotRefType.TAG,\n            max_ref_age_ms=max_ref_age_ms,\n        )\n        self._updates += update\n        self._requirements += requirement\n        return self\n\n    def remove_tag(self, tag_name: str) -&gt; ManageSnapshots:\n        \"\"\"\n        Remove a tag.\n\n        Args:\n            tag_name (str): name of tag to remove\n        Returns:\n            This for method chaining\n        \"\"\"\n        return self._remove_ref_snapshot(ref_name=tag_name)\n\n    def create_branch(\n        self,\n        snapshot_id: int,\n        branch_name: str,\n        max_ref_age_ms: int | None = None,\n        max_snapshot_age_ms: int | None = None,\n        min_snapshots_to_keep: int | None = None,\n    ) -&gt; ManageSnapshots:\n        \"\"\"\n        Create a new branch pointing to the given snapshot id.\n\n        Args:\n            snapshot_id (int): snapshot id of existing snapshot at which the branch is created.\n            branch_name (str): name of the new branch\n            max_ref_age_ms (Optional[int]): max ref age in milliseconds\n            max_snapshot_age_ms (Optional[int]): max age of snapshots to keep in milliseconds\n            min_snapshots_to_keep (Optional[int]): min number of snapshots to keep for the branch\n        Returns:\n            This for method chaining\n        \"\"\"\n        update, requirement = self._transaction._set_ref_snapshot(\n            snapshot_id=snapshot_id,\n            ref_name=branch_name,\n            type=SnapshotRefType.BRANCH,\n            max_ref_age_ms=max_ref_age_ms,\n            max_snapshot_age_ms=max_snapshot_age_ms,\n            min_snapshots_to_keep=min_snapshots_to_keep,\n        )\n        self._updates += update\n        self._requirements += requirement\n        return self\n\n    def remove_branch(self, branch_name: str) -&gt; ManageSnapshots:\n        \"\"\"\n        Remove a branch.\n\n        Args:\n            branch_name (str): name of branch to remove\n        Returns:\n            This for method chaining\n        \"\"\"\n        return self._remove_ref_snapshot(ref_name=branch_name)\n\n    def set_current_snapshot(self, snapshot_id: int | None = None, ref_name: str | None = None) -&gt; ManageSnapshots:\n        \"\"\"Set the current snapshot to a specific snapshot ID or ref.\n\n        Args:\n            snapshot_id: The ID of the snapshot to set as current.\n            ref_name: The snapshot reference (branch or tag) to set as current.\n\n        Returns:\n            This for method chaining.\n\n        Raises:\n            ValueError: If neither or both arguments are provided, or if the snapshot/ref does not exist.\n        \"\"\"\n        self._commit_if_ref_updates_exist()\n\n        if (snapshot_id is None) == (ref_name is None):\n            raise ValueError(\"Either snapshot_id or ref_name must be provided, not both\")\n\n        target_snapshot_id: int\n        if snapshot_id is not None:\n            target_snapshot_id = snapshot_id\n        else:\n            if ref_name not in self._transaction.table_metadata.refs:\n                raise ValueError(f\"Cannot find matching snapshot ID for ref: {ref_name}\")\n            target_snapshot_id = self._transaction.table_metadata.refs[ref_name].snapshot_id\n\n        if self._transaction.table_metadata.snapshot_by_id(target_snapshot_id) is None:\n            raise ValueError(f\"Cannot set current snapshot to unknown snapshot id: {target_snapshot_id}\")\n\n        update, requirement = self._transaction._set_ref_snapshot(\n            snapshot_id=target_snapshot_id,\n            ref_name=MAIN_BRANCH,\n            type=SnapshotRefType.BRANCH,\n        )\n        self._transaction._stage(update, requirement)\n        return self\n\n    def rollback_to_snapshot(self, snapshot_id: int) -&gt; ManageSnapshots:\n        \"\"\"Rollback the table to the given snapshot id.\n\n        The snapshot needs to be an ancestor of the current table state.\n\n        Args:\n            snapshot_id (int): rollback to this snapshot_id that used to be current.\n\n        Returns:\n            This for method chaining\n\n        Raises:\n            ValueError: If the snapshot does not exist or is not an ancestor of the current table state.\n        \"\"\"\n        if not self._transaction.table_metadata.snapshot_by_id(snapshot_id):\n            raise ValueError(f\"Cannot roll back to unknown snapshot id: {snapshot_id}\")\n\n        if not self._is_current_ancestor(snapshot_id):\n            raise ValueError(f\"Cannot roll back to snapshot, not an ancestor of the current state: {snapshot_id}\")\n\n        return self.set_current_snapshot(snapshot_id=snapshot_id)\n\n    def rollback_to_timestamp(self, timestamp_ms: int) -&gt; ManageSnapshots:\n        \"\"\"Rollback the table to the latest snapshot before the given timestamp.\n\n        Finds the latest ancestor snapshot whose timestamp is before the given timestamp and rolls back to it.\n\n        Args:\n            timestamp_ms: Rollback to the latest snapshot before this timestamp in milliseconds.\n\n        Returns:\n            This for method chaining\n\n        Raises:\n            ValueError: If no valid snapshot exists older than the given timestamp.\n        \"\"\"\n        snapshot = latest_ancestor_before_timestamp(self._transaction.table_metadata, timestamp_ms)\n        if snapshot is None:\n            raise ValueError(f\"Cannot roll back, no valid snapshot older than: {timestamp_ms}\")\n\n        return self.set_current_snapshot(snapshot_id=snapshot.snapshot_id)\n\n    def _is_current_ancestor(self, snapshot_id: int) -&gt; bool:\n        return snapshot_id in self._current_ancestors()\n\n    def _current_ancestors(self) -&gt; set[int]:\n        return {\n            a.snapshot_id\n            for a in ancestors_of(\n                self._transaction.table_metadata.current_snapshot(),\n                self._transaction.table_metadata,\n            )\n        }\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.create_branch","title":"<code>create_branch(snapshot_id, branch_name, max_ref_age_ms=None, max_snapshot_age_ms=None, min_snapshots_to_keep=None)</code>","text":"<p>Create a new branch pointing to the given snapshot id.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_id</code> <code>int</code> <p>snapshot id of existing snapshot at which the branch is created.</p> required <code>branch_name</code> <code>str</code> <p>name of the new branch</p> required <code>max_ref_age_ms</code> <code>Optional[int]</code> <p>max ref age in milliseconds</p> <code>None</code> <code>max_snapshot_age_ms</code> <code>Optional[int]</code> <p>max age of snapshots to keep in milliseconds</p> <code>None</code> <code>min_snapshots_to_keep</code> <code>Optional[int]</code> <p>min number of snapshots to keep for the branch</p> <code>None</code> <p>Returns:     This for method chaining</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def create_branch(\n    self,\n    snapshot_id: int,\n    branch_name: str,\n    max_ref_age_ms: int | None = None,\n    max_snapshot_age_ms: int | None = None,\n    min_snapshots_to_keep: int | None = None,\n) -&gt; ManageSnapshots:\n    \"\"\"\n    Create a new branch pointing to the given snapshot id.\n\n    Args:\n        snapshot_id (int): snapshot id of existing snapshot at which the branch is created.\n        branch_name (str): name of the new branch\n        max_ref_age_ms (Optional[int]): max ref age in milliseconds\n        max_snapshot_age_ms (Optional[int]): max age of snapshots to keep in milliseconds\n        min_snapshots_to_keep (Optional[int]): min number of snapshots to keep for the branch\n    Returns:\n        This for method chaining\n    \"\"\"\n    update, requirement = self._transaction._set_ref_snapshot(\n        snapshot_id=snapshot_id,\n        ref_name=branch_name,\n        type=SnapshotRefType.BRANCH,\n        max_ref_age_ms=max_ref_age_ms,\n        max_snapshot_age_ms=max_snapshot_age_ms,\n        min_snapshots_to_keep=min_snapshots_to_keep,\n    )\n    self._updates += update\n    self._requirements += requirement\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.create_tag","title":"<code>create_tag(snapshot_id, tag_name, max_ref_age_ms=None)</code>","text":"<p>Create a new tag pointing to the given snapshot id.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_id</code> <code>int</code> <p>snapshot id of the existing snapshot to tag</p> required <code>tag_name</code> <code>str</code> <p>name of the tag</p> required <code>max_ref_age_ms</code> <code>Optional[int]</code> <p>max ref age in milliseconds</p> <code>None</code> <p>Returns:</p> Type Description <code>ManageSnapshots</code> <p>This for method chaining</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def create_tag(self, snapshot_id: int, tag_name: str, max_ref_age_ms: int | None = None) -&gt; ManageSnapshots:\n    \"\"\"\n    Create a new tag pointing to the given snapshot id.\n\n    Args:\n        snapshot_id (int): snapshot id of the existing snapshot to tag\n        tag_name (str): name of the tag\n        max_ref_age_ms (Optional[int]): max ref age in milliseconds\n\n    Returns:\n        This for method chaining\n    \"\"\"\n    update, requirement = self._transaction._set_ref_snapshot(\n        snapshot_id=snapshot_id,\n        ref_name=tag_name,\n        type=SnapshotRefType.TAG,\n        max_ref_age_ms=max_ref_age_ms,\n    )\n    self._updates += update\n    self._requirements += requirement\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.remove_branch","title":"<code>remove_branch(branch_name)</code>","text":"<p>Remove a branch.</p> <p>Parameters:</p> Name Type Description Default <code>branch_name</code> <code>str</code> <p>name of branch to remove</p> required <p>Returns:     This for method chaining</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def remove_branch(self, branch_name: str) -&gt; ManageSnapshots:\n    \"\"\"\n    Remove a branch.\n\n    Args:\n        branch_name (str): name of branch to remove\n    Returns:\n        This for method chaining\n    \"\"\"\n    return self._remove_ref_snapshot(ref_name=branch_name)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.remove_tag","title":"<code>remove_tag(tag_name)</code>","text":"<p>Remove a tag.</p> <p>Parameters:</p> Name Type Description Default <code>tag_name</code> <code>str</code> <p>name of tag to remove</p> required <p>Returns:     This for method chaining</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def remove_tag(self, tag_name: str) -&gt; ManageSnapshots:\n    \"\"\"\n    Remove a tag.\n\n    Args:\n        tag_name (str): name of tag to remove\n    Returns:\n        This for method chaining\n    \"\"\"\n    return self._remove_ref_snapshot(ref_name=tag_name)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.rollback_to_snapshot","title":"<code>rollback_to_snapshot(snapshot_id)</code>","text":"<p>Rollback the table to the given snapshot id.</p> <p>The snapshot needs to be an ancestor of the current table state.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_id</code> <code>int</code> <p>rollback to this snapshot_id that used to be current.</p> required <p>Returns:</p> Type Description <code>ManageSnapshots</code> <p>This for method chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the snapshot does not exist or is not an ancestor of the current table state.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def rollback_to_snapshot(self, snapshot_id: int) -&gt; ManageSnapshots:\n    \"\"\"Rollback the table to the given snapshot id.\n\n    The snapshot needs to be an ancestor of the current table state.\n\n    Args:\n        snapshot_id (int): rollback to this snapshot_id that used to be current.\n\n    Returns:\n        This for method chaining\n\n    Raises:\n        ValueError: If the snapshot does not exist or is not an ancestor of the current table state.\n    \"\"\"\n    if not self._transaction.table_metadata.snapshot_by_id(snapshot_id):\n        raise ValueError(f\"Cannot roll back to unknown snapshot id: {snapshot_id}\")\n\n    if not self._is_current_ancestor(snapshot_id):\n        raise ValueError(f\"Cannot roll back to snapshot, not an ancestor of the current state: {snapshot_id}\")\n\n    return self.set_current_snapshot(snapshot_id=snapshot_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.rollback_to_timestamp","title":"<code>rollback_to_timestamp(timestamp_ms)</code>","text":"<p>Rollback the table to the latest snapshot before the given timestamp.</p> <p>Finds the latest ancestor snapshot whose timestamp is before the given timestamp and rolls back to it.</p> <p>Parameters:</p> Name Type Description Default <code>timestamp_ms</code> <code>int</code> <p>Rollback to the latest snapshot before this timestamp in milliseconds.</p> required <p>Returns:</p> Type Description <code>ManageSnapshots</code> <p>This for method chaining</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid snapshot exists older than the given timestamp.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def rollback_to_timestamp(self, timestamp_ms: int) -&gt; ManageSnapshots:\n    \"\"\"Rollback the table to the latest snapshot before the given timestamp.\n\n    Finds the latest ancestor snapshot whose timestamp is before the given timestamp and rolls back to it.\n\n    Args:\n        timestamp_ms: Rollback to the latest snapshot before this timestamp in milliseconds.\n\n    Returns:\n        This for method chaining\n\n    Raises:\n        ValueError: If no valid snapshot exists older than the given timestamp.\n    \"\"\"\n    snapshot = latest_ancestor_before_timestamp(self._transaction.table_metadata, timestamp_ms)\n    if snapshot is None:\n        raise ValueError(f\"Cannot roll back, no valid snapshot older than: {timestamp_ms}\")\n\n    return self.set_current_snapshot(snapshot_id=snapshot.snapshot_id)\n</code></pre>"},{"location":"reference/pyiceberg/table/update/snapshot/#pyiceberg.table.update.snapshot.ManageSnapshots.set_current_snapshot","title":"<code>set_current_snapshot(snapshot_id=None, ref_name=None)</code>","text":"<p>Set the current snapshot to a specific snapshot ID or ref.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot_id</code> <code>int | None</code> <p>The ID of the snapshot to set as current.</p> <code>None</code> <code>ref_name</code> <code>str | None</code> <p>The snapshot reference (branch or tag) to set as current.</p> <code>None</code> <p>Returns:</p> Type Description <code>ManageSnapshots</code> <p>This for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither or both arguments are provided, or if the snapshot/ref does not exist.</p> Source code in <code>pyiceberg/table/update/snapshot.py</code> <pre><code>def set_current_snapshot(self, snapshot_id: int | None = None, ref_name: str | None = None) -&gt; ManageSnapshots:\n    \"\"\"Set the current snapshot to a specific snapshot ID or ref.\n\n    Args:\n        snapshot_id: The ID of the snapshot to set as current.\n        ref_name: The snapshot reference (branch or tag) to set as current.\n\n    Returns:\n        This for method chaining.\n\n    Raises:\n        ValueError: If neither or both arguments are provided, or if the snapshot/ref does not exist.\n    \"\"\"\n    self._commit_if_ref_updates_exist()\n\n    if (snapshot_id is None) == (ref_name is None):\n        raise ValueError(\"Either snapshot_id or ref_name must be provided, not both\")\n\n    target_snapshot_id: int\n    if snapshot_id is not None:\n        target_snapshot_id = snapshot_id\n    else:\n        if ref_name not in self._transaction.table_metadata.refs:\n            raise ValueError(f\"Cannot find matching snapshot ID for ref: {ref_name}\")\n        target_snapshot_id = self._transaction.table_metadata.refs[ref_name].snapshot_id\n\n    if self._transaction.table_metadata.snapshot_by_id(target_snapshot_id) is None:\n        raise ValueError(f\"Cannot set current snapshot to unknown snapshot id: {target_snapshot_id}\")\n\n    update, requirement = self._transaction._set_ref_snapshot(\n        snapshot_id=target_snapshot_id,\n        ref_name=MAIN_BRANCH,\n        type=SnapshotRefType.BRANCH,\n    )\n    self._transaction._stage(update, requirement)\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/table/update/sorting/","title":"sorting","text":""},{"location":"reference/pyiceberg/table/update/sorting/#pyiceberg.table.update.sorting.UpdateSortOrder","title":"<code>UpdateSortOrder</code>","text":"<p>               Bases: <code>UpdateTableMetadata['UpdateSortOrder']</code></p> Source code in <code>pyiceberg/table/update/sorting.py</code> <pre><code>class UpdateSortOrder(UpdateTableMetadata[\"UpdateSortOrder\"]):\n    _transaction: Transaction\n    _last_assigned_order_id: int | None\n    _case_sensitive: bool\n    _fields: list[SortField]\n\n    def __init__(self, transaction: Transaction, case_sensitive: bool = True) -&gt; None:\n        super().__init__(transaction)\n        self._fields: list[SortField] = []\n        self._case_sensitive: bool = case_sensitive\n        self._last_assigned_order_id: int | None = None\n\n    def _column_name_to_id(self, column_name: str) -&gt; int:\n        \"\"\"Map the column name to the column field id.\"\"\"\n        return (\n            self._transaction.table_metadata.schema()\n            .find_field(\n                name_or_id=column_name,\n                case_sensitive=self._case_sensitive,\n            )\n            .field_id\n        )\n\n    def _add_sort_field(\n        self,\n        source_id: int,\n        transform: Transform[Any, Any],\n        direction: SortDirection,\n        null_order: NullOrder,\n    ) -&gt; UpdateSortOrder:\n        \"\"\"Add a sort field to the sort order list.\"\"\"\n        self._fields.append(\n            SortField(\n                source_id=source_id,\n                transform=transform,\n                direction=direction,\n                null_order=null_order,\n            )\n        )\n        return self\n\n    def _reuse_or_create_sort_order_id(self) -&gt; int:\n        \"\"\"Return the last assigned sort order id or create a new one.\"\"\"\n        new_sort_order_id = INITIAL_SORT_ORDER_ID\n        for sort_order in self._transaction.table_metadata.sort_orders:\n            new_sort_order_id = max(new_sort_order_id, sort_order.order_id)\n            if sort_order.fields == self._fields:\n                return sort_order.order_id\n            elif new_sort_order_id &lt;= sort_order.order_id:\n                new_sort_order_id = sort_order.order_id + 1\n        return new_sort_order_id\n\n    def asc(\n        self, source_column_name: str, transform: Transform[Any, Any], null_order: NullOrder = NullOrder.NULLS_LAST\n    ) -&gt; UpdateSortOrder:\n        \"\"\"Add a sort field with ascending order.\"\"\"\n        return self._add_sort_field(\n            source_id=self._column_name_to_id(source_column_name),\n            transform=transform,\n            direction=SortDirection.ASC,\n            null_order=null_order,\n        )\n\n    def desc(\n        self, source_column_name: str, transform: Transform[Any, Any], null_order: NullOrder = NullOrder.NULLS_LAST\n    ) -&gt; UpdateSortOrder:\n        \"\"\"Add a sort field with descending order.\"\"\"\n        return self._add_sort_field(\n            source_id=self._column_name_to_id(source_column_name),\n            transform=transform,\n            direction=SortDirection.DESC,\n            null_order=null_order,\n        )\n\n    def _apply(self) -&gt; SortOrder:\n        \"\"\"Return the sort order.\"\"\"\n        if next(iter(self._fields), None) is None:\n            return UNSORTED_SORT_ORDER\n        else:\n            return SortOrder(*self._fields, order_id=self._reuse_or_create_sort_order_id())\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        \"\"\"Apply the pending changes and commit.\"\"\"\n        new_sort_order = self._apply()\n        requirements: tuple[TableRequirement, ...] = ()\n        updates: tuple[TableUpdate, ...] = ()\n\n        if (\n            self._transaction.table_metadata.default_sort_order_id != new_sort_order.order_id\n            and self._transaction.table_metadata.sort_order_by_id(new_sort_order.order_id) is None\n        ):\n            self._last_assigned_order_id = new_sort_order.order_id\n            updates = (AddSortOrderUpdate(sort_order=new_sort_order), SetDefaultSortOrderUpdate(sort_order_id=-1))\n        else:\n            updates = (SetDefaultSortOrderUpdate(sort_order_id=new_sort_order.order_id),)\n\n        required_last_assigned_sort_order_id = self._transaction.table_metadata.default_sort_order_id\n        requirements = (AssertDefaultSortOrderId(default_sort_order_id=required_last_assigned_sort_order_id),)\n\n        return updates, requirements\n</code></pre>"},{"location":"reference/pyiceberg/table/update/sorting/#pyiceberg.table.update.sorting.UpdateSortOrder.asc","title":"<code>asc(source_column_name, transform, null_order=NullOrder.NULLS_LAST)</code>","text":"<p>Add a sort field with ascending order.</p> Source code in <code>pyiceberg/table/update/sorting.py</code> <pre><code>def asc(\n    self, source_column_name: str, transform: Transform[Any, Any], null_order: NullOrder = NullOrder.NULLS_LAST\n) -&gt; UpdateSortOrder:\n    \"\"\"Add a sort field with ascending order.\"\"\"\n    return self._add_sort_field(\n        source_id=self._column_name_to_id(source_column_name),\n        transform=transform,\n        direction=SortDirection.ASC,\n        null_order=null_order,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/sorting/#pyiceberg.table.update.sorting.UpdateSortOrder.desc","title":"<code>desc(source_column_name, transform, null_order=NullOrder.NULLS_LAST)</code>","text":"<p>Add a sort field with descending order.</p> Source code in <code>pyiceberg/table/update/sorting.py</code> <pre><code>def desc(\n    self, source_column_name: str, transform: Transform[Any, Any], null_order: NullOrder = NullOrder.NULLS_LAST\n) -&gt; UpdateSortOrder:\n    \"\"\"Add a sort field with descending order.\"\"\"\n    return self._add_sort_field(\n        source_id=self._column_name_to_id(source_column_name),\n        transform=transform,\n        direction=SortDirection.DESC,\n        null_order=null_order,\n    )\n</code></pre>"},{"location":"reference/pyiceberg/table/update/spec/","title":"spec","text":""},{"location":"reference/pyiceberg/table/update/statistics/","title":"statistics","text":""},{"location":"reference/pyiceberg/table/update/statistics/#pyiceberg.table.update.statistics.UpdateStatistics","title":"<code>UpdateStatistics</code>","text":"<p>               Bases: <code>UpdateTableMetadata['UpdateStatistics']</code></p> <p>Run statistics management operations using APIs.</p> <p>APIs include set_statistics and remove statistics operations.</p> <p>Use table.update_statistics().().commit() to run a specific operation. Use table.update_statistics().().().commit() to run multiple operations. <p>Pending changes are applied on commit.</p> <p>We can also use context managers to make more changes. For example:</p> <p>with table.update_statistics() as update:     update.set_statistics(statistics_file=statistics_file)     update.remove_statistics(snapshot_id=2)</p> Source code in <code>pyiceberg/table/update/statistics.py</code> <pre><code>class UpdateStatistics(UpdateTableMetadata[\"UpdateStatistics\"]):\n    \"\"\"\n    Run statistics management operations using APIs.\n\n    APIs include set_statistics and remove statistics operations.\n\n    Use table.update_statistics().&lt;operation&gt;().commit() to run a specific operation.\n    Use table.update_statistics().&lt;operation-one&gt;().&lt;operation-two&gt;().commit() to run multiple operations.\n\n    Pending changes are applied on commit.\n\n    We can also use context managers to make more changes. For example:\n\n    with table.update_statistics() as update:\n        update.set_statistics(statistics_file=statistics_file)\n        update.remove_statistics(snapshot_id=2)\n    \"\"\"\n\n    _updates: tuple[TableUpdate, ...] = ()\n\n    def __init__(self, transaction: \"Transaction\") -&gt; None:\n        super().__init__(transaction)\n\n    def set_statistics(self, statistics_file: StatisticsFile) -&gt; \"UpdateStatistics\":\n        self._updates += (\n            SetStatisticsUpdate(\n                statistics=statistics_file,\n            ),\n        )\n\n        return self\n\n    def remove_statistics(self, snapshot_id: int) -&gt; \"UpdateStatistics\":\n        self._updates = (\n            RemoveStatisticsUpdate(\n                snapshot_id=snapshot_id,\n            ),\n        )\n\n        return self\n\n    def _commit(self) -&gt; UpdatesAndRequirements:\n        return self._updates, ()\n</code></pre>"},{"location":"reference/pyiceberg/table/update/validate/","title":"validate","text":""},{"location":"reference/pyiceberg/utils/","title":"utils","text":""},{"location":"reference/pyiceberg/utils/bin_packing/","title":"bin_packing","text":""},{"location":"reference/pyiceberg/utils/bin_packing/#pyiceberg.utils.bin_packing.PackingIterator","title":"<code>PackingIterator</code>","text":"<p>               Bases: <code>Generic[T]</code></p> Source code in <code>pyiceberg/utils/bin_packing.py</code> <pre><code>class PackingIterator(Generic[T]):\n    bins: list[Bin[T]]\n\n    def __init__(\n        self,\n        items: Iterable[T],\n        target_weight: int,\n        lookback: int,\n        weight_func: Callable[[T], int],\n        largest_bin_first: bool = False,\n    ) -&gt; None:\n        self.items = iter(items)\n        self.target_weight = target_weight\n        self.lookback = lookback\n        self.weight_func = weight_func\n        self.largest_bin_first = largest_bin_first\n        self.bins = []\n\n    def __iter__(self) -&gt; PackingIterator[T]:\n        \"\"\"Return an iterator for the PackingIterator class.\"\"\"\n        return self\n\n    def __next__(self) -&gt; list[T]:\n        \"\"\"Return the next item when iterating over the PackingIterator class.\"\"\"\n        while True:\n            try:\n                item = next(self.items)\n                weight = self.weight_func(item)\n                bin_ = self.find_bin(weight)\n                if bin_ is not None:\n                    bin_.add(item, weight)\n                else:\n                    bin_ = Bin(self.target_weight)\n                    bin_.add(item, weight)\n                    self.bins.append(bin_)\n\n                    if len(self.bins) &gt; self.lookback:\n                        return self.remove_bin().items\n            except StopIteration:\n                break\n\n        if len(self.bins) == 0:\n            raise StopIteration()\n\n        return self.remove_bin().items\n\n    def find_bin(self, weight: int) -&gt; Bin[T] | None:\n        for bin_ in self.bins:\n            if bin_.can_add(weight):\n                return bin_\n        return None\n\n    def remove_bin(self) -&gt; Bin[T]:\n        if self.largest_bin_first:\n            bin_ = max(self.bins, key=lambda b: b.weight())\n            self.bins.remove(bin_)\n            return bin_\n        else:\n            return self.bins.pop(0)\n</code></pre>"},{"location":"reference/pyiceberg/utils/bin_packing/#pyiceberg.utils.bin_packing.PackingIterator.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator for the PackingIterator class.</p> Source code in <code>pyiceberg/utils/bin_packing.py</code> <pre><code>def __iter__(self) -&gt; PackingIterator[T]:\n    \"\"\"Return an iterator for the PackingIterator class.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/utils/bin_packing/#pyiceberg.utils.bin_packing.PackingIterator.__next__","title":"<code>__next__()</code>","text":"<p>Return the next item when iterating over the PackingIterator class.</p> Source code in <code>pyiceberg/utils/bin_packing.py</code> <pre><code>def __next__(self) -&gt; list[T]:\n    \"\"\"Return the next item when iterating over the PackingIterator class.\"\"\"\n    while True:\n        try:\n            item = next(self.items)\n            weight = self.weight_func(item)\n            bin_ = self.find_bin(weight)\n            if bin_ is not None:\n                bin_.add(item, weight)\n            else:\n                bin_ = Bin(self.target_weight)\n                bin_.add(item, weight)\n                self.bins.append(bin_)\n\n                if len(self.bins) &gt; self.lookback:\n                    return self.remove_bin().items\n        except StopIteration:\n            break\n\n    if len(self.bins) == 0:\n        raise StopIteration()\n\n    return self.remove_bin().items\n</code></pre>"},{"location":"reference/pyiceberg/utils/concurrent/","title":"concurrent","text":"<p>Concurrency concepts that support efficient multi-threading.</p>"},{"location":"reference/pyiceberg/utils/concurrent/#pyiceberg.utils.concurrent.ExecutorFactory","title":"<code>ExecutorFactory</code>","text":"Source code in <code>pyiceberg/utils/concurrent.py</code> <pre><code>class ExecutorFactory:\n    _instance: Executor | None = None\n    _instance_pid: int | None = None\n\n    @staticmethod\n    def max_workers() -&gt; int | None:\n        \"\"\"Return the max number of workers configured.\"\"\"\n        return Config().get_int(\"max-workers\")\n\n    @staticmethod\n    def get_or_create() -&gt; Executor:\n        \"\"\"Return the same executor in each call.\"\"\"\n        # ThreadPoolExecutor cannot be shared across processes.  If a new pid is found it means\n        # there is a new process so a new executor is needed.  Otherwise, the executor may be in\n        # an invalid state and tasks submitted will not be started.\n        if ExecutorFactory._instance_pid != os.getpid():\n            ExecutorFactory._instance_pid = os.getpid()\n            ExecutorFactory._instance = None\n\n        if ExecutorFactory._instance is None:\n            max_workers = ExecutorFactory.max_workers()\n            ExecutorFactory._instance = ThreadPoolExecutor(max_workers=max_workers)\n\n        return ExecutorFactory._instance\n</code></pre>"},{"location":"reference/pyiceberg/utils/concurrent/#pyiceberg.utils.concurrent.ExecutorFactory.get_or_create","title":"<code>get_or_create()</code>  <code>staticmethod</code>","text":"<p>Return the same executor in each call.</p> Source code in <code>pyiceberg/utils/concurrent.py</code> <pre><code>@staticmethod\ndef get_or_create() -&gt; Executor:\n    \"\"\"Return the same executor in each call.\"\"\"\n    # ThreadPoolExecutor cannot be shared across processes.  If a new pid is found it means\n    # there is a new process so a new executor is needed.  Otherwise, the executor may be in\n    # an invalid state and tasks submitted will not be started.\n    if ExecutorFactory._instance_pid != os.getpid():\n        ExecutorFactory._instance_pid = os.getpid()\n        ExecutorFactory._instance = None\n\n    if ExecutorFactory._instance is None:\n        max_workers = ExecutorFactory.max_workers()\n        ExecutorFactory._instance = ThreadPoolExecutor(max_workers=max_workers)\n\n    return ExecutorFactory._instance\n</code></pre>"},{"location":"reference/pyiceberg/utils/concurrent/#pyiceberg.utils.concurrent.ExecutorFactory.max_workers","title":"<code>max_workers()</code>  <code>staticmethod</code>","text":"<p>Return the max number of workers configured.</p> Source code in <code>pyiceberg/utils/concurrent.py</code> <pre><code>@staticmethod\ndef max_workers() -&gt; int | None:\n    \"\"\"Return the max number of workers configured.\"\"\"\n    return Config().get_int(\"max-workers\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/config/","title":"config","text":""},{"location":"reference/pyiceberg/utils/config/#pyiceberg.utils.config.Config","title":"<code>Config</code>","text":"Source code in <code>pyiceberg/utils/config.py</code> <pre><code>class Config:\n    config: RecursiveDict\n\n    def __init__(self) -&gt; None:\n        config = self._from_configuration_files() or {}\n        config = merge_config(config, self._from_environment_variables(config))\n        self.config = FrozenDict(**config)\n\n    @staticmethod\n    def _from_configuration_files() -&gt; RecursiveDict | None:\n        \"\"\"Load the first configuration file that its finds.\n\n        Will first look in the PYICEBERG_HOME env variable,\n        and then in the home directory.\n        \"\"\"\n\n        def _load_yaml(directory: str | None) -&gt; RecursiveDict | None:\n            if directory:\n                path = os.path.join(directory, PYICEBERG_YML)\n                if os.path.isfile(path):\n                    with open(path, encoding=UTF8) as f:\n                        yml_str = f.read()\n                    file_config = strictyaml.load(yml_str).data\n                    file_config_lowercase = _lowercase_dictionary_keys(file_config)\n                    return file_config_lowercase\n            return None\n\n        # Directories to search for the configuration file\n        # The current search order is: PYICEBERG_HOME, home directory, then current directory\n        search_dirs = [os.environ.get(PYICEBERG_HOME), os.path.expanduser(\"~\"), os.getcwd()]\n\n        for directory in search_dirs:\n            if config := _load_yaml(directory):\n                return config\n\n        # Didn't find a config\n        return None\n\n    @staticmethod\n    def _from_environment_variables(config: RecursiveDict) -&gt; RecursiveDict:\n        \"\"\"Read the environment variables, to check if there are any prepended by PYICEBERG_.\n\n        Args:\n            config: Existing configuration that's being amended with configuration from environment variables.\n\n        Returns:\n            Amended configuration.\n        \"\"\"\n\n        def set_property(_config: RecursiveDict, path: list[str], config_value: str) -&gt; None:\n            while len(path) &gt; 0:\n                element = path.pop(0)\n                if len(path) == 0:\n                    # We're at the end\n                    _config[element] = config_value\n                else:\n                    # We have to go deeper\n                    if element not in _config:\n                        _config[element] = {}\n                    if isinstance(_config[element], dict):\n                        _config = _config[element]  # type: ignore\n                    else:\n                        raise ValueError(\n                            f\"Incompatible configurations, merging dict with a value: {'.'.join(path)}, value: {config_value}\"\n                        )\n\n        for env_var, config_value in os.environ.items():\n            # Make it lowercase to make it case-insensitive\n            env_var_lower = env_var.lower()\n            if env_var_lower.startswith(PYICEBERG.lower()):\n                key = env_var_lower[len(PYICEBERG) :]\n                parts = key.split(\"__\", maxsplit=2)\n                parts_normalized = [part.replace(\"__\", \".\").replace(\"_\", \"-\") for part in parts]\n                set_property(config, parts_normalized, config_value)\n\n        return config\n\n    def get_default_catalog_name(self) -&gt; str:\n        \"\"\"Return the default catalog name.\n\n        Returns: The name of the default catalog in `default-catalog`.\n                 Returns `default` when the key cannot be found in the config file.\n        \"\"\"\n        if default_catalog_name := self.config.get(DEFAULT_CATALOG):\n            if not isinstance(default_catalog_name, str):\n                raise ValueError(f\"Default catalog name should be a str: {default_catalog_name}\")\n            return default_catalog_name\n        return DEFAULT\n\n    def get_catalog_config(self, catalog_name: str) -&gt; RecursiveDict | None:\n        if CATALOG in self.config:\n            catalog_name_lower = catalog_name.lower()\n            catalogs = self.config[CATALOG]\n            if not isinstance(catalogs, dict):\n                raise ValueError(f\"Catalog configurations needs to be an object: {catalog_name}\")\n            if catalog_name_lower in catalogs:\n                catalog_conf = catalogs[catalog_name_lower]\n                if not isinstance(catalog_conf, dict):\n                    raise ValueError(f\"Configuration path catalogs.{catalog_name_lower} needs to be an object\")\n                return catalog_conf\n        return None\n\n    def get_known_catalogs(self) -&gt; list[str]:\n        catalogs = self.config.get(CATALOG, {})\n        if not isinstance(catalogs, dict):\n            raise ValueError(\"Catalog configurations needs to be an object\")\n        return list(catalogs.keys())\n\n    def get_int(self, key: str) -&gt; int | None:\n        if (val := self.config.get(key)) is not None:\n            try:\n                return int(val)  # type: ignore\n            except ValueError as err:\n                raise ValueError(f\"{key} should be an integer or left unset. Current value: {val}\") from err\n        return None\n\n    def get_bool(self, key: str) -&gt; bool | None:\n        if (val := self.config.get(key)) is not None:\n            try:\n                return strtobool(val)  # type: ignore\n            except ValueError as err:\n                raise ValueError(f\"{key} should be a boolean or left unset. Current value: {val}\") from err\n        return None\n</code></pre>"},{"location":"reference/pyiceberg/utils/config/#pyiceberg.utils.config.Config.get_default_catalog_name","title":"<code>get_default_catalog_name()</code>","text":"<p>Return the default catalog name.</p> <p>The name of the default catalog in `default-catalog`.</p> Type Description <code>str</code> <p>Returns <code>default</code> when the key cannot be found in the config file.</p> Source code in <code>pyiceberg/utils/config.py</code> <pre><code>def get_default_catalog_name(self) -&gt; str:\n    \"\"\"Return the default catalog name.\n\n    Returns: The name of the default catalog in `default-catalog`.\n             Returns `default` when the key cannot be found in the config file.\n    \"\"\"\n    if default_catalog_name := self.config.get(DEFAULT_CATALOG):\n        if not isinstance(default_catalog_name, str):\n            raise ValueError(f\"Default catalog name should be a str: {default_catalog_name}\")\n        return default_catalog_name\n    return DEFAULT\n</code></pre>"},{"location":"reference/pyiceberg/utils/config/#pyiceberg.utils.config.merge_config","title":"<code>merge_config(lhs, rhs)</code>","text":"<p>Merge right-hand side into the left-hand side.</p> Source code in <code>pyiceberg/utils/config.py</code> <pre><code>def merge_config(lhs: RecursiveDict, rhs: RecursiveDict) -&gt; RecursiveDict:\n    \"\"\"Merge right-hand side into the left-hand side.\"\"\"\n    new_config = lhs.copy()\n    for rhs_key, rhs_value in rhs.items():\n        if rhs_key in new_config:\n            lhs_value = new_config[rhs_key]\n            if isinstance(lhs_value, dict) and isinstance(rhs_value, dict):\n                # If they are both dicts, then we have to go deeper\n                new_config[rhs_key] = merge_config(lhs_value, rhs_value)\n            else:\n                # Take the non-null value, with precedence on rhs\n                new_config[rhs_key] = rhs_value or lhs_value\n        else:\n            # New key\n            new_config[rhs_key] = rhs_value\n\n    return new_config\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/","title":"datetime","text":"<p>Helper methods for working with date/time representations.</p>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.date_str_to_days","title":"<code>date_str_to_days(date_str)</code>","text":"<p>Convert an ISO-8601 formatted date to days from 1970-01-01.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def date_str_to_days(date_str: str) -&gt; int:\n    \"\"\"Convert an ISO-8601 formatted date to days from 1970-01-01.\"\"\"\n    return (date.fromisoformat(date_str) - EPOCH_DATE).days\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.date_to_days","title":"<code>date_to_days(date_val)</code>","text":"<p>Convert a Python date object to days from 1970-01-01.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def date_to_days(date_val: date) -&gt; int:\n    \"\"\"Convert a Python date object to days from 1970-01-01.\"\"\"\n    return (date_val - EPOCH_DATE).days\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.datetime_to_micros","title":"<code>datetime_to_micros(dt)</code>","text":"<p>Convert a datetime to microseconds from 1970-01-01T00:00:00.000000.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def datetime_to_micros(dt: datetime) -&gt; int:\n    \"\"\"Convert a datetime to microseconds from 1970-01-01T00:00:00.000000.\"\"\"\n    if dt.tzinfo:\n        delta = dt - EPOCH_TIMESTAMPTZ\n    else:\n        delta = dt - EPOCH_TIMESTAMP\n    return (delta.days * 86400 + delta.seconds) * 1_000_000 + delta.microseconds\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.datetime_to_millis","title":"<code>datetime_to_millis(dt)</code>","text":"<p>Convert a datetime to milliseconds from 1970-01-01T00:00:00.000000.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def datetime_to_millis(dt: datetime) -&gt; int:\n    \"\"\"Convert a datetime to milliseconds from 1970-01-01T00:00:00.000000.\"\"\"\n    if dt.tzinfo:\n        delta = dt - EPOCH_TIMESTAMPTZ\n    else:\n        delta = dt - EPOCH_TIMESTAMP\n    return (delta.days * 86400 + delta.seconds) * 1_000 + delta.microseconds // 1_000\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.datetime_to_nanos","title":"<code>datetime_to_nanos(dt)</code>","text":"<p>Convert a datetime to nanoseconds from 1970-01-01T00:00:00.000000000.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def datetime_to_nanos(dt: datetime) -&gt; int:\n    \"\"\"Convert a datetime to nanoseconds from 1970-01-01T00:00:00.000000000.\"\"\"\n    # python datetime and time doesn't have nanoseconds support yet\n    # https://github.com/python/cpython/issues/59648\n    if dt.tzinfo:\n        delta = dt - EPOCH_TIMESTAMPTZ\n    else:\n        delta = dt - EPOCH_TIMESTAMP\n    return ((delta.days * 86400 + delta.seconds) * 1_000_000 + delta.microseconds) * 1_000\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.days_to_date","title":"<code>days_to_date(days)</code>","text":"<p>Create a date from the number of days from 1970-01-01.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def days_to_date(days: int) -&gt; date:\n    \"\"\"Create a date from the number of days from 1970-01-01.\"\"\"\n    return EPOCH_DATE + timedelta(days)\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_days","title":"<code>micros_to_days(timestamp)</code>","text":"<p>Convert a timestamp in microseconds to a date in days.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_days(timestamp: int) -&gt; int:\n    \"\"\"Convert a timestamp in microseconds to a date in days.\"\"\"\n    return timedelta(microseconds=timestamp).days\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_hours","title":"<code>micros_to_hours(micros)</code>","text":"<p>Convert a timestamp in microseconds to hours from 1970-01-01T00:00.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_hours(micros: int) -&gt; int:\n    \"\"\"Convert a timestamp in microseconds to hours from 1970-01-01T00:00.\"\"\"\n    return micros // 3_600_000_000\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_time","title":"<code>micros_to_time(micros)</code>","text":"<p>Convert a timestamp in microseconds to a time.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_time(micros: int) -&gt; time:\n    \"\"\"Convert a timestamp in microseconds to a time.\"\"\"\n    micros, microseconds = divmod(micros, 1000000)\n    micros, seconds = divmod(micros, 60)\n    micros, minutes = divmod(micros, 60)\n    hours = micros\n    return time(hour=hours, minute=minutes, second=seconds, microsecond=microseconds)\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_timestamp","title":"<code>micros_to_timestamp(micros)</code>","text":"<p>Convert microseconds from epoch to a timestamp.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_timestamp(micros: int) -&gt; datetime:\n    \"\"\"Convert microseconds from epoch to a timestamp.\"\"\"\n    dt = timedelta(microseconds=micros)\n    return EPOCH_TIMESTAMP + dt\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.micros_to_timestamptz","title":"<code>micros_to_timestamptz(micros)</code>","text":"<p>Convert microseconds from epoch to an utc timestamp.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def micros_to_timestamptz(micros: int) -&gt; datetime:\n    \"\"\"Convert microseconds from epoch to an utc timestamp.\"\"\"\n    dt = timedelta(microseconds=micros)\n    return EPOCH_TIMESTAMPTZ + dt\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.millis_to_datetime","title":"<code>millis_to_datetime(millis)</code>","text":"<p>Convert milliseconds from epoch to a timestamp.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def millis_to_datetime(millis: int) -&gt; datetime:\n    \"\"\"Convert milliseconds from epoch to a timestamp.\"\"\"\n    dt = timedelta(milliseconds=millis)\n    return EPOCH_TIMESTAMP + dt\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.nanos_to_days","title":"<code>nanos_to_days(nanos)</code>","text":"<p>Convert a timestamp in nanoseconds to a date in days.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def nanos_to_days(nanos: int) -&gt; int:\n    \"\"\"Convert a timestamp in nanoseconds to a date in days.\"\"\"\n    return timedelta(microseconds=nanos // 1000).days\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.nanos_to_hours","title":"<code>nanos_to_hours(nanos)</code>","text":"<p>Convert a timestamp in nanoseconds to hours from 1970-01-01T00:00.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def nanos_to_hours(nanos: int) -&gt; int:\n    \"\"\"Convert a timestamp in nanoseconds to hours from 1970-01-01T00:00.\"\"\"\n    return nanos // 3_600_000_000_000\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.nanos_to_micros","title":"<code>nanos_to_micros(nanos)</code>","text":"<p>Convert a nanoseconds timestamp to microsecond timestamp by dropping precision.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def nanos_to_micros(nanos: int) -&gt; int:\n    \"\"\"Convert a nanoseconds timestamp to microsecond timestamp by dropping precision.\"\"\"\n    return nanos // 1000\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.nanos_to_time","title":"<code>nanos_to_time(nanos)</code>","text":"<p>Convert a timestamp in nanoseconds to a microsecond precision time.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def nanos_to_time(nanos: int) -&gt; time:\n    \"\"\"Convert a timestamp in nanoseconds to a microsecond precision time.\"\"\"\n    micros = nanos_to_micros(nanos)\n    micros, microseconds = divmod(micros, 1000000)\n    micros, seconds = divmod(micros, 60)\n    micros, minutes = divmod(micros, 60)\n    hours = micros\n    return time(hour=hours, minute=minutes, second=seconds, microsecond=microseconds)\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.nanos_to_timestamp","title":"<code>nanos_to_timestamp(nanos)</code>","text":"<p>Convert nanoseconds from epoch to a microsecond timestamp.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def nanos_to_timestamp(nanos: int) -&gt; datetime:\n    \"\"\"Convert nanoseconds from epoch to a microsecond timestamp.\"\"\"\n    dt = timedelta(microseconds=nanos_to_micros(nanos))\n    return EPOCH_TIMESTAMP + dt\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.time_str_to_micros","title":"<code>time_str_to_micros(time_str)</code>","text":"<p>Convert an ISO-8601 formatted time to microseconds from midnight.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def time_str_to_micros(time_str: str) -&gt; int:\n    \"\"\"Convert an ISO-8601 formatted time to microseconds from midnight.\"\"\"\n    return time_to_micros(time.fromisoformat(time_str))\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.time_str_to_nanos","title":"<code>time_str_to_nanos(time_str)</code>","text":"<p>Convert an ISO-8601 formatted time to nanoseconds from midnight.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def time_str_to_nanos(time_str: str) -&gt; int:\n    \"\"\"Convert an ISO-8601 formatted time to nanoseconds from midnight.\"\"\"\n    return time_to_nanos(time.fromisoformat(time_str))\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.time_to_micros","title":"<code>time_to_micros(t)</code>","text":"<p>Convert a datetime.time object to microseconds from midnight.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def time_to_micros(t: time) -&gt; int:\n    \"\"\"Convert a datetime.time object to microseconds from midnight.\"\"\"\n    return (((t.hour * 60 + t.minute) * 60) + t.second) * 1_000_000 + t.microsecond\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.time_to_nanos","title":"<code>time_to_nanos(t)</code>","text":"<p>Convert a datetime.time object to nanoseconds from midnight.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def time_to_nanos(t: time) -&gt; int:\n    \"\"\"Convert a datetime.time object to nanoseconds from midnight.\"\"\"\n    # python datetime and time doesn't have nanoseconds support yet\n    # https://github.com/python/cpython/issues/59648\n    return ((((t.hour * 60 + t.minute) * 60) + t.second) * 1_000_000 + t.microsecond) * 1_000\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.timestamp_to_micros","title":"<code>timestamp_to_micros(timestamp_str)</code>","text":"<p>Convert an ISO-9601 formatted timestamp without zone to microseconds from 1970-01-01T00:00:00.000000.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def timestamp_to_micros(timestamp_str: str) -&gt; int:\n    \"\"\"Convert an ISO-9601 formatted timestamp without zone to microseconds from 1970-01-01T00:00:00.000000.\"\"\"\n    if ISO_TIMESTAMP.fullmatch(timestamp_str):\n        return datetime_to_micros(datetime.fromisoformat(timestamp_str))\n    if ISO_TIMESTAMPTZ.fullmatch(timestamp_str):\n        # When we can match a timestamp without a zone, we can give a more specific error\n        raise ValueError(f\"Zone offset provided, but not expected: {timestamp_str}\")\n    raise ValueError(f\"Invalid timestamp without zone: {timestamp_str} (must be ISO-8601)\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.timestamp_to_nanos","title":"<code>timestamp_to_nanos(timestamp_str)</code>","text":"<p>Convert an ISO-9601 formatted timestamp without zone to nanoseconds from 1970-01-01T00:00:00.000000000.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def timestamp_to_nanos(timestamp_str: str) -&gt; int:\n    \"\"\"Convert an ISO-9601 formatted timestamp without zone to nanoseconds from 1970-01-01T00:00:00.000000000.\"\"\"\n    if match := ISO_TIMESTAMP_NANO.fullmatch(timestamp_str):\n        # Python datetime does not have native nanoseconds support\n        # Hence we need to extract nanoseconds timestamp manually\n        ns_str = match.group(3) or \"0\"\n        ms_str = match.group(2) if match.group(2) else \"\"\n        timestamp_str_without_ns_str = match.group(1) + ms_str\n        return datetime_to_nanos(datetime.fromisoformat(timestamp_str_without_ns_str)) + int(ns_str)\n    if ISO_TIMESTAMPTZ_NANO.fullmatch(timestamp_str):\n        # When we can match a timestamp without a zone, we can give a more specific error\n        raise ValueError(f\"Zone offset provided, but not expected: {timestamp_str}\")\n    raise ValueError(f\"Invalid timestamp without zone: {timestamp_str} (must be ISO-8601)\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.timestamptz_to_micros","title":"<code>timestamptz_to_micros(timestamptz_str)</code>","text":"<p>Convert an ISO-8601 formatted timestamp with zone to microseconds from 1970-01-01T00:00:00.000000+00:00.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def timestamptz_to_micros(timestamptz_str: str) -&gt; int:\n    \"\"\"Convert an ISO-8601 formatted timestamp with zone to microseconds from 1970-01-01T00:00:00.000000+00:00.\"\"\"\n    if ISO_TIMESTAMPTZ.fullmatch(timestamptz_str):\n        return datetime_to_micros(datetime.fromisoformat(timestamptz_str))\n    if ISO_TIMESTAMP.fullmatch(timestamptz_str):\n        # When we can match a timestamp without a zone, we can give a more specific error\n        raise ValueError(f\"Missing zone offset: {timestamptz_str} (must be ISO-8601)\")\n    raise ValueError(f\"Invalid timestamp with zone: {timestamptz_str} (must be ISO-8601)\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.timestamptz_to_nanos","title":"<code>timestamptz_to_nanos(timestamptz_str)</code>","text":"<p>Convert an ISO-8601 formatted timestamp with zone to nanoseconds from 1970-01-01T00:00:00.000000000+00:00.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def timestamptz_to_nanos(timestamptz_str: str) -&gt; int:\n    \"\"\"Convert an ISO-8601 formatted timestamp with zone to nanoseconds from 1970-01-01T00:00:00.000000000+00:00.\"\"\"\n    if match := ISO_TIMESTAMPTZ_NANO.fullmatch(timestamptz_str):\n        # Python datetime does not have native nanoseconds support\n        # Hence we need to extract nanoseconds timestamp manually\n        ns_str = match.group(3) or \"0\"\n        ms_str = match.group(2) if match.group(2) else \"\"\n        timestamptz_str_without_ns_str = match.group(1) + ms_str + match.group(4)\n        return datetime_to_nanos(datetime.fromisoformat(timestamptz_str_without_ns_str)) + int(ns_str)\n    if ISO_TIMESTAMPTZ_NANO.fullmatch(timestamptz_str):\n        # When we can match a timestamp without a zone, we can give a more specific error\n        raise ValueError(f\"Missing zone offset: {timestamptz_str} (must be ISO-8601)\")\n    raise ValueError(f\"Invalid timestamp with zone: {timestamptz_str} (must be ISO-8601)\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_day","title":"<code>to_human_day(day_ordinal)</code>","text":"<p>Convert a DateType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_day(day_ordinal: int) -&gt; str:\n    \"\"\"Convert a DateType value to human string.\"\"\"\n    return (EPOCH_DATE + timedelta(days=day_ordinal)).isoformat()\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_hour","title":"<code>to_human_hour(hour_ordinal)</code>","text":"<p>Convert a DateType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_hour(hour_ordinal: int) -&gt; str:\n    \"\"\"Convert a DateType value to human string.\"\"\"\n    return (EPOCH_TIMESTAMP + timedelta(hours=hour_ordinal)).isoformat(\"-\", \"hours\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_month","title":"<code>to_human_month(month_ordinal)</code>","text":"<p>Convert a DateType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_month(month_ordinal: int) -&gt; str:\n    \"\"\"Convert a DateType value to human string.\"\"\"\n    return f\"{EPOCH_TIMESTAMP.year + month_ordinal // 12:0=4d}-{1 + month_ordinal % 12:0=2d}\"\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_time","title":"<code>to_human_time(micros_from_midnight)</code>","text":"<p>Convert a TimeType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_time(micros_from_midnight: int) -&gt; str:\n    \"\"\"Convert a TimeType value to human string.\"\"\"\n    return micros_to_time(micros_from_midnight).isoformat()\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_timestamp","title":"<code>to_human_timestamp(timestamp_micros)</code>","text":"<p>Convert a TimestampType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_timestamp(timestamp_micros: int) -&gt; str:\n    \"\"\"Convert a TimestampType value to human string.\"\"\"\n    return (EPOCH_TIMESTAMP + timedelta(microseconds=timestamp_micros)).isoformat()\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_timestamptz","title":"<code>to_human_timestamptz(timestamp_micros)</code>","text":"<p>Convert a TimestamptzType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_timestamptz(timestamp_micros: int) -&gt; str:\n    \"\"\"Convert a TimestamptzType value to human string.\"\"\"\n    return (EPOCH_TIMESTAMPTZ + timedelta(microseconds=timestamp_micros)).isoformat()\n</code></pre>"},{"location":"reference/pyiceberg/utils/datetime/#pyiceberg.utils.datetime.to_human_year","title":"<code>to_human_year(year_ordinal)</code>","text":"<p>Convert a DateType value to human string.</p> Source code in <code>pyiceberg/utils/datetime.py</code> <pre><code>def to_human_year(year_ordinal: int) -&gt; str:\n    \"\"\"Convert a DateType value to human string.\"\"\"\n    return f\"{EPOCH_TIMESTAMP.year + year_ordinal:0=4d}\"\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/","title":"decimal","text":"<p>Helper methods for working with Python Decimals.</p>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.bytes_required","title":"<code>bytes_required(value)</code>","text":"<p>Return the minimum number of bytes needed to serialize a decimal or unscaled value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int | Decimal</code> <p>a Decimal value or unscaled int value.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the minimum number of bytes needed to serialize the value.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def bytes_required(value: int | Decimal) -&gt; int:\n    \"\"\"Return the minimum number of bytes needed to serialize a decimal or unscaled value.\n\n    Args:\n        value (int | Decimal): a Decimal value or unscaled int value.\n\n    Returns:\n        int: the minimum number of bytes needed to serialize the value.\n    \"\"\"\n    if isinstance(value, int):\n        return (value.bit_length() + 8) // 8\n    elif isinstance(value, Decimal):\n        return (decimal_to_unscaled(value).bit_length() + 8) // 8\n\n    raise ValueError(f\"Unsupported value: {value}\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.bytes_to_decimal","title":"<code>bytes_to_decimal(value, scale)</code>","text":"<p>Return a decimal from the bytes.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>the bytes to be converted into a decimal.</p> required <code>scale</code> <code>int</code> <p>the scale of the decimal.</p> required <p>Returns:</p> Name Type Description <code>Decimal</code> <code>Decimal</code> <p>the scaled decimal.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def bytes_to_decimal(value: bytes, scale: int) -&gt; Decimal:\n    \"\"\"Return a decimal from the bytes.\n\n    Args:\n        value (bytes): the bytes to be converted into a decimal.\n        scale (int): the scale of the decimal.\n\n    Returns:\n        Decimal: the scaled decimal.\n    \"\"\"\n    unscaled_datum = int.from_bytes(value, byteorder=\"big\", signed=True)\n    return unscaled_to_decimal(unscaled_datum, scale)\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.decimal_required_bytes","title":"<code>decimal_required_bytes(precision)</code>","text":"<p>Compute the number of bytes required to store a precision.</p> <p>Parameters:</p> Name Type Description Default <code>precision</code> <code>int</code> <p>The number of digits to store.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of bytes required to store a decimal with a certain precision.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def decimal_required_bytes(precision: int) -&gt; int:\n    \"\"\"Compute the number of bytes required to store a precision.\n\n    Args:\n        precision: The number of digits to store.\n\n    Returns:\n        The number of bytes required to store a decimal with a certain precision.\n    \"\"\"\n    if precision &lt;= 0 or precision &gt;= 40:\n        raise ValueError(f\"Unsupported precision, outside of (0, 40]: {precision}\")\n\n    return REQUIRED_LENGTH[precision]\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.decimal_to_bytes","title":"<code>decimal_to_bytes(value, byte_length=None)</code>","text":"<p>Return a byte representation of a decimal.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Decimal</code> <p>a decimal value.</p> required <code>byte_length</code> <code>int</code> <p>The number of bytes.</p> <code>None</code> <p>Returns:     bytes: the unscaled value of the Decimal as bytes.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def decimal_to_bytes(value: Decimal, byte_length: int | None = None) -&gt; bytes:\n    \"\"\"Return a byte representation of a decimal.\n\n    Args:\n        value (Decimal): a decimal value.\n        byte_length (int): The number of bytes.\n    Returns:\n        bytes: the unscaled value of the Decimal as bytes.\n    \"\"\"\n    unscaled_value = decimal_to_unscaled(value)\n    if byte_length is None:\n        byte_length = bytes_required(unscaled_value)\n    return unscaled_value.to_bytes(byte_length, byteorder=\"big\", signed=True)\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.decimal_to_unscaled","title":"<code>decimal_to_unscaled(value)</code>","text":"<p>Get an unscaled value given a Decimal value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Decimal</code> <p>A Decimal instance.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The unscaled value.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def decimal_to_unscaled(value: Decimal) -&gt; int:\n    \"\"\"Get an unscaled value given a Decimal value.\n\n    Args:\n        value (Decimal): A Decimal instance.\n\n    Returns:\n        int: The unscaled value.\n    \"\"\"\n    sign, digits, _ = value.as_tuple()\n    return int(Decimal((sign, digits, 0)).to_integral_value())\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.truncate_decimal","title":"<code>truncate_decimal(value, width)</code>","text":"<p>Get a truncated Decimal value given a decimal value and a width.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Decimal</code> <p>a decimal value.</p> required <code>width</code> <code>int</code> <p>A width for the returned Decimal instance.</p> required <p>Returns:     Decimal: A truncated Decimal instance.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def truncate_decimal(value: Decimal, width: int) -&gt; Decimal:\n    \"\"\"Get a truncated Decimal value given a decimal value and a width.\n\n    Args:\n        value (Decimal): a decimal value.\n        width (int): A width for the returned Decimal instance.\n    Returns:\n        Decimal: A truncated Decimal instance.\n    \"\"\"\n    unscaled_value = decimal_to_unscaled(value)\n    applied_value = unscaled_value - (((unscaled_value % width) + width) % width)\n    return unscaled_to_decimal(applied_value, abs(int(value.as_tuple().exponent)))\n</code></pre>"},{"location":"reference/pyiceberg/utils/decimal/#pyiceberg.utils.decimal.unscaled_to_decimal","title":"<code>unscaled_to_decimal(unscaled, scale)</code>","text":"<p>Get a scaled Decimal value given an unscaled value and a scale.</p> <p>Parameters:</p> Name Type Description Default <code>unscaled</code> <code>int</code> <p>An unscaled value.</p> required <code>scale</code> <code>int</code> <p>A scale to set for the returned Decimal instance.</p> required <p>Returns:</p> Name Type Description <code>Decimal</code> <code>Decimal</code> <p>A scaled Decimal instance.</p> Source code in <code>pyiceberg/utils/decimal.py</code> <pre><code>def unscaled_to_decimal(unscaled: int, scale: int) -&gt; Decimal:\n    \"\"\"Get a scaled Decimal value given an unscaled value and a scale.\n\n    Args:\n        unscaled (int): An unscaled value.\n        scale (int): A scale to set for the returned Decimal instance.\n\n    Returns:\n        Decimal: A scaled Decimal instance.\n    \"\"\"\n    sign, digits, _ = Decimal(unscaled).as_tuple()\n    return Decimal((sign, digits, -scale))\n</code></pre>"},{"location":"reference/pyiceberg/utils/deprecated/","title":"deprecated","text":""},{"location":"reference/pyiceberg/utils/deprecated/#pyiceberg.utils.deprecated.deprecated","title":"<code>deprecated(deprecated_in, removed_in, help_message=None)</code>","text":"<p>Mark functions as deprecated.</p> <p>Adding this will result in a warning being emitted when the function is used.</p> Source code in <code>pyiceberg/utils/deprecated.py</code> <pre><code>def deprecated(deprecated_in: str, removed_in: str, help_message: str | None = None) -&gt; Callable:  # type: ignore\n    \"\"\"Mark functions as deprecated.\n\n    Adding this will result in a warning being emitted when the function is used.\n    \"\"\"\n    if help_message is not None:\n        help_message = f\" {help_message}.\"\n\n    def decorator(func: Callable):  # type: ignore\n        @functools.wraps(func)\n        def new_func(*args: Any, **kwargs: Any) -&gt; Any:\n            message = f\"Call to {func.__name__}, deprecated in {deprecated_in}, will be removed in {removed_in}.{help_message}\"\n\n            _deprecation_warning(message)\n\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return decorator\n</code></pre>"},{"location":"reference/pyiceberg/utils/deprecated/#pyiceberg.utils.deprecated.deprecation_message","title":"<code>deprecation_message(deprecated_in, removed_in, help_message)</code>","text":"<p>Mark properties or behaviors as deprecated.</p> <p>Adding this will result in a warning being emitted.</p> Source code in <code>pyiceberg/utils/deprecated.py</code> <pre><code>def deprecation_message(deprecated_in: str, removed_in: str, help_message: str | None) -&gt; None:\n    \"\"\"Mark properties or behaviors as deprecated.\n\n    Adding this will result in a warning being emitted.\n    \"\"\"\n    _deprecation_warning(deprecation_notice(deprecated_in, removed_in, help_message))\n</code></pre>"},{"location":"reference/pyiceberg/utils/deprecated/#pyiceberg.utils.deprecated.deprecation_notice","title":"<code>deprecation_notice(deprecated_in, removed_in, help_message)</code>","text":"<p>Return a deprecation notice.</p> Source code in <code>pyiceberg/utils/deprecated.py</code> <pre><code>def deprecation_notice(deprecated_in: str, removed_in: str, help_message: str | None) -&gt; str:\n    \"\"\"Return a deprecation notice.\"\"\"\n    return f\"Deprecated in {deprecated_in}, will be removed in {removed_in}. {help_message}\"\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/","title":"lazydict","text":""},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict","title":"<code>LazyDict</code>","text":"<p>               Bases: <code>Mapping[K, V]</code></p> <p>Lazily build a dictionary from an array of items.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>class LazyDict(Mapping[K, V]):\n    \"\"\"Lazily build a dictionary from an array of items.\"\"\"\n\n    __slots__ = (\"_contents\", \"_dict\")\n\n    # Since Python's type system is not powerful enough to express the type of the\n    # contents of the dictionary, we use specify the type as a sequence of either K or V\n    # values.\n    #\n    # Rather than spending the runtime cost of checking the type of each item, we presume\n    # that the developer has correctly used the class and that the contents are valid.\n    def __init__(self, contents: Sequence[Sequence[K | V]]):\n        self._contents = contents\n        self._dict: dict[K, V] | None = None\n\n    def _build_dict(self) -&gt; dict[K, V]:\n        self._dict = {}\n        for item in self._contents:\n            self._dict.update(dict(zip(cast(Sequence[K], item[::2]), cast(Sequence[V], item[1::2]), strict=True)))\n\n        return self._dict\n\n    def __getitem__(self, key: K, /) -&gt; V:\n        \"\"\"Return the value for the given key.\"\"\"\n        source = self._dict or self._build_dict()\n        return source[key]\n\n    def __iter__(self) -&gt; Iterator[K]:\n        \"\"\"Return an iterator over the keys of the dictionary.\"\"\"\n        source = self._dict or self._build_dict()\n        return iter(source)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of items in the dictionary.\"\"\"\n        source = self._dict or self._build_dict()\n        return len(source)\n\n    def __dict__(self) -&gt; dict[K, V]:  # type: ignore\n        \"\"\"Convert the lazy dict in a dict.\"\"\"\n        return self._dict or self._build_dict()\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict.__dict__","title":"<code>__dict__()</code>","text":"<p>Convert the lazy dict in a dict.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>def __dict__(self) -&gt; dict[K, V]:  # type: ignore\n    \"\"\"Convert the lazy dict in a dict.\"\"\"\n    return self._dict or self._build_dict()\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Return the value for the given key.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>def __getitem__(self, key: K, /) -&gt; V:\n    \"\"\"Return the value for the given key.\"\"\"\n    source = self._dict or self._build_dict()\n    return source[key]\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict.__iter__","title":"<code>__iter__()</code>","text":"<p>Return an iterator over the keys of the dictionary.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>def __iter__(self) -&gt; Iterator[K]:\n    \"\"\"Return an iterator over the keys of the dictionary.\"\"\"\n    source = self._dict or self._build_dict()\n    return iter(source)\n</code></pre>"},{"location":"reference/pyiceberg/utils/lazydict/#pyiceberg.utils.lazydict.LazyDict.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of items in the dictionary.</p> Source code in <code>pyiceberg/utils/lazydict.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of items in the dictionary.\"\"\"\n    source = self._dict or self._build_dict()\n    return len(source)\n</code></pre>"},{"location":"reference/pyiceberg/utils/parsing/","title":"parsing","text":""},{"location":"reference/pyiceberg/utils/parsing/#pyiceberg.utils.parsing.ParseNumberFromBrackets","title":"<code>ParseNumberFromBrackets</code>","text":"<p>Extracts the size from a string in the form of prefix[22].</p> Source code in <code>pyiceberg/utils/parsing.py</code> <pre><code>class ParseNumberFromBrackets:\n    \"\"\"Extracts the size from a string in the form of prefix[22].\"\"\"\n\n    regex: Pattern  # type: ignore\n    prefix: str\n\n    def __init__(self, prefix: str):\n        self.prefix = prefix\n        self.regex = re.compile(rf\"{prefix}\\[(\\d+)\\]\")\n\n    def match(self, str_repr: str) -&gt; int:\n        matches = self.regex.search(str_repr)\n        if matches:\n            return int(matches.group(1))\n        raise ValidationError(f\"Could not match {str_repr}, expected format {self.prefix}[22]\")\n</code></pre>"},{"location":"reference/pyiceberg/utils/properties/","title":"properties","text":""},{"location":"reference/pyiceberg/utils/schema_conversion/","title":"schema_conversion","text":"<p>Utility class for converting between Avro and Iceberg schemas.</p>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion","title":"<code>AvroSchemaConversion</code>","text":"Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>class AvroSchemaConversion:\n    def avro_to_iceberg(self, avro_schema: dict[str, Any]) -&gt; Schema:\n        \"\"\"Convert an Apache Avro into an Apache Iceberg schema equivalent.\n\n        This expects to have field id's to be encoded in the Avro schema:\n\n            {\n                \"type\": \"record\",\n                \"name\": \"manifest_file\",\n                \"fields\": [\n                    {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n                    {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n                ]\n            }\n\n        Example:\n            This converts an Avro schema into an Iceberg schema:\n\n            &gt;&gt;&gt; avro_schema = AvroSchemaConversion().avro_to_iceberg({\n            ...     \"type\": \"record\",\n            ...     \"name\": \"manifest_file\",\n            ...     \"fields\": [\n            ...         {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n            ...         {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n            ...     ]\n            ... })\n            &gt;&gt;&gt; iceberg_schema = Schema(\n            ...     NestedField(\n            ...         field_id=500, name=\"manifest_path\", field_type=StringType(),\n            ...         required=False, doc=\"Location URI with FS scheme\"\n            ...     ),\n            ...     NestedField(\n            ...         field_id=501, name=\"manifest_length\", field_type=LongType(),\n            ...         required=False, doc=\"Total file size in bytes\"\n            ...     ),\n            ...     schema_id=1\n            ... )\n            &gt;&gt;&gt; avro_schema == iceberg_schema\n            True\n\n        Args:\n            avro_schema (Dict[str, Any]): The JSON decoded Avro schema.\n\n        Returns:\n            Equivalent Iceberg schema.\n        \"\"\"\n        return Schema(*[self._convert_field(field) for field in avro_schema[\"fields\"]], schema_id=1)\n\n    def iceberg_to_avro(self, schema: Schema, schema_name: str | None = None) -&gt; AvroType:\n        \"\"\"Convert an Iceberg schema into an Avro dictionary that can be serialized to JSON.\"\"\"\n        return visit(schema, ConvertSchemaToAvro(schema_name))\n\n    def _resolve_union(self, type_union: dict[str, str] | list[str | dict[str, str]] | str) -&gt; tuple[str | dict[str, Any], bool]:\n        \"\"\"\n        Convert Unions into their type and resolves if the field is required.\n\n        Examples:\n            &gt;&gt;&gt; AvroSchemaConversion()._resolve_union('str')\n            ('str', True)\n            &gt;&gt;&gt; AvroSchemaConversion()._resolve_union(['null', 'str'])\n            ('str', False)\n            &gt;&gt;&gt; AvroSchemaConversion()._resolve_union([{'type': 'str'}])\n            ({'type': 'str'}, True)\n            &gt;&gt;&gt; AvroSchemaConversion()._resolve_union(['null', {'type': 'str'}])\n            ({'type': 'str'}, False)\n\n        Args:\n            type_union: The field, can be a string 'str', list ['null', 'str'], or dict {\"type\": 'str'}.\n\n        Returns:\n            A tuple containing the type and if required.\n\n        Raises:\n            TypeError: In the case non-optional union types are encountered.\n        \"\"\"\n        avro_types: dict[str, str] | list[dict[str, str] | str]\n        if isinstance(type_union, str):\n            # It is a primitive and required\n            return type_union, True\n        elif isinstance(type_union, dict):\n            # It is a context and required\n            return type_union, True\n        else:\n            avro_types = type_union\n\n        if len(avro_types) &gt; 2:\n            raise TypeError(f\"Non-optional types aren't part of the Iceberg specification: {avro_types}\")\n\n        # For the Iceberg spec it is required to set the default value to null\n        # From https://iceberg.apache.org/spec/#avro\n        # Optional fields must always set the Avro field default value to null.\n        #\n        # This means that null has to come first:\n        # https://avro.apache.org/docs/current/spec.html\n        # type of the default value must match the first element of the union.\n        if \"null\" != avro_types[0]:\n            raise TypeError(\"Only null-unions are supported\")\n\n        # Filter the null value and return the type\n        return list(filter(lambda t: t != \"null\", avro_types))[0], False\n\n    def _convert_schema(self, avro_type: str | dict[str, Any]) -&gt; IcebergType:\n        \"\"\"\n        Resolve the Avro type.\n\n        Args:\n            avro_type: The Avro type, can be simple or complex.\n\n        Returns:\n            The equivalent IcebergType.\n\n        Raises:\n            ValueError: When there are unknown types\n        \"\"\"\n        if isinstance(avro_type, str) and avro_type in PRIMITIVE_FIELD_TYPE_MAPPING:\n            return PRIMITIVE_FIELD_TYPE_MAPPING[avro_type]\n        elif isinstance(avro_type, dict):\n            if \"logicalType\" in avro_type:\n                return self._convert_logical_type(avro_type)\n            else:\n                # Resolve potential nested types\n                while \"type\" in avro_type and isinstance(avro_type[\"type\"], dict):\n                    avro_type = avro_type[\"type\"]\n                type_identifier = avro_type[\"type\"]\n                if type_identifier == \"record\":\n                    return self._convert_record_type(avro_type)\n                elif type_identifier == \"array\":\n                    return self._convert_array_type(avro_type)\n                elif type_identifier == \"map\":\n                    return self._convert_map_type(avro_type)\n                elif type_identifier == \"fixed\":\n                    return self._convert_fixed_type(avro_type)\n                elif isinstance(type_identifier, str) and type_identifier in PRIMITIVE_FIELD_TYPE_MAPPING:\n                    return PRIMITIVE_FIELD_TYPE_MAPPING[type_identifier]\n                else:\n                    raise TypeError(f\"Type not recognized: {avro_type}\")\n        else:\n            raise TypeError(f\"Type not recognized: {avro_type}\")\n\n    def _convert_field(self, field: dict[str, Any]) -&gt; NestedField:\n        \"\"\"Convert an Avro field into an Iceberg equivalent field.\n\n        Args:\n            field: The Avro field.\n\n        Returns:\n            The Iceberg equivalent field.\n        \"\"\"\n        if FIELD_ID_PROP not in field:\n            raise ValueError(f\"Cannot convert field, missing {FIELD_ID_PROP}: {field}\")\n\n        plain_type, required = self._resolve_union(field[\"type\"])\n\n        return NestedField(\n            field_id=field[FIELD_ID_PROP],\n            name=field[\"name\"],\n            field_type=self._convert_schema(plain_type),\n            required=required,\n            doc=field.get(\"doc\"),\n        )\n\n    def _convert_record_type(self, record_type: dict[str, Any]) -&gt; StructType:\n        \"\"\"\n        Convert the fields from a record into an Iceberg struct.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; record_type = {\n            ...     \"type\": \"record\",\n            ...     \"name\": \"r508\",\n            ...     \"fields\": [{\n            ...         \"name\": \"contains_null\",\n            ...         \"type\": \"boolean\",\n            ...         \"doc\": \"True if any file has a null partition value\",\n            ...         \"field-id\": 509,\n            ...      }, {\n            ...          \"name\": \"contains_nan\",\n            ...          \"type\": [\"null\", \"boolean\"],\n            ...          \"doc\": \"True if any file has a nan partition value\",\n            ...          \"default\": None,\n            ...          \"field-id\": 518,\n            ...      }],\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_record_type(record_type)\n            &gt;&gt;&gt; expected = StructType(\n            ...     fields=(\n            ...         NestedField(\n            ...             field_id=509,\n            ...             name=\"contains_null\",\n            ...             field_type=BooleanType(),\n            ...             required=False,\n            ...             doc=\"True if any file has a null partition value\",\n            ...         ),\n            ...         NestedField(\n            ...             field_id=518,\n            ...             name=\"contains_nan\",\n            ...             field_type=BooleanType(),\n            ...             required=True,\n            ...             doc=\"True if any file has a nan partition value\",\n            ...         ),\n            ...     )\n            ... )\n            &gt;&gt;&gt; expected == actual\n            True\n\n        Args:\n            record_type: The record type itself.\n\n        Returns: A StructType.\n        \"\"\"\n        if record_type[\"type\"] != \"record\":\n            raise ValueError(f\"Expected record type, got: {record_type}\")\n\n        return StructType(*[self._convert_field(field) for field in record_type[\"fields\"]])\n\n    def _convert_array_type(self, array_type: dict[str, Any]) -&gt; ListType:\n        if \"element-id\" not in array_type:\n            raise ValueError(f\"Cannot convert array-type, missing element-id: {array_type}\")\n\n        plain_type, element_required = self._resolve_union(array_type[\"items\"])\n\n        return ListType(\n            element_id=array_type[\"element-id\"],\n            element_type=self._convert_schema(plain_type),\n            element_required=element_required,\n        )\n\n    def _convert_map_type(self, map_type: dict[str, Any]) -&gt; MapType:\n        \"\"\"Convert an avro map type into an Iceberg MapType.\n\n        Args:\n            map_type: The dict that describes the Avro map type.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_field = {\n            ...     \"type\": \"map\",\n            ...     \"values\": [\"null\", \"long\"],\n            ...     \"key-id\": 101,\n            ...     \"value-id\": 102,\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_map_type(avro_field)\n            &gt;&gt;&gt; expected = MapType(\n            ...     key_id=101,\n            ...     key_type=StringType(),\n            ...     value_id=102,\n            ...     value_type=LongType(),\n            ...     value_required=True\n            ... )\n            &gt;&gt;&gt; actual == expected\n            True\n\n        Returns: A MapType.\n        \"\"\"\n        value_type, value_required = self._resolve_union(map_type[\"values\"])\n        return MapType(\n            key_id=map_type[\"key-id\"],\n            # Avro only supports string keys\n            key_type=StringType(),\n            value_id=map_type[\"value-id\"],\n            value_type=self._convert_schema(value_type),\n            value_required=value_required,\n        )\n\n    def _convert_logical_type(self, avro_logical_type: dict[str, Any]) -&gt; IcebergType:\n        \"\"\"Convert a schema with a logical type annotation into an IcebergType.\n\n        For the decimal and map we need to fetch more keys from the dict, and for\n        the simple ones we can just look it up in the mapping.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_logical_type = {\n            ...     \"type\": \"int\",\n            ...     \"logicalType\": \"date\"\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_type(avro_logical_type)\n            &gt;&gt;&gt; actual == DateType()\n            True\n\n        Args:\n            avro_logical_type: The logical type.\n\n        Returns:\n            The converted logical type.\n\n        Raises:\n            ValueError: When the logical type is unknown.\n        \"\"\"\n        logical_type = avro_logical_type[\"logicalType\"]\n        physical_type = avro_logical_type[\"type\"]\n        if logical_type == \"decimal\":\n            return self._convert_logical_decimal_type(avro_logical_type)\n        elif logical_type == \"map\":\n            return self._convert_logical_map_type(avro_logical_type)\n        elif logical_type == \"timestamp-micros\":\n            if avro_logical_type.get(\"adjust-to-utc\", False) is True:\n                return TimestamptzType()\n            else:\n                return TimestampType()\n        elif (logical_type, physical_type) in LOGICAL_FIELD_TYPE_MAPPING:\n            return LOGICAL_FIELD_TYPE_MAPPING[(logical_type, physical_type)]\n        else:\n            raise ValueError(f\"Unknown logical/physical type combination: {avro_logical_type}\")\n\n    def _convert_logical_decimal_type(self, avro_type: dict[str, Any]) -&gt; DecimalType:\n        \"\"\"Convert an avro type to an Iceberg DecimalType.\n\n        Args:\n            avro_type: The Avro type.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_decimal_type = {\n            ...     \"type\": \"bytes\",\n            ...     \"logicalType\": \"decimal\",\n            ...     \"precision\": 19,\n            ...     \"scale\": 25\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_decimal_type(avro_decimal_type)\n            &gt;&gt;&gt; expected = DecimalType(\n            ...     precision=19,\n            ...     scale=25\n            ... )\n            &gt;&gt;&gt; actual == expected\n            True\n\n        Returns:\n            A Iceberg DecimalType.\n        \"\"\"\n        return DecimalType(precision=avro_type[\"precision\"], scale=avro_type[\"scale\"])\n\n    def _convert_logical_map_type(self, avro_type: dict[str, Any]) -&gt; MapType:\n        \"\"\"Convert an avro map type to an Iceberg MapType.\n\n        In the case where a map hasn't a key as a type you can use a logical map to still encode this in Avro.\n\n        Args:\n            avro_type: The Avro Type.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_type = {\n            ...     \"type\": \"array\",\n            ...     \"logicalType\": \"map\",\n            ...     \"items\": {\n            ...         \"type\": \"record\",\n            ...         \"name\": \"k101_v102\",\n            ...         \"fields\": [\n            ...             {\"name\": \"key\", \"type\": \"int\", \"field-id\": 101},\n            ...             {\"name\": \"value\", \"type\": \"string\", \"field-id\": 102},\n            ...         ],\n            ...     },\n            ... }\n            &gt;&gt;&gt; actual = AvroSchemaConversion()._convert_logical_map_type(avro_type)\n            &gt;&gt;&gt; expected = MapType(\n            ...         key_id=101,\n            ...         key_type=IntegerType(),\n            ...         value_id=102,\n            ...         value_type=StringType(),\n            ...         value_required=False\n            ... )\n            &gt;&gt;&gt; actual == expected\n            True\n\n        .. _Apache Iceberg specification:\n            https://iceberg.apache.org/spec/#appendix-a-format-specific-requirements\n\n        Returns:\n            The logical map.\n        \"\"\"\n        fields = avro_type[\"items\"][\"fields\"]\n        if len(fields) != 2:\n            raise ValueError(f\"Invalid key-value pair schema: {avro_type['items']}\")\n        key = self._convert_field(list(filter(lambda f: f[\"name\"] == \"key\", fields))[0])\n        value = self._convert_field(list(filter(lambda f: f[\"name\"] == \"value\", fields))[0])\n        return MapType(\n            key_id=key.field_id,\n            key_type=key.field_type,\n            value_id=value.field_id,\n            value_type=value.field_type,\n            value_required=value.required,\n        )\n\n    def _convert_fixed_type(self, avro_type: dict[str, Any]) -&gt; FixedType:\n        \"\"\"\n        Convert Avro Type to the equivalent Iceberg fixed type.\n\n        - https://avro.apache.org/docs/current/spec.html#Fixed\n\n        Args:\n            avro_type: The Avro type.\n\n        Examples:\n            &gt;&gt;&gt; from pyiceberg.utils.schema_conversion import AvroSchemaConversion\n            &gt;&gt;&gt; avro_fixed_type = {\n            ...     \"name\": \"md5\",\n            ...     \"type\": \"fixed\",\n            ...     \"size\": 16\n            ... }\n            &gt;&gt;&gt; FixedType(length=16) == AvroSchemaConversion()._convert_fixed_type(avro_fixed_type)\n            True\n\n        Returns:\n            An Iceberg equivalent fixed type.\n        \"\"\"\n        return FixedType(length=avro_type[\"size\"])\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion.avro_to_iceberg","title":"<code>avro_to_iceberg(avro_schema)</code>","text":"<p>Convert an Apache Avro into an Apache Iceberg schema equivalent.</p> <p>This expects to have field id's to be encoded in the Avro schema:</p> <pre><code>{\n    \"type\": \"record\",\n    \"name\": \"manifest_file\",\n    \"fields\": [\n        {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n        {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n    ]\n}\n</code></pre> Example <p>This converts an Avro schema into an Iceberg schema:</p> <p>avro_schema = AvroSchemaConversion().avro_to_iceberg({ ...     \"type\": \"record\", ...     \"name\": \"manifest_file\", ...     \"fields\": [ ...         {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500}, ...         {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501} ...     ] ... }) iceberg_schema = Schema( ...     NestedField( ...         field_id=500, name=\"manifest_path\", field_type=StringType(), ...         required=False, doc=\"Location URI with FS scheme\" ...     ), ...     NestedField( ...         field_id=501, name=\"manifest_length\", field_type=LongType(), ...         required=False, doc=\"Total file size in bytes\" ...     ), ...     schema_id=1 ... ) avro_schema == iceberg_schema True</p> <p>Parameters:</p> Name Type Description Default <code>avro_schema</code> <code>Dict[str, Any]</code> <p>The JSON decoded Avro schema.</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Equivalent Iceberg schema.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def avro_to_iceberg(self, avro_schema: dict[str, Any]) -&gt; Schema:\n    \"\"\"Convert an Apache Avro into an Apache Iceberg schema equivalent.\n\n    This expects to have field id's to be encoded in the Avro schema:\n\n        {\n            \"type\": \"record\",\n            \"name\": \"manifest_file\",\n            \"fields\": [\n                {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n                {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n            ]\n        }\n\n    Example:\n        This converts an Avro schema into an Iceberg schema:\n\n        &gt;&gt;&gt; avro_schema = AvroSchemaConversion().avro_to_iceberg({\n        ...     \"type\": \"record\",\n        ...     \"name\": \"manifest_file\",\n        ...     \"fields\": [\n        ...         {\"name\": \"manifest_path\", \"type\": \"string\", \"doc\": \"Location URI with FS scheme\", \"field-id\": 500},\n        ...         {\"name\": \"manifest_length\", \"type\": \"long\", \"doc\": \"Total file size in bytes\", \"field-id\": 501}\n        ...     ]\n        ... })\n        &gt;&gt;&gt; iceberg_schema = Schema(\n        ...     NestedField(\n        ...         field_id=500, name=\"manifest_path\", field_type=StringType(),\n        ...         required=False, doc=\"Location URI with FS scheme\"\n        ...     ),\n        ...     NestedField(\n        ...         field_id=501, name=\"manifest_length\", field_type=LongType(),\n        ...         required=False, doc=\"Total file size in bytes\"\n        ...     ),\n        ...     schema_id=1\n        ... )\n        &gt;&gt;&gt; avro_schema == iceberg_schema\n        True\n\n    Args:\n        avro_schema (Dict[str, Any]): The JSON decoded Avro schema.\n\n    Returns:\n        Equivalent Iceberg schema.\n    \"\"\"\n    return Schema(*[self._convert_field(field) for field in avro_schema[\"fields\"]], schema_id=1)\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.AvroSchemaConversion.iceberg_to_avro","title":"<code>iceberg_to_avro(schema, schema_name=None)</code>","text":"<p>Convert an Iceberg schema into an Avro dictionary that can be serialized to JSON.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def iceberg_to_avro(self, schema: Schema, schema_name: str | None = None) -&gt; AvroType:\n    \"\"\"Convert an Iceberg schema into an Avro dictionary that can be serialized to JSON.\"\"\"\n    return visit(schema, ConvertSchemaToAvro(schema_name))\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.ConvertSchemaToAvro","title":"<code>ConvertSchemaToAvro</code>","text":"<p>               Bases: <code>SchemaVisitorPerPrimitiveType[AvroType]</code></p> <p>Convert an Iceberg schema to an Avro schema.</p> Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>class ConvertSchemaToAvro(SchemaVisitorPerPrimitiveType[AvroType]):\n    \"\"\"Convert an Iceberg schema to an Avro schema.\"\"\"\n\n    schema_name: str | None\n    last_list_field_id: int\n    last_map_key_field_id: int\n    last_map_value_field_id: int\n\n    def __init__(self, schema_name: str | None) -&gt; None:\n        \"\"\"Convert an Iceberg schema to an Avro schema.\n\n        Args:\n            schema_name: The name of the root record.\n        \"\"\"\n        self.schema_name = schema_name\n\n    def schema(self, schema: Schema, struct_result: AvroType) -&gt; AvroType:\n        if isinstance(struct_result, dict) and self.schema_name is not None:\n            struct_result[\"name\"] = self.schema_name\n        return struct_result\n\n    def before_list_element(self, element: NestedField) -&gt; None:\n        self.last_list_field_id = element.field_id\n\n    def before_map_key(self, key: NestedField) -&gt; None:\n        self.last_map_key_field_id = key.field_id\n\n    def before_map_value(self, value: NestedField) -&gt; None:\n        self.last_map_value_field_id = value.field_id\n\n    def struct(self, struct: StructType, field_results: list[AvroType]) -&gt; AvroType:\n        return {\"type\": \"record\", \"fields\": field_results}\n\n    def field(self, field: NestedField, field_result: AvroType) -&gt; AvroType:\n        # Sets the schema name\n        if isinstance(field_result, dict) and field_result.get(\"type\") == \"record\":\n            field_result[\"name\"] = f\"r{field.field_id}\"\n\n        original_name = field.name\n        sanitized_name = make_compatible_name(original_name)\n\n        result = {\n            \"name\": sanitized_name,\n            FIELD_ID_PROP: field.field_id,\n            \"type\": field_result if field.required else [\"null\", field_result],\n        }\n\n        if original_name != sanitized_name:\n            result[ICEBERG_FIELD_NAME_PROP] = original_name\n\n        if field.write_default is not None:\n            result[\"default\"] = field.write_default\n        elif field.optional:\n            result[\"default\"] = None\n\n        if field.doc is not None:\n            result[\"doc\"] = field.doc\n\n        return result\n\n    def list(self, list_type: ListType, element_result: AvroType) -&gt; AvroType:\n        # Sets the schema name in case of a record\n        if isinstance(element_result, dict) and element_result.get(\"type\") == \"record\":\n            element_result[\"name\"] = f\"r{self.last_list_field_id}\"\n        return {\"type\": \"array\", \"element-id\": self.last_list_field_id, \"items\": element_result}\n\n    def map(self, map_type: MapType, key_result: AvroType, value_result: AvroType) -&gt; AvroType:\n        if isinstance(key_result, StringType):\n            # Avro Maps does not support other keys than a String,\n            return {\n                \"type\": \"map\",\n                \"values\": value_result,\n                \"key-id\": self.last_map_key_field_id,\n                \"value-id\": self.last_map_value_field_id,\n            }\n        else:\n            # Creates a logical map that's a list of schema's\n            # binary compatible\n            return {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"record\",\n                    \"name\": f\"k{self.last_map_key_field_id}_v{self.last_map_value_field_id}\",\n                    \"fields\": [\n                        {\"name\": \"key\", \"type\": key_result, FIELD_ID_PROP: self.last_map_key_field_id},\n                        {\"name\": \"value\", \"type\": value_result, FIELD_ID_PROP: self.last_map_value_field_id},\n                    ],\n                },\n                \"logicalType\": \"map\",\n            }\n\n    def visit_fixed(self, fixed_type: FixedType) -&gt; AvroType:\n        return {\"type\": \"fixed\", \"size\": len(fixed_type), \"name\": f\"fixed_{len(fixed_type)}\"}\n\n    def visit_decimal(self, decimal_type: DecimalType) -&gt; AvroType:\n        return {\n            \"type\": \"fixed\",\n            \"size\": decimal_required_bytes(decimal_type.precision),\n            \"logicalType\": \"decimal\",\n            \"precision\": decimal_type.precision,\n            \"scale\": decimal_type.scale,\n            \"name\": f\"decimal_{decimal_type.precision}_{decimal_type.scale}\",\n        }\n\n    def visit_boolean(self, boolean_type: BooleanType) -&gt; AvroType:\n        return \"boolean\"\n\n    def visit_integer(self, integer_type: IntegerType) -&gt; AvroType:\n        return \"int\"\n\n    def visit_long(self, long_type: LongType) -&gt; AvroType:\n        return \"long\"\n\n    def visit_float(self, float_type: FloatType) -&gt; AvroType:\n        return \"float\"\n\n    def visit_double(self, double_type: DoubleType) -&gt; AvroType:\n        return \"double\"\n\n    def visit_date(self, date_type: DateType) -&gt; AvroType:\n        return {\"type\": \"int\", \"logicalType\": \"date\"}\n\n    def visit_time(self, time_type: TimeType) -&gt; AvroType:\n        return {\"type\": \"long\", \"logicalType\": \"time-micros\"}\n\n    def visit_timestamp(self, timestamp_type: TimestampType) -&gt; AvroType:\n        return {\"type\": \"long\", \"logicalType\": \"timestamp-micros\", \"adjust-to-utc\": False}\n\n    def visit_timestamp_ns(self, timestamp_type: TimestampType) -&gt; AvroType:\n        return {\"type\": \"long\", \"logicalType\": \"timestamp-nanos\", \"adjust-to-utc\": False}\n\n    def visit_timestamptz(self, timestamptz_type: TimestamptzType) -&gt; AvroType:\n        return {\"type\": \"long\", \"logicalType\": \"timestamp-micros\", \"adjust-to-utc\": True}\n\n    def visit_timestamptz_ns(self, timestamptz_type: TimestamptzType) -&gt; AvroType:\n        return {\"type\": \"long\", \"logicalType\": \"timestamp-nanos\", \"adjust-to-utc\": True}\n\n    def visit_string(self, string_type: StringType) -&gt; AvroType:\n        return \"string\"\n\n    def visit_uuid(self, uuid_type: UUIDType) -&gt; AvroType:\n        return {\"type\": \"fixed\", \"size\": 16, \"logicalType\": \"uuid\", \"name\": \"uuid_fixed\"}\n\n    def visit_binary(self, binary_type: BinaryType) -&gt; AvroType:\n        return \"bytes\"\n\n    def visit_unknown(self, unknown_type: UnknownType) -&gt; AvroType:\n        return \"null\"\n</code></pre>"},{"location":"reference/pyiceberg/utils/schema_conversion/#pyiceberg.utils.schema_conversion.ConvertSchemaToAvro.__init__","title":"<code>__init__(schema_name)</code>","text":"<p>Convert an Iceberg schema to an Avro schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str | None</code> <p>The name of the root record.</p> required Source code in <code>pyiceberg/utils/schema_conversion.py</code> <pre><code>def __init__(self, schema_name: str | None) -&gt; None:\n    \"\"\"Convert an Iceberg schema to an Avro schema.\n\n    Args:\n        schema_name: The name of the root record.\n    \"\"\"\n    self.schema_name = schema_name\n</code></pre>"},{"location":"reference/pyiceberg/utils/singleton/","title":"singleton","text":"<p>This is a singleton metaclass that can be used to cache and reuse existing objects.</p> <p>In the Iceberg codebase we have a lot of objects that are stateless (for example Types such as StringType, BooleanType etc). FixedTypes have arguments (eg. Fixed[22]) that we also make part of the key when caching the newly created object.</p> <p>The Singleton uses a metaclass which essentially defines a new type. When the Type gets created, it will first evaluate the <code>__call__</code> method with all the arguments. If we already initialized a class earlier, we'll just return it.</p> <p>More information on metaclasses: https://docs.python.org/3/reference/datamodel.html#metaclasses</p>"},{"location":"reference/pyiceberg/utils/singleton/#pyiceberg.utils.singleton.Singleton","title":"<code>Singleton</code>","text":"Source code in <code>pyiceberg/utils/singleton.py</code> <pre><code>class Singleton:\n    _instances: ClassVar[dict] = {}  # type: ignore\n\n    def __new__(cls, *args, **kwargs):  # type: ignore\n        key = (cls, tuple(args), _convert_to_hashable_type(kwargs))\n        if key not in cls._instances:\n            cls._instances[key] = super().__new__(cls)\n        return cls._instances[key]\n\n    def __deepcopy__(self, memo: dict[int, Any]) -&gt; Any:\n        \"\"\"\n        Prevent deep copy operations for singletons.\n\n        The IcebergRootModel inherits from Pydantic RootModel,\n        which has its own implementation of deepcopy. When deepcopy\n        runs, it calls the RootModel __deepcopy__ method and ignores\n        that it's a Singleton. To handle this, the order of inheritance\n        is adjusted and a __deepcopy__ method is implemented for\n        singletons that simply returns itself.\n        \"\"\"\n        return self\n</code></pre>"},{"location":"reference/pyiceberg/utils/singleton/#pyiceberg.utils.singleton.Singleton.__deepcopy__","title":"<code>__deepcopy__(memo)</code>","text":"<p>Prevent deep copy operations for singletons.</p> <p>The IcebergRootModel inherits from Pydantic RootModel, which has its own implementation of deepcopy. When deepcopy runs, it calls the RootModel deepcopy method and ignores that it's a Singleton. To handle this, the order of inheritance is adjusted and a deepcopy method is implemented for singletons that simply returns itself.</p> Source code in <code>pyiceberg/utils/singleton.py</code> <pre><code>def __deepcopy__(self, memo: dict[int, Any]) -&gt; Any:\n    \"\"\"\n    Prevent deep copy operations for singletons.\n\n    The IcebergRootModel inherits from Pydantic RootModel,\n    which has its own implementation of deepcopy. When deepcopy\n    runs, it calls the RootModel __deepcopy__ method and ignores\n    that it's a Singleton. To handle this, the order of inheritance\n    is adjusted and a __deepcopy__ method is implemented for\n    singletons that simply returns itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"reference/pyiceberg/utils/truncate/","title":"truncate","text":""}]}